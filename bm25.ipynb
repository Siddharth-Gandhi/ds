{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = 'cl100k_base'\n",
    "encoding = 'p50k_base'\n",
    "enc = tiktoken.get_encoding(encoding)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4776\n",
      "   0 drwxr-xr-x@  9 siddharth  staff   288B Oct  3 01:48 \u001b[1m\u001b[36m.\u001b[m\u001b[m/\n",
      "   0 drwxr-xr-x@ 13 siddharth  staff   416B Oct  3 03:09 \u001b[1m\u001b[36m..\u001b[m\u001b[m/\n",
      "   0 drwxr-xr-x@  3 siddharth  staff    96B Oct  3 01:45 \u001b[1m\u001b[36mjsonl\u001b[m\u001b[m/\n",
      "1752 -rw-r--r--@  1 siddharth  staff   868K Oct  2 22:02 karpathy_llama2.c_commit_data_0.parquet\n",
      "1232 -rw-r--r--@  1 siddharth  staff   614K Oct  2 22:02 karpathy_llama2.c_commit_data_1.parquet\n",
      " 744 -rw-r--r--@  1 siddharth  staff   372K Oct  2 22:03 karpathy_llama2.c_commit_data_2.parquet\n",
      " 568 -rw-r--r--@  1 siddharth  staff   282K Oct  2 22:03 karpathy_llama2.c_commit_data_3.parquet\n",
      " 480 -rw-r--r--@  1 siddharth  staff   237K Oct  2 22:03 karpathy_llama2.c_commit_data_4.parquet\n",
      "   0 drwxr-xr-x@ 21 siddharth  staff   672B Oct  3 00:52 \u001b[1m\u001b[36msearcher\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "!ls -GFlash data/karpathy_llama2.c/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "df = pd.read_parquet('data/karpathy_llama2.c/karpathy_llama2.c_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  85 non-null     string             \n",
      " 1   repo_name              85 non-null     string             \n",
      " 2   commit_date            85 non-null     datetime64[ns, UTC]\n",
      " 3   commit_id              85 non-null     string             \n",
      " 4   commit_message         85 non-null     string             \n",
      " 5   file_path              85 non-null     string             \n",
      " 6   previous_commit_id     85 non-null     string             \n",
      " 7   previous_file_content  82 non-null     string             \n",
      " 8   cur_file_content       79 non-null     string             \n",
      " 9   diff                   76 non-null     string             \n",
      " 10  status                 85 non-null     category           \n",
      " 11  is_merge_request       85 non-null     bool               \n",
      " 12  file_extension         85 non-null     category           \n",
      "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 7.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.41 MB\n"
     ]
    }
   ],
   "source": [
    " # print just the memory usage in human readable format (MB) to 2 decimal places\n",
    "print(f'{df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique commits stored (others excluded for not being code commits): 72\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique commits stored (others excluded for not being code commits):', df.commit_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = 'data/karpathy_llama2.c/'\n",
    "REPO_LIST = ['karpathy_llama2.c', 'siddharth-gandhi_refpred', 'facebook_react', 'apache_kafka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_data_to_jsonl(data_dir, output_file):\n",
    "#     all_files = glob.glob(os.path.join(data_dir, '*.parquet'))\n",
    "#     all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "#     combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#     # replace NaN with empty string\n",
    "#     combined_df.fillna('', inplace=True)\n",
    "    \n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for index, row in combined_df.iterrows():\n",
    "#             doc = {\n",
    "#                 'id': row['commit_id'],\n",
    "#                 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "#                 # Optionally include source code\n",
    "#                 # 'source_code': row['cur_file_content']  \n",
    "#             }\n",
    "#             f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return ' '.join(map(str,enc.encode(text, disallowed_special=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_repo_to_jsonl(repo_dir, output_file, use_tokenizer=False):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    # replace NaN with empty string in non-category columns\n",
    "    # combined_df.fillna('', inplace=True)\n",
    "\n",
    "    combined_df['commit_message'] = combined_df['commit_message'].fillna('')\n",
    "    combined_df['cur_file_content'] = combined_df['cur_file_content'].fillna('')\n",
    "\n",
    "    # print combined_df memory usage\n",
    "    # print(combined_df.info(memory_usage='deep'))\n",
    "    print(f'Combined Memory Usage: {combined_df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB for {len(combined_df)} rows')\n",
    "    print(output_file)\n",
    "    with open(output_file, 'x') as f:\n",
    "        for index, row in combined_df.iterrows():\n",
    "            doc = {\n",
    "                'id': row['commit_id'],\n",
    "                # 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "                # 'source_code': row['cur_file_content'],  # Optionally include source code\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']),\n",
    "                'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']) if use_tokenizer else row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "                'repo_name': row['repo_name'],\n",
    "                'file_path': row['file_path'],\n",
    "            }\n",
    "            f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty data/jsonl if it has data\n",
    "# !rm -rf data/jsonl_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Memory Usage: 18.29 MB for 402 rows\n",
      "data/jsonl_tiktoken/karpathy_llama2.c.jsonl\n",
      "Combined Memory Usage: 0.94 MB for 108 rows\n",
      "data/jsonl_tiktoken/siddharth-gandhi_refpred.jsonl\n",
      "Combined Memory Usage: 2699.89 MB for 73551 rows\n",
      "data/jsonl_tiktoken/facebook_react.jsonl\n",
      "Combined Memory Usage: 3645.70 MB for 75870 rows\n",
      "data/jsonl_tiktoken/apache_kafka.jsonl\n"
     ]
    }
   ],
   "source": [
    "jsonl_dir_name = 'jsonl_tiktoken'\n",
    "for repo_name in REPO_LIST:\n",
    "    repo_dir = os.path.join('data', repo_name)\n",
    "    # create data/jsonl directory if it doesn't exist\n",
    "    os.makedirs(os.path.join('data', jsonl_dir_name), exist_ok=True)\n",
    "\n",
    "    # store in data/jsonl\n",
    "    output_jsonl_file = os.path.join('data', jsonl_dir_name, f'{repo_name}.jsonl')\n",
    "    convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# jsonl_file_path = f'{BASE_DIR}/jsonl/llama2.jsonl'\n",
    "# convert_data_to_jsonl(BASE_DIR, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of jsonl files which are present in data/repo_name/jsonl/repo_name.jsonl\n",
    "# jsonl_files = glob.glob('data/*/*/*.jsonl')\n",
    "# print(jsonl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normal untokenized\n",
    "- Parquet -> JSONL 22s\n",
    "- Index build 1m26s\n",
    "\n",
    "For tokenized\n",
    "- Parquet -> JSONL 8m3s\n",
    "- Index Build 2m12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b35c000-0x000000016b368000).\n",
      "[0.004s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-03 03:30:53,450 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-03 03:30:53,451 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-03 03:30:53,451 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/jsonl_tiktoken/\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-03 03:30:53,452 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-03 03:30:53,453 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-03 03:30:53,453 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-03 03:30:53,453 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-03 03:30:53,453 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-03 03:30:53,453 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-03 03:30:53,453 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
      "2023-10-03 03:30:53,453 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: ./bm25_index_tiktoken/\n",
      "2023-10-03 03:30:53,455 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-03 03:30:53,462 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
      "2023-10-03 03:30:53,463 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
      "2023-10-03 03:30:53,463 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
      "2023-10-03 03:30:53,463 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
      "2023-10-03 03:30:53,536 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-03 03:30:53,536 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/jsonl_tiktoken\n",
      "2023-10-03 03:30:53,537 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 4 files found\n",
      "2023-10-03 03:30:53,538 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-03 03:30:53,755 DEBUG [pool-2-thread-4] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl_tiktoken/siddharth-gandhi_refpred.jsonl: 108 docs added.\n",
      "2023-10-03 03:30:54,213 DEBUG [pool-2-thread-3] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl_tiktoken/karpathy_llama2.c.jsonl: 402 docs added.\n",
      "2023-10-03 03:31:53,545 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 50.00% of files completed, 80,510 documents indexed\n",
      "2023-10-03 03:32:10,762 DEBUG [pool-2-thread-2] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl_tiktoken/facebook_react.jsonl: 73551 docs added.\n",
      "2023-10-03 03:32:48,669 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl_tiktoken/apache_kafka.jsonl: 75870 docs added.\n",
      "2023-10-03 03:33:04,335 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 149,931 documents indexed\n",
      "2023-10-03 03:33:04,335 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-03 03:33:04,336 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:          149,931\n",
      "2023-10-03 03:33:04,336 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-03 03:33:04,336 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-03 03:33:04,336 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-03 03:33:04,336 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-03 03:33:04,339 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 149,931 documents indexed in 00:02:10\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Directory to store the index\n",
    "# index_dir=\"./bm25_index/\"\n",
    "# jsonl_dir_name=\"jsonl\"\n",
    "\n",
    "index_dir=\"./bm25_index_tiktoken/\"\n",
    "jsonl_dir_name=\"jsonl_tiktoken\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "mkdir -p \"$index_dir\"\n",
    "\n",
    "# Remove any existing indexes\n",
    "rm -rf \"$index_dir/*\"\n",
    "\n",
    "# build the index from data/jsonl\n",
    "python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
    " -threads 4 -input data/\"$jsonl_dir_name\"/ -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pyserini.index.lucene \\\n",
    "#   --collection JsonCollection \\\n",
    "#   --input data/karpathy_llama2.c/jsonl/ \\\n",
    "#   --index data/karpathy_llama2.c/searcher/ \\\n",
    "#   --generator DefaultLuceneDocumentGenerator \\\n",
    "#   --threads 1 \\\n",
    "#   --storePositions --storeDocvectors --storeRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ce91b1b3b56ff7d43d894c204f965bfbf5d63c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'nInference for Llama-2 Transformer model in pure C'\n",
    "# query = 'if is_arxiv:\\n return f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper_id}/references?fields=title,abstract,url,venue,publicationVenue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess'\n",
    "# query = \"export {default} from './npm/Circle';\"\n",
    "# query = \"\"\"\n",
    "# public class MockKafkaLog4jAppender extends KafkaLog4jAppender {\n",
    "#     private MockProducer<byte[], byte[]> mockProducer =\n",
    "#             new MockProducer<>(false, new MockSerializer(), new MockSerializer());\n",
    "\n",
    "#     private Properties producerProperties;\n",
    "\n",
    "#     @Override\n",
    "#     protected Producer<byte[], byte[]> getKafkaProducer(Properties props) {\n",
    "#         producerProperties = props;\n",
    "#         return mockProducer;\n",
    "#     }\n",
    "\n",
    "#     void setKafkaProducer(MockProducer<byte[], byte[]> producer) {\n",
    "#         this.mockProducer = producer;\n",
    "#     }\n",
    "# \"\"\"\n",
    "query = \"\"\"\n",
    "/**\n",
    " * Local file based quorum state store. It takes the JSON format of {@link QuorumStateData}\n",
    " * with an extra data version number as part of the data for easy deserialization.\n",
    " *\n",
    " * Example format:\n",
    " * <pre>\n",
    " * {\"clusterId\":\"\",\n",
    " *   \"leaderId\":1,\n",
    " *   \"leaderEpoch\":2,\n",
    " *   \"votedId\":-1,\n",
    " *   \"appliedOffset\":0,\n",
    " *   \"currentVoters\":[],\n",
    " *   \"data_version\":0}\n",
    " * </pre>\n",
    " * */\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 2e3ff21c2e3674ece50c2a8a4053b93024e12b4a 74.94260 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 2 7b669e8806ce9d122233afeec03eb4e15bde808a 74.90920 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 3 4b7ad7b14d04e1e362b8100f43375d1630ded1b4 74.65520 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 4 b7c8490cf47b0c18253d6a776b2b35c76c71c65d 74.57880 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 5 0927049a617fa2937a211aab895f6590403130fb 40.86640 kafka/raft/src/test/java/org/apache/kafka/raft/FileBasedStateStoreTest.java\n",
      " 6 7b669e8806ce9d122233afeec03eb4e15bde808a 38.03240 kafka/raft/src/test/java/org/apache/kafka/raft/FileBasedStateStoreTest.java\n",
      " 7 2e3ff21c2e3674ece50c2a8a4053b93024e12b4a 37.48210 kafka/raft/src/test/java/org/apache/kafka/raft/FileBasedStateStoreTest.java\n",
      " 8 ae0c6e58e5a2c545ba54eea5fb4d5dd103d237ff 27.19650 kafka/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java\n",
      " 9 5bd06f1d542e6b588a1d402d059bc24690017d32 27.02200 kafka/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java\n",
      "10 2f3600198722dd5a01a210bc78b7d43b33967c7f 26.82320 kafka/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java\n"
     ]
    }
   ],
   "source": [
    "bm25searcher = LuceneSearcher('bm25_index/')\n",
    "hits = bm25searcher.search(query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 2e3ff21c2e3674ece50c2a8a4053b93024e12b4a 136.07840 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 2 b7c8490cf47b0c18253d6a776b2b35c76c71c65d 135.86909 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 3 4b7ad7b14d04e1e362b8100f43375d1630ded1b4 135.61960 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 4 7b669e8806ce9d122233afeec03eb4e15bde808a 135.35139 kafka/raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java\n",
      " 5 db490707606855c265bc938e1b236070e0e2eba5 82.86220 kafka/core/src/main/scala/kafka/zk/ZkData.scala\n",
      " 6 c2759df0676cef252596239baf8f1f361e76c49f 82.14700 kafka/core/src/main/scala/kafka/zk/ZkData.scala\n",
      " 7 d40561e90ab5b1f5c79d174393645c22b5797eff 82.11960 kafka/core/src/main/scala/kafka/zk/ZkData.scala\n",
      " 8 87b9c572c685f95d57b28749698dcd017381aec2 82.07420 kafka/core/src/main/scala/kafka/zk/ZkData.scala\n",
      " 9 8cdf9564ab0f56845bb0cc0192a43b39c26c3375 82.02510 kafka/core/src/main/scala/kafka/zk/ZkData.scala\n",
      "10 7b7e40a536a79cebf35cc278b9375c8352d342b9 82.00710 kafka/core/src/main/scala/kafka/zk/ZkData.scala\n"
     ]
    }
   ],
   "source": [
    "tiktoken_searcher = LuceneSearcher('bm25_index_tiktoken/')\n",
    "# get tokenized query with enc.encode\n",
    "tokeninzed_query = tokenize(query)\n",
    "hits = tiktoken_searcher.search(tokeninzed_query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21774 5637 1243 284 3706 15319 17426 1507 20986 8 198 198 9 10028 5637 1243 284 3706 15319 198 198 464 3499 1339 994 318 262 645 404 9851 19288 13 383 7917 11799 1088 262 198 260 1102 2856 263 783 3421 284 779 257 1957 10784 326 3011 48865 13 198 198 3041 529 2949 404 290 21492 2949 404 30946 7609 783 423 284 2244 5620 262 2134 284 198 4868 503 262 3891 340 338 1016 284 10784 13 775 815 2192 1006 11218 198 3041 529 2949 404 1497 422 2251 3041 529 2949 404 13 18948 1201 340 338 635 407 27782 198 774 9124 13 198 198 9 14645 12213 284 3491 15319 198 198 1212 481 423 1658 26796 17764 6056 319 606 13 1119 815 30274 198 39344 4277 2427 13 628\n",
      "35343 198 1635 15069 357 66 8 3203 11 3457 13 290 663 29116 13 198 1635 198 1635 770 2723 2438 318 11971 739 262 17168 5964 1043 287 262 198 1635 38559 24290 2393 287 262 6808 8619 286 428 2723 5509 13 198 1635 198 1635 2488 11125 198 9466 198 198 39344 1391 12286 92 422 705 19571 77 4426 14 31560 293 17020 198\n"
     ]
    }
   ],
   "source": [
    "# print the document source code inside the first hit raw\n",
    "print(json.loads(hits[0].raw)['contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = '''\n",
    "A crawler for the Semantic Scholar API.\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import logging.config\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "import httpx  # https://github.com/encode/httpx\n",
    "import requests\n",
    "from crawl_utils import get_batch_url, get_reference_url\n",
    "from db import MongoDBClient\n",
    "\n",
    "from config import S2_API_KEY, S2_RATE_LIMIT\n",
    "\n",
    "logging.config.fileConfig(fname=\"logging.conf\", disable_existing_loggers=False)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RateLimitExceededException(Exception):\n",
    "    \"\"\"Exception raised when rate limit is exceeded\"\"\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"RateLimitExceededException: {self.message}\"\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    \"\"\"Exception raised when a request times out\"\"\"\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"TimeoutException: {self.message}\"\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class Crawler:\n",
    "    \"\"\"A crawler for the Semantic Scholar API\"\"\"\n",
    "\n",
    "    client: httpx.AsyncClient = field(repr=False)\n",
    "    initial_papers: List[str] = field(default_factory=list)\n",
    "    num_workers: int = 10\n",
    "    max_papers: int = 100\n",
    "    mongodb_client: MongoDBClient = field(default_factory=MongoDBClient)\n",
    "    headers: dict = field(repr=False, default_factory=dict)\n",
    "    todo: asyncio.Queue = field(init=False, repr=False, default_factory=asyncio.Queue)\n",
    "    seen: Set[str]= field(init=False, default_factory=set)\n",
    "    done: Set[str] = field(init=False, default_factory=set)\n",
    "    retry: Dict[str, int] = field(init=False, default_factory=dict)\n",
    "    total: int = field(init=False, default=0)\n",
    "    MAX_RETRIES: int = field(init=False, default=3)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, settings: dict) -> \"Crawler\":\n",
    "        \"\"\"\n",
    "        Create a Crawler instance from a dict of settings\"\"\"\n",
    "        return cls(**settings)\n",
    "\n",
    "    async def run(self) -> None:\n",
    "        \"\"\"Run the crawler by creating workers until todo queue is empty\"\"\"\n",
    "        self.init_done()\n",
    "        await self.init_queue()\n",
    "        workers = [asyncio.create_task(self.worker()) for _ in range(self.num_workers)]\n",
    "        await self.todo.join()\n",
    "        for worker in workers:\n",
    "            worker.cancel()\n",
    "\n",
    "    async def init_queue(self) -> None:\n",
    "        \"\"\"Initialize the queue with the initial papers\"\"\"\n",
    "        batch_url = get_batch_url()\n",
    "        data = json.dumps({\"ids\": self.initial_papers})\n",
    "        response = requests.post(url=batch_url, data=data, headers=self.headers, timeout=10)\n",
    "        # initial_paper_id = self.initial_papers[0]\n",
    "        # initial_url = get_paper_url(initial_paper_id)\n",
    "        # response = requests.get(initial_url, headers=self.headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            logger.error(\"Error fetching initial papers\")\n",
    "            sys.exit(1)\n",
    "        logger.debug(f\"Fetching data for intial papers {self.initial_papers}\")\n",
    "        result_data = response.json()\n",
    "        # result_data[\"_id\"] = result_data[\"paperId\"]\n",
    "        for paper in result_data:\n",
    "            paper[\"_id\"] = paper[\"paperId\"]\n",
    "        # prime the queue\n",
    "        await self.on_found_papers(result_data, initial=True)\n",
    "\n",
    "    def init_done(self) -> None:\n",
    "        \"\"\"Initialize the seen set with already stored papers from DB\"\"\"\n",
    "        # self.seen = set(self.initial_papers)\n",
    "        self.done = self.mongodb_client.get_ids()\n",
    "        logger.info(f\"Already stored {len(self.done)} papers\")\n",
    "\n",
    "    async def worker(self) -> None:\n",
    "        \"\"\"One worker processes one paper at a time from the queue in a loop until cancelled\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                await self.process_one()\n",
    "            except asyncio.CancelledError:\n",
    "                return\n",
    "\n",
    "    async def retry_crawl(self, paper) -> None:\n",
    "        \"\"\"Retry crawling a paper in case of an exception\"\"\"\n",
    "        if paper[\"_id\"] in self.retry and self.retry[paper[\"_id\"]] > self.MAX_RETRIES:\n",
    "            logger.error(f\"Error processing {paper['_id']} even after retrying {self.MAX_RETRIES} times\")\n",
    "            return\n",
    "        # self.retry.add(paper[\"_id\"])\n",
    "        self.retry[paper[\"_id\"]] = self.retry.get(paper[\"_id\"], 0) + 1\n",
    "        logger.info(f\"Retry #{self.retry[paper['_id']]} for {paper['_id']}\")\n",
    "        # await self.todo.put_nowait(cur_paper)\n",
    "        await asyncio.sleep(1)\n",
    "        await self.crawl(paper)\n",
    "\n",
    "    async def process_one(self) -> None:\n",
    "        \"\"\"Gets one paper from the queue and processes it\"\"\"\n",
    "        # cur_paper is a dict\n",
    "        cur_paper = await self.todo.get()\n",
    "        try:\n",
    "            await self.crawl(cur_paper)\n",
    "        except TimeoutException as te:\n",
    "            # logger.warning(f\"Timeout for {cur_paper['_id']}\")\n",
    "            logger.warning(te)\n",
    "            await self.retry_crawl(cur_paper)\n",
    "        except RateLimitExceededException as rlee:\n",
    "            logger.critical(\"Rate limit exceeded, retrying in 2 second\")\n",
    "            logger.critical(rlee)\n",
    "            await asyncio.sleep(2)\n",
    "            await self.retry_crawl(cur_paper)\n",
    "        finally:\n",
    "            self.todo.task_done()\n",
    "\n",
    "    async def crawl(self, cur_paper: dict) -> None:\n",
    "        \"\"\"\n",
    "        Crawl a paper and its references, stores them in the database.\n",
    "        \"\"\"\n",
    "        # TODO proper rate limiting to 100 requests / second\n",
    "        # await asyncio.sleep(1 / self.num_workers)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "        cur_paper_id = cur_paper[\"paperId\"]\n",
    "        ref_url = get_reference_url(cur_paper_id)\n",
    "        cur_paper[\"_id\"] = cur_paper_id\n",
    "        if cur_paper[\"title\"] is None or cur_paper[\"abstract\"] is None:\n",
    "            logger.debug(f\"Skipping {cur_paper_id} as empty title or abstract\")\n",
    "            # I have no clue why this total -= 1 is here, it shouldn't be required, but crawler just prematurely stops\n",
    "            self.total -= 1\n",
    "            return\n",
    "        # async with self.semaphore:\n",
    "        # async with self.client.get(ref_url, headers=self.headers) as response:\n",
    "\n",
    "        response = await self.client.get(ref_url, headers=self.headers)\n",
    "\n",
    "        # if self.semaphore.locked():\n",
    "        #     logger.warning(f\"Semaphore locked for {cur_paper_id}\")\n",
    "        #     await asyncio.sleep(1)\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            # logger.critical(\n",
    "            #     f\"Rated limited for {cur_paper_id} - {response.status_code}\"\n",
    "            # )\n",
    "            # # await self.todo.put_nowait(cur_paper)\n",
    "            # await asyncio.sleep(1)\n",
    "            # await self.crawl(cur_paper)\n",
    "            raise RateLimitExceededException(\n",
    "                f\"Rated limited for {cur_paper_id} - {response.status_code}\"\n",
    "            )\n",
    "\n",
    "        if response.status_code == 504:\n",
    "            # raise asyncio.exceptions.TimeoutError(\n",
    "            #     f\"Timeout for {cur_paper_id} - {response.status_code}\"\n",
    "            # )\n",
    "            raise TimeoutException(f\"Timeout for {cur_paper_id} - {response.status_code}\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Error fetching references for {cur_paper_id} - {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        logger.debug(f\"Fetching references for {cur_paper_id} - {response.status_code}\")\n",
    "\n",
    "        result_data = response.json()\n",
    "        found_references = result_data[\"data\"]\n",
    "        found_references = [ref[\"citedPaper\"] for ref in found_references]\n",
    "        found_references = sorted(found_references, key=lambda x: x[\"citationCount\"] or 0, reverse=True)\n",
    "        ref_ids = [ref[\"paperId\"] for ref in found_references if ref[\"paperId\"] is not None]\n",
    "        cur_paper[\"references\"] = ref_ids\n",
    "        cur_paper[\"allReferencesStored\"] = True\n",
    "        if len(ref_ids) != cur_paper[\"referenceCount\"]:\n",
    "            cur_paper[\"allReferencesStored\"] = False\n",
    "\n",
    "        # self.collection.insert_one(cur_paper)\n",
    "        self.mongodb_client.insert_one(cur_paper)\n",
    "        self.done.add(cur_paper[\"paperId\"])\n",
    "        # self.stored += 1\n",
    "        # if self.stored % 100 == 0:\n",
    "        #     logger.info(f\"Stored {self.stored} papers\")\n",
    "\n",
    "        await self.on_found_papers(found_references)\n",
    "\n",
    "    # async def get_paper_references(self, base: str, text: str) -> set[str]:\n",
    "    #     parser = UrlParser(base, self.filter_url)\n",
    "    #     parser.feed(text)\n",
    "    #     return parser.found_references\n",
    "\n",
    "    async def on_found_papers(self, papers: List[dict], initial: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Called when new papers are found. Filters out papers that have already been seen and puts the new ones in the queue.\n",
    "        \"\"\"\n",
    "        if initial:\n",
    "            for paper in papers:\n",
    "                await self.put_todo(paper)\n",
    "            return\n",
    "        ids = {paper[\"paperId\"] for paper in papers if paper[\"paperId\"] is not None}\n",
    "        new = ids - self.seen\n",
    "        self.seen.update(new)\n",
    "\n",
    "        for paper in papers:\n",
    "            if paper[\"paperId\"] in new:\n",
    "                await self.put_todo(paper)\n",
    "\n",
    "    async def put_todo(self, paper: dict) -> None:\n",
    "        \"\"\"Put a paper in the queue\"\"\"\n",
    "        # paper is a dict with fields like paper_id, title, abstract, etc.\n",
    "        if self.total >= self.max_papers:\n",
    "            return\n",
    "        self.total += 1\n",
    "        await self.todo.put(paper)\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    \"\"\"Main function\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    headers={\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"x-api-key\": S2_API_KEY,\n",
    "    }\n",
    "    mongodb_client = MongoDBClient(mongo_url='mongodb://localhost:27017', db_name='refpred', collection_name='review3_demo', init_new=True)\n",
    "    timeout = httpx.Timeout(10, connect=10, read=None, write=10)\n",
    "    # based on https://towardsdatascience.com/top-10-research-papers-in-ai-1f02cf844e26\n",
    "    initial_papers = [\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"bee044c8e8903fb67523c1f8c105ab4718600cdb\", \"36eff562f65125511b5dfab68ce7f7a943c27478\", \"8388f1be26329fa45e5807e968a641ce170ea078\", \"846aedd869a00c09b40f1f1f35673cb22bc87490\", \"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\", \"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\", \"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\", \"4d376d6978dad0374edfa6709c9556b42d3594d3\", \"a6cb366736791bcccc5c8639de5a8f9636bf87e8\", \"df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"913f54b44dfb9202955fe296cf5586e1105565ea\", \"156d217b0a911af97fa1b5a71dc909ccef7a8028\", \"a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031\", \"5c5751d45e298cea054f32b392c12c61027d2fe7\", \"bc1586a2e74d6d1cf87b083c4cbd1eede2b09ea5\", \"921b2958cac4138d188fd5047aa12bbcf37ac867\", \"cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1\"]\n",
    "    MAX_PAPERS = 10000\n",
    "    async with httpx.AsyncClient(timeout=timeout) as client:\n",
    "        # starting with the famous paper 'Attention is all you need'\n",
    "        crawler = Crawler(\n",
    "            client=client,\n",
    "            initial_papers=initial_papers,\n",
    "            num_workers=S2_RATE_LIMIT,\n",
    "            max_papers=MAX_PAPERS,\n",
    "            mongodb_client=mongodb_client,\n",
    "            headers=headers,\n",
    "        )\n",
    "        await crawler.run()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    logger.info(\"Results:\")\n",
    "    logger.info(f\"Crawled: {len(crawler.done)} Papers\")\n",
    "    logger.info(f\"Found: {len(crawler.seen)} Papers\")\n",
    "    logger.info(f\"Done in {end - start:.2f}s\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n",
    "# TODO\n",
    "# 1. Batch processing of seed papers\n",
    "# 2. Initialize seen from dataset to avoid restarting over\n",
    "# 3. Null abstract papers need to be removed from the dataset âœ…'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_msg = '''# A simple hello world program in python with docstring\n",
    "def hello_world():\n",
    "    \"\"\"A simple hello world program in python with docstring\"\"\"\n",
    "    print(\"Hello World!\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 362, 4382, 24748, 1917, 2068, 304, 10344, 449, 4733]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see tokenized output of the above code\n",
    "enc.encode(simple_msg)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 317, 2829, 23748, 995, 1430, 287, 21015, 351, 2205]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see tokenized output of the above code\n",
    "enc.encode(simple_msg)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
