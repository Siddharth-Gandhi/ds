{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "import tiktoken\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = 'cl100k_base'\n",
    "encoding = 'p50k_base'\n",
    "enc = tiktoken.get_encoding(encoding)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return ' '.join(map(str,enc.encode(text, disallowed_special=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.4M\n",
      "   0 drwxr-xr-x 10 siddharth  320 Oct  5 22:02 ./\n",
      "   0 drwxr-xr-x 12 siddharth  384 Oct  5 22:04 ../\n",
      "8.0K -rw-r--r--  1 siddharth 6.1K Oct  5 22:02 .DS_Store\n",
      "   0 drwxr-xr-x  3 siddharth   96 Oct  5 22:02 jsonl/\n",
      "868K -rw-r--r--  1 siddharth 868K Oct  2 22:02 karpathy_llama2.c_commit_data_0.parquet\n",
      "616K -rw-r--r--  1 siddharth 615K Oct  2 22:02 karpathy_llama2.c_commit_data_1.parquet\n",
      "372K -rw-r--r--  1 siddharth 372K Oct  2 22:03 karpathy_llama2.c_commit_data_2.parquet\n",
      "284K -rw-r--r--  1 siddharth 283K Oct  2 22:03 karpathy_llama2.c_commit_data_3.parquet\n",
      "240K -rw-r--r--  1 siddharth 238K Oct  2 22:03 karpathy_llama2.c_commit_data_4.parquet\n",
      "   0 drwxr-xr-x 21 siddharth  672 Oct  3 00:52 searcher/\n"
     ]
    }
   ],
   "source": [
    "!ls -GFlash data/karpathy_llama2.c/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "df = pd.read_parquet('data/karpathy_llama2.c/karpathy_llama2.c_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/facebook_react/facebook_react_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get commit 7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "# df[df['commit_id'] == '7022e8d6a3222c97d287dfa0f2361acc8a30683a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2023-09-12 08:18:43+00:00\n",
       "Name: commit_date, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (df.head(1)['commit_date'].astype('int64')/1e6).astype('int64')\n",
    "df.head(1)['commit_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  85 non-null     string             \n",
      " 1   repo_name              85 non-null     string             \n",
      " 2   commit_date            85 non-null     datetime64[ns, UTC]\n",
      " 3   commit_id              85 non-null     string             \n",
      " 4   commit_message         85 non-null     string             \n",
      " 5   file_path              85 non-null     string             \n",
      " 6   previous_commit_id     85 non-null     string             \n",
      " 7   previous_file_content  82 non-null     string             \n",
      " 8   cur_file_content       79 non-null     string             \n",
      " 9   diff                   76 non-null     string             \n",
      " 10  status                 85 non-null     category           \n",
      " 11  is_merge_request       85 non-null     bool               \n",
      " 12  file_extension         85 non-null     category           \n",
      "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 7.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.41 MB\n"
     ]
    }
   ],
   "source": [
    " # print just the memory usage in human readable format (MB) to 2 decimal places\n",
    "print(f'{df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique commits stored (others excluded for not being code commits): 72\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique commits stored (others excluded for not being code commits):', df.commit_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = 'data/karpathy_llama2.c/'\n",
    "REPO_LIST = ['karpathy_llama2.c', 'siddharth-gandhi_refpred', 'facebook_react', 'apache_kafka', 'ggerganov_llama.cpp', 'nodejs_node']\n",
    "# REPO_LIST = ['karpathy_llama2.c', 'siddharth-gandhi_refpred', 'facebook_react', 'apache_kafka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_LIST = ['karpathy_llama2.c']\n",
    "REPONAME = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_data_to_jsonl(data_dir, output_file):\n",
    "#     all_files = glob.glob(os.path.join(data_dir, '*.parquet'))\n",
    "#     all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "#     combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#     # replace NaN with empty string\n",
    "#     combined_df.fillna('', inplace=True)\n",
    "\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for index, row in combined_df.iterrows():\n",
    "#             doc = {\n",
    "#                 'id': row['commit_id'],\n",
    "#                 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "#                 # Optionally include source code\n",
    "#                 # 'source_code': row['cur_file_content']\n",
    "#             }\n",
    "#             f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_commits(repo_dir):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # number of unique commit_id columns\n",
    "    return combined_df.commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_commits = 0\n",
    "for repo in REPO_LIST:\n",
    "    total_commits += count_commits('data/' + repo + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of commits: 11595\n"
     ]
    }
   ],
   "source": [
    "print('Total number of commits:', total_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_repo_to_jsonl(repo_dir, output_file, use_tokenizer=False):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    # replace NaN with empty string in non-category columns\n",
    "    # combined_df.fillna('', inplace=True)\n",
    "\n",
    "    combined_df['commit_message'] = combined_df['commit_message'].fillna('')\n",
    "    combined_df['cur_file_content'] = combined_df['cur_file_content'].fillna('')\n",
    "    # convert commit_date to int64 (unix timestamp in milliseconds)\n",
    "    combined_df['commit_date'] = (combined_df['commit_date'].astype('int64') / 1e6).astype('int64')\n",
    "    # df['commit_date'] = df['commit_date'].astype(str)\n",
    "    # print(type(df['commit_date'][0]))\n",
    "    # print combined_df memory usage\n",
    "    # print(combined_df.info(memory_usage='deep'))\n",
    "    print(f'Combined Memory Usage: {combined_df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB for {len(combined_df)} rows')\n",
    "    print(output_file)\n",
    "    with open(output_file, 'x') as f:\n",
    "        for index, row in combined_df.iterrows():\n",
    "            doc = {\n",
    "                'id': row['commit_id'],\n",
    "                'contents': row['commit_message'] if not use_tokenizer else tokenize(row['commit_message']),\n",
    "                # 'source_code': row['cur_file_content'],  # Optionally include source code\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']),\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']) if use_tokenizer else row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "                'repo_name': row['repo_name'],\n",
    "                'file_path': row['file_path'],\n",
    "                'commit_date': row['commit_date'],\n",
    "            }\n",
    "            f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty data/jsonl if it has data\n",
    "# !rm -rf data/jsonl_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl_dir_name = 'jsonl_6'\n",
    "# for repo_name in REPO_LIST:\n",
    "#     repo_dir = os.path.join('data', repo_name)\n",
    "#     # create data/jsonl directory if it doesn't exist\n",
    "#     os.makedirs(os.path.join('data', jsonl_dir_name), exist_ok=True)\n",
    "\n",
    "#     # store in data/jsonl\n",
    "#     output_jsonl_file = os.path.join('data', jsonl_dir_name, f'{repo_name}.jsonl')\n",
    "#     convert_repo_to_jsonl(repo_dir, output_jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_LIST = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Memory Usage: 2699.89 MB for 73551 rows\n",
      "data/facebook_react/jsonl/facebook_react_commit_only_tk.jsonl\n"
     ]
    }
   ],
   "source": [
    "# store in data/repo_dir/jsonl\n",
    "jsonl_dir_name = 'jsonl'\n",
    "for repo_name in REPO_LIST:\n",
    "    repo_dir = os.path.join('data', repo_name)\n",
    "    # create data/jsonl directory if it doesn't exist\n",
    "    os.makedirs(os.path.join(repo_dir, jsonl_dir_name), exist_ok=True)\n",
    "    output_name = f'{repo_name}_commit_only_tk.jsonl'\n",
    "    # store in data/jsonl\n",
    "    output_jsonl_file = os.path.join(repo_dir, jsonl_dir_name, output_name)\n",
    "    convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# jsonl_file_path = f'{BASE_DIR}/jsonl/llama2.jsonl'\n",
    "# convert_data_to_jsonl(BASE_DIR, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of jsonl files which are present in data/repo_name/jsonl/repo_name.jsonl\n",
    "# jsonl_files = glob.glob('data/*/*/*.jsonl')\n",
    "# print(jsonl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normal untokenized\n",
    "- Parquet -> JSONL 22s\n",
    "- Index build 1m26s\n",
    "- 6 repos\n",
    "    Parquet -> JSONL 1m11s\n",
    "    Same mem usage as before, just lower time since no need for tokenization\n",
    "    Index Build 3m51s\n",
    "    Index Size 5Gb\n",
    "\n",
    "For tokenized\n",
    "- Parquet -> JSONL 8m3s\n",
    "- Index Build 2m12s\n",
    "- 6 repos:\n",
    "    Parquert -> JSONL 24m\n",
    "        - Combined Memory Usage: 18.29 MB for 402 rows data/isonl_6/karpathy_llama2.c.jsonl\n",
    "        - Combined Memory Usage: 0.94 MB for 108 rows data/json1_6/siddharth-gandhi_refpred.jsonl \\\\\n",
    "        - Combined Memory Usage: 2699.89 MB for 73551 rows data/jsonl_6/facebook_react.jsonl \\\\\n",
    "        - Combined Memory Usage: 3645.70 MB for 75870 rows data/jsonl_6/apache_kafka. jsonl \\\\\n",
    "        - Combined Memory Usage: 605.11 MB for 2111 rows data/jsonl_6/ggerganov_llama.cpp.jsonl \\\\\n",
    "        - Combined Memory Usage: 11010.96 MB for 208188 rows data/jsonl_6/nodejs_node.json\n",
    "        - 36731 total commits \n",
    "        - Total ~360K rows\n",
    "        - Interesting heuristic, on avg 10 files edited per commit?\n",
    "    Index build 6m42s\n",
    "    Index Size 10GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b810000-0x000000016b81c000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-06 02:11:09,885 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-06 02:11:09,887 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-06 02:11:09,887 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-06 02:11:09,887 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/facebook_react//jsonl/final/\n",
      "2023-10-06 02:11:09,887 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-06 02:11:09,887 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-06 02:11:09,887 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-06 02:11:09,888 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-06 02:11:09,889 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-06 02:11:09,889 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-06 02:11:09,889 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-06 02:11:09,889 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/facebook_react//index_tk\n",
      "2023-10-06 02:11:09,891 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-06 02:11:09,963 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-06 02:11:09,963 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/facebook_react/jsonl/final\n",
      "2023-10-06 02:11:09,964 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-06 02:11:09,964 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-06 02:11:12,455 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - final/facebook_react_commit_only_tk.jsonl: 73551 docs added.\n",
      "2023-10-06 02:11:12,921 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 73,551 documents indexed\n",
      "2023-10-06 02:11:12,921 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-06 02:11:12,922 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:           73,551\n",
      "2023-10-06 02:11:12,922 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-06 02:11:12,922 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-06 02:11:12,922 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-06 02:11:12,922 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-06 02:11:12,925 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 73,551 documents indexed in 00:00:03\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Directory to store the index\n",
    "# index_dir=\"./bm25_index_6/\"\n",
    "# jsonl_dir_name=\"jsonl_6\"\n",
    "repo_dir=\"data/facebook_react/\"\n",
    "index_dir=\"$repo_dir/index_tk\"\n",
    "# jsonl_dir_name=\"jsonl_tiktoken_6\"\n",
    "jsonl_dir_name=\"$repo_dir/jsonl/final/\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "mkdir -p \"$index_dir\"\n",
    "\n",
    "# Remove any existing indexes\n",
    "rm -rf \"$index_dir/*\"\n",
    "\n",
    "# build the index from data/jsonl\n",
    "python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
    " -threads 4 -input \"$jsonl_dir_name\" -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir=f\"data/{REPO_LIST[0]}\"\n",
    "# repo_dir=f\"data/karpathy_llama2.c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Refactors Resources to have a more compact and memory efficient struture.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_to_timestamp(date_str):\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "    # Convert the datetime object to a UNIX timestamp\n",
    "    # Method 1: Using timestamp() method\n",
    "    unix_timestamp_1 = int(date_obj.timestamp())\n",
    "    return unix_timestamp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebook/react/commit/7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls. I've encountered an issue in Firefox where multiple calls to chrome.panels.create result in the creation of duplicate panels. This seems to happen every time chrome.panels.create is called, even if a panel already exists. This leads to a cluttered interface with many duplicate panels. Ideally, chrome.panels.create should only create a new panel if there isn't one already existing. I believe a check should be implemented to ensure that chrome.panels.create is only called if no panels have been created yet to prevent this duplication issue.\"\n",
    "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls.\"\n",
    "query_date = \"2023-08-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1693454400"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_to_timestamp(query_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tokenize(text):\n",
    "    text = json.loads(text)\n",
    "    # print(list(text['contents'].split(' ')))\n",
    "    text['contents'] = enc.decode([int(i) for i in text['contents'].split(' ')])\n",
    "    # return string\n",
    "    return json.dumps(text, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/facebook_react/index/\n",
      "{'total_terms': 4461997, 'documents': 73551, 'non_empty_documents': 73551, 'unique_terms': 42932}\n",
      "{\n",
      "  \"id\" : \"1f74eca9937ad6f19b6291d21edfb8747bae88ca\",\n",
      "  \"contents\" : \"Add warning for rendering into container that was updated manually (#10210)\\n\\n* RFC Add warning for rendering into container that was updated manually\\n\\nRFC because we still need to tidy this up and verify that all tests\\npass.\\n\\n**what is the change?:**\\nWe want to warn when users render into a container which was manually\\nemptied or updated outside of React. This can lead to the cryptic error\\nabout not being able to remove a node, or just lead to silent failures\\nof render. This warning should make things more clear.\\n\\nNote that this covers the case where the contents of the root container\\nare manually updated, but does not cover the case where something was\\nmanually updated deeper in the tree.\\n\\n**why make this change?:**\\nTo maintain parity and increase clarity before releasing v16.0 beta.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\nlast item under the '16 beta' checklist.\\n\\n* Add test and tweak check for rendering into manually updated container\\n\\nSTILL TODO: figure out how to skip this warning when the component\\nrenders to a portal.\\n\\nUnfortunately 'ReactPortal.isPortal(children)' returns false, even in\\nthe failing test where we are rendering to a portal.\\n\\n**what is the change?:**\\n- added a test for the case where we call 'ReactDOM.render' with a new\\n  container, using a key or a different type, after the contents of the\\n  first container were messed with outside of React. This case throws,\\n  and now at least there will be an informative warning along with the\\n  error.\\n- We updated the check to compare the parent of the 'hostInstance' to\\n  the container; this seems less fragile\\n- tweaked some comments\\n\\n**why make this change?:**\\nContinue improving this to make it more final.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Stub our `console.error` in one of the portal tests\\n\\n**what is the change?:**\\nSee title\\n\\n**why make this change?:**\\nSee comment in the code\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Skip warning in 'ReactDOMFiberEntry' when mounting to Comment node\\n\\n**what is the change?:**\\nWe have a warning for cases when the container doesn't match the parent\\nwhich we remembered the previously rendered content being rendered into.\\n\\nWe are skipping that warning when you render into a 'comment' node.\\n\\n**why make this change?:**\\nBasically, if you render into a 'comment' node, then the parent of the\\ncomment node is the container for your rendered content. We could check\\nfor similarity there but rendering into a comment node seems like a\\ncorner case and I'd rather skip the warning without knowing more about\\nwhat could happen in that case.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Improve warning message, remove dedup check, and unmock console.error\\n\\n**what is the change?:**\\nVarious changes to get this closer to being finished;\\n- Improved warning message (thanks @spicyj!!!)\\n- Removed dedup check on warning\\n- Remove mocking of 'console.error' in portals test\\n\\n**why make this change?:**\\n- warning message improvement: communicates better with users\\n- Remove dedup check: it wasn't important in this case\\n- Remove mocking of 'console.error'; we don't want to ignore an\\n  inaccurate warning, even for an \\\"unstable\\\" feature.\\n\\n**test plan:**\\n`yarn test` -> follow-up commits will fix the remaining tests\\n\\n**issue:**\\nissue #8854\\n\\n* Possible fix for issue of incorrect warning for portal re-render\\n\\n**what is the change?:**\\nAdd a property to a container which was rendered into using\\n`ReactDOM.unstable_createPortal`.\\n\\n**why make this change?:**\\nWe don't want to warn for mismatching container nodes in this case - the\\nuser intentionally rendered into the portal container instead of the\\noriginal container.\\n\\nconcerns;\\n- will this affect React Native badly?\\n- will this add bloat to the portal code? seems small enough but not\\n  sure.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Fix logic for checking if the host instance container is a portal\\n\\n**what is the change?:**\\nWhen focusing on fixing the warning to not check when we are using\\nportals, I missed checking for the existence of the host instance parent\\nbefore checking if it was a portal. This adds the missing null checks.\\n\\n**why make this change?:**\\nTo fix a bug that the previous commit introduced.\\n\\n**test plan:**\\n`yarn test`\\n-> follow-up commits fix more of the test failures\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Clean up new tests in ReactDOMFiber-test\\n\\n**what is the change?:**\\n- removed extra single quotes, downgrade double quotes to single\\n- update expected warning message to match latest warning message\\n- fix indentation\\n\\n**why make this change?:**\\n- get tests passing\\n- code maintainability/readability\\n\\n**test plan:**\\n`yarn test`\\nfollow up commits will fix the remaining tests\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Add 'unmountComponentAtNode' call in test for reconciling pre-rendered markup\\n\\n**what is the change?:**\\nWe have a test that verifies React can reconcile text from pre-rendered\\nmark-up. It tests React doing this for three strings and three empty\\nstrings.\\n\\nThis adds a call to 'unmountComponentAtNode' between the two\\nexpectations for strings and empty strings.\\n\\n**why make this change?:**\\nWe now warn when someone messes with the DOM inside of a node in such a\\nway that removes the React-rendered content. This test was doing that. I\\ncan't think of a situation where this would happen with server-side\\nrendering without the need to call 'unmountComponentAtNode' before\\ninserting the server-side rendered content.\\n\\n**test plan:**\\n`yarn test`\\n\\nOnly one more failing test, will fix that in the next commit.\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* ran prettier\\n\\n* remove unused variable\\n\\n* run scripts/fiber/record-tests\\n\\n* Fix type error and improve name of portal container flag\\n\\n**NOTE:** I am still looking for a good place to move this flag\\nassignment to, or a better approach. This does some intermediate fixes.\\n\\n**what is the change?:**\\n- fixed flow error by allowing optional flag on a DOMContainer that\\n  indicates it was used as a portal container.\\n- renamed the flag to something which makes more sense\\n\\n**why make this change?:**\\n- get Flow passing\\n- make this change make more sense\\n\\nWe are still not sure about adding this flag; a follow-up diff may move\\nit or take a different approach.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Add flag to portalContainer on mount instead of in `createPortal`\\n\\n**what is the change?:**\\nWe add a flag to the container of a 'portal' in the 'commit work' phase\\nin Fiber. This is right before we call `appendChildToContainer`.\\n\\n**why make this change?:**\\n- Sometimes people call `ReactDOM.render(... container)`, then manually\\nclear the content of the `container`, and then try to call another\\n`ReactDOM.render(... container)`.\\n- This leads to cryptic errors or silent failure because we hold a\\n  reference to the node that was rendered the first time, and expect it\\n  to still be inside the container.\\n- We added a warning for this issue in `renderSubtreeIntoContainer`, but\\n  when a component renders something returned by\\n  `ReactDOM.unstable_createPortal(<Component />, portalContainer);`,\\n  then the child is inside the `portalContainer` and not the `container,\\n  but that is valid and we want to skip warning in that case.\\n\\nInside `renderSubtreeIntoContainer` we don't have the info to determine\\nif a child was rendered into a `portalContainer` or a `container`, and\\nadding this flag lets us figure that out and skip the warning.\\n\\nWe originally added the flag in the call to\\n`ReactDOM.unstable_createPortal` but that seemed like a method that\\nshould be \\\"pure\\\" and free of side-effects. This commit moves the\\nflag-adding to happen when we mount the portal component.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Force an 'any' type for the `hostInstance.parentNode` in warning check\\n\\n**what is the change?:**\\nThis is awful. :(\\nI'm not sure how else to let Flow know that we expect that this might be\\na sort of `DOMContainer` type and not just a normal `Node` type.\\n\\nTo at least make the type information clear we added a comment.\\n\\n**why make this change?:**\\nTo get `flow` passing. Looks like we have `any` types sprinkled\\nthroughout this file. phooey. :(\\n\\n**test plan:**\\n`yarn flow`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Ignore portals in `DOMRenderer.findHostInstance`\\n\\n**what is the change?:**\\nWe want to ignore portals when firing a certain warning.\\n\\nThis allows us to get the host instance and ignore portals.\\n\\nAlso added a new snapshot recording while fixing things.\\n\\n**why make this change?:**\\nOriginally we had added a flag to the DOM node which was used for\\n\\nrendering the portal, and then could notice and ignore children rendered\\ninto those nodes.\\n\\nHowever, it's better to just ignore portals in\\n`DOMRenderer.findHostInstance` because\\n - we will not ignore a non-portal second child with this approach\\n - we meant to ignore portals in this method anyway (according to a\\n   'TODO' comment)\\n - this change only affects the DOM renderer, instead of changing code\\n   which is shared with RN and other renderers\\n - we avoid adding unneeded expandos\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Ran prettier\\n\\n* Remove error snapshot test\\n\\nI think there is a bug where an empty snapshot is treated as an 'outdated snapshot'.\\n\\nIf I delete the obsolute snapshot, and run ReactDOMFiber-test.js it generates a new snapshot.\\nBut then when I run the test with the newly generated snapshot, it says \\\"1 obsolete snapshot found\\\",\\nAt some point I will file an issue with Jest. For now going to skip the snapshot generation for the error message in the new test.\\n\\n* Remove expando that we were adding to portal container\\n\\n**what is the change?:**\\nsee title\\n\\n**why make this change?:**\\nthis is part of an old approach to detecting portals, and we have\\ninstead added a check in the `findHostInstance` method to filter out\\nportals.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n* Fork `findHostInstance` to make `findHostInstanceWithNoPortals`\\n\\n**what is the change?:**\\nWe need to get host instances, but filter out portals. There is not\\ncurrently a method for that.\\n\\n**why make this change?:**\\nRather than change the existing `findHostInstance` method, which would\\naffect the behavior of the public `findDOMNode` method, we are forking.\\n\\n**test plan:**\\n`yarn test`\\n\\n**issue:**\\nhttps://github.com/facebook/react/issues/8854\\n\\n\",\n",
      "  \"repo_name\" : \"react\",\n",
      "  \"file_path\" : \"src/renderers/dom/fiber/ReactDOMFiberEntry.js\",\n",
      "  \"commit_date\" : 1500652126\n",
      "}\n",
      "Score: 39.50899887084961\n",
      "\n",
      "data/facebook_react/index_tk/\n",
      "{'total_terms': 7567855, 'documents': 73551, 'non_empty_documents': 73551, 'unique_terms': 14588}\n",
      "{\n",
      "  \"id\": \"7022e8d6a3222c97d287dfa0f2361acc8a30683a\",\n",
      "  \"contents\": \"fix[devtools/extension]: fixed duplicating panels in firefox (#27320)\\n\\nMultiple `chrome.panels.create` calls result into having duplicate\\npanels created in Firefox, these changes fix that.\\n\\nNow calling `chrome.panels.create` only if there are no panels created\\nyet.\\n\",\n",
      "  \"repo_name\": \"react\",\n",
      "  \"file_path\": \"packages/react-devtools-extensions/src/main/index.js\",\n",
      "  \"commit_date\": 1693502666\n",
      "}\n",
      "Score: 170.09060668945312\n",
      "\n",
      "data/facebook_react/index_nf/\n",
      "{'total_terms': 3338660, 'documents': 73551, 'non_empty_documents': 73551, 'unique_terms': 20393}\n",
      "{\n",
      "  \"id\" : \"7022e8d6a3222c97d287dfa0f2361acc8a30683a\",\n",
      "  \"contents\" : \"fix[devtools/extension]: fixed duplicating panels in firefox (#27320)\\n\\nMultiple `chrome.panels.create` calls result into having duplicate\\npanels created in Firefox, these changes fix that.\\n\\nNow calling `chrome.panels.create` only if there are no panels created\\nyet.\\n\",\n",
      "  \"repo_name\" : \"react\",\n",
      "  \"file_path\" : \"packages/react-devtools-extensions/src/main/index.js\",\n",
      "  \"commit_date\" : 1693502666\n",
      "}\n",
      "Score: 123.9469985961914\n",
      "\n",
      "data/facebook_react/index_tk_nf/\n",
      "{'total_terms': 7567855, 'documents': 73551, 'non_empty_documents': 73551, 'unique_terms': 14588}\n",
      "{\n",
      "  \"id\": \"7022e8d6a3222c97d287dfa0f2361acc8a30683a\",\n",
      "  \"contents\": \"fix[devtools/extension]: fixed duplicating panels in firefox (#27320)\\n\\nMultiple `chrome.panels.create` calls result into having duplicate\\npanels created in Firefox, these changes fix that.\\n\\nNow calling `chrome.panels.create` only if there are no panels created\\nyet.\\n\",\n",
      "  \"repo_name\": \"react\",\n",
      "  \"file_path\": \"packages/react-devtools-extensions/src/main/index.js\",\n",
      "  \"commit_date\": 1693502666\n",
      "}\n",
      "Score: 170.09060668945312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lst = [f'{repo_dir}/index/', f'{repo_dir}/index_tk/', f'{repo_dir}/index_nf/', f'{repo_dir}/index_tk_nf/']\n",
    "for i in lst:\n",
    "    index_reader = IndexReader(i)\n",
    "    search = LuceneSearcher(i)\n",
    "    print(i)\n",
    "    print(index_reader.stats())\n",
    "    search_res = search.search(query, k=10) if 'tk' not in i else search.search(tokenize(query), k=10)\n",
    "    if 'tk' in i:\n",
    "        print(reverse_tokenize(search_res[0].raw))\n",
    "    else:\n",
    "        print(search_res[0].raw)\n",
    "    print(f'Score: {search_res[0].score}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 5426af3d50ff706f3ebeb4764f838e0a3812bf9a 23.37060 react/packages/react-devtools-extensions/src/main.js 1681238533\n",
      " 3 72e690ecfb89dd86db17d0a17160680d6b6dfa19 15.52670 react/src/core/ReactCompositeComponent.js 1408580827\n",
      " 4 72e690ecfb89dd86db17d0a17160680d6b6dfa19 15.52670 react/src/core/__tests__/ReactCompositeComponent-test.js 1408580827\n",
      " 5 dcdc35fab69fa28a6cda7b3fe5dc904ccfbe5455 13.89970 react/src/core/ReactCompositeComponent.js 1407963580\n",
      " 6 dcdc35fab69fa28a6cda7b3fe5dc904ccfbe5455 13.89970 react/src/core/__tests__/ReactCompositeComponent-test.js 1407963580\n",
      " 7 51947a14bb24bd151f76f6fc0acdbbc404de13f7 13.62390 react/packages/react-devtools-shared/src/__tests__/TimelineProfiler-test.js 1642103754\n",
      " 8 51947a14bb24bd151f76f6fc0acdbbc404de13f7 13.62390 react/packages/react-devtools-shared/src/__tests__/preprocessData-test.js 1642103754\n",
      " 9 51947a14bb24bd151f76f6fc0acdbbc404de13f7 13.62390 react/packages/react-devtools-shared/src/__tests__/setupTests.js 1642103754\n",
      "10 51947a14bb24bd151f76f6fc0acdbbc404de13f7 13.62390 react/packages/react-devtools-shared/src/__tests__/utils.js 1642103754\n"
     ]
    }
   ],
   "source": [
    "idx_path = f'{repo_dir}/index_tk/'\n",
    "bm25searcher = LuceneSearcher(idx_path)\n",
    "hits = bm25searcher.search(tokenize(query), k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    commit_date = int(obj[\"commit_date\"])\n",
    "    if commit_date > convert_date_to_timestamp(query_date):\n",
    "        continue\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2.c\n",
    "# query = 'nInference for Llama-2 Transformer model in pure C'\n",
    "\n",
    "# refpred\n",
    "# query = 'if is_arxiv:\\n return f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper_id}/references?fields=title,\n",
    "# abstract,url,venue,publicationVenue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess'\n",
    "\n",
    "# react\n",
    "# query = \"export {default} from './npm/Circle';\"\n",
    "\n",
    "# kafka\n",
    "# public class MockKafkaLog4jAppender extends KafkaLog4jAppender {\n",
    "#     private MockProducer<byte[], byte[]> mockProducer =\n",
    "#             new MockProducer<>(false, new MockSerializer(), new MockSerializer());\n",
    "\n",
    "#     private Properties producerProperties;\n",
    "\n",
    "#     @Override\n",
    "#     protected Producer<byte[], byte[]> getKafkaProducer(Properties props) {\n",
    "#         producerProperties = props;\n",
    "#         return mockProducer;\n",
    "#     }\n",
    "\n",
    "#     void setKafkaProducer(MockProducer<byte[], byte[]> producer) {\n",
    "#         this.mockProducer = producer;\n",
    "#     }\n",
    "# \"\"\"\n",
    "\n",
    "# Kakfa\n",
    "# query = \"\"\"\n",
    "# /**\n",
    "#  * Local file based quorum state store. It takes the JSON format of {@link QuorumStateData}\n",
    "#  * with an extra data version number as part of the data for easy deserialization.\n",
    "#  *\n",
    "#  * Example format:\n",
    "#  * <pre>\n",
    "#  * {\"clusterId\":\"\",\n",
    "#  *   \"leaderId\":1,\n",
    "#  *   \"leaderEpoch\":2,\n",
    "#  *   \"votedId\":-1,\n",
    "#  *   \"appliedOffset\":0,\n",
    "#  *   \"currentVoters\":[],\n",
    "#  *   \"data_version\":0}\n",
    "#  * </pre>\n",
    "#  * */\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# kakfa\n",
    "query = \"\"\"Convert coordinator retriable errors to a known producer…\n",
    "… response error (#14378)\n",
    "\n",
    "KIP-890 Part 1 tries to address hanging transactions on old clients. Thus, the produce version can not be bumped and no new errors can be added. Before we used the java client's notion of retriable and abortable errors -- retriable errors are defined as such by extending the retriable error class, fatal errors are defined explicitly, and abortable errors are the remaining. However, many other clients treat non specified errors as fatal and that means many retriable errors kill the application.\"\"\"\n",
    "\n",
    "# kakfa\n",
    "# query = \"\"\"Fix flaky TopicAdminTest::retryEndOffsetsShouldRetryWhenTopicNotFound test case\"\"\"\n",
    "\n",
    "# nodejs\n",
    "# query = \"\"\"bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
    "#   DebugSealHandleScope scope(isolate);\n",
    "#   Environment* env = Environment::GetCurrent(isolate);\n",
    "#   return env != nullptr &&\n",
    "#          (env->is_main_thread() || !env->is_stopping()) &&\n",
    "#          env->abort_on_uncaught_exception() &&\n",
    "#          env->should_abort_on_uncaught_toggle()[0] &&\n",
    "#          !env->inside_should_not_abort_on_uncaught_scope();\n",
    "# }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 99.51060 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 82.57980 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 81.72260 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 5aecd2825644728f68a26558c957f5dfd4643423 81.36090 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 5 ef09a2e3fc11a738f6681fd57fb84ad109593fd3 80.57710 kafka/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala\n",
      " 6 f5d5f654db359af077088685e29fbe5ea69616cf 79.69870 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 2b6365c78b6e659f8df0651a24013d028f39edd9 79.64400 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 78.68580 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 1a10c3445e157da1d2fd670c043f19c385465eb0 78.48480 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 69d2a177101eb1c29b59b4c64d8c22f6d5e3d281 78.27240 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "bm25searcher = LuceneSearcher('bm25_index_6/')\n",
    "hits = bm25searcher.search(query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 696778,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 6840}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [00:02<00:00, 190.05it/s]\n"
     ]
    }
   ],
   "source": [
    "index_reader.dump_documents_BM25('tmp/idx_karpathy_double.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 578447,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 3034}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 141.63670 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 5aecd2825644728f68a26558c957f5dfd4643423 112.99820 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 111.59350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 111.57550 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 5 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 110.54000 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 6 ea0bb001262320bc9233221955a2be31c85993b9 109.68660 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 f5d5f654db359af077088685e29fbe5ea69616cf 109.62250 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 b937ec75677f8af13bf6fda686f07e9c62cdd20f 109.10350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 a81f35c1c8f9dc594aa585618c36f92ade0f86e2 109.03760 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 b49013b73efa25466652d8d8122974e60c927ec4 108.96060 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "tiktoken_searcher = LuceneSearcher('bm25_index_tiktoken_6/')\n",
    "# get tokenized query with enc.encode\n",
    "tokeninzed_query = tokenize(query)\n",
    "hits = tiktoken_searcher.search(tokeninzed_query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 2698903862,\n",
       " 'documents': 360230,\n",
       " 'non_empty_documents': 360230,\n",
       " 'unique_terms': -1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_index_reader = IndexReader('bm25_index_tiktoken_6/')\n",
    "tiktoken_index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the document source code inside the first hit raw\n",
    "content = json.loads(hits[0].raw)['contents']\n",
    "\n",
    "# print the document source code inside the first hit raw by decoding the tokenized string with enc.decode (convert to array of int and then decode)\n",
    "# print(enc.decode(json.loads(hits[0].raw)['contents']))\n",
    "\n",
    "# convert content to array of int\n",
    "content_arr = [int(i) for i in content.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: fix --abort-on-uncaught-exception handling\n",
      "\n",
      "The `set_abort_on_uncaught_exception(false)` line was supposed to\n",
      "prevent aborting when running Workers in\n",
      "`--abort-on-uncaught-exception` mode, but it was incorrectly set\n",
      "and not checked properly in the should-abort callback.\n",
      "\n",
      "PR-URL: https://github.com/nodejs/node/pull/34724\n",
      "Reviewed-By: Colin Ihrig <cjihrig@gmail.com>\n",
      "Reviewed-By: Richard Lau <riclau@uk.ibm.com>\n",
      "Reviewed-By: James M Snell <jasnell@gmail.com>\n",
      "Reviewed-By: Mary Marchini <oss@mmarchini.me>\n",
      "\n",
      "#include \"node.h\"\n",
      "#include \"node_context_data.h\"\n",
      "#include \"node_errors.h\"\n",
      "#include \"node_internals.h\"\n",
      "#include \"node_native_module_env.h\"\n",
      "#include \"node_platform.h\"\n",
      "#include \"node_v8_platform-inl.h\"\n",
      "#include \"uv.h\"\n",
      "\n",
      "#if HAVE_INSPECTOR\n",
      "#include \"inspector/worker_inspector.h\"  // ParentInspectorHandle\n",
      "#endif\n",
      "\n",
      "namespace node {\n",
      "using errors::TryCatchScope;\n",
      "using v8::Array;\n",
      "using v8::Context;\n",
      "using v8::EscapableHandleScope;\n",
      "using v8::Function;\n",
      "using v8::FunctionCallbackInfo;\n",
      "using v8::HandleScope;\n",
      "using v8::Isolate;\n",
      "using v8::Local;\n",
      "using v8::MaybeLocal;\n",
      "using v8::Null;\n",
      "using v8::Object;\n",
      "using v8::ObjectTemplate;\n",
      "using v8::Private;\n",
      "using v8::PropertyDescriptor;\n",
      "using v8::SealHandleScope;\n",
      "using v8::String;\n",
      "using v8::Value;\n",
      "\n",
      "static bool AllowWasmCodeGenerationCallback(Local<Context> context,\n",
      "                                            Local<String>) {\n",
      "  Local<Value> wasm_code_gen =\n",
      "      context->GetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration);\n",
      "  return wasm_code_gen->IsUndefined() || wasm_code_gen->IsTrue();\n",
      "}\n",
      "\n",
      "static bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
      "  DebugSealHandleScope scope(isolate);\n",
      "  Environment* env = Environment::GetCurrent(isolate);\n",
      "  return env != nullptr &&\n",
      "         (env->is_main_thread() || !env->is_stopping()) &&\n",
      "         env->abort_on_uncaught_exception() &&\n",
      "         env->should_abort_on_uncaught_toggle()[0] &&\n",
      "         !env->inside_should_not_abort_on_uncaught_scope();\n",
      "}\n",
      "\n",
      "static MaybeLocal<Value> PrepareStackTraceCallback(Local<Context> context,\n",
      "                                      Local<Value> exception,\n",
      "                                      Local<Array> trace) {\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Function> prepare = env->prepare_stack_trace_callback();\n",
      "  if (prepare.IsEmpty()) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Value> args[] = {\n",
      "      context->Global(),\n",
      "      exception,\n",
      "      trace,\n",
      "  };\n",
      "  // This TryCatch + Rethrow is required by V8 due to details around exception\n",
      "  // handling there. For C++ callbacks, V8 expects a scheduled exception (which\n",
      "  // is what ReThrow gives us). Just returning the empty MaybeLocal would leave\n",
      "  // us with a pending exception.\n",
      "  TryCatchScope try_catch(env);\n",
      "  MaybeLocal<Value> result = prepare->Call(\n",
      "      context, Undefined(env->isolate()), arraysize(args), args);\n",
      "  if (try_catch.HasCaught() && !try_catch.HasTerminated()) {\n",
      "    try_catch.ReThrow();\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Allocate(size_t size) {\n",
      "  void* ret;\n",
      "  if (zero_fill_field_ || per_process::cli_options->zero_fill_all_buffers)\n",
      "    ret = UncheckedCalloc(size);\n",
      "  else\n",
      "    ret = UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  void* ret = node::UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Reallocate(\n",
      "    void* data, size_t old_size, size_t size) {\n",
      "  void* ret = UncheckedRealloc<char>(static_cast<char*>(data), size);\n",
      "  if (LIKELY(ret != nullptr) || UNLIKELY(size == 0))\n",
      "    total_mem_usage_.fetch_add(size - old_size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void NodeArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  total_mem_usage_.fetch_sub(size, std::memory_order_relaxed);\n",
      "  free(data);\n",
      "}\n",
      "\n",
      "DebuggingArrayBufferAllocator::~DebuggingArrayBufferAllocator() {\n",
      "  CHECK(allocations_.empty());\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Allocate(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::Allocate(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::AllocateUninitialized(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "  NodeArrayBufferAllocator::Free(data, size);\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Reallocate(void* data,\n",
      "                                                size_t old_size,\n",
      "                                                size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* ret = NodeArrayBufferAllocator::Reallocate(data, old_size, size);\n",
      "  if (ret == nullptr) {\n",
      "    if (size == 0)  // i.e. equivalent to free().\n",
      "      UnregisterPointerInternal(data, old_size);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  if (data != nullptr) {\n",
      "    auto it = allocations_.find(data);\n",
      "    CHECK_NE(it, allocations_.end());\n",
      "    allocations_.erase(it);\n",
      "  }\n",
      "\n",
      "  RegisterPointerInternal(ret, size);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::RegisterPointer(data, size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::UnregisterPointer(data, size);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointerInternal(void* data,\n",
      "                                                              size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  auto it = allocations_.find(data);\n",
      "  CHECK_NE(it, allocations_.end());\n",
      "  if (size > 0) {\n",
      "    // We allow allocations with size 1 for 0-length buffers to avoid having\n",
      "    // to deal with nullptr values.\n",
      "    CHECK_EQ(it->second, size);\n",
      "  }\n",
      "  allocations_.erase(it);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointerInternal(void* data,\n",
      "                                                            size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  CHECK_EQ(allocations_.count(data), 0);\n",
      "  allocations_[data] = size;\n",
      "}\n",
      "\n",
      "std::unique_ptr<ArrayBufferAllocator> ArrayBufferAllocator::Create(bool debug) {\n",
      "  if (debug || per_process::cli_options->debug_arraybuffer_allocations)\n",
      "    return std::make_unique<DebuggingArrayBufferAllocator>();\n",
      "  else\n",
      "    return std::make_unique<NodeArrayBufferAllocator>();\n",
      "}\n",
      "\n",
      "ArrayBufferAllocator* CreateArrayBufferAllocator() {\n",
      "  return ArrayBufferAllocator::Create().release();\n",
      "}\n",
      "\n",
      "void FreeArrayBufferAllocator(ArrayBufferAllocator* allocator) {\n",
      "  delete allocator;\n",
      "}\n",
      "\n",
      "void SetIsolateCreateParamsForNode(Isolate::CreateParams* params) {\n",
      "  const uint64_t constrained_memory = uv_get_constrained_memory();\n",
      "  const uint64_t total_memory = constrained_memory > 0 ?\n",
      "      std::min(uv_get_total_memory(), constrained_memory) :\n",
      "      uv_get_total_memory();\n",
      "  if (total_memory > 0) {\n",
      "    // V8 defaults to 700MB or 1.4GB on 32 and 64 bit platforms respectively.\n",
      "    // This default is based on browser use-cases. Tell V8 to configure the\n",
      "    // heap based on the actual physical memory.\n",
      "    params->constraints.ConfigureDefaults(total_memory, 0);\n",
      "  }\n",
      "  params->embedder_wrapper_object_index = BaseObject::InternalFields::kSlot;\n",
      "  params->embedder_wrapper_type_index = std::numeric_limits<int>::max();\n",
      "}\n",
      "\n",
      "void SetIsolateErrorHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  if (s.flags & MESSAGE_LISTENER_WITH_ERROR_LEVEL)\n",
      "    isolate->AddMessageListenerWithErrorLevel(\n",
      "            errors::PerIsolateMessageListener,\n",
      "            Isolate::MessageErrorLevel::kMessageError |\n",
      "                Isolate::MessageErrorLevel::kMessageWarning);\n",
      "\n",
      "  auto* abort_callback = s.should_abort_on_uncaught_exception_callback ?\n",
      "      s.should_abort_on_uncaught_exception_callback :\n",
      "      ShouldAbortOnUncaughtException;\n",
      "  isolate->SetAbortOnUncaughtExceptionCallback(abort_callback);\n",
      "\n",
      "  auto* fatal_error_cb = s.fatal_error_callback ?\n",
      "      s.fatal_error_callback : OnFatalError;\n",
      "  isolate->SetFatalErrorHandler(fatal_error_cb);\n",
      "\n",
      "  auto* prepare_stack_trace_cb = s.prepare_stack_trace_callback ?\n",
      "      s.prepare_stack_trace_callback : PrepareStackTraceCallback;\n",
      "  isolate->SetPrepareStackTraceCallback(prepare_stack_trace_cb);\n",
      "}\n",
      "\n",
      "void SetIsolateMiscHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  isolate->SetMicrotasksPolicy(s.policy);\n",
      "\n",
      "  auto* allow_wasm_codegen_cb = s.allow_wasm_code_generation_callback ?\n",
      "    s.allow_wasm_code_generation_callback : AllowWasmCodeGenerationCallback;\n",
      "  isolate->SetAllowWasmCodeGenerationCallback(allow_wasm_codegen_cb);\n",
      "\n",
      "  if ((s.flags & SHOULD_NOT_SET_PROMISE_REJECTION_CALLBACK) == 0) {\n",
      "    auto* promise_reject_cb = s.promise_reject_callback ?\n",
      "      s.promise_reject_callback : task_queue::PromiseRejectCallback;\n",
      "    isolate->SetPromiseRejectCallback(promise_reject_cb);\n",
      "  }\n",
      "\n",
      "  if (s.flags & DETAILED_SOURCE_POSITIONS_FOR_PROFILING)\n",
      "    v8::CpuProfiler::UseDetailedSourcePositionsForProfiling(isolate);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate,\n",
      "                         const IsolateSettings& settings) {\n",
      "  SetIsolateErrorHandlers(isolate, settings);\n",
      "  SetIsolateMiscHandlers(isolate, settings);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate) {\n",
      "  IsolateSettings settings;\n",
      "  SetIsolateUpForNode(isolate, settings);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator, uv_loop_t* event_loop) {\n",
      "  return NewIsolate(allocator, event_loop, GetMainThreadMultiIsolatePlatform());\n",
      "}\n",
      "\n",
      "// TODO(joyeecheung): we may want to expose this, but then we need to be\n",
      "// careful about what we override in the params.\n",
      "Isolate* NewIsolate(Isolate::CreateParams* params,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate* isolate = Isolate::Allocate();\n",
      "  if (isolate == nullptr) return nullptr;\n",
      "\n",
      "  // Register the isolate on the platform before the isolate gets initialized,\n",
      "  // so that the isolate can access the platform during initialization.\n",
      "  platform->RegisterIsolate(isolate, event_loop);\n",
      "\n",
      "  SetIsolateCreateParamsForNode(params);\n",
      "  Isolate::Initialize(isolate, *params);\n",
      "  SetIsolateUpForNode(isolate);\n",
      "\n",
      "  return isolate;\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator != nullptr) params.array_buffer_allocator = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(std::shared_ptr<ArrayBufferAllocator> allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator) params.array_buffer_allocator_shared = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "IsolateData* CreateIsolateData(Isolate* isolate,\n",
      "                               uv_loop_t* loop,\n",
      "                               MultiIsolatePlatform* platform,\n",
      "                               ArrayBufferAllocator* allocator) {\n",
      "  return new IsolateData(isolate, loop, platform, allocator);\n",
      "}\n",
      "\n",
      "void FreeIsolateData(IsolateData* isolate_data) {\n",
      "  delete isolate_data;\n",
      "}\n",
      "\n",
      "InspectorParentHandle::~InspectorParentHandle() {}\n",
      "\n",
      "// Hide the internal handle class from the public API.\n",
      "#if HAVE_INSPECTOR\n",
      "struct InspectorParentHandleImpl : public InspectorParentHandle {\n",
      "  std::unique_ptr<inspector::ParentInspectorHandle> impl;\n",
      "\n",
      "  explicit InspectorParentHandleImpl(\n",
      "      std::unique_ptr<inspector::ParentInspectorHandle>&& impl)\n",
      "    : impl(std::move(impl)) {}\n",
      "};\n",
      "#endif\n",
      "\n",
      "Environment* CreateEnvironment(IsolateData* isolate_data,\n",
      "                               Local<Context> context,\n",
      "                               int argc,\n",
      "                               const char* const* argv,\n",
      "                               int exec_argc,\n",
      "                               const char* const* exec_argv) {\n",
      "  return CreateEnvironment(\n",
      "      isolate_data, context,\n",
      "      std::vector<std::string>(argv, argv + argc),\n",
      "      std::vector<std::string>(exec_argv, exec_argv + exec_argc));\n",
      "}\n",
      "\n",
      "Environment* CreateEnvironment(\n",
      "    IsolateData* isolate_data,\n",
      "    Local<Context> context,\n",
      "    const std::vector<std::string>& args,\n",
      "    const std::vector<std::string>& exec_args,\n",
      "    EnvironmentFlags::Flags flags,\n",
      "    ThreadId thread_id,\n",
      "    std::unique_ptr<InspectorParentHandle> inspector_parent_handle) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Context::Scope context_scope(context);\n",
      "  // TODO(addaleax): This is a much better place for parsing per-Environment\n",
      "  // options than the global parse call.\n",
      "  Environment* env = new Environment(\n",
      "      isolate_data, context, args, exec_args, nullptr, flags, thread_id);\n",
      "#if HAVE_INSPECTOR\n",
      "  if (inspector_parent_handle) {\n",
      "    env->InitializeInspector(\n",
      "        std::move(static_cast<InspectorParentHandleImpl*>(\n",
      "            inspector_parent_handle.get())->impl));\n",
      "  } else {\n",
      "    env->InitializeInspector({});\n",
      "  }\n",
      "#endif\n",
      "\n",
      "  if (env->RunBootstrapping().IsEmpty()) {\n",
      "    FreeEnvironment(env);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  return env;\n",
      "}\n",
      "\n",
      "void FreeEnvironment(Environment* env) {\n",
      "  Isolate::DisallowJavascriptExecutionScope disallow_js(env->isolate(),\n",
      "      Isolate::DisallowJavascriptExecutionScope::THROW_ON_FAILURE);\n",
      "  {\n",
      "    HandleScope handle_scope(env->isolate());  // For env->context().\n",
      "    Context::Scope context_scope(env->context());\n",
      "    SealHandleScope seal_handle_scope(env->isolate());\n",
      "\n",
      "    env->set_stopping(true);\n",
      "    env->stop_sub_worker_contexts();\n",
      "    env->RunCleanup();\n",
      "    RunAtExit(env);\n",
      "  }\n",
      "\n",
      "  // This call needs to be made while the `Environment` is still alive\n",
      "  // because we assume that it is available for async tracking in the\n",
      "  // NodePlatform implementation.\n",
      "  MultiIsolatePlatform* platform = env->isolate_data()->platform();\n",
      "  if (platform != nullptr)\n",
      "    platform->DrainTasks(env->isolate());\n",
      "\n",
      "  delete env;\n",
      "}\n",
      "\n",
      "NODE_EXTERN std::unique_ptr<InspectorParentHandle> GetInspectorParentHandle(\n",
      "    Environment* env,\n",
      "    ThreadId thread_id,\n",
      "    const char* url) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  CHECK_NE(thread_id.id, static_cast<uint64_t>(-1));\n",
      "#if HAVE_INSPECTOR\n",
      "  return std::make_unique<InspectorParentHandleImpl>(\n",
      "      env->inspector_agent()->GetParentHandle(thread_id.id, url));\n",
      "#else\n",
      "  return {};\n",
      "#endif\n",
      "}\n",
      "\n",
      "void LoadEnvironment(Environment* env) {\n",
      "  USE(LoadEnvironment(env,\n",
      "                      StartExecutionCallback{},\n",
      "                      {}));\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    StartExecutionCallback cb,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  env->InitializeLibuv();\n",
      "  env->InitializeDiagnostics();\n",
      "\n",
      "  return StartExecution(env, cb);\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    const char* main_script_source_utf8,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  CHECK_NOT_NULL(main_script_source_utf8);\n",
      "  return LoadEnvironment(\n",
      "      env,\n",
      "      [&](const StartExecutionCallbackInfo& info) -> MaybeLocal<Value> {\n",
      "        // This is a slightly hacky way to convert UTF-8 to UTF-16.\n",
      "        Local<String> str =\n",
      "            String::NewFromUtf8(env->isolate(),\n",
      "                                main_script_source_utf8).ToLocalChecked();\n",
      "        auto main_utf16 = std::make_unique<String::Value>(env->isolate(), str);\n",
      "\n",
      "        // TODO(addaleax): Avoid having a global table for all scripts.\n",
      "        std::string name = \"embedder_main_\" + std::to_string(env->thread_id());\n",
      "        native_module::NativeModuleEnv::Add(\n",
      "            name.c_str(),\n",
      "            UnionBytes(**main_utf16, main_utf16->length()));\n",
      "        env->set_main_utf16(std::move(main_utf16));\n",
      "        std::vector<Local<String>> params = {\n",
      "            env->process_string(),\n",
      "            env->require_string()};\n",
      "        std::vector<Local<Value>> args = {\n",
      "            env->process_object(),\n",
      "            env->native_module_require()};\n",
      "        return ExecuteBootstrapper(env, name.c_str(), &params, &args);\n",
      "      });\n",
      "}\n",
      "\n",
      "Environment* GetCurrentEnvironment(Local<Context> context) {\n",
      "  return Environment::GetCurrent(context);\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMainThreadMultiIsolatePlatform() {\n",
      "  return per_process::v8_platform.Platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(Environment* env) {\n",
      "  return GetMultiIsolatePlatform(env->isolate_data());\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(IsolateData* env) {\n",
      "  return env->platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    node::tracing::TracingController* tracing_controller) {\n",
      "  return CreatePlatform(\n",
      "      thread_pool_size,\n",
      "      static_cast<v8::TracingController*>(tracing_controller));\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return MultiIsolatePlatform::Create(thread_pool_size, tracing_controller)\n",
      "      .release();\n",
      "}\n",
      "\n",
      "void FreePlatform(MultiIsolatePlatform* platform) {\n",
      "  delete platform;\n",
      "}\n",
      "\n",
      "std::unique_ptr<MultiIsolatePlatform> MultiIsolatePlatform::Create(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return std::make_unique<NodePlatform>(thread_pool_size, tracing_controller);\n",
      "}\n",
      "\n",
      "MaybeLocal<Object> GetPerContextExports(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  EscapableHandleScope handle_scope(isolate);\n",
      "\n",
      "  Local<Object> global = context->Global();\n",
      "  Local<Private> key = Private::ForApi(isolate,\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"node:per_context_binding_exports\"));\n",
      "\n",
      "  Local<Value> existing_value;\n",
      "  if (!global->GetPrivate(context, key).ToLocal(&existing_value))\n",
      "    return MaybeLocal<Object>();\n",
      "  if (existing_value->IsObject())\n",
      "    return handle_scope.Escape(existing_value.As<Object>());\n",
      "\n",
      "  Local<Object> exports = Object::New(isolate);\n",
      "  if (context->Global()->SetPrivate(context, key, exports).IsNothing() ||\n",
      "      !InitializePrimordials(context))\n",
      "    return MaybeLocal<Object>();\n",
      "  return handle_scope.Escape(exports);\n",
      "}\n",
      "\n",
      "// Any initialization logic should be performed in\n",
      "// InitializeContext, because embedders don't necessarily\n",
      "// call NewContext and so they will experience breakages.\n",
      "Local<Context> NewContext(Isolate* isolate,\n",
      "                          Local<ObjectTemplate> object_template) {\n",
      "  auto context = Context::New(isolate, nullptr, object_template);\n",
      "  if (context.IsEmpty()) return context;\n",
      "\n",
      "  if (!InitializeContext(context)) {\n",
      "    return Local<Context>();\n",
      "  }\n",
      "\n",
      "  return context;\n",
      "}\n",
      "\n",
      "void ProtoThrower(const FunctionCallbackInfo<Value>& info) {\n",
      "  THROW_ERR_PROTO_ACCESS(info.GetIsolate());\n",
      "}\n",
      "\n",
      "// This runs at runtime, regardless of whether the context\n",
      "// is created from a snapshot.\n",
      "void InitializeContextRuntime(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  // Delete `Intl.v8BreakIterator`\n",
      "  // https://github.com/nodejs/node/issues/14909\n",
      "  Local<String> intl_string = FIXED_ONE_BYTE_STRING(isolate, \"Intl\");\n",
      "  Local<String> break_iter_string =\n",
      "    FIXED_ONE_BYTE_STRING(isolate, \"v8BreakIterator\");\n",
      "  Local<Value> intl_v;\n",
      "  if (context->Global()->Get(context, intl_string).ToLocal(&intl_v) &&\n",
      "      intl_v->IsObject()) {\n",
      "    Local<Object> intl = intl_v.As<Object>();\n",
      "    intl->Delete(context, break_iter_string).Check();\n",
      "  }\n",
      "\n",
      "  // Delete `Atomics.wake`\n",
      "  // https://github.com/nodejs/node/issues/21219\n",
      "  Local<String> atomics_string = FIXED_ONE_BYTE_STRING(isolate, \"Atomics\");\n",
      "  Local<String> wake_string = FIXED_ONE_BYTE_STRING(isolate, \"wake\");\n",
      "  Local<Value> atomics_v;\n",
      "  if (context->Global()->Get(context, atomics_string).ToLocal(&atomics_v) &&\n",
      "      atomics_v->IsObject()) {\n",
      "    Local<Object> atomics = atomics_v.As<Object>();\n",
      "    atomics->Delete(context, wake_string).Check();\n",
      "  }\n",
      "\n",
      "  // Remove __proto__\n",
      "  // https://github.com/nodejs/node/issues/31951\n",
      "  Local<String> object_string = FIXED_ONE_BYTE_STRING(isolate, \"Object\");\n",
      "  Local<String> prototype_string = FIXED_ONE_BYTE_STRING(isolate, \"prototype\");\n",
      "  Local<Object> prototype = context->Global()\n",
      "                                ->Get(context, object_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>()\n",
      "                                ->Get(context, prototype_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>();\n",
      "  Local<String> proto_string = FIXED_ONE_BYTE_STRING(isolate, \"__proto__\");\n",
      "  if (per_process::cli_options->disable_proto == \"delete\") {\n",
      "    prototype->Delete(context, proto_string).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto == \"throw\") {\n",
      "    Local<Value> thrower =\n",
      "        Function::New(context, ProtoThrower).ToLocalChecked();\n",
      "    PropertyDescriptor descriptor(thrower, thrower);\n",
      "    descriptor.set_enumerable(false);\n",
      "    descriptor.set_configurable(true);\n",
      "    prototype->DefineProperty(context, proto_string, descriptor).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto != \"\") {\n",
      "    // Validated in ProcessGlobalArgs\n",
      "    FatalError(\"InitializeContextRuntime()\", \"invalid --disable-proto mode\");\n",
      "  }\n",
      "}\n",
      "\n",
      "bool InitializeContextForSnapshot(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  context->SetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration,\n",
      "                           True(isolate));\n",
      "  return InitializePrimordials(context);\n",
      "}\n",
      "\n",
      "bool InitializePrimordials(Local<Context> context) {\n",
      "  // Run per-context JS files.\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  Context::Scope context_scope(context);\n",
      "  Local<Object> exports;\n",
      "\n",
      "  Local<String> primordials_string =\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"primordials\");\n",
      "  Local<String> global_string = FIXED_ONE_BYTE_STRING(isolate, \"global\");\n",
      "  Local<String> exports_string = FIXED_ONE_BYTE_STRING(isolate, \"exports\");\n",
      "\n",
      "  // Create primordials first and make it available to per-context scripts.\n",
      "  Local<Object> primordials = Object::New(isolate);\n",
      "  if (!primordials->SetPrototype(context, Null(isolate)).FromJust() ||\n",
      "      !GetPerContextExports(context).ToLocal(&exports) ||\n",
      "      !exports->Set(context, primordials_string, primordials).FromJust()) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  static const char* context_files[] = {\"internal/per_context/primordials\",\n",
      "                                        \"internal/per_context/domexception\",\n",
      "                                        \"internal/per_context/messageport\",\n",
      "                                        nullptr};\n",
      "\n",
      "  for (const char** module = context_files; *module != nullptr; module++) {\n",
      "    std::vector<Local<String>> parameters = {\n",
      "        global_string, exports_string, primordials_string};\n",
      "    Local<Value> arguments[] = {context->Global(), exports, primordials};\n",
      "    MaybeLocal<Function> maybe_fn =\n",
      "        native_module::NativeModuleEnv::LookupAndCompile(\n",
      "            context, *module, &parameters, nullptr);\n",
      "    Local<Function> fn;\n",
      "    if (!maybe_fn.ToLocal(&fn)) {\n",
      "      return false;\n",
      "    }\n",
      "    MaybeLocal<Value> result =\n",
      "        fn->Call(context, Undefined(isolate), arraysize(arguments), arguments);\n",
      "    // Execution failed during context creation.\n",
      "    // TODO(joyeecheung): deprecate this signature and return a MaybeLocal.\n",
      "    if (result.IsEmpty()) {\n",
      "      return false;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return true;\n",
      "}\n",
      "\n",
      "bool InitializeContext(Local<Context> context) {\n",
      "  if (!InitializeContextForSnapshot(context)) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  InitializeContextRuntime(context);\n",
      "  return true;\n",
      "}\n",
      "\n",
      "uv_loop_t* GetCurrentEventLoop(Isolate* isolate) {\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Local<Context> context = isolate->GetCurrentContext();\n",
      "  if (context.IsEmpty()) return nullptr;\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) return nullptr;\n",
      "  return env->event_loop();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env, const node_module& mod) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  Mutex::ScopedLock lock(env->extra_linked_bindings_mutex());\n",
      "\n",
      "  node_module* prev_head = env->extra_linked_bindings_head();\n",
      "  env->extra_linked_bindings()->push_back(mod);\n",
      "  if (prev_head != nullptr)\n",
      "    prev_head->nm_link = &env->extra_linked_bindings()->back();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env,\n",
      "                      const char* name,\n",
      "                      addon_context_register_func fn,\n",
      "                      void* priv) {\n",
      "  node_module mod = {\n",
      "    NODE_MODULE_VERSION,\n",
      "    NM_F_LINKED,\n",
      "    nullptr,  // nm_dso_handle\n",
      "    nullptr,  // nm_filename\n",
      "    nullptr,  // nm_register_func\n",
      "    fn,\n",
      "    name,\n",
      "    priv,\n",
      "    nullptr   // nm_link\n",
      "  };\n",
      "  AddLinkedBinding(env, mod);\n",
      "}\n",
      "\n",
      "static std::atomic<uint64_t> next_thread_id{0};\n",
      "\n",
      "ThreadId AllocateEnvironmentThreadId() {\n",
      "  return ThreadId { next_thread_id++ };\n",
      "}\n",
      "\n",
      "void DefaultProcessExitHandler(Environment* env, int exit_code) {\n",
      "  env->set_can_call_into_js(false);\n",
      "  env->stop_sub_worker_contexts();\n",
      "  DisposePlatform();\n",
      "  exit(exit_code);\n",
      "}\n",
      "\n",
      "\n",
      "void SetProcessExitHandler(Environment* env,\n",
      "                           std::function<void(Environment*, int)>&& handler) {\n",
      "  env->set_process_exit_handler(std::move(handler));\n",
      "}\n",
      "\n",
      "}  // namespace node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(content_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = '''\n",
    "A crawler for the Semantic Scholar API.\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import logging.config\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "import httpx  # https://github.com/encode/httpx\n",
    "import requests\n",
    "from crawl_utils import get_batch_url, get_reference_url\n",
    "from db import MongoDBClient\n",
    "\n",
    "from config import S2_API_KEY, S2_RATE_LIMIT\n",
    "\n",
    "logging.config.fileConfig(fname=\"logging.conf\", disable_existing_loggers=False)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RateLimitExceededException(Exception):\n",
    "    \"\"\"Exception raised when rate limit is exceeded\"\"\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"RateLimitExceededException: {self.message}\"\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    \"\"\"Exception raised when a request times out\"\"\"\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"TimeoutException: {self.message}\"\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class Crawler:\n",
    "    \"\"\"A crawler for the Semantic Scholar API\"\"\"\n",
    "\n",
    "    client: httpx.AsyncClient = field(repr=False)\n",
    "    initial_papers: List[str] = field(default_factory=list)\n",
    "    num_workers: int = 10\n",
    "    max_papers: int = 100\n",
    "    mongodb_client: MongoDBClient = field(default_factory=MongoDBClient)\n",
    "    headers: dict = field(repr=False, default_factory=dict)\n",
    "    todo: asyncio.Queue = field(init=False, repr=False, default_factory=asyncio.Queue)\n",
    "    seen: Set[str]= field(init=False, default_factory=set)\n",
    "    done: Set[str] = field(init=False, default_factory=set)\n",
    "    retry: Dict[str, int] = field(init=False, default_factory=dict)\n",
    "    total: int = field(init=False, default=0)\n",
    "    MAX_RETRIES: int = field(init=False, default=3)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, settings: dict) -> \"Crawler\":\n",
    "        \"\"\"\n",
    "        Create a Crawler instance from a dict of settings\"\"\"\n",
    "        return cls(**settings)\n",
    "\n",
    "    async def run(self) -> None:\n",
    "        \"\"\"Run the crawler by creating workers until todo queue is empty\"\"\"\n",
    "        self.init_done()\n",
    "        await self.init_queue()\n",
    "        workers = [asyncio.create_task(self.worker()) for _ in range(self.num_workers)]\n",
    "        await self.todo.join()\n",
    "        for worker in workers:\n",
    "            worker.cancel()\n",
    "\n",
    "    async def init_queue(self) -> None:\n",
    "        \"\"\"Initialize the queue with the initial papers\"\"\"\n",
    "        batch_url = get_batch_url()\n",
    "        data = json.dumps({\"ids\": self.initial_papers})\n",
    "        response = requests.post(url=batch_url, data=data, headers=self.headers, timeout=10)\n",
    "        # initial_paper_id = self.initial_papers[0]\n",
    "        # initial_url = get_paper_url(initial_paper_id)\n",
    "        # response = requests.get(initial_url, headers=self.headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            logger.error(\"Error fetching initial papers\")\n",
    "            sys.exit(1)\n",
    "        logger.debug(f\"Fetching data for intial papers {self.initial_papers}\")\n",
    "        result_data = response.json()\n",
    "        # result_data[\"_id\"] = result_data[\"paperId\"]\n",
    "        for paper in result_data:\n",
    "            paper[\"_id\"] = paper[\"paperId\"]\n",
    "        # prime the queue\n",
    "        await self.on_found_papers(result_data, initial=True)\n",
    "\n",
    "    def init_done(self) -> None:\n",
    "        \"\"\"Initialize the seen set with already stored papers from DB\"\"\"\n",
    "        # self.seen = set(self.initial_papers)\n",
    "        self.done = self.mongodb_client.get_ids()\n",
    "        logger.info(f\"Already stored {len(self.done)} papers\")\n",
    "\n",
    "    async def worker(self) -> None:\n",
    "        \"\"\"One worker processes one paper at a time from the queue in a loop until cancelled\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                await self.process_one()\n",
    "            except asyncio.CancelledError:\n",
    "                return\n",
    "\n",
    "    async def retry_crawl(self, paper) -> None:\n",
    "        \"\"\"Retry crawling a paper in case of an exception\"\"\"\n",
    "        if paper[\"_id\"] in self.retry and self.retry[paper[\"_id\"]] > self.MAX_RETRIES:\n",
    "            logger.error(f\"Error processing {paper['_id']} even after retrying {self.MAX_RETRIES} times\")\n",
    "            return\n",
    "        # self.retry.add(paper[\"_id\"])\n",
    "        self.retry[paper[\"_id\"]] = self.retry.get(paper[\"_id\"], 0) + 1\n",
    "        logger.info(f\"Retry #{self.retry[paper['_id']]} for {paper['_id']}\")\n",
    "        # await self.todo.put_nowait(cur_paper)\n",
    "        await asyncio.sleep(1)\n",
    "        await self.crawl(paper)\n",
    "\n",
    "    async def process_one(self) -> None:\n",
    "        \"\"\"Gets one paper from the queue and processes it\"\"\"\n",
    "        # cur_paper is a dict\n",
    "        cur_paper = await self.todo.get()\n",
    "        try:\n",
    "            await self.crawl(cur_paper)\n",
    "        except TimeoutException as te:\n",
    "            # logger.warning(f\"Timeout for {cur_paper['_id']}\")\n",
    "            logger.warning(te)\n",
    "            await self.retry_crawl(cur_paper)\n",
    "        except RateLimitExceededException as rlee:\n",
    "            logger.critical(\"Rate limit exceeded, retrying in 2 second\")\n",
    "            logger.critical(rlee)\n",
    "            await asyncio.sleep(2)\n",
    "            await self.retry_crawl(cur_paper)\n",
    "        finally:\n",
    "            self.todo.task_done()\n",
    "\n",
    "    async def crawl(self, cur_paper: dict) -> None:\n",
    "        \"\"\"\n",
    "        Crawl a paper and its references, stores them in the database.\n",
    "        \"\"\"\n",
    "        # TODO proper rate limiting to 100 requests / second\n",
    "        # await asyncio.sleep(1 / self.num_workers)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "        cur_paper_id = cur_paper[\"paperId\"]\n",
    "        ref_url = get_reference_url(cur_paper_id)\n",
    "        cur_paper[\"_id\"] = cur_paper_id\n",
    "        if cur_paper[\"title\"] is None or cur_paper[\"abstract\"] is None:\n",
    "            logger.debug(f\"Skipping {cur_paper_id} as empty title or abstract\")\n",
    "            # I have no clue why this total -= 1 is here, it shouldn't be required, but crawler just prematurely stops\n",
    "            self.total -= 1\n",
    "            return\n",
    "        # async with self.semaphore:\n",
    "        # async with self.client.get(ref_url, headers=self.headers) as response:\n",
    "\n",
    "        response = await self.client.get(ref_url, headers=self.headers)\n",
    "\n",
    "        # if self.semaphore.locked():\n",
    "        #     logger.warning(f\"Semaphore locked for {cur_paper_id}\")\n",
    "        #     await asyncio.sleep(1)\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            # logger.critical(\n",
    "            #     f\"Rated limited for {cur_paper_id} - {response.status_code}\"\n",
    "            # )\n",
    "            # # await self.todo.put_nowait(cur_paper)\n",
    "            # await asyncio.sleep(1)\n",
    "            # await self.crawl(cur_paper)\n",
    "            raise RateLimitExceededException(\n",
    "                f\"Rated limited for {cur_paper_id} - {response.status_code}\"\n",
    "            )\n",
    "\n",
    "        if response.status_code == 504:\n",
    "            # raise asyncio.exceptions.TimeoutError(\n",
    "            #     f\"Timeout for {cur_paper_id} - {response.status_code}\"\n",
    "            # )\n",
    "            raise TimeoutException(f\"Timeout for {cur_paper_id} - {response.status_code}\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Error fetching references for {cur_paper_id} - {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        logger.debug(f\"Fetching references for {cur_paper_id} - {response.status_code}\")\n",
    "\n",
    "        result_data = response.json()\n",
    "        found_references = result_data[\"data\"]\n",
    "        found_references = [ref[\"citedPaper\"] for ref in found_references]\n",
    "        found_references = sorted(found_references, key=lambda x: x[\"citationCount\"] or 0, reverse=True)\n",
    "        ref_ids = [ref[\"paperId\"] for ref in found_references if ref[\"paperId\"] is not None]\n",
    "        cur_paper[\"references\"] = ref_ids\n",
    "        cur_paper[\"allReferencesStored\"] = True\n",
    "        if len(ref_ids) != cur_paper[\"referenceCount\"]:\n",
    "            cur_paper[\"allReferencesStored\"] = False\n",
    "\n",
    "        # self.collection.insert_one(cur_paper)\n",
    "        self.mongodb_client.insert_one(cur_paper)\n",
    "        self.done.add(cur_paper[\"paperId\"])\n",
    "        # self.stored += 1\n",
    "        # if self.stored % 100 == 0:\n",
    "        #     logger.info(f\"Stored {self.stored} papers\")\n",
    "\n",
    "        await self.on_found_papers(found_references)\n",
    "\n",
    "    # async def get_paper_references(self, base: str, text: str) -> set[str]:\n",
    "    #     parser = UrlParser(base, self.filter_url)\n",
    "    #     parser.feed(text)\n",
    "    #     return parser.found_references\n",
    "\n",
    "    async def on_found_papers(self, papers: List[dict], initial: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Called when new papers are found. Filters out papers that have already been seen and puts the new ones in the queue.\n",
    "        \"\"\"\n",
    "        if initial:\n",
    "            for paper in papers:\n",
    "                await self.put_todo(paper)\n",
    "            return\n",
    "        ids = {paper[\"paperId\"] for paper in papers if paper[\"paperId\"] is not None}\n",
    "        new = ids - self.seen\n",
    "        self.seen.update(new)\n",
    "\n",
    "        for paper in papers:\n",
    "            if paper[\"paperId\"] in new:\n",
    "                await self.put_todo(paper)\n",
    "\n",
    "    async def put_todo(self, paper: dict) -> None:\n",
    "        \"\"\"Put a paper in the queue\"\"\"\n",
    "        # paper is a dict with fields like paper_id, title, abstract, etc.\n",
    "        if self.total >= self.max_papers:\n",
    "            return\n",
    "        self.total += 1\n",
    "        await self.todo.put(paper)\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    \"\"\"Main function\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    headers={\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"x-api-key\": S2_API_KEY,\n",
    "    }\n",
    "    mongodb_client = MongoDBClient(mongo_url='mongodb://localhost:27017', db_name='refpred', collection_name='review3_demo', init_new=True)\n",
    "    timeout = httpx.Timeout(10, connect=10, read=None, write=10)\n",
    "    # based on https://towardsdatascience.com/top-10-research-papers-in-ai-1f02cf844e26\n",
    "    initial_papers = [\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"bee044c8e8903fb67523c1f8c105ab4718600cdb\", \"36eff562f65125511b5dfab68ce7f7a943c27478\", \"8388f1be26329fa45e5807e968a641ce170ea078\", \"846aedd869a00c09b40f1f1f35673cb22bc87490\", \"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\", \"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\", \"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\", \"4d376d6978dad0374edfa6709c9556b42d3594d3\", \"a6cb366736791bcccc5c8639de5a8f9636bf87e8\", \"df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"913f54b44dfb9202955fe296cf5586e1105565ea\", \"156d217b0a911af97fa1b5a71dc909ccef7a8028\", \"a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031\", \"5c5751d45e298cea054f32b392c12c61027d2fe7\", \"bc1586a2e74d6d1cf87b083c4cbd1eede2b09ea5\", \"921b2958cac4138d188fd5047aa12bbcf37ac867\", \"cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1\"]\n",
    "    MAX_PAPERS = 10000\n",
    "    async with httpx.AsyncClient(timeout=timeout) as client:\n",
    "        # starting with the famous paper 'Attention is all you need'\n",
    "        crawler = Crawler(\n",
    "            client=client,\n",
    "            initial_papers=initial_papers,\n",
    "            num_workers=S2_RATE_LIMIT,\n",
    "            max_papers=MAX_PAPERS,\n",
    "            mongodb_client=mongodb_client,\n",
    "            headers=headers,\n",
    "        )\n",
    "        await crawler.run()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    logger.info(\"Results:\")\n",
    "    logger.info(f\"Crawled: {len(crawler.done)} Papers\")\n",
    "    logger.info(f\"Found: {len(crawler.seen)} Papers\")\n",
    "    logger.info(f\"Done in {end - start:.2f}s\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n",
    "# TODO\n",
    "# 1. Batch processing of seed papers\n",
    "# 2. Initialize seen from dataset to avoid restarting over\n",
    "# 3. Null abstract papers need to be removed from the dataset ✅'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_msg = '''# A simple hello world program in python with docstring\n",
    "def hello_world():\n",
    "    \"\"\"A simple hello world program in python with docstring\"\"\"\n",
    "    print(\"Hello World!\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 362, 4382, 24748, 1917, 2068, 304, 10344, 449, 4733]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see tokenized output of the above code\n",
    "enc.encode(simple_msg)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 317, 2829, 23748, 995, 1430, 287, 21015, 351, 2205]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see tokenized output of the above code\n",
    "enc.encode(simple_msg)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
