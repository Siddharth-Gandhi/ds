{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b190000-0x000000016b19c000).\n",
      "[0.006s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "import tiktoken\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = 'cl100k_base'\n",
    "encoding = 'p50k_base'\n",
    "enc = tiktoken.get_encoding(encoding)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return ' '.join(map(str,enc.encode(text, disallowed_special=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.4M\n",
      "   0 drwxr-xr-x 10 siddharth  320 Oct  5 22:02 ./\n",
      "   0 drwxr-xr-x 12 siddharth  384 Oct  5 22:04 ../\n",
      "8.0K -rw-r--r--  1 siddharth 6.1K Oct  5 22:02 .DS_Store\n",
      "   0 drwxr-xr-x  3 siddharth   96 Oct  5 22:02 jsonl/\n",
      "868K -rw-r--r--  1 siddharth 868K Oct  2 22:02 karpathy_llama2.c_commit_data_0.parquet\n",
      "616K -rw-r--r--  1 siddharth 615K Oct  2 22:02 karpathy_llama2.c_commit_data_1.parquet\n",
      "372K -rw-r--r--  1 siddharth 372K Oct  2 22:03 karpathy_llama2.c_commit_data_2.parquet\n",
      "284K -rw-r--r--  1 siddharth 283K Oct  2 22:03 karpathy_llama2.c_commit_data_3.parquet\n",
      "240K -rw-r--r--  1 siddharth 238K Oct  2 22:03 karpathy_llama2.c_commit_data_4.parquet\n",
      "   0 drwxr-xr-x 21 siddharth  672 Oct  3 00:52 searcher/\n"
     ]
    }
   ],
   "source": [
    "!ls -GFlash data/karpathy_llama2.c/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1774 entries, 0 to 1773\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  1774 non-null   string             \n",
      " 1   repo_name              1774 non-null   string             \n",
      " 2   commit_date            1774 non-null   datetime64[ns, UTC]\n",
      " 3   commit_id              1774 non-null   string             \n",
      " 4   commit_message         1774 non-null   string             \n",
      " 5   file_path              1774 non-null   string             \n",
      " 6   previous_commit_id     1774 non-null   string             \n",
      " 7   previous_file_content  1774 non-null   string             \n",
      " 8   cur_file_content       1774 non-null   string             \n",
      " 9   diff                   1626 non-null   string             \n",
      " 10  status                 1774 non-null   category           \n",
      " 11  is_merge_request       1774 non-null   bool               \n",
      " 12  file_extension         1774 non-null   category           \n",
      "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 144.7 KB\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet('data/ggerganov_llama.cpp/ggerganov_llama.cpp_commit_data_0.parquet').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9418 entries, 0 to 9417\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  9418 non-null   string             \n",
      " 1   repo_name              9418 non-null   string             \n",
      " 2   commit_date            9418 non-null   datetime64[ns, UTC]\n",
      " 3   commit_id              9418 non-null   string             \n",
      " 4   commit_message         9418 non-null   string             \n",
      " 5   file_path              9418 non-null   string             \n",
      " 6   previous_commit_id     9418 non-null   string             \n",
      " 7   previous_file_content  9418 non-null   string             \n",
      " 8   cur_file_content       9418 non-null   string             \n",
      " 9   diff                   8591 non-null   string             \n",
      " 10  status                 9418 non-null   category           \n",
      " 11  is_merge_request       9418 non-null   bool               \n",
      " 12  file_extension         9418 non-null   category           \n",
      "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 763.8 KB\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet('data/facebook_react/facebook_react_commit_data_0.parquet').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5948 entries, 0 to 5947\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  5948 non-null   string             \n",
      " 1   repo_name              5948 non-null   string             \n",
      " 2   commit_date            5948 non-null   datetime64[ns, UTC]\n",
      " 3   commit_id              5948 non-null   string             \n",
      " 4   commit_message         5948 non-null   string             \n",
      " 5   file_path              5948 non-null   string             \n",
      " 6   previous_commit_id     5948 non-null   string             \n",
      " 7   previous_file_content  5159 non-null   string             \n",
      " 8   cur_file_content       5860 non-null   string             \n",
      " 9   diff                   5072 non-null   string             \n",
      " 10  status                 5948 non-null   category           \n",
      " 11  is_merge_request       5948 non-null   bool               \n",
      "dtypes: bool(1), category(1), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 476.6 KB\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet('data/apache_kafka/apache_kafka_commit_data_0.parquet').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "# tempdf = pd.read_parquet('data/karpathy_llama2.c/karpathy_llama2.c_commit_data_0.parquet')\n",
    "tempdf = pd.read_parquet('data/apache_kafka/apache_kafka_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5948 entries, 0 to 5947\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  5948 non-null   string             \n",
      " 1   repo_name              5948 non-null   string             \n",
      " 2   commit_date            5948 non-null   datetime64[ns, UTC]\n",
      " 3   commit_id              5948 non-null   string             \n",
      " 4   commit_message         5948 non-null   string             \n",
      " 5   file_path              5948 non-null   string             \n",
      " 6   previous_commit_id     5948 non-null   string             \n",
      " 7   previous_file_content  5159 non-null   string             \n",
      " 8   cur_file_content       5860 non-null   string             \n",
      " 9   diff                   5072 non-null   string             \n",
      " 10  status                 5948 non-null   category           \n",
      " 11  is_merge_request       5948 non-null   bool               \n",
      "dtypes: bool(1), category(1), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 476.6 KB\n"
     ]
    }
   ],
   "source": [
    "tempdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/facebook_react/facebook_react_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get commit 7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "# df[df['commit_id'] == '7022e8d6a3222c97d287dfa0f2361acc8a30683a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2023-09-29 18:24:38-04:00\n",
       "Name: commit_date, dtype: datetime64[us, pytz.FixedOffset(-240)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (df.head(1)['commit_date'].astype('int64')/1e6).astype('int64')\n",
    "df.head(1)['commit_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73551 entries, 0 to 73550\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype                                 \n",
      "---  ------                 --------------  -----                                 \n",
      " 0   owner                  73551 non-null  string                                \n",
      " 1   repo_name              73551 non-null  string                                \n",
      " 2   commit_date            73551 non-null  datetime64[us, pytz.FixedOffset(-240)]\n",
      " 3   commit_id              73551 non-null  string                                \n",
      " 4   commit_message         73551 non-null  string                                \n",
      " 5   file_path              73551 non-null  string                                \n",
      " 6   previous_commit_id     73325 non-null  string                                \n",
      " 7   previous_file_content  60606 non-null  string                                \n",
      " 8   cur_file_content       67356 non-null  string                                \n",
      " 9   diff                   57351 non-null  string                                \n",
      " 10  status                 73551 non-null  category                              \n",
      " 11  is_merge_request       73551 non-null  bool                                  \n",
      "dtypes: bool(1), category(1), datetime64[us, pytz.FixedOffset(-240)](1), string(9)\n",
      "memory usage: 5.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2699.89 MB\n"
     ]
    }
   ],
   "source": [
    " # print just the memory usage in human readable format (MB) to 2 decimal places\n",
    "print(f'{df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique commits stored (others excluded for not being code commits): 11595\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique commits stored (others excluded for not being code commits):', df.commit_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = 'data/karpathy_llama2.c/'\n",
    "# REPO_LIST = ['karpathy_llama2.c', 'facebook_react', 'apache_kafka', 'ggerganov_llama.cpp', 'nodejs_node']\n",
    "REPO_LIST = ['karpathy_llama2.c', 'apache_kafka', 'ggerganov_llama.cpp', 'nodejs_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_LIST = ['karpathy_llama2.c']\n",
    "REPONAME = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_data_to_jsonl(data_dir, output_file):\n",
    "#     all_files = glob.glob(os.path.join(data_dir, '*.parquet'))\n",
    "#     all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "#     combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#     # replace NaN with empty string\n",
    "#     combined_df.fillna('', inplace=True)\n",
    "\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for index, row in combined_df.iterrows():\n",
    "#             doc = {\n",
    "#                 'id': row['commit_id'],\n",
    "#                 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "#                 # Optionally include source code\n",
    "#                 # 'source_code': row['cur_file_content']\n",
    "#             }\n",
    "#             f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_commits(repo_dir):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # number of unique commit_id columns\n",
    "    return combined_df.commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_commits = 0\n",
    "for repo in REPO_LIST:\n",
    "    total_commits += count_commits('data/' + repo + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of commits: 11595\n"
     ]
    }
   ],
   "source": [
    "print('Total number of commits:', total_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_df(repo_dir):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    combined_df['commit_date'] = (combined_df['commit_date'].astype('int64') / 1e9).astype('int64')\n",
    "    # replace NaN with empty string\n",
    "    # combined_df.fillna('', inplace=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_repo_to_jsonl(repo_dir, output_file, use_tokenizer=False):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    print(f\"Found {len(all_files)} parquet files\")\n",
    "    print(all_files)\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    # replace NaN with empty string in non-category columns\n",
    "    # combined_df.fillna('', inplace=True)\n",
    "\n",
    "    combined_df['commit_message'] = combined_df['commit_message'].fillna('')\n",
    "    combined_df['cur_file_content'] = combined_df['cur_file_content'].fillna('')\n",
    "    # convert commit_date to int64 (unix timestamp in milliseconds)\n",
    "    # 1e9 is very important AND it HAS to be in UTC\n",
    "    combined_df['commit_date'] = (combined_df['commit_date'].astype('int64') / 1e9).astype('int64')\n",
    "    # combined_df['commit_date'] = combined_df['commit_date'].astype(str)\n",
    "    # df['commit_date'] = df['commit_date'].astype(str)\n",
    "    # print(type(df['commit_date'][0]))\n",
    "    # print combined_df memory usage\n",
    "    # print(combined_df.info(memory_usage='deep'))\n",
    "    print(f'Combined Memory Usage: {combined_df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB for {len(combined_df)} rows')\n",
    "    with open(output_file, 'x') as f:\n",
    "        for index, row in combined_df.iterrows():\n",
    "            doc = {\n",
    "                'id': row['commit_id'],\n",
    "                'contents': row['commit_message'] if not use_tokenizer else tokenize(row['commit_message']),\n",
    "                # 'source_code': row['cur_file_content'],  # Optionally include source code\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']),\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']) if use_tokenizer else row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "                'repo_name': row['repo_name'],\n",
    "                'file_path': row['file_path'],\n",
    "                'commit_date': row['commit_date'],\n",
    "            }\n",
    "            f.write(json.dumps(doc) + '\\n')\n",
    "    print(f'Wrote {len(combined_df)} rows to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty data/jsonl if it has data\n",
    "# !rm -rf data/jsonl_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl_dir_name = 'jsonl_6'\n",
    "# for repo_name in REPO_LIST:\n",
    "#     repo_dir = os.path.join('data', repo_name)\n",
    "#     # create data/jsonl directory if it doesn't exist\n",
    "#     os.makedirs(os.path.join('data', jsonl_dir_name), exist_ok=True)\n",
    "\n",
    "#     # store in data/jsonl\n",
    "#     output_jsonl_file = os.path.join('data', jsonl_dir_name, f'{repo_name}.jsonl')\n",
    "#     convert_repo_to_jsonl(repo_dir, output_jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_LIST = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karpathy_llama2.c\n",
      "Found 5 parquet files\n",
      "['data/karpathy_llama2.c/karpathy_llama2.c_commit_data_3.parquet', 'data/karpathy_llama2.c/karpathy_llama2.c_commit_data_2.parquet', 'data/karpathy_llama2.c/karpathy_llama2.c_commit_data_0.parquet', 'data/karpathy_llama2.c/karpathy_llama2.c_commit_data_1.parquet', 'data/karpathy_llama2.c/karpathy_llama2.c_commit_data_4.parquet']\n",
      "Combined Memory Usage: 18.29 MB for 402 rows\n",
      "Wrote 402 rows to data/karpathy_llama2.c/jsonl/karpathy_llama2.c_commit_only_tk.jsonl\n",
      "apache_kafka\n",
      "Found 12 parquet files\n",
      "['data/apache_kafka/apache_kafka_commit_data_4.parquet', 'data/apache_kafka/apache_kafka_commit_data_5.parquet', 'data/apache_kafka/apache_kafka_commit_data_7.parquet', 'data/apache_kafka/apache_kafka_commit_data_6.parquet', 'data/apache_kafka/apache_kafka_commit_data_3.parquet', 'data/apache_kafka/apache_kafka_commit_data_2.parquet', 'data/apache_kafka/apache_kafka_commit_data_11.parquet', 'data/apache_kafka/apache_kafka_commit_data_9.parquet', 'data/apache_kafka/apache_kafka_commit_data_0.parquet', 'data/apache_kafka/apache_kafka_commit_data_1.parquet', 'data/apache_kafka/apache_kafka_commit_data_10.parquet', 'data/apache_kafka/apache_kafka_commit_data_8.parquet']\n",
      "Combined Memory Usage: 3645.70 MB for 75870 rows\n",
      "Wrote 75870 rows to data/apache_kafka/jsonl/apache_kafka_commit_only_tk.jsonl\n",
      "ggerganov_llama.cpp\n",
      "Found 2 parquet files\n",
      "['data/ggerganov_llama.cpp/ggerganov_llama.cpp_commit_data_1.parquet', 'data/ggerganov_llama.cpp/ggerganov_llama.cpp_commit_data_0.parquet']\n",
      "Combined Memory Usage: 604.98 MB for 2111 rows\n",
      "Wrote 2111 rows to data/ggerganov_llama.cpp/jsonl/ggerganov_llama.cpp_commit_only_tk.jsonl\n",
      "nodejs_node\n",
      "Found 19 parquet files\n",
      "['data/nodejs_node/nodejs_node_commit_data_6.parquet', 'data/nodejs_node/nodejs_node_commit_data_16.parquet', 'data/nodejs_node/nodejs_node_commit_data_17.parquet', 'data/nodejs_node/nodejs_node_commit_data_7.parquet', 'data/nodejs_node/nodejs_node_commit_data_15.parquet', 'data/nodejs_node/nodejs_node_commit_data_5.parquet', 'data/nodejs_node/nodejs_node_commit_data_4.parquet', 'data/nodejs_node/nodejs_node_commit_data_14.parquet', 'data/nodejs_node/nodejs_node_commit_data_1.parquet', 'data/nodejs_node/nodejs_node_commit_data_11.parquet', 'data/nodejs_node/nodejs_node_commit_data_8.parquet', 'data/nodejs_node/nodejs_node_commit_data_18.parquet', 'data/nodejs_node/nodejs_node_commit_data_9.parquet', 'data/nodejs_node/nodejs_node_commit_data_10.parquet', 'data/nodejs_node/nodejs_node_commit_data_0.parquet', 'data/nodejs_node/nodejs_node_commit_data_12.parquet', 'data/nodejs_node/nodejs_node_commit_data_2.parquet', 'data/nodejs_node/nodejs_node_commit_data_3.parquet', 'data/nodejs_node/nodejs_node_commit_data_13.parquet']\n",
      "Combined Memory Usage: 11010.96 MB for 208188 rows\n",
      "Wrote 208188 rows to data/nodejs_node/jsonl/nodejs_node_commit_only_tk.jsonl\n"
     ]
    }
   ],
   "source": [
    "# store in data/repo_dir/jsonl\n",
    "jsonl_dir_name = 'jsonl'\n",
    "for repo_name in REPO_LIST:\n",
    "    print(repo_name)\n",
    "    repo_dir = os.path.join('data', repo_name)\n",
    "    # create data/jsonl directory if it doesn't exist\n",
    "    os.makedirs(os.path.join(repo_dir, jsonl_dir_name), exist_ok=True)\n",
    "    output_name = f'{repo_name}_commit_only_tk.jsonl'\n",
    "    # store in data/jsonl\n",
    "    output_jsonl_file = os.path.join(repo_dir, jsonl_dir_name, output_name)\n",
    "    # if file exists, delete it\n",
    "    if os.path.exists(output_jsonl_file):\n",
    "        os.remove(output_jsonl_file)\n",
    "    convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=True)\n",
    "    # if not os.path.exists(output_jsonl_file):\n",
    "    #     convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=True)\n",
    "    # else:\n",
    "    #     print('File already exists:', output_jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# jsonl_file_path = f'{BASE_DIR}/jsonl/llama2.jsonl'\n",
    "# convert_data_to_jsonl(BASE_DIR, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of jsonl files which are present in data/repo_name/jsonl/repo_name.jsonl\n",
    "# jsonl_files = glob.glob('data/*/*/*.jsonl')\n",
    "# print(jsonl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normal untokenized\n",
    "- Parquet -> JSONL 22s\n",
    "- Index build 1m26s\n",
    "- 6 repos\n",
    "    Parquet -> JSONL 1m11s\n",
    "    Same mem usage as before, just lower time since no need for tokenization\n",
    "    Index Build 3m51s\n",
    "    Index Size 5Gb\n",
    "\n",
    "For tokenized\n",
    "- Parquet -> JSONL 8m3s\n",
    "- Index Build 2m12s\n",
    "- 6 repos:\n",
    "    Parquert -> JSONL 24m\n",
    "        - Combined Memory Usage: 18.29 MB for 402 rows data/isonl_6/karpathy_llama2.c.jsonl\n",
    "        - Combined Memory Usage: 0.94 MB for 108 rows data/json1_6/siddharth-gandhi_refpred.jsonl \\\\\n",
    "        - Combined Memory Usage: 2699.89 MB for 73551 rows data/jsonl_6/facebook_react.jsonl \\\\\n",
    "        - Combined Memory Usage: 3645.70 MB for 75870 rows data/jsonl_6/apache_kafka. jsonl \\\\\n",
    "        - Combined Memory Usage: 605.11 MB for 2111 rows data/jsonl_6/ggerganov_llama.cpp.jsonl \\\\\n",
    "        - Combined Memory Usage: 11010.96 MB for 208188 rows data/jsonl_6/nodejs_node.json\n",
    "        - 36731 total commits \n",
    "        - Total ~360K rows\n",
    "        - Interesting heuristic, on avg 10 files edited per commit?\n",
    "    Index build 6m42s\n",
    "    Index Size 10GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_LIST = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016bdb8000-0x000000016bdc4000).\n",
      "[0.008s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 03:28:20,406 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 03:28:20,408 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 03:28:20,408 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 03:28:20,409 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 03:28:20,410 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/karpathy_llama2.c/index_tk\n",
      "2023-10-10 03:28:20,413 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 03:28:20,491 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 03:28:20,491 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 03:28:20,492 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 03:28:20,492 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 03:28:20,625 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/karpathy_llama2.c_commit_only_tk.jsonl: 402 docs added.\n",
      "2023-10-10 03:28:20,695 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 402 documents indexed\n",
      "2023-10-10 03:28:20,695 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 03:28:20,695 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:              402\n",
      "2023-10-10 03:28:20,695 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 03:28:20,695 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 03:28:20,695 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 03:28:20,695 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 03:28:20,701 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 402 documents indexed in 00:00:00\n",
      "Processing karpathy_llama2.c\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000169488000-0x0000000169494000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 03:28:21,604 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 03:28:21,605 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 03:28:21,605 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 03:28:21,605 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/apache_kafka/jsonl\n",
      "2023-10-10 03:28:21,605 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 03:28:21,605 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 03:28:21,605 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 03:28:21,605 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 03:28:21,606 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 03:28:21,607 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 03:28:21,607 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/apache_kafka/index_tk\n",
      "2023-10-10 03:28:21,608 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 03:28:21,674 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 03:28:21,674 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/apache_kafka/jsonl\n",
      "2023-10-10 03:28:21,675 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 03:28:21,675 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 03:28:24,958 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/apache_kafka_commit_only_tk.jsonl: 75870 docs added.\n",
      "2023-10-10 03:28:25,651 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 75,870 documents indexed\n",
      "2023-10-10 03:28:25,652 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 03:28:25,652 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:           75,870\n",
      "2023-10-10 03:28:25,652 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 03:28:25,652 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 03:28:25,652 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 03:28:25,652 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 03:28:25,655 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 75,870 documents indexed in 00:00:04\n",
      "Processing apache_kafka\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b14c000-0x000000016b158000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 03:28:26,533 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/ggerganov_llama.cpp/jsonl\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 03:28:26,534 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 03:28:26,535 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/ggerganov_llama.cpp/index_tk\n",
      "2023-10-10 03:28:26,537 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 03:28:26,603 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 03:28:26,603 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/ggerganov_llama.cpp/jsonl\n",
      "2023-10-10 03:28:26,604 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 03:28:26,604 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 03:28:26,955 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/ggerganov_llama.cpp_commit_only_tk.jsonl: 2111 docs added.\n",
      "2023-10-10 03:28:27,059 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 2,111 documents indexed\n",
      "2023-10-10 03:28:27,060 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 03:28:27,060 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:            2,111\n",
      "2023-10-10 03:28:27,060 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 03:28:27,060 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 03:28:27,060 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 03:28:27,060 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 03:28:27,063 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 2,111 documents indexed in 00:00:00\n",
      "Processing ggerganov_llama.cpp\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b7ec000-0x000000016b7f8000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 03:28:27,943 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 03:28:27,943 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 03:28:27,943 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/nodejs_node/jsonl\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 03:28:27,944 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 03:28:27,945 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/nodejs_node/index_tk\n",
      "2023-10-10 03:28:27,947 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 03:28:28,012 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 03:28:28,013 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/nodejs_node/jsonl\n",
      "2023-10-10 03:28:28,014 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 03:28:28,014 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 03:28:34,698 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/nodejs_node_commit_only_tk.jsonl: 208188 docs added.\n",
      "2023-10-10 03:28:35,747 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 208,188 documents indexed\n",
      "2023-10-10 03:28:35,747 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 03:28:35,747 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:          208,188\n",
      "2023-10-10 03:28:35,747 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 03:28:35,747 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 03:28:35,747 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 03:28:35,747 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 03:28:35,750 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 208,188 documents indexed in 00:00:07\n",
      "Processing nodejs_node\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Specify the repository list here\n",
    "# REPO_LIST=(\"karpathy_llama2.c\" \"facebook_react\" \"apache_kafka\" \"ggerganov_llama.cpp\" \"nodejs_node\")\n",
    "REPO_LIST=(\"karpathy_llama2.c\" \"apache_kafka\" \"ggerganov_llama.cpp\" \"nodejs_node\")\n",
    "\n",
    "# Loop over each repo in the REPO_LIST array\n",
    "for repo in \"${REPO_LIST[@]}\"\n",
    "do\n",
    "    # Directory paths\n",
    "    repo_dir=\"data/$repo\"\n",
    "    index_dir=\"$repo_dir/index_tk\"\n",
    "    jsonl_dir_name=\"$repo_dir/jsonl\"\n",
    "\n",
    "    # Check if the index directory already exists\n",
    "    # if [ -d \"$index_dir\" ]; then\n",
    "    #     echo \"Index directory $index_dir already exists. Not doing $repo.\"\n",
    "    #     continue  # Skip to the next iteration of the loop\n",
    "    # fi\n",
    "\n",
    "    # remove all fiiles in the index directory\n",
    "    rm -rf \"$index_dir\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    mkdir -p \"$index_dir\"\n",
    "\n",
    "    # Build the index from data/jsonl\n",
    "    python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
    "     -threads 4 -input \"$jsonl_dir_name\" -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized\n",
    "\n",
    "    # Log the repo being processed\n",
    "    echo \"Processing $repo\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonl_dir_name: data/karpathy_llama2.c/jsonl\n",
      "index_dir: data/karpathy_llama2.c/index_tk\n",
      "total 100\n",
      "-rw-r--r-- 1 siddharth staff 101236 Oct 10 03:02 karpathy_llama2.c_commit_only_tk.jsonl\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000169900000-0x000000016990c000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 03:04:25,567 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 03:04:25,571 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/karpathy_llama2.c/index_tk\n",
      "2023-10-10 03:04:25,573 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 03:04:25,641 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 03:04:25,641 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 03:04:25,642 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 03:04:25,642 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 03:04:25,766 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/karpathy_llama2.c_commit_only_tk.jsonl: 402 docs added.\n",
      "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 402 documents indexed\n",
      "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:              402\n",
      "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 03:04:25,838 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 402 documents indexed in 00:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Directory to store the index\n",
    "# index_dir=\"./bm25_index_6/\"\n",
    "# jsonl_dir_name=\"jsonl_6\"\n",
    "repo_dir=\"data/karpathy_llama2.c\"\n",
    "index_dir=\"$repo_dir/index_tk\"\n",
    "# jsonl_dir_name=\"jsonl_tiktoken_6\"\n",
    "jsonl_dir_name=\"$repo_dir/jsonl\"\n",
    "# Create the directory if it doesn't exist\n",
    "mkdir -p \"$index_dir\"\n",
    "\n",
    "# Remove any existing indexes\n",
    "rm -rf \"$index_dir/*\"\n",
    "\n",
    "echo jsonl_dir_name: \"$jsonl_dir_name\"\n",
    "echo index_dir: \"$index_dir\"\n",
    "ls -l \"$jsonl_dir_name\"\n",
    "\n",
    "# build the index from data/jsonl\n",
    "python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
    " -threads 4 -input \"$jsonl_dir_name\" -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_dir=f\"data/{REPO_LIST[0]}\"\n",
    "# repo_dir=f\"data/karpathy_llama2.c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Refactors Resources to have a more compact and memory efficient struture.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692975245\n"
     ]
    }
   ],
   "source": [
    "# def convert_date_to_timestamp(date_str):\n",
    "#     date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "#     # Convert the datetime object to a UNIX timestamp\n",
    "#     # Method 1: Using timestamp() method\n",
    "#     unix_timestamp_1 = int(date_obj.timestamp())\n",
    "#     return unix_timestamp_1\n",
    "\n",
    "import datetime\n",
    "\n",
    "def convert_date_to_timestamp(date_str, precise_timestamp=True):\n",
    "    if precise_timestamp:\n",
    "        # Parse date string with time and timezone information\n",
    "        date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    else:\n",
    "        # Parse date string without time information\n",
    "        date_obj = datetime.datetime.strptime(date_str.split()[0], '%Y-%m-%d')\n",
    "\n",
    "    # Convert the datetime object to a UNIX timestamp\n",
    "    unix_timestamp = int(date_obj.timestamp())\n",
    "    return unix_timestamp\n",
    "\n",
    "# Example Usage:\n",
    "date_str = \"2023-08-25 14:54:05+00:00\"\n",
    "timestamp = convert_date_to_timestamp(date_str)\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebook/react/commit/7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls. I've encountered an issue in Firefox where multiple calls to chrome.panels.create result in the creation of duplicate panels. This seems to happen every time chrome.panels.create is called, even if a panel already exists. This leads to a cluttered interface with many duplicate panels. Ideally, chrome.panels.create should only create a new panel if there isn't one already existing. I believe a check should be implemented to ensure that chrome.panels.create is only called if no panels have been created yet to prevent this duplication issue.\"\n",
    "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls.\"\n",
    "query_date = \"2023-08-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '2023-08-31' does not match format '%Y-%m-%d %H:%M:%S%z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/siddharth/dev/ds/bm25.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m convert_date_to_timestamp(query_date, precise_timestamp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/siddharth/dev/ds/bm25.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_date_to_timestamp\u001b[39m(date_str, precise_timestamp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m precise_timestamp:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# Parse date string with time and timezone information\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         date_obj \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39;49mstrptime(date_str, \u001b[39m'\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mz\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m# Parse date string without time information\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         date_obj \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mstrptime(date_str\u001b[39m.\u001b[39msplit()[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.9/_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_strptime_datetime\u001b[39m(\u001b[39mcls\u001b[39m, data_string, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%a\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mb \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS \u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    566\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m    format string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m     tt, fraction, gmtoff_fraction \u001b[39m=\u001b[39m _strptime(data_string, \u001b[39mformat\u001b[39;49m)\n\u001b[1;32m    569\u001b[0m     tzname, gmtoff \u001b[39m=\u001b[39m tt[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[1;32m    570\u001b[0m     args \u001b[39m=\u001b[39m tt[:\u001b[39m6\u001b[39m] \u001b[39m+\u001b[39m (fraction,)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.9/_strptime.py:349\u001b[0m, in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    347\u001b[0m found \u001b[39m=\u001b[39m format_regex\u001b[39m.\u001b[39mmatch(data_string)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m found:\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtime data \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m does not match format \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    350\u001b[0m                      (data_string, \u001b[39mformat\u001b[39m))\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_string) \u001b[39m!=\u001b[39m found\u001b[39m.\u001b[39mend():\n\u001b[1;32m    352\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munconverted data remains: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    353\u001b[0m                       data_string[found\u001b[39m.\u001b[39mend():])\n",
      "\u001b[0;31mValueError\u001b[0m: time data '2023-08-31' does not match format '%Y-%m-%d %H:%M:%S%z'"
     ]
    }
   ],
   "source": [
    "convert_date_to_timestamp(query_date, precise_timestamp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tokenize(text):\n",
    "    text = json.loads(text)\n",
    "    # print(list(text['contents'].split(' ')))\n",
    "    text['contents'] = enc.decode([int(i) for i in text['contents'].split(' ')])\n",
    "    # return string\n",
    "    return json.dumps(text, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst = [f'{repo_dir}/index/', f'{repo_dir}/index_tk/', f'{repo_dir}/index_nf/', f'{repo_dir}/index_tk_nf/']\n",
    "# for i in lst:\n",
    "#     index_reader = IndexReader(i)\n",
    "#     search = LuceneSearcher(i)\n",
    "#     print(i)\n",
    "#     print(index_reader.stats())\n",
    "#     search_res = search.search(query, k=10) if 'tk' not in i else search.search(tokenize(query), k=10)\n",
    "#     if 'tk' in i:\n",
    "#         print(reverse_tokenize(search_res[0].raw))\n",
    "#     else:\n",
    "#         print(search_res[0].raw)\n",
    "#     print(f'Score: {search_res[0].score}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/facebook/react/commit/7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "# query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls.\"\n",
    "# query_date = \"2023-08-31\"\n",
    "\n",
    "\n",
    "# modified_query = \"I've encountered an issue in Firefox where multiple calls to chrome.panels.create result in the creation of duplicate panels. This seems to happen every time chrome.panels.create is called, even if a panel already exists. This leads to a cluttered interface with many duplicate panels. Ideally, chrome.panels.create should only create a new panel if there isn't one already existing. I believe a check should be implemented to ensure that chrome.panels.create is only called if no panels have been created yet to prevent this duplication issue.\"\n",
    "\n",
    "# actual_modified_files = ['packages/react-devtools-extensions/src/main/index.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('data/facebook_react/facebook_react_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebook/react/commit/d9e00f795b77676fb14f2a3c6f421f48f73bec2a\n",
    "query = \"Stop flowing and then abort if a stream is cancelled\"\n",
    "query_date = \"2023-09-22\"\n",
    "query_commit_id = 'd9e00f795b77676fb14f2a3c6f421f48f73bec2a'\n",
    "actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df to only include commits with commit_id d9e00f795b77676fb14f2a3c6f421f48f73bec2a & get the file_path column as a list to get actual_modified_files\n",
    "# df[df['commit_id'] == 'd9e00f795b77676fb14f2a3c6f421f48f73bec2a']\n",
    "# actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['packages/react-dom/src/__tests__/ReactDOMFizzServerBrowser-test.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerBrowser.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerBun.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerEdge.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerNode.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzStaticBrowser.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzStaticEdge.js',\n",
       " 'packages/react-server-dom-esm/src/ReactFlightDOMServerNode.js',\n",
       " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerBrowser.js',\n",
       " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerEdge.js',\n",
       " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerNode.js',\n",
       " 'packages/react-server-dom-webpack/src/__tests__/ReactFlightDOMBrowser-test.js',\n",
       " 'packages/react-server/src/ReactFizzServer.js',\n",
       " 'packages/react-server/src/ReactFlightServer.js']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_modified_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized with or without flag is the same, so let's just use with flag to avoid recomputing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = f\"data/facebook_react/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/nodejs_node/index_tk/\n"
     ]
    }
   ],
   "source": [
    "print(idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "karpathy_repo = get_combined_df('data/karpathy_llama2.c/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_q = 'adjust things a bit'\n",
    "# k_q_d = '2023-08-25'\n",
    "k_q_c = 'fbe324fc5ab61eaab3b8f74694be5b3870d3d5ee'\n",
    "k_q_d = karpathy_repo[karpathy_repo['commit_id'] == k_q_c]['commit_date'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1692975245"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_q_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>commit_date</th>\n",
       "      <th>commit_id</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>file_path</th>\n",
       "      <th>previous_commit_id</th>\n",
       "      <th>previous_file_content</th>\n",
       "      <th>cur_file_content</th>\n",
       "      <th>diff</th>\n",
       "      <th>status</th>\n",
       "      <th>is_merge_request</th>\n",
       "      <th>file_extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>karpathy</td>\n",
       "      <td>llama2.c</td>\n",
       "      <td>2023-08-25 14:54:05+00:00</td>\n",
       "      <td>fbe324fc5ab61eaab3b8f74694be5b3870d3d5ee</td>\n",
       "      <td>adjust things a bit</td>\n",
       "      <td>run.c</td>\n",
       "      <td>3d787b24635a7031f933ed42afc58e8117ee4504</td>\n",
       "      <td>/* Inference for Llama-2 Transformer model in ...</td>\n",
       "      <td>/* Inference for Llama-2 Transformer model in ...</td>\n",
       "      <td>@@ -800,16 +800,20 @@ void read_stdin(const ch...</td>\n",
       "      <td>modified</td>\n",
       "      <td>False</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        owner repo_name               commit_date  \\\n",
       "169  karpathy  llama2.c 2023-08-25 14:54:05+00:00   \n",
       "\n",
       "                                    commit_id         commit_message  \\\n",
       "169  fbe324fc5ab61eaab3b8f74694be5b3870d3d5ee  adjust things a bit\n",
       "\n",
       "   \n",
       "\n",
       "    file_path                         previous_commit_id  \\\n",
       "169     run.c  3d787b24635a7031f933ed42afc58e8117ee4504\n",
       "   \n",
       "\n",
       "                                 previous_file_content  \\\n",
       "169  /* Inference for Llama-2 Transformer model in ...   \n",
       "\n",
       "                                      cur_file_content  \\\n",
       "169  /* Inference for Llama-2 Transformer model in ...   \n",
       "\n",
       "                                                  diff    status  \\\n",
       "169  @@ -800,16 +800,20 @@ void read_stdin(const ch...  modified   \n",
       "\n",
       "     is_merge_request file_extension  \n",
       "169             False              c  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find commit id bf9a1162e1bd2e1d492bd637eb26fd1c04d22d79\n",
    "karpathy_repo[karpathy_repo['commit_id'] == 'fbe324fc5ab61eaab3b8f74694be5b3870d3d5ee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "1692975245 1692975245\n",
      " 2 c74456f3f084c73a2865f758e341f4cfe5b54a87 6.86510 llama2.c/run.c 1692555503\n",
      " 3 3868f732a43aed3290dc855fddea31f6d0e43ec1 4.34990 llama2.c/run.c 1692591782\n",
      " 4 d73b917d3ba7a1d933c65b1f33ecb58fd8d78a92 4.29890 llama2.c/run.c 1692670671\n",
      " 5 4c6f0af9ff3671b0b8053c6a3a512a06bad5c676 4.00620 llama2.c/tinystories.py 1691726302\n",
      " 6 4c6f0af9ff3671b0b8053c6a3a512a06bad5c676 4.00620 llama2.c/tokenizer.py 1691726302\n",
      " 7 4c6f0af9ff3671b0b8053c6a3a512a06bad5c676 4.00620 llama2.c/train_vocab.sh 1691726302\n",
      " 8 c42641205ffe17871af3464f35f51b201e58ebeb 3.81360 llama2.c/run.c 1691680985\n",
      " 9 ad7a1ef52547263debd8074ea30a4e3b550abe28 3.53140 llama2.c/run.c 1692671541\n",
      "10 b4bb47bb7baf0a5fb98a131d80b4e1a84ad72597 3.53140 llama2.c/run.c 1690517574\n",
      "11 b4bb47bb7baf0a5fb98a131d80b4e1a84ad72597 3.53140 llama2.c/tokenizer.py 1690517574\n",
      "12 dcef5ff7c720265752b7963c244c70846d6be252 3.53140 llama2.c/run.c 1691255291\n",
      "13 fd68dd222f8ea2b0a1fb75302a9fdba746bfca12 3.53140 llama2.c/run.c 1690522663\n",
      "14 2c0134f6699b7bc5d494f7dcc49a88fd2b3891d0 3.26790 llama2.c/run.c 1690805475\n",
      "15 00a61dc7f92a94069c0b03bc83c8bf30db1b4aa2 1.58040 llama2.c/tinyshakespeare.py 1691893110\n",
      "16 00a61dc7f92a94069c0b03bc83c8bf30db1b4aa2 1.58040 llama2.c/train.py 1691893110\n",
      "17 8b472ded1f354be093518bbc3c07283ea368167f 1.58040 llama2.c/model.py 1691897481\n",
      "18 8b472ded1f354be093518bbc3c07283ea368167f 1.58040 llama2.c/run.c 1691897481\n",
      "19 8b472ded1f354be093518bbc3c07283ea368167f 1.58040 llama2.c/sample.py 1691897481\n",
      "20 8b472ded1f354be093518bbc3c07283ea368167f 1.58039 llama2.c/tinyshakespeare.py 1691897481\n",
      "21 8b472ded1f354be093518bbc3c07283ea368167f 1.58039 llama2.c/tinystories.py 1691897481\n",
      "22 8b472ded1f354be093518bbc3c07283ea368167f 1.58039 llama2.c/tokenizer.py 1691897481\n",
      "23 8b472ded1f354be093518bbc3c07283ea368167f 1.58039 llama2.c/train.py 1691897481\n",
      "24 8b472ded1f354be093518bbc3c07283ea368167f 1.58039 llama2.c/train_vocab.sh 1691897481\n",
      "25 b0cfa2458d65747424fb4712f072680e2b3bc330 1.58039 llama2.c/model.py 1691772449\n",
      "26 b0cfa2458d65747424fb4712f072680e2b3bc330 1.58039 llama2.c/sample.py 1691772449\n",
      "27 b0cfa2458d65747424fb4712f072680e2b3bc330 1.58039 llama2.c/tinystories.py 1691772449\n",
      "28 b0cfa2458d65747424fb4712f072680e2b3bc330 1.58039 llama2.c/train.py 1691772449\n",
      "29 d04336cac88b93c91057e179ad46192600e50f1d 1.58039 llama2.c/run.c 1690522394\n",
      "30 f0024cfc885a1f5bac58200ee4aaf00caefcf0b4 1.58038 llama2.c/test_all.py 1691961764\n",
      "31 0e362f735f097a7c8306b67b56cc41c57ef9a091 1.29830 llama2.c/run.c 1692670956\n",
      "32 14275bd623df8ebb9ef8628df460db624b7940fa 1.29830 llama2.c/run.c 1692593004\n",
      "33 1bdf5af74316bcaeeff6390bc7a3b901dc6d66ba 1.29830 llama2.c/run.c 1690456448\n",
      "34 379f083b85740ccabe80b61259e7346ad27ede3d 1.29830 llama2.c/run.c 1692669411\n",
      "35 37e8c20f4fbfe471b33419b77f5ce0b2868f82ff 1.29830 llama2.c/run.c 1690435189\n",
      "36 45afa91dca8808f4d767d132210e7093c42f004c 1.29830 llama2.c/run.c 1691981667\n",
      "38 502f6816d61a5860c0d5ab98895d64ed6694b3c2 1.29829 llama2.c/export_meta_llama_bin.py 1690903514\n",
      "39 502f6816d61a5860c0d5ab98895d64ed6694b3c2 1.29829 llama2.c/model.py 1690903514\n",
      "40 530ef8e778382b23ba5472b6c796203fde7053cb 1.29829 llama2.c/export_meta_llama_bin.py 1690434525\n",
      "41 570789aa04e2c487c18778d71f16c33f1bf45d04 1.29829 llama2.c/tinystories.py 1691938150\n",
      "42 8a377a1d3110875ce3d6fdeda31a86489303b12a 1.29829 llama2.c/run.c 1692590112\n",
      "43 8c93c7a30e360164847843e7bafb6fbb6c40e9b7 1.29829 llama2.c/export.py 1692551312\n",
      "44 8c93c7a30e360164847843e7bafb6fbb6c40e9b7 1.29829 llama2.c/model.py 1692551312\n",
      "45 8c93c7a30e360164847843e7bafb6fbb6c40e9b7 1.29829 llama2.c/run.c 1692551312\n",
      "46 8c93c7a30e360164847843e7bafb6fbb6c40e9b7 1.29829 llama2.c/train.py 1692551312\n",
      "47 9414e7a45eb1cd492b1f158598dc06e60146fdba 1.29828 llama2.c/model.py 1690123928\n",
      "48 9414e7a45eb1cd492b1f158598dc06e60146fdba 1.29828 llama2.c/run.c 1690123928\n",
      "49 9414e7a45eb1cd492b1f158598dc06e60146fdba 1.29828 llama2.c/run_wrap.py 1690123928\n",
      "50 9414e7a45eb1cd492b1f158598dc06e60146fdba 1.29828 llama2.c/sample.py 1690123928\n",
      "51 9414e7a45eb1cd492b1f158598dc06e60146fdba 1.29828 llama2.c/test_all.py 1690123928\n",
      "52 9414e7a45eb1cd492b1f158598dc06e60146fdba 1.29828 llama2.c/train.py 1690123928\n",
      "53 98b515e44d23687258c08ec19e1e2458b57aa5ae 1.29828 llama2.c/model.py 1691358527\n",
      "54 f5650891d53e15605bb066a978c0182b12e91cd8 1.29828 llama2.c/model.py 1690329423\n"
     ]
    }
   ],
   "source": [
    "# idx_path = f'{repo_dir}/index_tk/'\n",
    "idx_path = 'data/karpathy_llama2.c/index_tk'\n",
    "bm25searcher = LuceneSearcher(idx_path)\n",
    "hits = bm25searcher.search(tokenize(k_q), k=1000)\n",
    "print(len(hits))\n",
    "# unix_date = convert_date_to_timestamp(k_q_d, precise_timestamp=False)\n",
    "unix_date = k_q_d\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    # print(obj)\n",
    "    commit_date = int(obj[\"commit_date\"])\n",
    "    if obj['id'] == 'fbe324fc5ab61eaab3b8f74694be5b3870d3d5ee':\n",
    "        print(commit_date, unix_date)\n",
    "\n",
    "    if commit_date >= unix_date:\n",
    "        continue\n",
    "    # if obj[\"file_path\"] in actual_modified_files:\n",
    "    #     print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to write a function to evaluate this behaviour. For now just focus on the perfomance of normal query (without modification)\n",
    "# the way we do this is by randomly sampling 1000 queries from df and then running the query on the index and then checking if the file is present in the actual_modified_files list. We want to store all hits and return IR metrics like MAP, MRR, P@10, P@100, P@1K, P@10K, NDCG@10, NDCG@100, NDCG@1K, NDCG@10K\n",
    "\n",
    "# write 2 functions, one for searching and one for evaluating\n",
    "\n",
    "def search(query, idx_path, query_date, k=1000, precise_timestamp=True):\n",
    "    bm25searcher = LuceneSearcher(idx_path)\n",
    "    hits = bm25searcher.search(tokenize(query), k)\n",
    "    # filter hits based on date\n",
    "    # unix_date = convert_date_to_timestamp(query_date, precise_timestamp=precise_timestamp)\n",
    "    unix_date = query_date\n",
    "    filtered_hits = []\n",
    "    for i in range(len(hits)):\n",
    "        obj = json.loads(hits[i].raw)\n",
    "        commit_date = int(obj[\"commit_date\"])\n",
    "        if commit_date >= unix_date:\n",
    "            continue\n",
    "        filtered_hits.append(hits[i])\n",
    "    return filtered_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "def evaluate(query, idx_path, query_date, actual_modified_files, k=1000):\n",
    "    hits = search(query, idx_path, query_date, k)\n",
    "\n",
    "    # Convert the hits to a list of filenames\n",
    "    retrieved_files = [json.loads(hit.raw)['file_path'] for hit in hits]\n",
    "\n",
    "    # Generate binary relevance judgments based on the actual_modified_files\n",
    "    relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "\n",
    "    if sum(relevant) == 0:\n",
    "        return {\n",
    "            'MAP': 0,\n",
    "            'P@10': 0,\n",
    "            'P@100': 0,\n",
    "            'P@1K': 0,\n",
    "            # 'P@10K': 0,\n",
    "            'MRR': 0,\n",
    "            'Recall@1K': 0\n",
    "            # 'NDCG@10': 0,\n",
    "            # 'NDCG@100': 0,\n",
    "            # 'NDCG@1K': 0,\n",
    "            # 'NDCG@10K': 0\n",
    "        }\n",
    "    # Calculate the metrics\n",
    "    MAP = average_precision_score(relevant, [1]*len(relevant))\n",
    "    unique_relevant_files = {\n",
    "        file for idx, file in enumerate(retrieved_files) if relevant[idx] == 1\n",
    "    }\n",
    "    recall = len(unique_relevant_files) / len(actual_modified_files)\n",
    "    # recall = sum(relevant) / len(actual_modified_files)\n",
    "    # also calculate MRR\n",
    "    MRR = mean_reciprocal_rank(relevant)\n",
    "    precision_values = [precision_at_k(relevant, k_val) for k_val in [10, 100, 1000]]\n",
    "\n",
    "    #todo NDCG calculations - no multi-label support as of now\n",
    "    # true_relevance = [[rel] for rel in relevant]\n",
    "    # scores = [[1] for _ in relevant]  # assuming all the retrieved files are equally relevant\n",
    "    # NDCG_values = [ndcg_score(true_relevance, scores, k=k_val) for k_val in [10, 100, 1000, 10000]]\n",
    "\n",
    "    metrics = {\n",
    "        'MAP': MAP,\n",
    "        'P@10': precision_values[0],\n",
    "        'P@100': precision_values[1],\n",
    "        'P@1K': precision_values[2],\n",
    "        # 'P@10K': precision_values[3],\n",
    "        'MRR': MRR,\n",
    "        'Recall@1K': recall\n",
    "        # 'NDCG@10': NDCG_values[0],\n",
    "        # 'NDCG@100': NDCG_values[1],\n",
    "        # 'NDCG@1K': NDCG_values[2],\n",
    "        # 'NDCG@10K': NDCG_values[3]\n",
    "    }\n",
    "    # round all the values to 4 decimal places\n",
    "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
    "    return metrics\n",
    "\n",
    "def precision_at_k(relevant, k):\n",
    "    return sum(relevant[:k]) / k\n",
    "\n",
    "def mean_reciprocal_rank(relevant):\n",
    "    for idx, value in enumerate(relevant):\n",
    "        if value == 1:\n",
    "            return 1 / (idx + 1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/facebook_react//index_tk_nf/'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0723,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.06,\n",
       " 'P@1K': 0.006,\n",
       " 'MRR': 0.0455,\n",
       " 'Recall@1K': 0.3571}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(query, idx_path, query_date, actual_modified_files, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0219, 'MRR': 0.121, 'P@10': 0.052, 'P@100': 0.0188, 'P@1K': 0.0068, 'Recall@1K': 0.5148}\n"
     ]
    }
   ],
   "source": [
    "# # Assuming df is your data frame\n",
    "# sampled_commits = df.drop_duplicates(subset='commit_id').sample(100, replace=False, random_state=42)\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for index, row in sampled_commits.iterrows():\n",
    "#     query = row['commit_message']\n",
    "#     query_date = row['commit_date'].strftime('%Y-%m-%d')\n",
    "#     query_commit_id = row['commit_id']\n",
    "#     actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()\n",
    "\n",
    "#     result = evaluate(query, idx_path, query_date, actual_modified_files)\n",
    "#     results.append(result)\n",
    "\n",
    "# # Compute average scores\n",
    "# avg_scores = {}\n",
    "# metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
    "# for metric in metrics:\n",
    "#     avg_scores[metric] = np.mean([result[metric] for result in results])\n",
    "\n",
    "# # round all the values to 4 decimal places\n",
    "# avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
    "# print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalize sampling across all repos by making a function which does it for each repo_name in REPO_LIST\n",
    "\n",
    "def evaluate_sampling(repo_dir, idx_path, n=100):\n",
    "    metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
    "    # all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    # all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    # combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    combined_df = get_combined_df(repo_dir)\n",
    "\n",
    "    print(f'Index path: {idx_path}')\n",
    "    total_commits = combined_df.commit_id.nunique()\n",
    "    print(f'Total commits: {total_commits}')\n",
    "    if total_commits < 100:\n",
    "        print(f'Not enough commits to sample for {repo_dir}, skipping...')\n",
    "        return {metric: 0 for metric in metrics}\n",
    "    # n = total_commits // 10 if total_commits > 10 else 1\n",
    "    print(f'Processing {repo_dir} with {n} samples')\n",
    "\n",
    "    sampled_commits = combined_df.drop_duplicates(subset='commit_id').sample(n, replace=False, random_state=42)\n",
    "    print(f'Number of commits sampled: {len(sampled_commits)}')\n",
    "    results = []\n",
    "    for index, row in sampled_commits.iterrows():\n",
    "        query = row['commit_message']\n",
    "        # query_date = row['commit_date'].strftime('%Y-%m-%d')\n",
    "        query_date = row['commit_date']\n",
    "        # query_date = row['commit_date'].strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "        query_commit_id = row['commit_id']\n",
    "        actual_modified_files = combined_df[combined_df['commit_id'] == query_commit_id]['file_path'].tolist()\n",
    "\n",
    "        result = evaluate(query, idx_path, query_date, actual_modified_files)\n",
    "        results.append(result)\n",
    "    avg_scores = {\n",
    "        metric: np.mean([result[metric] for result in results])\n",
    "        for metric in metrics\n",
    "    }\n",
    "    # round all the values to 4 decimal places\n",
    "    avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index path: data/apache_kafka/index_tk\n",
      "Total commits: 10438\n",
      "Processing data/apache_kafka/ with 100 samples\n",
      "Number of commits sampled: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.025,\n",
       " 'MRR': 0.2009,\n",
       " 'P@10': 0.065,\n",
       " 'P@100': 0.0315,\n",
       " 'P@1K': 0.0105,\n",
       " 'Recall@1K': 0.6829}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_sampling('data/apache_kafka/', 'data/apache_kafka/index_tk', n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index path: data/apache_kafka/index_tk\n",
    "Total commits: 10438\n",
    "Processing data/apache_kafka/ with 100 samples\n",
    "Number of commits sampled: 100\n",
    "{'MAP': 0.0284,\n",
    " 'MRR': 0.3324,\n",
    " 'P@10': 0.142,\n",
    " 'P@100': 0.0422,\n",
    " 'P@1K': 0.0116,\n",
    " 'Recall@1K': 0.7336}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_LIST = ['karpathy_llama2.c',\n",
    "#  'facebook_react',\n",
    " 'apache_kafka',\n",
    " 'ggerganov_llama.cpp',\n",
    " 'nodejs_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/karpathy_llama2.c/\n",
      "Index path: data/karpathy_llama2.c//index_tk/\n",
      "Total commits: 273\n",
      "Processing data/karpathy_llama2.c/ with 100 samples\n",
      "Number of commits sampled: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.2585, 'MRR': 0.6278, 'P@10': 0.335, 'P@100': 0.2065, 'P@1K': 0.0388, 'Recall@1K': 0.9717}\n",
      "\n",
      "Processing data/apache_kafka/\n",
      "Index path: data/apache_kafka//index_tk/\n",
      "Total commits: 10438\n",
      "Processing data/apache_kafka/ with 100 samples\n",
      "Number of commits sampled: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:10<00:11,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.025, 'MRR': 0.2009, 'P@10': 0.065, 'P@100': 0.0315, 'P@1K': 0.0105, 'Recall@1K': 0.6829}\n",
      "\n",
      "Processing data/ggerganov_llama.cpp/\n",
      "Index path: data/ggerganov_llama.cpp//index_tk/\n",
      "Total commits: 960\n",
      "Processing data/ggerganov_llama.cpp/ with 100 samples\n",
      "Number of commits sampled: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:12<00:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1405, 'MRR': 0.3986, 'P@10': 0.157, 'P@100': 0.1192, 'P@1K': 0.0466, 'Recall@1K': 0.9172}\n",
      "\n",
      "Processing data/nodejs_node/\n",
      "Index path: data/nodejs_node//index_tk/\n",
      "Total commits: 13446\n",
      "Processing data/nodejs_node/ with 100 samples\n",
      "Number of commits sampled: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0321, 'MRR': 0.2734, 'P@10': 0.087, 'P@100': 0.0423, 'P@1K': 0.0099, 'Recall@1K': 0.6001}\n",
      "\n",
      "Average scores for all repos:\n",
      " {'MAP': 0.114, 'MRR': 0.3752, 'P@10': 0.161, 'P@100': 0.0999, 'P@1K': 0.0265, 'Recall@1K': 0.793}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
    "res = []\n",
    "for repo_name in tqdm(REPO_LIST):\n",
    "    repo_dir = f'data/{repo_name}/'\n",
    "    idx_path = f'{repo_dir}/index_tk/'\n",
    "    print(f'Processing {repo_dir}')\n",
    "    avg_scores = evaluate_sampling(repo_dir, idx_path)\n",
    "    res.append(avg_scores)\n",
    "    print(avg_scores)\n",
    "    print()\n",
    "\n",
    "# avg scores for all repos\n",
    "avg_scores = {}\n",
    "for metric in metrics:\n",
    "    avg_scores[metric] = np.mean([result[metric] for result in res])\n",
    "# round all the values to 4 decimal places\n",
    "avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
    "print(f'Average scores for all repos:\\n {avg_scores}')\n",
    "# evaluate_sampling(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2.c\n",
    "# query = 'nInference for Llama-2 Transformer model in pure C'\n",
    "\n",
    "# refpred\n",
    "# query = 'if is_arxiv:\\n return f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper_id}/references?fields=title,\n",
    "# abstract,url,venue,publicationVenue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess'\n",
    "\n",
    "# react\n",
    "# query = \"export {default} from './npm/Circle';\"\n",
    "\n",
    "# kafka\n",
    "# public class MockKafkaLog4jAppender extends KafkaLog4jAppender {\n",
    "#     private MockProducer<byte[], byte[]> mockProducer =\n",
    "#             new MockProducer<>(false, new MockSerializer(), new MockSerializer());\n",
    "\n",
    "#     private Properties producerProperties;\n",
    "\n",
    "#     @Override\n",
    "#     protected Producer<byte[], byte[]> getKafkaProducer(Properties props) {\n",
    "#         producerProperties = props;\n",
    "#         return mockProducer;\n",
    "#     }\n",
    "\n",
    "#     void setKafkaProducer(MockProducer<byte[], byte[]> producer) {\n",
    "#         this.mockProducer = producer;\n",
    "#     }\n",
    "# \"\"\"\n",
    "\n",
    "# Kakfa\n",
    "# query = \"\"\"\n",
    "# /**\n",
    "#  * Local file based quorum state store. It takes the JSON format of {@link QuorumStateData}\n",
    "#  * with an extra data version number as part of the data for easy deserialization.\n",
    "#  *\n",
    "#  * Example format:\n",
    "#  * <pre>\n",
    "#  * {\"clusterId\":\"\",\n",
    "#  *   \"leaderId\":1,\n",
    "#  *   \"leaderEpoch\":2,\n",
    "#  *   \"votedId\":-1,\n",
    "#  *   \"appliedOffset\":0,\n",
    "#  *   \"currentVoters\":[],\n",
    "#  *   \"data_version\":0}\n",
    "#  * </pre>\n",
    "#  * */\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# kakfa\n",
    "query = \"\"\"Convert coordinator retriable errors to a known producer…\n",
    "… response error (#14378)\n",
    "\n",
    "KIP-890 Part 1 tries to address hanging transactions on old clients. Thus, the produce version can not be bumped and no new errors can be added. Before we used the java client's notion of retriable and abortable errors -- retriable errors are defined as such by extending the retriable error class, fatal errors are defined explicitly, and abortable errors are the remaining. However, many other clients treat non specified errors as fatal and that means many retriable errors kill the application.\"\"\"\n",
    "\n",
    "# kakfa\n",
    "# query = \"\"\"Fix flaky TopicAdminTest::retryEndOffsetsShouldRetryWhenTopicNotFound test case\"\"\"\n",
    "\n",
    "# nodejs\n",
    "# query = \"\"\"bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
    "#   DebugSealHandleScope scope(isolate);\n",
    "#   Environment* env = Environment::GetCurrent(isolate);\n",
    "#   return env != nullptr &&\n",
    "#          (env->is_main_thread() || !env->is_stopping()) &&\n",
    "#          env->abort_on_uncaught_exception() &&\n",
    "#          env->should_abort_on_uncaught_toggle()[0] &&\n",
    "#          !env->inside_should_not_abort_on_uncaught_scope();\n",
    "# }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 99.51060 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 82.57980 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 81.72260 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 5aecd2825644728f68a26558c957f5dfd4643423 81.36090 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 5 ef09a2e3fc11a738f6681fd57fb84ad109593fd3 80.57710 kafka/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala\n",
      " 6 f5d5f654db359af077088685e29fbe5ea69616cf 79.69870 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 2b6365c78b6e659f8df0651a24013d028f39edd9 79.64400 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 78.68580 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 1a10c3445e157da1d2fd670c043f19c385465eb0 78.48480 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 69d2a177101eb1c29b59b4c64d8c22f6d5e3d281 78.27240 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "bm25searcher = LuceneSearcher('bm25_index_6/')\n",
    "hits = bm25searcher.search(query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 696778,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 6840}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [00:02<00:00, 190.05it/s]\n"
     ]
    }
   ],
   "source": [
    "index_reader.dump_documents_BM25('tmp/idx_karpathy_double.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 578447,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 3034}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 141.63670 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 5aecd2825644728f68a26558c957f5dfd4643423 112.99820 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 111.59350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 111.57550 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 5 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 110.54000 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 6 ea0bb001262320bc9233221955a2be31c85993b9 109.68660 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 f5d5f654db359af077088685e29fbe5ea69616cf 109.62250 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 b937ec75677f8af13bf6fda686f07e9c62cdd20f 109.10350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 a81f35c1c8f9dc594aa585618c36f92ade0f86e2 109.03760 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 b49013b73efa25466652d8d8122974e60c927ec4 108.96060 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "tiktoken_searcher = LuceneSearcher('bm25_index_tiktoken_6/')\n",
    "# get tokenized query with enc.encode\n",
    "tokeninzed_query = tokenize(query)\n",
    "hits = tiktoken_searcher.search(tokeninzed_query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 2698903862,\n",
       " 'documents': 360230,\n",
       " 'non_empty_documents': 360230,\n",
       " 'unique_terms': -1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_index_reader = IndexReader('bm25_index_tiktoken_6/')\n",
    "tiktoken_index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the document source code inside the first hit raw\n",
    "content = json.loads(hits[0].raw)['contents']\n",
    "\n",
    "# print the document source code inside the first hit raw by decoding the tokenized string with enc.decode (convert to array of int and then decode)\n",
    "# print(enc.decode(json.loads(hits[0].raw)['contents']))\n",
    "\n",
    "# convert content to array of int\n",
    "content_arr = [int(i) for i in content.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: fix --abort-on-uncaught-exception handling\n",
      "\n",
      "The `set_abort_on_uncaught_exception(false)` line was supposed to\n",
      "prevent aborting when running Workers in\n",
      "`--abort-on-uncaught-exception` mode, but it was incorrectly set\n",
      "and not checked properly in the should-abort callback.\n",
      "\n",
      "PR-URL: https://github.com/nodejs/node/pull/34724\n",
      "Reviewed-By: Colin Ihrig <cjihrig@gmail.com>\n",
      "Reviewed-By: Richard Lau <riclau@uk.ibm.com>\n",
      "Reviewed-By: James M Snell <jasnell@gmail.com>\n",
      "Reviewed-By: Mary Marchini <oss@mmarchini.me>\n",
      "\n",
      "#include \"node.h\"\n",
      "#include \"node_context_data.h\"\n",
      "#include \"node_errors.h\"\n",
      "#include \"node_internals.h\"\n",
      "#include \"node_native_module_env.h\"\n",
      "#include \"node_platform.h\"\n",
      "#include \"node_v8_platform-inl.h\"\n",
      "#include \"uv.h\"\n",
      "\n",
      "#if HAVE_INSPECTOR\n",
      "#include \"inspector/worker_inspector.h\"  // ParentInspectorHandle\n",
      "#endif\n",
      "\n",
      "namespace node {\n",
      "using errors::TryCatchScope;\n",
      "using v8::Array;\n",
      "using v8::Context;\n",
      "using v8::EscapableHandleScope;\n",
      "using v8::Function;\n",
      "using v8::FunctionCallbackInfo;\n",
      "using v8::HandleScope;\n",
      "using v8::Isolate;\n",
      "using v8::Local;\n",
      "using v8::MaybeLocal;\n",
      "using v8::Null;\n",
      "using v8::Object;\n",
      "using v8::ObjectTemplate;\n",
      "using v8::Private;\n",
      "using v8::PropertyDescriptor;\n",
      "using v8::SealHandleScope;\n",
      "using v8::String;\n",
      "using v8::Value;\n",
      "\n",
      "static bool AllowWasmCodeGenerationCallback(Local<Context> context,\n",
      "                                            Local<String>) {\n",
      "  Local<Value> wasm_code_gen =\n",
      "      context->GetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration);\n",
      "  return wasm_code_gen->IsUndefined() || wasm_code_gen->IsTrue();\n",
      "}\n",
      "\n",
      "static bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
      "  DebugSealHandleScope scope(isolate);\n",
      "  Environment* env = Environment::GetCurrent(isolate);\n",
      "  return env != nullptr &&\n",
      "         (env->is_main_thread() || !env->is_stopping()) &&\n",
      "         env->abort_on_uncaught_exception() &&\n",
      "         env->should_abort_on_uncaught_toggle()[0] &&\n",
      "         !env->inside_should_not_abort_on_uncaught_scope();\n",
      "}\n",
      "\n",
      "static MaybeLocal<Value> PrepareStackTraceCallback(Local<Context> context,\n",
      "                                      Local<Value> exception,\n",
      "                                      Local<Array> trace) {\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Function> prepare = env->prepare_stack_trace_callback();\n",
      "  if (prepare.IsEmpty()) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Value> args[] = {\n",
      "      context->Global(),\n",
      "      exception,\n",
      "      trace,\n",
      "  };\n",
      "  // This TryCatch + Rethrow is required by V8 due to details around exception\n",
      "  // handling there. For C++ callbacks, V8 expects a scheduled exception (which\n",
      "  // is what ReThrow gives us). Just returning the empty MaybeLocal would leave\n",
      "  // us with a pending exception.\n",
      "  TryCatchScope try_catch(env);\n",
      "  MaybeLocal<Value> result = prepare->Call(\n",
      "      context, Undefined(env->isolate()), arraysize(args), args);\n",
      "  if (try_catch.HasCaught() && !try_catch.HasTerminated()) {\n",
      "    try_catch.ReThrow();\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Allocate(size_t size) {\n",
      "  void* ret;\n",
      "  if (zero_fill_field_ || per_process::cli_options->zero_fill_all_buffers)\n",
      "    ret = UncheckedCalloc(size);\n",
      "  else\n",
      "    ret = UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  void* ret = node::UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Reallocate(\n",
      "    void* data, size_t old_size, size_t size) {\n",
      "  void* ret = UncheckedRealloc<char>(static_cast<char*>(data), size);\n",
      "  if (LIKELY(ret != nullptr) || UNLIKELY(size == 0))\n",
      "    total_mem_usage_.fetch_add(size - old_size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void NodeArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  total_mem_usage_.fetch_sub(size, std::memory_order_relaxed);\n",
      "  free(data);\n",
      "}\n",
      "\n",
      "DebuggingArrayBufferAllocator::~DebuggingArrayBufferAllocator() {\n",
      "  CHECK(allocations_.empty());\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Allocate(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::Allocate(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::AllocateUninitialized(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "  NodeArrayBufferAllocator::Free(data, size);\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Reallocate(void* data,\n",
      "                                                size_t old_size,\n",
      "                                                size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* ret = NodeArrayBufferAllocator::Reallocate(data, old_size, size);\n",
      "  if (ret == nullptr) {\n",
      "    if (size == 0)  // i.e. equivalent to free().\n",
      "      UnregisterPointerInternal(data, old_size);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  if (data != nullptr) {\n",
      "    auto it = allocations_.find(data);\n",
      "    CHECK_NE(it, allocations_.end());\n",
      "    allocations_.erase(it);\n",
      "  }\n",
      "\n",
      "  RegisterPointerInternal(ret, size);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::RegisterPointer(data, size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::UnregisterPointer(data, size);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointerInternal(void* data,\n",
      "                                                              size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  auto it = allocations_.find(data);\n",
      "  CHECK_NE(it, allocations_.end());\n",
      "  if (size > 0) {\n",
      "    // We allow allocations with size 1 for 0-length buffers to avoid having\n",
      "    // to deal with nullptr values.\n",
      "    CHECK_EQ(it->second, size);\n",
      "  }\n",
      "  allocations_.erase(it);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointerInternal(void* data,\n",
      "                                                            size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  CHECK_EQ(allocations_.count(data), 0);\n",
      "  allocations_[data] = size;\n",
      "}\n",
      "\n",
      "std::unique_ptr<ArrayBufferAllocator> ArrayBufferAllocator::Create(bool debug) {\n",
      "  if (debug || per_process::cli_options->debug_arraybuffer_allocations)\n",
      "    return std::make_unique<DebuggingArrayBufferAllocator>();\n",
      "  else\n",
      "    return std::make_unique<NodeArrayBufferAllocator>();\n",
      "}\n",
      "\n",
      "ArrayBufferAllocator* CreateArrayBufferAllocator() {\n",
      "  return ArrayBufferAllocator::Create().release();\n",
      "}\n",
      "\n",
      "void FreeArrayBufferAllocator(ArrayBufferAllocator* allocator) {\n",
      "  delete allocator;\n",
      "}\n",
      "\n",
      "void SetIsolateCreateParamsForNode(Isolate::CreateParams* params) {\n",
      "  const uint64_t constrained_memory = uv_get_constrained_memory();\n",
      "  const uint64_t total_memory = constrained_memory > 0 ?\n",
      "      std::min(uv_get_total_memory(), constrained_memory) :\n",
      "      uv_get_total_memory();\n",
      "  if (total_memory > 0) {\n",
      "    // V8 defaults to 700MB or 1.4GB on 32 and 64 bit platforms respectively.\n",
      "    // This default is based on browser use-cases. Tell V8 to configure the\n",
      "    // heap based on the actual physical memory.\n",
      "    params->constraints.ConfigureDefaults(total_memory, 0);\n",
      "  }\n",
      "  params->embedder_wrapper_object_index = BaseObject::InternalFields::kSlot;\n",
      "  params->embedder_wrapper_type_index = std::numeric_limits<int>::max();\n",
      "}\n",
      "\n",
      "void SetIsolateErrorHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  if (s.flags & MESSAGE_LISTENER_WITH_ERROR_LEVEL)\n",
      "    isolate->AddMessageListenerWithErrorLevel(\n",
      "            errors::PerIsolateMessageListener,\n",
      "            Isolate::MessageErrorLevel::kMessageError |\n",
      "                Isolate::MessageErrorLevel::kMessageWarning);\n",
      "\n",
      "  auto* abort_callback = s.should_abort_on_uncaught_exception_callback ?\n",
      "      s.should_abort_on_uncaught_exception_callback :\n",
      "      ShouldAbortOnUncaughtException;\n",
      "  isolate->SetAbortOnUncaughtExceptionCallback(abort_callback);\n",
      "\n",
      "  auto* fatal_error_cb = s.fatal_error_callback ?\n",
      "      s.fatal_error_callback : OnFatalError;\n",
      "  isolate->SetFatalErrorHandler(fatal_error_cb);\n",
      "\n",
      "  auto* prepare_stack_trace_cb = s.prepare_stack_trace_callback ?\n",
      "      s.prepare_stack_trace_callback : PrepareStackTraceCallback;\n",
      "  isolate->SetPrepareStackTraceCallback(prepare_stack_trace_cb);\n",
      "}\n",
      "\n",
      "void SetIsolateMiscHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  isolate->SetMicrotasksPolicy(s.policy);\n",
      "\n",
      "  auto* allow_wasm_codegen_cb = s.allow_wasm_code_generation_callback ?\n",
      "    s.allow_wasm_code_generation_callback : AllowWasmCodeGenerationCallback;\n",
      "  isolate->SetAllowWasmCodeGenerationCallback(allow_wasm_codegen_cb);\n",
      "\n",
      "  if ((s.flags & SHOULD_NOT_SET_PROMISE_REJECTION_CALLBACK) == 0) {\n",
      "    auto* promise_reject_cb = s.promise_reject_callback ?\n",
      "      s.promise_reject_callback : task_queue::PromiseRejectCallback;\n",
      "    isolate->SetPromiseRejectCallback(promise_reject_cb);\n",
      "  }\n",
      "\n",
      "  if (s.flags & DETAILED_SOURCE_POSITIONS_FOR_PROFILING)\n",
      "    v8::CpuProfiler::UseDetailedSourcePositionsForProfiling(isolate);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate,\n",
      "                         const IsolateSettings& settings) {\n",
      "  SetIsolateErrorHandlers(isolate, settings);\n",
      "  SetIsolateMiscHandlers(isolate, settings);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate) {\n",
      "  IsolateSettings settings;\n",
      "  SetIsolateUpForNode(isolate, settings);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator, uv_loop_t* event_loop) {\n",
      "  return NewIsolate(allocator, event_loop, GetMainThreadMultiIsolatePlatform());\n",
      "}\n",
      "\n",
      "// TODO(joyeecheung): we may want to expose this, but then we need to be\n",
      "// careful about what we override in the params.\n",
      "Isolate* NewIsolate(Isolate::CreateParams* params,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate* isolate = Isolate::Allocate();\n",
      "  if (isolate == nullptr) return nullptr;\n",
      "\n",
      "  // Register the isolate on the platform before the isolate gets initialized,\n",
      "  // so that the isolate can access the platform during initialization.\n",
      "  platform->RegisterIsolate(isolate, event_loop);\n",
      "\n",
      "  SetIsolateCreateParamsForNode(params);\n",
      "  Isolate::Initialize(isolate, *params);\n",
      "  SetIsolateUpForNode(isolate);\n",
      "\n",
      "  return isolate;\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator != nullptr) params.array_buffer_allocator = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(std::shared_ptr<ArrayBufferAllocator> allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator) params.array_buffer_allocator_shared = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "IsolateData* CreateIsolateData(Isolate* isolate,\n",
      "                               uv_loop_t* loop,\n",
      "                               MultiIsolatePlatform* platform,\n",
      "                               ArrayBufferAllocator* allocator) {\n",
      "  return new IsolateData(isolate, loop, platform, allocator);\n",
      "}\n",
      "\n",
      "void FreeIsolateData(IsolateData* isolate_data) {\n",
      "  delete isolate_data;\n",
      "}\n",
      "\n",
      "InspectorParentHandle::~InspectorParentHandle() {}\n",
      "\n",
      "// Hide the internal handle class from the public API.\n",
      "#if HAVE_INSPECTOR\n",
      "struct InspectorParentHandleImpl : public InspectorParentHandle {\n",
      "  std::unique_ptr<inspector::ParentInspectorHandle> impl;\n",
      "\n",
      "  explicit InspectorParentHandleImpl(\n",
      "      std::unique_ptr<inspector::ParentInspectorHandle>&& impl)\n",
      "    : impl(std::move(impl)) {}\n",
      "};\n",
      "#endif\n",
      "\n",
      "Environment* CreateEnvironment(IsolateData* isolate_data,\n",
      "                               Local<Context> context,\n",
      "                               int argc,\n",
      "                               const char* const* argv,\n",
      "                               int exec_argc,\n",
      "                               const char* const* exec_argv) {\n",
      "  return CreateEnvironment(\n",
      "      isolate_data, context,\n",
      "      std::vector<std::string>(argv, argv + argc),\n",
      "      std::vector<std::string>(exec_argv, exec_argv + exec_argc));\n",
      "}\n",
      "\n",
      "Environment* CreateEnvironment(\n",
      "    IsolateData* isolate_data,\n",
      "    Local<Context> context,\n",
      "    const std::vector<std::string>& args,\n",
      "    const std::vector<std::string>& exec_args,\n",
      "    EnvironmentFlags::Flags flags,\n",
      "    ThreadId thread_id,\n",
      "    std::unique_ptr<InspectorParentHandle> inspector_parent_handle) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Context::Scope context_scope(context);\n",
      "  // TODO(addaleax): This is a much better place for parsing per-Environment\n",
      "  // options than the global parse call.\n",
      "  Environment* env = new Environment(\n",
      "      isolate_data, context, args, exec_args, nullptr, flags, thread_id);\n",
      "#if HAVE_INSPECTOR\n",
      "  if (inspector_parent_handle) {\n",
      "    env->InitializeInspector(\n",
      "        std::move(static_cast<InspectorParentHandleImpl*>(\n",
      "            inspector_parent_handle.get())->impl));\n",
      "  } else {\n",
      "    env->InitializeInspector({});\n",
      "  }\n",
      "#endif\n",
      "\n",
      "  if (env->RunBootstrapping().IsEmpty()) {\n",
      "    FreeEnvironment(env);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  return env;\n",
      "}\n",
      "\n",
      "void FreeEnvironment(Environment* env) {\n",
      "  Isolate::DisallowJavascriptExecutionScope disallow_js(env->isolate(),\n",
      "      Isolate::DisallowJavascriptExecutionScope::THROW_ON_FAILURE);\n",
      "  {\n",
      "    HandleScope handle_scope(env->isolate());  // For env->context().\n",
      "    Context::Scope context_scope(env->context());\n",
      "    SealHandleScope seal_handle_scope(env->isolate());\n",
      "\n",
      "    env->set_stopping(true);\n",
      "    env->stop_sub_worker_contexts();\n",
      "    env->RunCleanup();\n",
      "    RunAtExit(env);\n",
      "  }\n",
      "\n",
      "  // This call needs to be made while the `Environment` is still alive\n",
      "  // because we assume that it is available for async tracking in the\n",
      "  // NodePlatform implementation.\n",
      "  MultiIsolatePlatform* platform = env->isolate_data()->platform();\n",
      "  if (platform != nullptr)\n",
      "    platform->DrainTasks(env->isolate());\n",
      "\n",
      "  delete env;\n",
      "}\n",
      "\n",
      "NODE_EXTERN std::unique_ptr<InspectorParentHandle> GetInspectorParentHandle(\n",
      "    Environment* env,\n",
      "    ThreadId thread_id,\n",
      "    const char* url) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  CHECK_NE(thread_id.id, static_cast<uint64_t>(-1));\n",
      "#if HAVE_INSPECTOR\n",
      "  return std::make_unique<InspectorParentHandleImpl>(\n",
      "      env->inspector_agent()->GetParentHandle(thread_id.id, url));\n",
      "#else\n",
      "  return {};\n",
      "#endif\n",
      "}\n",
      "\n",
      "void LoadEnvironment(Environment* env) {\n",
      "  USE(LoadEnvironment(env,\n",
      "                      StartExecutionCallback{},\n",
      "                      {}));\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    StartExecutionCallback cb,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  env->InitializeLibuv();\n",
      "  env->InitializeDiagnostics();\n",
      "\n",
      "  return StartExecution(env, cb);\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    const char* main_script_source_utf8,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  CHECK_NOT_NULL(main_script_source_utf8);\n",
      "  return LoadEnvironment(\n",
      "      env,\n",
      "      [&](const StartExecutionCallbackInfo& info) -> MaybeLocal<Value> {\n",
      "        // This is a slightly hacky way to convert UTF-8 to UTF-16.\n",
      "        Local<String> str =\n",
      "            String::NewFromUtf8(env->isolate(),\n",
      "                                main_script_source_utf8).ToLocalChecked();\n",
      "        auto main_utf16 = std::make_unique<String::Value>(env->isolate(), str);\n",
      "\n",
      "        // TODO(addaleax): Avoid having a global table for all scripts.\n",
      "        std::string name = \"embedder_main_\" + std::to_string(env->thread_id());\n",
      "        native_module::NativeModuleEnv::Add(\n",
      "            name.c_str(),\n",
      "            UnionBytes(**main_utf16, main_utf16->length()));\n",
      "        env->set_main_utf16(std::move(main_utf16));\n",
      "        std::vector<Local<String>> params = {\n",
      "            env->process_string(),\n",
      "            env->require_string()};\n",
      "        std::vector<Local<Value>> args = {\n",
      "            env->process_object(),\n",
      "            env->native_module_require()};\n",
      "        return ExecuteBootstrapper(env, name.c_str(), &params, &args);\n",
      "      });\n",
      "}\n",
      "\n",
      "Environment* GetCurrentEnvironment(Local<Context> context) {\n",
      "  return Environment::GetCurrent(context);\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMainThreadMultiIsolatePlatform() {\n",
      "  return per_process::v8_platform.Platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(Environment* env) {\n",
      "  return GetMultiIsolatePlatform(env->isolate_data());\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(IsolateData* env) {\n",
      "  return env->platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    node::tracing::TracingController* tracing_controller) {\n",
      "  return CreatePlatform(\n",
      "      thread_pool_size,\n",
      "      static_cast<v8::TracingController*>(tracing_controller));\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return MultiIsolatePlatform::Create(thread_pool_size, tracing_controller)\n",
      "      .release();\n",
      "}\n",
      "\n",
      "void FreePlatform(MultiIsolatePlatform* platform) {\n",
      "  delete platform;\n",
      "}\n",
      "\n",
      "std::unique_ptr<MultiIsolatePlatform> MultiIsolatePlatform::Create(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return std::make_unique<NodePlatform>(thread_pool_size, tracing_controller);\n",
      "}\n",
      "\n",
      "MaybeLocal<Object> GetPerContextExports(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  EscapableHandleScope handle_scope(isolate);\n",
      "\n",
      "  Local<Object> global = context->Global();\n",
      "  Local<Private> key = Private::ForApi(isolate,\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"node:per_context_binding_exports\"));\n",
      "\n",
      "  Local<Value> existing_value;\n",
      "  if (!global->GetPrivate(context, key).ToLocal(&existing_value))\n",
      "    return MaybeLocal<Object>();\n",
      "  if (existing_value->IsObject())\n",
      "    return handle_scope.Escape(existing_value.As<Object>());\n",
      "\n",
      "  Local<Object> exports = Object::New(isolate);\n",
      "  if (context->Global()->SetPrivate(context, key, exports).IsNothing() ||\n",
      "      !InitializePrimordials(context))\n",
      "    return MaybeLocal<Object>();\n",
      "  return handle_scope.Escape(exports);\n",
      "}\n",
      "\n",
      "// Any initialization logic should be performed in\n",
      "// InitializeContext, because embedders don't necessarily\n",
      "// call NewContext and so they will experience breakages.\n",
      "Local<Context> NewContext(Isolate* isolate,\n",
      "                          Local<ObjectTemplate> object_template) {\n",
      "  auto context = Context::New(isolate, nullptr, object_template);\n",
      "  if (context.IsEmpty()) return context;\n",
      "\n",
      "  if (!InitializeContext(context)) {\n",
      "    return Local<Context>();\n",
      "  }\n",
      "\n",
      "  return context;\n",
      "}\n",
      "\n",
      "void ProtoThrower(const FunctionCallbackInfo<Value>& info) {\n",
      "  THROW_ERR_PROTO_ACCESS(info.GetIsolate());\n",
      "}\n",
      "\n",
      "// This runs at runtime, regardless of whether the context\n",
      "// is created from a snapshot.\n",
      "void InitializeContextRuntime(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  // Delete `Intl.v8BreakIterator`\n",
      "  // https://github.com/nodejs/node/issues/14909\n",
      "  Local<String> intl_string = FIXED_ONE_BYTE_STRING(isolate, \"Intl\");\n",
      "  Local<String> break_iter_string =\n",
      "    FIXED_ONE_BYTE_STRING(isolate, \"v8BreakIterator\");\n",
      "  Local<Value> intl_v;\n",
      "  if (context->Global()->Get(context, intl_string).ToLocal(&intl_v) &&\n",
      "      intl_v->IsObject()) {\n",
      "    Local<Object> intl = intl_v.As<Object>();\n",
      "    intl->Delete(context, break_iter_string).Check();\n",
      "  }\n",
      "\n",
      "  // Delete `Atomics.wake`\n",
      "  // https://github.com/nodejs/node/issues/21219\n",
      "  Local<String> atomics_string = FIXED_ONE_BYTE_STRING(isolate, \"Atomics\");\n",
      "  Local<String> wake_string = FIXED_ONE_BYTE_STRING(isolate, \"wake\");\n",
      "  Local<Value> atomics_v;\n",
      "  if (context->Global()->Get(context, atomics_string).ToLocal(&atomics_v) &&\n",
      "      atomics_v->IsObject()) {\n",
      "    Local<Object> atomics = atomics_v.As<Object>();\n",
      "    atomics->Delete(context, wake_string).Check();\n",
      "  }\n",
      "\n",
      "  // Remove __proto__\n",
      "  // https://github.com/nodejs/node/issues/31951\n",
      "  Local<String> object_string = FIXED_ONE_BYTE_STRING(isolate, \"Object\");\n",
      "  Local<String> prototype_string = FIXED_ONE_BYTE_STRING(isolate, \"prototype\");\n",
      "  Local<Object> prototype = context->Global()\n",
      "                                ->Get(context, object_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>()\n",
      "                                ->Get(context, prototype_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>();\n",
      "  Local<String> proto_string = FIXED_ONE_BYTE_STRING(isolate, \"__proto__\");\n",
      "  if (per_process::cli_options->disable_proto == \"delete\") {\n",
      "    prototype->Delete(context, proto_string).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto == \"throw\") {\n",
      "    Local<Value> thrower =\n",
      "        Function::New(context, ProtoThrower).ToLocalChecked();\n",
      "    PropertyDescriptor descriptor(thrower, thrower);\n",
      "    descriptor.set_enumerable(false);\n",
      "    descriptor.set_configurable(true);\n",
      "    prototype->DefineProperty(context, proto_string, descriptor).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto != \"\") {\n",
      "    // Validated in ProcessGlobalArgs\n",
      "    FatalError(\"InitializeContextRuntime()\", \"invalid --disable-proto mode\");\n",
      "  }\n",
      "}\n",
      "\n",
      "bool InitializeContextForSnapshot(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  context->SetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration,\n",
      "                           True(isolate));\n",
      "  return InitializePrimordials(context);\n",
      "}\n",
      "\n",
      "bool InitializePrimordials(Local<Context> context) {\n",
      "  // Run per-context JS files.\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  Context::Scope context_scope(context);\n",
      "  Local<Object> exports;\n",
      "\n",
      "  Local<String> primordials_string =\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"primordials\");\n",
      "  Local<String> global_string = FIXED_ONE_BYTE_STRING(isolate, \"global\");\n",
      "  Local<String> exports_string = FIXED_ONE_BYTE_STRING(isolate, \"exports\");\n",
      "\n",
      "  // Create primordials first and make it available to per-context scripts.\n",
      "  Local<Object> primordials = Object::New(isolate);\n",
      "  if (!primordials->SetPrototype(context, Null(isolate)).FromJust() ||\n",
      "      !GetPerContextExports(context).ToLocal(&exports) ||\n",
      "      !exports->Set(context, primordials_string, primordials).FromJust()) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  static const char* context_files[] = {\"internal/per_context/primordials\",\n",
      "                                        \"internal/per_context/domexception\",\n",
      "                                        \"internal/per_context/messageport\",\n",
      "                                        nullptr};\n",
      "\n",
      "  for (const char** module = context_files; *module != nullptr; module++) {\n",
      "    std::vector<Local<String>> parameters = {\n",
      "        global_string, exports_string, primordials_string};\n",
      "    Local<Value> arguments[] = {context->Global(), exports, primordials};\n",
      "    MaybeLocal<Function> maybe_fn =\n",
      "        native_module::NativeModuleEnv::LookupAndCompile(\n",
      "            context, *module, &parameters, nullptr);\n",
      "    Local<Function> fn;\n",
      "    if (!maybe_fn.ToLocal(&fn)) {\n",
      "      return false;\n",
      "    }\n",
      "    MaybeLocal<Value> result =\n",
      "        fn->Call(context, Undefined(isolate), arraysize(arguments), arguments);\n",
      "    // Execution failed during context creation.\n",
      "    // TODO(joyeecheung): deprecate this signature and return a MaybeLocal.\n",
      "    if (result.IsEmpty()) {\n",
      "      return false;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return true;\n",
      "}\n",
      "\n",
      "bool InitializeContext(Local<Context> context) {\n",
      "  if (!InitializeContextForSnapshot(context)) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  InitializeContextRuntime(context);\n",
      "  return true;\n",
      "}\n",
      "\n",
      "uv_loop_t* GetCurrentEventLoop(Isolate* isolate) {\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Local<Context> context = isolate->GetCurrentContext();\n",
      "  if (context.IsEmpty()) return nullptr;\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) return nullptr;\n",
      "  return env->event_loop();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env, const node_module& mod) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  Mutex::ScopedLock lock(env->extra_linked_bindings_mutex());\n",
      "\n",
      "  node_module* prev_head = env->extra_linked_bindings_head();\n",
      "  env->extra_linked_bindings()->push_back(mod);\n",
      "  if (prev_head != nullptr)\n",
      "    prev_head->nm_link = &env->extra_linked_bindings()->back();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env,\n",
      "                      const char* name,\n",
      "                      addon_context_register_func fn,\n",
      "                      void* priv) {\n",
      "  node_module mod = {\n",
      "    NODE_MODULE_VERSION,\n",
      "    NM_F_LINKED,\n",
      "    nullptr,  // nm_dso_handle\n",
      "    nullptr,  // nm_filename\n",
      "    nullptr,  // nm_register_func\n",
      "    fn,\n",
      "    name,\n",
      "    priv,\n",
      "    nullptr   // nm_link\n",
      "  };\n",
      "  AddLinkedBinding(env, mod);\n",
      "}\n",
      "\n",
      "static std::atomic<uint64_t> next_thread_id{0};\n",
      "\n",
      "ThreadId AllocateEnvironmentThreadId() {\n",
      "  return ThreadId { next_thread_id++ };\n",
      "}\n",
      "\n",
      "void DefaultProcessExitHandler(Environment* env, int exit_code) {\n",
      "  env->set_can_call_into_js(false);\n",
      "  env->stop_sub_worker_contexts();\n",
      "  DisposePlatform();\n",
      "  exit(exit_code);\n",
      "}\n",
      "\n",
      "\n",
      "void SetProcessExitHandler(Environment* env,\n",
      "                           std::function<void(Environment*, int)>&& handler) {\n",
      "  env->set_process_exit_handler(std::move(handler));\n",
      "}\n",
      "\n",
      "}  // namespace node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(content_arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
