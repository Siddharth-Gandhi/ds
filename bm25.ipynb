{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "import tiktoken\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = 'cl100k_base'\n",
    "encoding = 'p50k_base'\n",
    "enc = tiktoken.get_encoding(encoding)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return ' '.join(map(str,enc.encode(text, disallowed_special=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.4M\n",
      "   0 drwxr-xr-x 10 siddharth  320 Oct  5 22:02 ./\n",
      "   0 drwxr-xr-x 12 siddharth  384 Oct  5 22:04 ../\n",
      "8.0K -rw-r--r--  1 siddharth 6.1K Oct  5 22:02 .DS_Store\n",
      "   0 drwxr-xr-x  3 siddharth   96 Oct  5 22:02 jsonl/\n",
      "868K -rw-r--r--  1 siddharth 868K Oct  2 22:02 karpathy_llama2.c_commit_data_0.parquet\n",
      "616K -rw-r--r--  1 siddharth 615K Oct  2 22:02 karpathy_llama2.c_commit_data_1.parquet\n",
      "372K -rw-r--r--  1 siddharth 372K Oct  2 22:03 karpathy_llama2.c_commit_data_2.parquet\n",
      "284K -rw-r--r--  1 siddharth 283K Oct  2 22:03 karpathy_llama2.c_commit_data_3.parquet\n",
      "240K -rw-r--r--  1 siddharth 238K Oct  2 22:03 karpathy_llama2.c_commit_data_4.parquet\n",
      "   0 drwxr-xr-x 21 siddharth  672 Oct  3 00:52 searcher/\n"
     ]
    }
   ],
   "source": [
    "!ls -GFlash data/karpathy_llama2.c/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "# tempdf = pd.read_parquet('data/karpathy_llama2.c/karpathy_llama2.c_commit_data_0.parquet')\n",
    "tempdf = pd.read_parquet('data/apache_kafka/apache_kafka_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5948 entries, 0 to 5947\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  5948 non-null   string             \n",
      " 1   repo_name              5948 non-null   string             \n",
      " 2   commit_date            5948 non-null   datetime64[ns, UTC]\n",
      " 3   commit_id              5948 non-null   string             \n",
      " 4   commit_message         5948 non-null   string             \n",
      " 5   file_path              5948 non-null   string             \n",
      " 6   previous_commit_id     5948 non-null   string             \n",
      " 7   previous_file_content  5159 non-null   string             \n",
      " 8   cur_file_content       5860 non-null   string             \n",
      " 9   diff                   5072 non-null   string             \n",
      " 10  status                 5948 non-null   category           \n",
      " 11  is_merge_request       5948 non-null   bool               \n",
      "dtypes: bool(1), category(1), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 476.6 KB\n"
     ]
    }
   ],
   "source": [
    "tempdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/facebook_react/facebook_react_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get commit 7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "# df[df['commit_id'] == '7022e8d6a3222c97d287dfa0f2361acc8a30683a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2023-09-29 18:24:38-04:00\n",
       "Name: commit_date, dtype: datetime64[us, pytz.FixedOffset(-240)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (df.head(1)['commit_date'].astype('int64')/1e6).astype('int64')\n",
    "df.head(1)['commit_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73551 entries, 0 to 73550\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype                                 \n",
      "---  ------                 --------------  -----                                 \n",
      " 0   owner                  73551 non-null  string                                \n",
      " 1   repo_name              73551 non-null  string                                \n",
      " 2   commit_date            73551 non-null  datetime64[us, pytz.FixedOffset(-240)]\n",
      " 3   commit_id              73551 non-null  string                                \n",
      " 4   commit_message         73551 non-null  string                                \n",
      " 5   file_path              73551 non-null  string                                \n",
      " 6   previous_commit_id     73325 non-null  string                                \n",
      " 7   previous_file_content  60606 non-null  string                                \n",
      " 8   cur_file_content       67356 non-null  string                                \n",
      " 9   diff                   57351 non-null  string                                \n",
      " 10  status                 73551 non-null  category                              \n",
      " 11  is_merge_request       73551 non-null  bool                                  \n",
      "dtypes: bool(1), category(1), datetime64[us, pytz.FixedOffset(-240)](1), string(9)\n",
      "memory usage: 5.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2699.89 MB\n"
     ]
    }
   ],
   "source": [
    " # print just the memory usage in human readable format (MB) to 2 decimal places\n",
    "print(f'{df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique commits stored (others excluded for not being code commits): 11595\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique commits stored (others excluded for not being code commits):', df.commit_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = 'data/karpathy_llama2.c/'\n",
    "REPO_LIST = ['karpathy_llama2.c', 'facebook_react', 'apache_kafka', 'ggerganov_llama.cpp', 'nodejs_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_LIST = ['karpathy_llama2.c']\n",
    "REPONAME = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_data_to_jsonl(data_dir, output_file):\n",
    "#     all_files = glob.glob(os.path.join(data_dir, '*.parquet'))\n",
    "#     all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "#     combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#     # replace NaN with empty string\n",
    "#     combined_df.fillna('', inplace=True)\n",
    "\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for index, row in combined_df.iterrows():\n",
    "#             doc = {\n",
    "#                 'id': row['commit_id'],\n",
    "#                 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "#                 # Optionally include source code\n",
    "#                 # 'source_code': row['cur_file_content']\n",
    "#             }\n",
    "#             f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_commits(repo_dir):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # number of unique commit_id columns\n",
    "    return combined_df.commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_commits = 0\n",
    "for repo in REPO_LIST:\n",
    "    total_commits += count_commits('data/' + repo + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of commits: 11595\n"
     ]
    }
   ],
   "source": [
    "print('Total number of commits:', total_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_repo_to_jsonl(repo_dir, output_file, use_tokenizer=False):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    # replace NaN with empty string in non-category columns\n",
    "    # combined_df.fillna('', inplace=True)\n",
    "\n",
    "    combined_df['commit_message'] = combined_df['commit_message'].fillna('')\n",
    "    combined_df['cur_file_content'] = combined_df['cur_file_content'].fillna('')\n",
    "    # convert commit_date to int64 (unix timestamp in milliseconds)\n",
    "    combined_df['commit_date'] = (combined_df['commit_date'].astype('int64') / 1e6).astype('int64')\n",
    "    # df['commit_date'] = df['commit_date'].astype(str)\n",
    "    # print(type(df['commit_date'][0]))\n",
    "    # print combined_df memory usage\n",
    "    # print(combined_df.info(memory_usage='deep'))\n",
    "    print(f'Combined Memory Usage: {combined_df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB for {len(combined_df)} rows')\n",
    "    print(output_file)\n",
    "    with open(output_file, 'x') as f:\n",
    "        for index, row in combined_df.iterrows():\n",
    "            doc = {\n",
    "                'id': row['commit_id'],\n",
    "                'contents': row['commit_message'] if not use_tokenizer else tokenize(row['commit_message']),\n",
    "                # 'source_code': row['cur_file_content'],  # Optionally include source code\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']),\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']) if use_tokenizer else row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "                'repo_name': row['repo_name'],\n",
    "                'file_path': row['file_path'],\n",
    "                'commit_date': row['commit_date'],\n",
    "            }\n",
    "            f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty data/jsonl if it has data\n",
    "# !rm -rf data/jsonl_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl_dir_name = 'jsonl_6'\n",
    "# for repo_name in REPO_LIST:\n",
    "#     repo_dir = os.path.join('data', repo_name)\n",
    "#     # create data/jsonl directory if it doesn't exist\n",
    "#     os.makedirs(os.path.join('data', jsonl_dir_name), exist_ok=True)\n",
    "\n",
    "#     # store in data/jsonl\n",
    "#     output_jsonl_file = os.path.join('data', jsonl_dir_name, f'{repo_name}.jsonl')\n",
    "#     convert_repo_to_jsonl(repo_dir, output_jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_LIST = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karpathy_llama2.c\n",
      "Combined Memory Usage: 18.29 MB for 402 rows\n",
      "data/karpathy_llama2.c/jsonl/karpathy_llama2.c_commit_only_tk.jsonl\n",
      "facebook_react\n",
      "Combined Memory Usage: 2699.89 MB for 73551 rows\n",
      "data/facebook_react/jsonl/facebook_react_commit_only_tk.jsonl\n",
      "apache_kafka\n",
      "Combined Memory Usage: 3645.70 MB for 75870 rows\n",
      "data/apache_kafka/jsonl/apache_kafka_commit_only_tk.jsonl\n",
      "ggerganov_llama.cpp\n",
      "Combined Memory Usage: 604.98 MB for 2111 rows\n",
      "data/ggerganov_llama.cpp/jsonl/ggerganov_llama.cpp_commit_only_tk.jsonl\n",
      "nodejs_node\n",
      "Combined Memory Usage: 11010.96 MB for 208188 rows\n",
      "data/nodejs_node/jsonl/nodejs_node_commit_only_tk.jsonl\n"
     ]
    }
   ],
   "source": [
    "# store in data/repo_dir/jsonl\n",
    "jsonl_dir_name = 'jsonl'\n",
    "for repo_name in REPO_LIST:\n",
    "    print(repo_name)\n",
    "    repo_dir = os.path.join('data', repo_name)\n",
    "    # create data/jsonl directory if it doesn't exist\n",
    "    os.makedirs(os.path.join(repo_dir, jsonl_dir_name), exist_ok=True)\n",
    "    output_name = f'{repo_name}_commit_only_tk.jsonl'\n",
    "    # store in data/jsonl\n",
    "    output_jsonl_file = os.path.join(repo_dir, jsonl_dir_name, output_name)\n",
    "    # if file exists, delete it\n",
    "    if os.path.exists(output_jsonl_file):\n",
    "        os.remove(output_jsonl_file)\n",
    "    convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=True)\n",
    "    # if not os.path.exists(output_jsonl_file):\n",
    "    #     convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=True)\n",
    "    # else:\n",
    "    #     print('File already exists:', output_jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# jsonl_file_path = f'{BASE_DIR}/jsonl/llama2.jsonl'\n",
    "# convert_data_to_jsonl(BASE_DIR, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of jsonl files which are present in data/repo_name/jsonl/repo_name.jsonl\n",
    "# jsonl_files = glob.glob('data/*/*/*.jsonl')\n",
    "# print(jsonl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normal untokenized\n",
    "- Parquet -> JSONL 22s\n",
    "- Index build 1m26s\n",
    "- 6 repos\n",
    "    Parquet -> JSONL 1m11s\n",
    "    Same mem usage as before, just lower time since no need for tokenization\n",
    "    Index Build 3m51s\n",
    "    Index Size 5Gb\n",
    "\n",
    "For tokenized\n",
    "- Parquet -> JSONL 8m3s\n",
    "- Index Build 2m12s\n",
    "- 6 repos:\n",
    "    Parquert -> JSONL 24m\n",
    "        - Combined Memory Usage: 18.29 MB for 402 rows data/isonl_6/karpathy_llama2.c.jsonl\n",
    "        - Combined Memory Usage: 0.94 MB for 108 rows data/json1_6/siddharth-gandhi_refpred.jsonl \\\\\n",
    "        - Combined Memory Usage: 2699.89 MB for 73551 rows data/jsonl_6/facebook_react.jsonl \\\\\n",
    "        - Combined Memory Usage: 3645.70 MB for 75870 rows data/jsonl_6/apache_kafka. jsonl \\\\\n",
    "        - Combined Memory Usage: 605.11 MB for 2111 rows data/jsonl_6/ggerganov_llama.cpp.jsonl \\\\\n",
    "        - Combined Memory Usage: 11010.96 MB for 208188 rows data/jsonl_6/nodejs_node.json\n",
    "        - 36731 total commits \n",
    "        - Total ~360K rows\n",
    "        - Interesting heuristic, on avg 10 files edited per commit?\n",
    "    Index build 6m42s\n",
    "    Index Size 10GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPO_LIST = ['facebook_react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016d8c4000-0x000000016d8d0000).\n",
      "[0.004s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 02:45:48,832 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 02:45:48,833 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 02:45:48,833 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 02:45:48,833 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 02:45:48,833 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 02:45:48,833 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 02:45:48,834 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 02:45:48,835 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 02:45:48,835 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 02:45:48,835 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 02:45:48,835 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/karpathy_llama2.c/index_tk\n",
      "2023-10-10 02:45:48,836 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 02:45:48,900 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 02:45:48,900 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 02:45:48,901 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 02:45:48,901 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 02:45:49,028 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/karpathy_llama2.c_commit_only_tk.jsonl: 402 docs added.\n",
      "2023-10-10 02:45:49,095 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 402 documents indexed\n",
      "2023-10-10 02:45:49,095 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 02:45:49,095 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:              402\n",
      "2023-10-10 02:45:49,095 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 02:45:49,095 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 02:45:49,096 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 02:45:49,096 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 02:45:49,098 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 402 documents indexed in 00:00:00\n",
      "Processing karpathy_llama2.c\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000168f50000-0x0000000168f5c000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 02:45:49,983 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 02:45:49,984 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 02:45:49,984 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 02:45:49,984 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/facebook_react/jsonl\n",
      "2023-10-10 02:45:49,985 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 02:45:49,985 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 02:45:49,985 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 02:45:49,985 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 02:45:49,985 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 02:45:49,985 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 02:45:49,985 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 02:45:49,986 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/facebook_react/index_tk\n",
      "2023-10-10 02:45:49,988 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 02:45:50,055 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 02:45:50,056 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/facebook_react/jsonl\n",
      "2023-10-10 02:45:50,057 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 02:45:50,057 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 02:45:52,728 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/facebook_react_commit_only_tk.jsonl: 73551 docs added.\n",
      "2023-10-10 02:45:53,230 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 73,551 documents indexed\n",
      "2023-10-10 02:45:53,230 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 02:45:53,230 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:           73,551\n",
      "2023-10-10 02:45:53,230 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 02:45:53,230 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 02:45:53,230 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 02:45:53,230 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 02:45:53,233 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 73,551 documents indexed in 00:00:03\n",
      "Processing facebook_react\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000169734000-0x0000000169740000).\n",
      "[0.004s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 02:45:54,141 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/apache_kafka/jsonl\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 02:45:54,143 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 02:45:54,144 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 02:45:54,145 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/apache_kafka/index_tk\n",
      "2023-10-10 02:45:54,147 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 02:45:54,213 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 02:45:54,213 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/apache_kafka/jsonl\n",
      "2023-10-10 02:45:54,214 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 02:45:54,214 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 02:45:57,488 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/apache_kafka_commit_only_tk.jsonl: 75870 docs added.\n",
      "2023-10-10 02:45:58,011 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 75,870 documents indexed\n",
      "2023-10-10 02:45:58,011 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 02:45:58,011 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:           75,870\n",
      "2023-10-10 02:45:58,011 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 02:45:58,011 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 02:45:58,011 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 02:45:58,011 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 02:45:58,014 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 75,870 documents indexed in 00:00:03\n",
      "Processing apache_kafka\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b888000-0x000000016b894000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 02:45:58,896 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 02:45:58,896 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 02:45:58,896 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/ggerganov_llama.cpp/jsonl\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 02:45:58,897 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 02:45:58,898 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/ggerganov_llama.cpp/index_tk\n",
      "2023-10-10 02:45:58,900 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 02:45:58,968 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 02:45:58,968 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/ggerganov_llama.cpp/jsonl\n",
      "2023-10-10 02:45:58,969 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 02:45:58,969 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 02:45:59,361 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/ggerganov_llama.cpp_commit_only_tk.jsonl: 2111 docs added.\n",
      "2023-10-10 02:45:59,477 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 2,111 documents indexed\n",
      "2023-10-10 02:45:59,477 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 02:45:59,477 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:            2,111\n",
      "2023-10-10 02:45:59,478 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 02:45:59,478 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 02:45:59,478 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 02:45:59,478 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 02:45:59,481 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 2,111 documents indexed in 00:00:00\n",
      "Processing ggerganov_llama.cpp\n",
      "[0.004s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b69c000-0x000000016b6a8000).\n",
      "[0.004s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 02:46:00,375 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 02:46:00,375 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/nodejs_node/jsonl\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 02:46:00,376 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 02:46:00,377 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 02:46:00,378 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/nodejs_node/index_tk\n",
      "2023-10-10 02:46:00,379 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 02:46:00,442 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 02:46:00,442 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/nodejs_node/jsonl\n",
      "2023-10-10 02:46:00,443 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 02:46:00,443 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 02:46:07,289 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/nodejs_node_commit_only_tk.jsonl: 208188 docs added.\n",
      "2023-10-10 02:46:08,259 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 208,188 documents indexed\n",
      "2023-10-10 02:46:08,259 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 02:46:08,260 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:          208,188\n",
      "2023-10-10 02:46:08,260 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 02:46:08,260 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 02:46:08,260 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 02:46:08,260 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 02:46:08,263 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 208,188 documents indexed in 00:00:07\n",
      "Processing nodejs_node\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Specify the repository list here\n",
    "REPO_LIST=(\"karpathy_llama2.c\" \"facebook_react\" \"apache_kafka\" \"ggerganov_llama.cpp\" \"nodejs_node\")\n",
    "\n",
    "# Loop over each repo in the REPO_LIST array\n",
    "for repo in \"${REPO_LIST[@]}\"\n",
    "do\n",
    "    # Directory paths\n",
    "    repo_dir=\"data/$repo\"\n",
    "    index_dir=\"$repo_dir/index_tk\"\n",
    "    jsonl_dir_name=\"$repo_dir/jsonl\"\n",
    "\n",
    "    # Check if the index directory already exists\n",
    "    # if [ -d \"$index_dir\" ]; then\n",
    "    #     echo \"Index directory $index_dir already exists. Not doing $repo.\"\n",
    "    #     continue  # Skip to the next iteration of the loop\n",
    "    # fi\n",
    "\n",
    "    # remove all fiiles in the index directory\n",
    "    rm -rf \"$index_dir\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    mkdir -p \"$index_dir\"\n",
    "\n",
    "    # Build the index from data/jsonl\n",
    "    python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
    "     -threads 4 -input \"$jsonl_dir_name\" -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized\n",
    "\n",
    "    # Log the repo being processed\n",
    "    echo \"Processing $repo\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonl_dir_name: data/karpathy_llama2.c/jsonl\n",
      "total 100\n",
      "-rw-r--r-- 1 siddharth staff 101236 Oct 10 02:18 karpathy_llama2.c_commit_only_tk.jsonl\n",
      "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016b2ac000-0x000000016b2b8000).\n",
      "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-10 02:44:47,203 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-10 02:44:47,204 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-10 02:44:47,204 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-10 02:44:47,204 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 02:44:47,204 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-10 02:44:47,204 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-10 02:44:47,204 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-10 02:44:47,204 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-10 02:44:47,205 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-10 02:44:47,206 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-10 02:44:47,206 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/karpathy_llama2.c/index_tk\n",
      "2023-10-10 02:44:47,207 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-10 02:44:47,281 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-10 02:44:47,281 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/karpathy_llama2.c/jsonl\n",
      "2023-10-10 02:44:47,282 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-10 02:44:47,282 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-10 02:44:47,406 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/karpathy_llama2.c_commit_only_tk.jsonl: 402 docs added.\n",
      "2023-10-10 02:44:47,475 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 402 documents indexed\n",
      "2023-10-10 02:44:47,475 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-10 02:44:47,475 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:              402\n",
      "2023-10-10 02:44:47,475 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-10 02:44:47,475 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-10 02:44:47,475 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-10 02:44:47,475 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-10 02:44:47,477 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 402 documents indexed in 00:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Directory to store the index\n",
    "# index_dir=\"./bm25_index_6/\"\n",
    "# jsonl_dir_name=\"jsonl_6\"\n",
    "repo_dir=\"data/karpathy_llama2.c\"\n",
    "index_dir=\"$repo_dir/index_tk\"\n",
    "# jsonl_dir_name=\"jsonl_tiktoken_6\"\n",
    "jsonl_dir_name=\"$repo_dir/jsonl\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "mkdir -p \"$index_dir\"\n",
    "\n",
    "# Remove any existing indexes\n",
    "rm -rf \"$index_dir/*\"\n",
    "\n",
    "echo jsonl_dir_name: \"$jsonl_dir_name\"\n",
    "ls -l \"$jsonl_dir_name\"\n",
    "\n",
    "# build the index from data/jsonl\n",
    "python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
    " -threads 4 -input \"$jsonl_dir_name\" -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_dir=f\"data/{REPO_LIST[0]}\"\n",
    "# repo_dir=f\"data/karpathy_llama2.c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Refactors Resources to have a more compact and memory efficient struture.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_to_timestamp(date_str):\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "    # Convert the datetime object to a UNIX timestamp\n",
    "    # Method 1: Using timestamp() method\n",
    "    unix_timestamp_1 = int(date_obj.timestamp())\n",
    "    return unix_timestamp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebook/react/commit/7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls. I've encountered an issue in Firefox where multiple calls to chrome.panels.create result in the creation of duplicate panels. This seems to happen every time chrome.panels.create is called, even if a panel already exists. This leads to a cluttered interface with many duplicate panels. Ideally, chrome.panels.create should only create a new panel if there isn't one already existing. I believe a check should be implemented to ensure that chrome.panels.create is only called if no panels have been created yet to prevent this duplication issue.\"\n",
    "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls.\"\n",
    "query_date = \"2023-08-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1693454400"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_to_timestamp(query_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tokenize(text):\n",
    "    text = json.loads(text)\n",
    "    # print(list(text['contents'].split(' ')))\n",
    "    text['contents'] = enc.decode([int(i) for i in text['contents'].split(' ')])\n",
    "    # return string\n",
    "    return json.dumps(text, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst = [f'{repo_dir}/index/', f'{repo_dir}/index_tk/', f'{repo_dir}/index_nf/', f'{repo_dir}/index_tk_nf/']\n",
    "# for i in lst:\n",
    "#     index_reader = IndexReader(i)\n",
    "#     search = LuceneSearcher(i)\n",
    "#     print(i)\n",
    "#     print(index_reader.stats())\n",
    "#     search_res = search.search(query, k=10) if 'tk' not in i else search.search(tokenize(query), k=10)\n",
    "#     if 'tk' in i:\n",
    "#         print(reverse_tokenize(search_res[0].raw))\n",
    "#     else:\n",
    "#         print(search_res[0].raw)\n",
    "#     print(f'Score: {search_res[0].score}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebook/react/commit/7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
    "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls.\"\n",
    "query_date = \"2023-08-31\"\n",
    "\n",
    "\n",
    "modified_query = \"I've encountered an issue in Firefox where multiple calls to chrome.panels.create result in the creation of duplicate panels. This seems to happen every time chrome.panels.create is called, even if a panel already exists. This leads to a cluttered interface with many duplicate panels. Ideally, chrome.panels.create should only create a new panel if there isn't one already existing. I believe a check should be implemented to ensure that chrome.panels.create is only called if no panels have been created yet to prevent this duplication issue.\"\n",
    "\n",
    "actual_modified_files = ['packages/react-devtools-extensions/src/main/index.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/facebook_react/facebook_react_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebook/react/commit/d9e00f795b77676fb14f2a3c6f421f48f73bec2a\n",
    "query = \"Stop flowing and then abort if a stream is cancelled\"\n",
    "query_date = \"2023-09-22\"\n",
    "query_commit_id = 'd9e00f795b77676fb14f2a3c6f421f48f73bec2a'\n",
    "actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df to only include commits with commit_id d9e00f795b77676fb14f2a3c6f421f48f73bec2a & get the file_path column as a list to get actual_modified_files\n",
    "# df[df['commit_id'] == 'd9e00f795b77676fb14f2a3c6f421f48f73bec2a']\n",
    "# actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['packages/react-dom/src/__tests__/ReactDOMFizzServerBrowser-test.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerBrowser.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerBun.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerEdge.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzServerNode.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzStaticBrowser.js',\n",
       " 'packages/react-dom/src/server/ReactDOMFizzStaticEdge.js',\n",
       " 'packages/react-server-dom-esm/src/ReactFlightDOMServerNode.js',\n",
       " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerBrowser.js',\n",
       " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerEdge.js',\n",
       " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerNode.js',\n",
       " 'packages/react-server-dom-webpack/src/__tests__/ReactFlightDOMBrowser-test.js',\n",
       " 'packages/react-server/src/ReactFizzServer.js',\n",
       " 'packages/react-server/src/ReactFlightServer.js']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_modified_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized with or without flag is the same, so let's just use with flag to avoid recomputing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = f\"data/facebook_react/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/nodejs_node/index_tk/\n"
     ]
    }
   ],
   "source": [
    "print(idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 848e802d203e531daf2b9b0edb281a1eb6c5415d 108.84327 react/packages/react-server/src/ReactFizzServer.js 1643990253\n",
      "39 848e802d203e531daf2b9b0edb281a1eb6c5415d 108.84327 react/packages/react-server/src/ReactFlightServer.js 1643990253\n",
      "137 ef8bdbecb6dbb9743b895c2e867e5a5264dd6651 87.51058 react/packages/react-server-dom-webpack/src/ReactFlightDOMServerBrowser.js 1678466175\n",
      "138 ef8bdbecb6dbb9743b895c2e867e5a5264dd6651 87.51058 react/packages/react-server-dom-webpack/src/ReactFlightDOMServerEdge.js 1678466175\n",
      "139 ef8bdbecb6dbb9743b895c2e867e5a5264dd6651 87.51058 react/packages/react-server-dom-webpack/src/ReactFlightDOMServerNode.js 1678466175\n",
      "141 ef8bdbecb6dbb9743b895c2e867e5a5264dd6651 87.51058 react/packages/react-server-dom-webpack/src/__tests__/ReactFlightDOMBrowser-test.js 1678466175\n",
      "146 ef8bdbecb6dbb9743b895c2e867e5a5264dd6651 87.51057 react/packages/react-server/src/ReactFlightServer.js 1678466175\n",
      "213 c88fb49d37fd01024e0a254a37b7810d107bdd1d 85.80286 react/packages/react-server/src/ReactFlightServer.js 1632762307\n",
      "417 f181ba8aa6339d62f6e2572109c61242606f16b3 78.05207 react/packages/react-server-dom-esm/src/ReactFlightDOMServerNode.js 1685822304\n",
      "553 a724a3b578dce77d427bef313102a4d0e978d9b4 77.29090 react/packages/react-server/src/ReactFizzServer.js 1633028488\n",
      "554 a724a3b578dce77d427bef313102a4d0e978d9b4 77.29090 react/packages/react-server/src/ReactFlightServer.js 1633028488\n",
      "703 b6006201b5fdfcc5720160f169b80ddb7b8d7467 74.42069 react/packages/react-server-dom-webpack/src/__tests__/ReactFlightDOMBrowser-test.js 1681528835\n",
      "820 6396b664118442f3c2eae7bf13732fcb27bda98f 71.80569 react/packages/react-dom/src/__tests__/ReactDOMFizzServerBrowser-test.js 1676012369\n",
      "845 6396b664118442f3c2eae7bf13732fcb27bda98f 71.80566 react/packages/react-server/src/ReactFizzServer.js 1676012369\n",
      "932 d4f58c3b8123f1654ca1b06692c3165928bef8df 69.22440 react/packages/react-server/src/ReactFizzServer.js 1678572687\n"
     ]
    }
   ],
   "source": [
    "# idx_path = f'{repo_dir}/index_tk/'\n",
    "idx_path = f'data/facebook_react/index_tk'\n",
    "bm25searcher = LuceneSearcher(idx_path)\n",
    "hits = bm25searcher.search(tokenize(modified_query), k=1000)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    # print(obj)\n",
    "    commit_date = int(obj[\"commit_date\"])\n",
    "    if commit_date > convert_date_to_timestamp(query_date):\n",
    "        continue\n",
    "    if obj[\"file_path\"] in actual_modified_files:\n",
    "        print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to write a function to evaluate this behaviour. For now just focus on the perfomance of normal query (without modification)\n",
    "# the way we do this is by randomly sampling 1000 queries from df and then running the query on the index and then checking if the file is present in the actual_modified_files list. We want to store all hits and return IR metrics like MAP, MRR, P@10, P@100, P@1K, P@10K, NDCG@10, NDCG@100, NDCG@1K, NDCG@10K\n",
    "\n",
    "# write 2 functions, one for searching and one for evaluating\n",
    "\n",
    "def search(query, idx_path, query_date, k=1000):\n",
    "    bm25searcher = LuceneSearcher(idx_path)\n",
    "    hits = bm25searcher.search(tokenize(query), k)\n",
    "    # filter hits based on date\n",
    "    filtered_hits = []\n",
    "    for i in range(len(hits)):\n",
    "        obj = json.loads(hits[i].raw)\n",
    "        commit_date = int(obj[\"commit_date\"])\n",
    "        if commit_date > convert_date_to_timestamp(query_date):\n",
    "            continue\n",
    "        filtered_hits.append(hits[i])\n",
    "    return filtered_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "def evaluate(query, idx_path, query_date, actual_modified_files, k=1000):\n",
    "    hits = search(query, idx_path, query_date, k)\n",
    "\n",
    "    # Convert the hits to a list of filenames\n",
    "    retrieved_files = [json.loads(hit.raw)['file_path'] for hit in hits]\n",
    "\n",
    "    # Generate binary relevance judgments based on the actual_modified_files\n",
    "    relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "\n",
    "    if sum(relevant) == 0:\n",
    "        return {\n",
    "            'MAP': 0,\n",
    "            'P@10': 0,\n",
    "            'P@100': 0,\n",
    "            'P@1K': 0,\n",
    "            # 'P@10K': 0,\n",
    "            'MRR': 0,\n",
    "            'Recall@1K': 0\n",
    "            # 'NDCG@10': 0,\n",
    "            # 'NDCG@100': 0,\n",
    "            # 'NDCG@1K': 0,\n",
    "            # 'NDCG@10K': 0\n",
    "        }\n",
    "    # Calculate the metrics\n",
    "    MAP = average_precision_score(relevant, [1]*len(relevant))\n",
    "    unique_relevant_files = {\n",
    "        file for idx, file in enumerate(retrieved_files) if relevant[idx] == 1\n",
    "    }\n",
    "    recall = len(unique_relevant_files) / len(actual_modified_files)\n",
    "    # recall = sum(relevant) / len(actual_modified_files)\n",
    "    # also calculate MRR\n",
    "    MRR = mean_reciprocal_rank(relevant)\n",
    "    precision_values = [precision_at_k(relevant, k_val) for k_val in [10, 100, 1000]]\n",
    "\n",
    "    #todo NDCG calculations - no multi-label support as of now\n",
    "    # true_relevance = [[rel] for rel in relevant]\n",
    "    # scores = [[1] for _ in relevant]  # assuming all the retrieved files are equally relevant\n",
    "    # NDCG_values = [ndcg_score(true_relevance, scores, k=k_val) for k_val in [10, 100, 1000, 10000]]\n",
    "\n",
    "    metrics = {\n",
    "        'MAP': MAP,\n",
    "        'P@10': precision_values[0],\n",
    "        'P@100': precision_values[1],\n",
    "        'P@1K': precision_values[2],\n",
    "        # 'P@10K': precision_values[3],\n",
    "        'MRR': MRR,\n",
    "        'Recall@1K': recall\n",
    "        # 'NDCG@10': NDCG_values[0],\n",
    "        # 'NDCG@100': NDCG_values[1],\n",
    "        # 'NDCG@1K': NDCG_values[2],\n",
    "        # 'NDCG@10K': NDCG_values[3]\n",
    "    }\n",
    "    # round all the values to 4 decimal places\n",
    "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
    "    return metrics\n",
    "\n",
    "def precision_at_k(relevant, k):\n",
    "    return sum(relevant[:k]) / k\n",
    "\n",
    "def mean_reciprocal_rank(relevant):\n",
    "    for idx, value in enumerate(relevant):\n",
    "        if value == 1:\n",
    "            return 1 / (idx + 1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/facebook_react//index_tk_nf/'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0723,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.06,\n",
       " 'P@1K': 0.006,\n",
       " 'MRR': 0.0455,\n",
       " 'Recall@1K': 0.3571}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(query, idx_path, query_date, actual_modified_files, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0219, 'MRR': 0.121, 'P@10': 0.052, 'P@100': 0.0188, 'P@1K': 0.0068, 'Recall@1K': 0.5148}\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your data frame\n",
    "sampled_commits = df.drop_duplicates(subset='commit_id').sample(100, replace=False, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in sampled_commits.iterrows():\n",
    "    query = row['commit_message']\n",
    "    query_date = row['commit_date'].strftime('%Y-%m-%d')\n",
    "    query_commit_id = row['commit_id']\n",
    "    actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()\n",
    "\n",
    "    result = evaluate(query, idx_path, query_date, actual_modified_files)\n",
    "    results.append(result)\n",
    "\n",
    "# Compute average scores\n",
    "avg_scores = {}\n",
    "metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
    "for metric in metrics:\n",
    "    avg_scores[metric] = np.mean([result[metric] for result in results])\n",
    "\n",
    "# round all the values to 4 decimal places\n",
    "avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalize sampling across all repos by making a function which does it for each repo_name in REPO_LIST\n",
    "\n",
    "def evaluate_sampling(repo_dir, idx_path, n=100):\n",
    "    metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    print(f'Index path: {idx_path}')\n",
    "    total_commits = combined_df.commit_id.nunique()\n",
    "    print(f'Total commits: {total_commits}')\n",
    "    if total_commits < 100:\n",
    "        print(f'Not enough commits to sample for {repo_dir}, skipping...')\n",
    "        return {metric: 0 for metric in metrics}\n",
    "    # n = total_commits // 10 if total_commits > 10 else 1\n",
    "    print(f'Processing {repo_dir} with {n} samples')\n",
    "\n",
    "    sampled_commits = combined_df.drop_duplicates(subset='commit_id').sample(n, replace=False, random_state=42)\n",
    "    print(f'Number of commits sampled: {len(sampled_commits)}')\n",
    "    results = []\n",
    "    for index, row in sampled_commits.iterrows():\n",
    "        query = row['commit_message']\n",
    "        query_date = row['commit_date'].strftime('%Y-%m-%d')\n",
    "        query_commit_id = row['commit_id']\n",
    "        actual_modified_files = combined_df[combined_df['commit_id'] == query_commit_id]['file_path'].tolist()\n",
    "\n",
    "        result = evaluate(query, idx_path, query_date, actual_modified_files)\n",
    "        results.append(result)\n",
    "    avg_scores = {\n",
    "        metric: np.mean([result[metric] for result in results])\n",
    "        for metric in metrics\n",
    "    }\n",
    "    # round all the values to 4 decimal places\n",
    "    avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index path: data/apache_kafka/index_tk\n",
      "Total commits: 10438\n",
      "Processing data/apache_kafka/ with 1000 samples\n",
      "Number of commits sampled: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0,\n",
       " 'MRR': 0.0,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.0,\n",
       " 'P@1K': 0.0,\n",
       " 'Recall@1K': 0.0}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_sampling('data/apache_kafka/', 'data/apache_kafka/index_tk', n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/facebook_react/'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_LIST = ['karpathy_llama2.c',\n",
    " 'facebook_react',\n",
    " 'apache_kafka',\n",
    " 'ggerganov_llama.cpp',\n",
    " 'nodejs_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/karpathy_llama2.c/\n",
      "Index path: data/karpathy_llama2.c//index_tk/\n",
      "Processing data/karpathy_llama2.c/ with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:04,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0, 'MRR': 0.0, 'P@10': 0.0, 'P@100': 0.0, 'P@1K': 0.0, 'Recall@1K': 0.0}\n",
      "Processing data/facebook_react/\n",
      "Index path: data/facebook_react//index_tk/\n",
      "Processing data/facebook_react/ with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:11<00:19,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0219, 'MRR': 0.121, 'P@10': 0.052, 'P@100': 0.0188, 'P@1K': 0.0068, 'Recall@1K': 0.5148}\n",
      "Processing data/apache_kafka/\n",
      "Index path: data/apache_kafka//index_tk/\n",
      "Processing data/apache_kafka/ with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:20<00:14,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0, 'MRR': 0.0, 'P@10': 0.0, 'P@100': 0.0, 'P@1K': 0.0, 'Recall@1K': 0.0}\n",
      "Processing data/ggerganov_llama.cpp/\n",
      "Index path: data/ggerganov_llama.cpp//index_tk/\n",
      "Processing data/ggerganov_llama.cpp/ with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:24<00:06,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0, 'MRR': 0.0, 'P@10': 0.0, 'P@100': 0.0, 'P@1K': 0.0, 'Recall@1K': 0.0}\n",
      "Processing data/nodejs_node/\n",
      "Index path: data/nodejs_node//index_tk/\n",
      "Processing data/nodejs_node/ with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:51<00:00, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.0, 'MRR': 0.0, 'P@10': 0.0, 'P@100': 0.0, 'P@1K': 0.0, 'Recall@1K': 0.0}\n",
      "Average scores for all repos: {'MAP': 0.00438, 'MRR': 0.0242, 'P@10': 0.0104, 'P@100': 0.0037600000000000003, 'P@1K': 0.0013599999999999999, 'Recall@1K': 0.10296000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
    "res = []\n",
    "for repo_name in tqdm(REPO_LIST):\n",
    "    repo_dir = f'data/{repo_name}/'\n",
    "    idx_path = f'{repo_dir}/index_tk/'\n",
    "    print(f'Processing {repo_dir}')\n",
    "    avg_scores = evaluate_sampling(repo_dir, idx_path)\n",
    "    res.append(avg_scores)\n",
    "    print(avg_scores)\n",
    "\n",
    "# avg scores for all repos\n",
    "avg_scores = {}\n",
    "for metric in metrics:\n",
    "    avg_scores[metric] = np.mean([result[metric] for result in res])\n",
    "print(f'Average scores for all repos: {avg_scores}')\n",
    "# evaluate_sampling(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2.c\n",
    "# query = 'nInference for Llama-2 Transformer model in pure C'\n",
    "\n",
    "# refpred\n",
    "# query = 'if is_arxiv:\\n return f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper_id}/references?fields=title,\n",
    "# abstract,url,venue,publicationVenue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess'\n",
    "\n",
    "# react\n",
    "# query = \"export {default} from './npm/Circle';\"\n",
    "\n",
    "# kafka\n",
    "# public class MockKafkaLog4jAppender extends KafkaLog4jAppender {\n",
    "#     private MockProducer<byte[], byte[]> mockProducer =\n",
    "#             new MockProducer<>(false, new MockSerializer(), new MockSerializer());\n",
    "\n",
    "#     private Properties producerProperties;\n",
    "\n",
    "#     @Override\n",
    "#     protected Producer<byte[], byte[]> getKafkaProducer(Properties props) {\n",
    "#         producerProperties = props;\n",
    "#         return mockProducer;\n",
    "#     }\n",
    "\n",
    "#     void setKafkaProducer(MockProducer<byte[], byte[]> producer) {\n",
    "#         this.mockProducer = producer;\n",
    "#     }\n",
    "# \"\"\"\n",
    "\n",
    "# Kakfa\n",
    "# query = \"\"\"\n",
    "# /**\n",
    "#  * Local file based quorum state store. It takes the JSON format of {@link QuorumStateData}\n",
    "#  * with an extra data version number as part of the data for easy deserialization.\n",
    "#  *\n",
    "#  * Example format:\n",
    "#  * <pre>\n",
    "#  * {\"clusterId\":\"\",\n",
    "#  *   \"leaderId\":1,\n",
    "#  *   \"leaderEpoch\":2,\n",
    "#  *   \"votedId\":-1,\n",
    "#  *   \"appliedOffset\":0,\n",
    "#  *   \"currentVoters\":[],\n",
    "#  *   \"data_version\":0}\n",
    "#  * </pre>\n",
    "#  * */\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# kakfa\n",
    "query = \"\"\"Convert coordinator retriable errors to a known producer…\n",
    "… response error (#14378)\n",
    "\n",
    "KIP-890 Part 1 tries to address hanging transactions on old clients. Thus, the produce version can not be bumped and no new errors can be added. Before we used the java client's notion of retriable and abortable errors -- retriable errors are defined as such by extending the retriable error class, fatal errors are defined explicitly, and abortable errors are the remaining. However, many other clients treat non specified errors as fatal and that means many retriable errors kill the application.\"\"\"\n",
    "\n",
    "# kakfa\n",
    "# query = \"\"\"Fix flaky TopicAdminTest::retryEndOffsetsShouldRetryWhenTopicNotFound test case\"\"\"\n",
    "\n",
    "# nodejs\n",
    "# query = \"\"\"bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
    "#   DebugSealHandleScope scope(isolate);\n",
    "#   Environment* env = Environment::GetCurrent(isolate);\n",
    "#   return env != nullptr &&\n",
    "#          (env->is_main_thread() || !env->is_stopping()) &&\n",
    "#          env->abort_on_uncaught_exception() &&\n",
    "#          env->should_abort_on_uncaught_toggle()[0] &&\n",
    "#          !env->inside_should_not_abort_on_uncaught_scope();\n",
    "# }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 99.51060 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 82.57980 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 81.72260 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 5aecd2825644728f68a26558c957f5dfd4643423 81.36090 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 5 ef09a2e3fc11a738f6681fd57fb84ad109593fd3 80.57710 kafka/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala\n",
      " 6 f5d5f654db359af077088685e29fbe5ea69616cf 79.69870 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 2b6365c78b6e659f8df0651a24013d028f39edd9 79.64400 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 78.68580 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 1a10c3445e157da1d2fd670c043f19c385465eb0 78.48480 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 69d2a177101eb1c29b59b4c64d8c22f6d5e3d281 78.27240 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "bm25searcher = LuceneSearcher('bm25_index_6/')\n",
    "hits = bm25searcher.search(query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 696778,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 6840}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [00:02<00:00, 190.05it/s]\n"
     ]
    }
   ],
   "source": [
    "index_reader.dump_documents_BM25('tmp/idx_karpathy_double.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 578447,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 3034}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 141.63670 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 5aecd2825644728f68a26558c957f5dfd4643423 112.99820 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 111.59350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 111.57550 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 5 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 110.54000 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 6 ea0bb001262320bc9233221955a2be31c85993b9 109.68660 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 f5d5f654db359af077088685e29fbe5ea69616cf 109.62250 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 b937ec75677f8af13bf6fda686f07e9c62cdd20f 109.10350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 a81f35c1c8f9dc594aa585618c36f92ade0f86e2 109.03760 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 b49013b73efa25466652d8d8122974e60c927ec4 108.96060 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "tiktoken_searcher = LuceneSearcher('bm25_index_tiktoken_6/')\n",
    "# get tokenized query with enc.encode\n",
    "tokeninzed_query = tokenize(query)\n",
    "hits = tiktoken_searcher.search(tokeninzed_query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 2698903862,\n",
       " 'documents': 360230,\n",
       " 'non_empty_documents': 360230,\n",
       " 'unique_terms': -1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_index_reader = IndexReader('bm25_index_tiktoken_6/')\n",
    "tiktoken_index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the document source code inside the first hit raw\n",
    "content = json.loads(hits[0].raw)['contents']\n",
    "\n",
    "# print the document source code inside the first hit raw by decoding the tokenized string with enc.decode (convert to array of int and then decode)\n",
    "# print(enc.decode(json.loads(hits[0].raw)['contents']))\n",
    "\n",
    "# convert content to array of int\n",
    "content_arr = [int(i) for i in content.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: fix --abort-on-uncaught-exception handling\n",
      "\n",
      "The `set_abort_on_uncaught_exception(false)` line was supposed to\n",
      "prevent aborting when running Workers in\n",
      "`--abort-on-uncaught-exception` mode, but it was incorrectly set\n",
      "and not checked properly in the should-abort callback.\n",
      "\n",
      "PR-URL: https://github.com/nodejs/node/pull/34724\n",
      "Reviewed-By: Colin Ihrig <cjihrig@gmail.com>\n",
      "Reviewed-By: Richard Lau <riclau@uk.ibm.com>\n",
      "Reviewed-By: James M Snell <jasnell@gmail.com>\n",
      "Reviewed-By: Mary Marchini <oss@mmarchini.me>\n",
      "\n",
      "#include \"node.h\"\n",
      "#include \"node_context_data.h\"\n",
      "#include \"node_errors.h\"\n",
      "#include \"node_internals.h\"\n",
      "#include \"node_native_module_env.h\"\n",
      "#include \"node_platform.h\"\n",
      "#include \"node_v8_platform-inl.h\"\n",
      "#include \"uv.h\"\n",
      "\n",
      "#if HAVE_INSPECTOR\n",
      "#include \"inspector/worker_inspector.h\"  // ParentInspectorHandle\n",
      "#endif\n",
      "\n",
      "namespace node {\n",
      "using errors::TryCatchScope;\n",
      "using v8::Array;\n",
      "using v8::Context;\n",
      "using v8::EscapableHandleScope;\n",
      "using v8::Function;\n",
      "using v8::FunctionCallbackInfo;\n",
      "using v8::HandleScope;\n",
      "using v8::Isolate;\n",
      "using v8::Local;\n",
      "using v8::MaybeLocal;\n",
      "using v8::Null;\n",
      "using v8::Object;\n",
      "using v8::ObjectTemplate;\n",
      "using v8::Private;\n",
      "using v8::PropertyDescriptor;\n",
      "using v8::SealHandleScope;\n",
      "using v8::String;\n",
      "using v8::Value;\n",
      "\n",
      "static bool AllowWasmCodeGenerationCallback(Local<Context> context,\n",
      "                                            Local<String>) {\n",
      "  Local<Value> wasm_code_gen =\n",
      "      context->GetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration);\n",
      "  return wasm_code_gen->IsUndefined() || wasm_code_gen->IsTrue();\n",
      "}\n",
      "\n",
      "static bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
      "  DebugSealHandleScope scope(isolate);\n",
      "  Environment* env = Environment::GetCurrent(isolate);\n",
      "  return env != nullptr &&\n",
      "         (env->is_main_thread() || !env->is_stopping()) &&\n",
      "         env->abort_on_uncaught_exception() &&\n",
      "         env->should_abort_on_uncaught_toggle()[0] &&\n",
      "         !env->inside_should_not_abort_on_uncaught_scope();\n",
      "}\n",
      "\n",
      "static MaybeLocal<Value> PrepareStackTraceCallback(Local<Context> context,\n",
      "                                      Local<Value> exception,\n",
      "                                      Local<Array> trace) {\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Function> prepare = env->prepare_stack_trace_callback();\n",
      "  if (prepare.IsEmpty()) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Value> args[] = {\n",
      "      context->Global(),\n",
      "      exception,\n",
      "      trace,\n",
      "  };\n",
      "  // This TryCatch + Rethrow is required by V8 due to details around exception\n",
      "  // handling there. For C++ callbacks, V8 expects a scheduled exception (which\n",
      "  // is what ReThrow gives us). Just returning the empty MaybeLocal would leave\n",
      "  // us with a pending exception.\n",
      "  TryCatchScope try_catch(env);\n",
      "  MaybeLocal<Value> result = prepare->Call(\n",
      "      context, Undefined(env->isolate()), arraysize(args), args);\n",
      "  if (try_catch.HasCaught() && !try_catch.HasTerminated()) {\n",
      "    try_catch.ReThrow();\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Allocate(size_t size) {\n",
      "  void* ret;\n",
      "  if (zero_fill_field_ || per_process::cli_options->zero_fill_all_buffers)\n",
      "    ret = UncheckedCalloc(size);\n",
      "  else\n",
      "    ret = UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  void* ret = node::UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Reallocate(\n",
      "    void* data, size_t old_size, size_t size) {\n",
      "  void* ret = UncheckedRealloc<char>(static_cast<char*>(data), size);\n",
      "  if (LIKELY(ret != nullptr) || UNLIKELY(size == 0))\n",
      "    total_mem_usage_.fetch_add(size - old_size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void NodeArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  total_mem_usage_.fetch_sub(size, std::memory_order_relaxed);\n",
      "  free(data);\n",
      "}\n",
      "\n",
      "DebuggingArrayBufferAllocator::~DebuggingArrayBufferAllocator() {\n",
      "  CHECK(allocations_.empty());\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Allocate(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::Allocate(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::AllocateUninitialized(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "  NodeArrayBufferAllocator::Free(data, size);\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Reallocate(void* data,\n",
      "                                                size_t old_size,\n",
      "                                                size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* ret = NodeArrayBufferAllocator::Reallocate(data, old_size, size);\n",
      "  if (ret == nullptr) {\n",
      "    if (size == 0)  // i.e. equivalent to free().\n",
      "      UnregisterPointerInternal(data, old_size);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  if (data != nullptr) {\n",
      "    auto it = allocations_.find(data);\n",
      "    CHECK_NE(it, allocations_.end());\n",
      "    allocations_.erase(it);\n",
      "  }\n",
      "\n",
      "  RegisterPointerInternal(ret, size);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::RegisterPointer(data, size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::UnregisterPointer(data, size);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointerInternal(void* data,\n",
      "                                                              size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  auto it = allocations_.find(data);\n",
      "  CHECK_NE(it, allocations_.end());\n",
      "  if (size > 0) {\n",
      "    // We allow allocations with size 1 for 0-length buffers to avoid having\n",
      "    // to deal with nullptr values.\n",
      "    CHECK_EQ(it->second, size);\n",
      "  }\n",
      "  allocations_.erase(it);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointerInternal(void* data,\n",
      "                                                            size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  CHECK_EQ(allocations_.count(data), 0);\n",
      "  allocations_[data] = size;\n",
      "}\n",
      "\n",
      "std::unique_ptr<ArrayBufferAllocator> ArrayBufferAllocator::Create(bool debug) {\n",
      "  if (debug || per_process::cli_options->debug_arraybuffer_allocations)\n",
      "    return std::make_unique<DebuggingArrayBufferAllocator>();\n",
      "  else\n",
      "    return std::make_unique<NodeArrayBufferAllocator>();\n",
      "}\n",
      "\n",
      "ArrayBufferAllocator* CreateArrayBufferAllocator() {\n",
      "  return ArrayBufferAllocator::Create().release();\n",
      "}\n",
      "\n",
      "void FreeArrayBufferAllocator(ArrayBufferAllocator* allocator) {\n",
      "  delete allocator;\n",
      "}\n",
      "\n",
      "void SetIsolateCreateParamsForNode(Isolate::CreateParams* params) {\n",
      "  const uint64_t constrained_memory = uv_get_constrained_memory();\n",
      "  const uint64_t total_memory = constrained_memory > 0 ?\n",
      "      std::min(uv_get_total_memory(), constrained_memory) :\n",
      "      uv_get_total_memory();\n",
      "  if (total_memory > 0) {\n",
      "    // V8 defaults to 700MB or 1.4GB on 32 and 64 bit platforms respectively.\n",
      "    // This default is based on browser use-cases. Tell V8 to configure the\n",
      "    // heap based on the actual physical memory.\n",
      "    params->constraints.ConfigureDefaults(total_memory, 0);\n",
      "  }\n",
      "  params->embedder_wrapper_object_index = BaseObject::InternalFields::kSlot;\n",
      "  params->embedder_wrapper_type_index = std::numeric_limits<int>::max();\n",
      "}\n",
      "\n",
      "void SetIsolateErrorHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  if (s.flags & MESSAGE_LISTENER_WITH_ERROR_LEVEL)\n",
      "    isolate->AddMessageListenerWithErrorLevel(\n",
      "            errors::PerIsolateMessageListener,\n",
      "            Isolate::MessageErrorLevel::kMessageError |\n",
      "                Isolate::MessageErrorLevel::kMessageWarning);\n",
      "\n",
      "  auto* abort_callback = s.should_abort_on_uncaught_exception_callback ?\n",
      "      s.should_abort_on_uncaught_exception_callback :\n",
      "      ShouldAbortOnUncaughtException;\n",
      "  isolate->SetAbortOnUncaughtExceptionCallback(abort_callback);\n",
      "\n",
      "  auto* fatal_error_cb = s.fatal_error_callback ?\n",
      "      s.fatal_error_callback : OnFatalError;\n",
      "  isolate->SetFatalErrorHandler(fatal_error_cb);\n",
      "\n",
      "  auto* prepare_stack_trace_cb = s.prepare_stack_trace_callback ?\n",
      "      s.prepare_stack_trace_callback : PrepareStackTraceCallback;\n",
      "  isolate->SetPrepareStackTraceCallback(prepare_stack_trace_cb);\n",
      "}\n",
      "\n",
      "void SetIsolateMiscHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  isolate->SetMicrotasksPolicy(s.policy);\n",
      "\n",
      "  auto* allow_wasm_codegen_cb = s.allow_wasm_code_generation_callback ?\n",
      "    s.allow_wasm_code_generation_callback : AllowWasmCodeGenerationCallback;\n",
      "  isolate->SetAllowWasmCodeGenerationCallback(allow_wasm_codegen_cb);\n",
      "\n",
      "  if ((s.flags & SHOULD_NOT_SET_PROMISE_REJECTION_CALLBACK) == 0) {\n",
      "    auto* promise_reject_cb = s.promise_reject_callback ?\n",
      "      s.promise_reject_callback : task_queue::PromiseRejectCallback;\n",
      "    isolate->SetPromiseRejectCallback(promise_reject_cb);\n",
      "  }\n",
      "\n",
      "  if (s.flags & DETAILED_SOURCE_POSITIONS_FOR_PROFILING)\n",
      "    v8::CpuProfiler::UseDetailedSourcePositionsForProfiling(isolate);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate,\n",
      "                         const IsolateSettings& settings) {\n",
      "  SetIsolateErrorHandlers(isolate, settings);\n",
      "  SetIsolateMiscHandlers(isolate, settings);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate) {\n",
      "  IsolateSettings settings;\n",
      "  SetIsolateUpForNode(isolate, settings);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator, uv_loop_t* event_loop) {\n",
      "  return NewIsolate(allocator, event_loop, GetMainThreadMultiIsolatePlatform());\n",
      "}\n",
      "\n",
      "// TODO(joyeecheung): we may want to expose this, but then we need to be\n",
      "// careful about what we override in the params.\n",
      "Isolate* NewIsolate(Isolate::CreateParams* params,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate* isolate = Isolate::Allocate();\n",
      "  if (isolate == nullptr) return nullptr;\n",
      "\n",
      "  // Register the isolate on the platform before the isolate gets initialized,\n",
      "  // so that the isolate can access the platform during initialization.\n",
      "  platform->RegisterIsolate(isolate, event_loop);\n",
      "\n",
      "  SetIsolateCreateParamsForNode(params);\n",
      "  Isolate::Initialize(isolate, *params);\n",
      "  SetIsolateUpForNode(isolate);\n",
      "\n",
      "  return isolate;\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator != nullptr) params.array_buffer_allocator = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(std::shared_ptr<ArrayBufferAllocator> allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator) params.array_buffer_allocator_shared = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "IsolateData* CreateIsolateData(Isolate* isolate,\n",
      "                               uv_loop_t* loop,\n",
      "                               MultiIsolatePlatform* platform,\n",
      "                               ArrayBufferAllocator* allocator) {\n",
      "  return new IsolateData(isolate, loop, platform, allocator);\n",
      "}\n",
      "\n",
      "void FreeIsolateData(IsolateData* isolate_data) {\n",
      "  delete isolate_data;\n",
      "}\n",
      "\n",
      "InspectorParentHandle::~InspectorParentHandle() {}\n",
      "\n",
      "// Hide the internal handle class from the public API.\n",
      "#if HAVE_INSPECTOR\n",
      "struct InspectorParentHandleImpl : public InspectorParentHandle {\n",
      "  std::unique_ptr<inspector::ParentInspectorHandle> impl;\n",
      "\n",
      "  explicit InspectorParentHandleImpl(\n",
      "      std::unique_ptr<inspector::ParentInspectorHandle>&& impl)\n",
      "    : impl(std::move(impl)) {}\n",
      "};\n",
      "#endif\n",
      "\n",
      "Environment* CreateEnvironment(IsolateData* isolate_data,\n",
      "                               Local<Context> context,\n",
      "                               int argc,\n",
      "                               const char* const* argv,\n",
      "                               int exec_argc,\n",
      "                               const char* const* exec_argv) {\n",
      "  return CreateEnvironment(\n",
      "      isolate_data, context,\n",
      "      std::vector<std::string>(argv, argv + argc),\n",
      "      std::vector<std::string>(exec_argv, exec_argv + exec_argc));\n",
      "}\n",
      "\n",
      "Environment* CreateEnvironment(\n",
      "    IsolateData* isolate_data,\n",
      "    Local<Context> context,\n",
      "    const std::vector<std::string>& args,\n",
      "    const std::vector<std::string>& exec_args,\n",
      "    EnvironmentFlags::Flags flags,\n",
      "    ThreadId thread_id,\n",
      "    std::unique_ptr<InspectorParentHandle> inspector_parent_handle) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Context::Scope context_scope(context);\n",
      "  // TODO(addaleax): This is a much better place for parsing per-Environment\n",
      "  // options than the global parse call.\n",
      "  Environment* env = new Environment(\n",
      "      isolate_data, context, args, exec_args, nullptr, flags, thread_id);\n",
      "#if HAVE_INSPECTOR\n",
      "  if (inspector_parent_handle) {\n",
      "    env->InitializeInspector(\n",
      "        std::move(static_cast<InspectorParentHandleImpl*>(\n",
      "            inspector_parent_handle.get())->impl));\n",
      "  } else {\n",
      "    env->InitializeInspector({});\n",
      "  }\n",
      "#endif\n",
      "\n",
      "  if (env->RunBootstrapping().IsEmpty()) {\n",
      "    FreeEnvironment(env);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  return env;\n",
      "}\n",
      "\n",
      "void FreeEnvironment(Environment* env) {\n",
      "  Isolate::DisallowJavascriptExecutionScope disallow_js(env->isolate(),\n",
      "      Isolate::DisallowJavascriptExecutionScope::THROW_ON_FAILURE);\n",
      "  {\n",
      "    HandleScope handle_scope(env->isolate());  // For env->context().\n",
      "    Context::Scope context_scope(env->context());\n",
      "    SealHandleScope seal_handle_scope(env->isolate());\n",
      "\n",
      "    env->set_stopping(true);\n",
      "    env->stop_sub_worker_contexts();\n",
      "    env->RunCleanup();\n",
      "    RunAtExit(env);\n",
      "  }\n",
      "\n",
      "  // This call needs to be made while the `Environment` is still alive\n",
      "  // because we assume that it is available for async tracking in the\n",
      "  // NodePlatform implementation.\n",
      "  MultiIsolatePlatform* platform = env->isolate_data()->platform();\n",
      "  if (platform != nullptr)\n",
      "    platform->DrainTasks(env->isolate());\n",
      "\n",
      "  delete env;\n",
      "}\n",
      "\n",
      "NODE_EXTERN std::unique_ptr<InspectorParentHandle> GetInspectorParentHandle(\n",
      "    Environment* env,\n",
      "    ThreadId thread_id,\n",
      "    const char* url) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  CHECK_NE(thread_id.id, static_cast<uint64_t>(-1));\n",
      "#if HAVE_INSPECTOR\n",
      "  return std::make_unique<InspectorParentHandleImpl>(\n",
      "      env->inspector_agent()->GetParentHandle(thread_id.id, url));\n",
      "#else\n",
      "  return {};\n",
      "#endif\n",
      "}\n",
      "\n",
      "void LoadEnvironment(Environment* env) {\n",
      "  USE(LoadEnvironment(env,\n",
      "                      StartExecutionCallback{},\n",
      "                      {}));\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    StartExecutionCallback cb,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  env->InitializeLibuv();\n",
      "  env->InitializeDiagnostics();\n",
      "\n",
      "  return StartExecution(env, cb);\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    const char* main_script_source_utf8,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  CHECK_NOT_NULL(main_script_source_utf8);\n",
      "  return LoadEnvironment(\n",
      "      env,\n",
      "      [&](const StartExecutionCallbackInfo& info) -> MaybeLocal<Value> {\n",
      "        // This is a slightly hacky way to convert UTF-8 to UTF-16.\n",
      "        Local<String> str =\n",
      "            String::NewFromUtf8(env->isolate(),\n",
      "                                main_script_source_utf8).ToLocalChecked();\n",
      "        auto main_utf16 = std::make_unique<String::Value>(env->isolate(), str);\n",
      "\n",
      "        // TODO(addaleax): Avoid having a global table for all scripts.\n",
      "        std::string name = \"embedder_main_\" + std::to_string(env->thread_id());\n",
      "        native_module::NativeModuleEnv::Add(\n",
      "            name.c_str(),\n",
      "            UnionBytes(**main_utf16, main_utf16->length()));\n",
      "        env->set_main_utf16(std::move(main_utf16));\n",
      "        std::vector<Local<String>> params = {\n",
      "            env->process_string(),\n",
      "            env->require_string()};\n",
      "        std::vector<Local<Value>> args = {\n",
      "            env->process_object(),\n",
      "            env->native_module_require()};\n",
      "        return ExecuteBootstrapper(env, name.c_str(), &params, &args);\n",
      "      });\n",
      "}\n",
      "\n",
      "Environment* GetCurrentEnvironment(Local<Context> context) {\n",
      "  return Environment::GetCurrent(context);\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMainThreadMultiIsolatePlatform() {\n",
      "  return per_process::v8_platform.Platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(Environment* env) {\n",
      "  return GetMultiIsolatePlatform(env->isolate_data());\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(IsolateData* env) {\n",
      "  return env->platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    node::tracing::TracingController* tracing_controller) {\n",
      "  return CreatePlatform(\n",
      "      thread_pool_size,\n",
      "      static_cast<v8::TracingController*>(tracing_controller));\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return MultiIsolatePlatform::Create(thread_pool_size, tracing_controller)\n",
      "      .release();\n",
      "}\n",
      "\n",
      "void FreePlatform(MultiIsolatePlatform* platform) {\n",
      "  delete platform;\n",
      "}\n",
      "\n",
      "std::unique_ptr<MultiIsolatePlatform> MultiIsolatePlatform::Create(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return std::make_unique<NodePlatform>(thread_pool_size, tracing_controller);\n",
      "}\n",
      "\n",
      "MaybeLocal<Object> GetPerContextExports(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  EscapableHandleScope handle_scope(isolate);\n",
      "\n",
      "  Local<Object> global = context->Global();\n",
      "  Local<Private> key = Private::ForApi(isolate,\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"node:per_context_binding_exports\"));\n",
      "\n",
      "  Local<Value> existing_value;\n",
      "  if (!global->GetPrivate(context, key).ToLocal(&existing_value))\n",
      "    return MaybeLocal<Object>();\n",
      "  if (existing_value->IsObject())\n",
      "    return handle_scope.Escape(existing_value.As<Object>());\n",
      "\n",
      "  Local<Object> exports = Object::New(isolate);\n",
      "  if (context->Global()->SetPrivate(context, key, exports).IsNothing() ||\n",
      "      !InitializePrimordials(context))\n",
      "    return MaybeLocal<Object>();\n",
      "  return handle_scope.Escape(exports);\n",
      "}\n",
      "\n",
      "// Any initialization logic should be performed in\n",
      "// InitializeContext, because embedders don't necessarily\n",
      "// call NewContext and so they will experience breakages.\n",
      "Local<Context> NewContext(Isolate* isolate,\n",
      "                          Local<ObjectTemplate> object_template) {\n",
      "  auto context = Context::New(isolate, nullptr, object_template);\n",
      "  if (context.IsEmpty()) return context;\n",
      "\n",
      "  if (!InitializeContext(context)) {\n",
      "    return Local<Context>();\n",
      "  }\n",
      "\n",
      "  return context;\n",
      "}\n",
      "\n",
      "void ProtoThrower(const FunctionCallbackInfo<Value>& info) {\n",
      "  THROW_ERR_PROTO_ACCESS(info.GetIsolate());\n",
      "}\n",
      "\n",
      "// This runs at runtime, regardless of whether the context\n",
      "// is created from a snapshot.\n",
      "void InitializeContextRuntime(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  // Delete `Intl.v8BreakIterator`\n",
      "  // https://github.com/nodejs/node/issues/14909\n",
      "  Local<String> intl_string = FIXED_ONE_BYTE_STRING(isolate, \"Intl\");\n",
      "  Local<String> break_iter_string =\n",
      "    FIXED_ONE_BYTE_STRING(isolate, \"v8BreakIterator\");\n",
      "  Local<Value> intl_v;\n",
      "  if (context->Global()->Get(context, intl_string).ToLocal(&intl_v) &&\n",
      "      intl_v->IsObject()) {\n",
      "    Local<Object> intl = intl_v.As<Object>();\n",
      "    intl->Delete(context, break_iter_string).Check();\n",
      "  }\n",
      "\n",
      "  // Delete `Atomics.wake`\n",
      "  // https://github.com/nodejs/node/issues/21219\n",
      "  Local<String> atomics_string = FIXED_ONE_BYTE_STRING(isolate, \"Atomics\");\n",
      "  Local<String> wake_string = FIXED_ONE_BYTE_STRING(isolate, \"wake\");\n",
      "  Local<Value> atomics_v;\n",
      "  if (context->Global()->Get(context, atomics_string).ToLocal(&atomics_v) &&\n",
      "      atomics_v->IsObject()) {\n",
      "    Local<Object> atomics = atomics_v.As<Object>();\n",
      "    atomics->Delete(context, wake_string).Check();\n",
      "  }\n",
      "\n",
      "  // Remove __proto__\n",
      "  // https://github.com/nodejs/node/issues/31951\n",
      "  Local<String> object_string = FIXED_ONE_BYTE_STRING(isolate, \"Object\");\n",
      "  Local<String> prototype_string = FIXED_ONE_BYTE_STRING(isolate, \"prototype\");\n",
      "  Local<Object> prototype = context->Global()\n",
      "                                ->Get(context, object_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>()\n",
      "                                ->Get(context, prototype_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>();\n",
      "  Local<String> proto_string = FIXED_ONE_BYTE_STRING(isolate, \"__proto__\");\n",
      "  if (per_process::cli_options->disable_proto == \"delete\") {\n",
      "    prototype->Delete(context, proto_string).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto == \"throw\") {\n",
      "    Local<Value> thrower =\n",
      "        Function::New(context, ProtoThrower).ToLocalChecked();\n",
      "    PropertyDescriptor descriptor(thrower, thrower);\n",
      "    descriptor.set_enumerable(false);\n",
      "    descriptor.set_configurable(true);\n",
      "    prototype->DefineProperty(context, proto_string, descriptor).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto != \"\") {\n",
      "    // Validated in ProcessGlobalArgs\n",
      "    FatalError(\"InitializeContextRuntime()\", \"invalid --disable-proto mode\");\n",
      "  }\n",
      "}\n",
      "\n",
      "bool InitializeContextForSnapshot(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  context->SetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration,\n",
      "                           True(isolate));\n",
      "  return InitializePrimordials(context);\n",
      "}\n",
      "\n",
      "bool InitializePrimordials(Local<Context> context) {\n",
      "  // Run per-context JS files.\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  Context::Scope context_scope(context);\n",
      "  Local<Object> exports;\n",
      "\n",
      "  Local<String> primordials_string =\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"primordials\");\n",
      "  Local<String> global_string = FIXED_ONE_BYTE_STRING(isolate, \"global\");\n",
      "  Local<String> exports_string = FIXED_ONE_BYTE_STRING(isolate, \"exports\");\n",
      "\n",
      "  // Create primordials first and make it available to per-context scripts.\n",
      "  Local<Object> primordials = Object::New(isolate);\n",
      "  if (!primordials->SetPrototype(context, Null(isolate)).FromJust() ||\n",
      "      !GetPerContextExports(context).ToLocal(&exports) ||\n",
      "      !exports->Set(context, primordials_string, primordials).FromJust()) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  static const char* context_files[] = {\"internal/per_context/primordials\",\n",
      "                                        \"internal/per_context/domexception\",\n",
      "                                        \"internal/per_context/messageport\",\n",
      "                                        nullptr};\n",
      "\n",
      "  for (const char** module = context_files; *module != nullptr; module++) {\n",
      "    std::vector<Local<String>> parameters = {\n",
      "        global_string, exports_string, primordials_string};\n",
      "    Local<Value> arguments[] = {context->Global(), exports, primordials};\n",
      "    MaybeLocal<Function> maybe_fn =\n",
      "        native_module::NativeModuleEnv::LookupAndCompile(\n",
      "            context, *module, &parameters, nullptr);\n",
      "    Local<Function> fn;\n",
      "    if (!maybe_fn.ToLocal(&fn)) {\n",
      "      return false;\n",
      "    }\n",
      "    MaybeLocal<Value> result =\n",
      "        fn->Call(context, Undefined(isolate), arraysize(arguments), arguments);\n",
      "    // Execution failed during context creation.\n",
      "    // TODO(joyeecheung): deprecate this signature and return a MaybeLocal.\n",
      "    if (result.IsEmpty()) {\n",
      "      return false;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return true;\n",
      "}\n",
      "\n",
      "bool InitializeContext(Local<Context> context) {\n",
      "  if (!InitializeContextForSnapshot(context)) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  InitializeContextRuntime(context);\n",
      "  return true;\n",
      "}\n",
      "\n",
      "uv_loop_t* GetCurrentEventLoop(Isolate* isolate) {\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Local<Context> context = isolate->GetCurrentContext();\n",
      "  if (context.IsEmpty()) return nullptr;\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) return nullptr;\n",
      "  return env->event_loop();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env, const node_module& mod) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  Mutex::ScopedLock lock(env->extra_linked_bindings_mutex());\n",
      "\n",
      "  node_module* prev_head = env->extra_linked_bindings_head();\n",
      "  env->extra_linked_bindings()->push_back(mod);\n",
      "  if (prev_head != nullptr)\n",
      "    prev_head->nm_link = &env->extra_linked_bindings()->back();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env,\n",
      "                      const char* name,\n",
      "                      addon_context_register_func fn,\n",
      "                      void* priv) {\n",
      "  node_module mod = {\n",
      "    NODE_MODULE_VERSION,\n",
      "    NM_F_LINKED,\n",
      "    nullptr,  // nm_dso_handle\n",
      "    nullptr,  // nm_filename\n",
      "    nullptr,  // nm_register_func\n",
      "    fn,\n",
      "    name,\n",
      "    priv,\n",
      "    nullptr   // nm_link\n",
      "  };\n",
      "  AddLinkedBinding(env, mod);\n",
      "}\n",
      "\n",
      "static std::atomic<uint64_t> next_thread_id{0};\n",
      "\n",
      "ThreadId AllocateEnvironmentThreadId() {\n",
      "  return ThreadId { next_thread_id++ };\n",
      "}\n",
      "\n",
      "void DefaultProcessExitHandler(Environment* env, int exit_code) {\n",
      "  env->set_can_call_into_js(false);\n",
      "  env->stop_sub_worker_contexts();\n",
      "  DisposePlatform();\n",
      "  exit(exit_code);\n",
      "}\n",
      "\n",
      "\n",
      "void SetProcessExitHandler(Environment* env,\n",
      "                           std::function<void(Environment*, int)>&& handler) {\n",
      "  env->set_process_exit_handler(std::move(handler));\n",
      "}\n",
      "\n",
      "}  // namespace node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(content_arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
