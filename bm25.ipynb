{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = 'cl100k_base'\n",
    "encoding = 'p50k_base'\n",
    "enc = tiktoken.get_encoding(encoding)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return ' '.join(map(str,enc.encode(text, disallowed_special=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md             hs_err_pid34690.log         scrape.py\n",
      "bm25.ipynb            hs_err_pid73328.log         scrape_local.py\n",
      "clone_repos.py        \u001b[0m\u001b[01;34mindex\u001b[0m                       temp.ipynb\n",
      "clone_repos.sh        \u001b[01;34mlogs\u001b[0m                        temp.py\n",
      "code_extensions.json  \u001b[01;34mmisc\u001b[0m                        test.json\n",
      "\u001b[01;34mdata\u001b[0m                  plot.py                     test.parquet\n",
      "\u001b[01;34mdata_api\u001b[0m              programming_languages.json  test_repos.txt\n",
      "\u001b[01;34mdata_test\u001b[0m             \u001b[01;34mrepos\u001b[0m                       \u001b[01;34mtmp\u001b[0m\n",
      "diff_json.py          requirements.txt            tmp.txt\n",
      "hs_err_pid11351.log   run_scrape.sh               top_repos.txt\n",
      "hs_err_pid11926.log   sbatch_scrape.sh\n"
     ]
    }
   ],
   "source": [
    "!ls --color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.4M\n",
      "   0 drwxr-xr-x  9 siddharth  288 Oct  3 01:48 ./\n",
      "   0 drwxr-xr-x 17 siddharth  544 Oct  5 03:14 ../\n",
      "   0 drwxr-xr-x  3 siddharth   96 Oct  3 01:45 jsonl/\n",
      "868K -rw-r--r--  1 siddharth 868K Oct  2 22:02 karpathy_llama2.c_commit_data_0.parquet\n",
      "616K -rw-r--r--  1 siddharth 615K Oct  2 22:02 karpathy_llama2.c_commit_data_1.parquet\n",
      "372K -rw-r--r--  1 siddharth 372K Oct  2 22:03 karpathy_llama2.c_commit_data_2.parquet\n",
      "284K -rw-r--r--  1 siddharth 283K Oct  2 22:03 karpathy_llama2.c_commit_data_3.parquet\n",
      "240K -rw-r--r--  1 siddharth 238K Oct  2 22:03 karpathy_llama2.c_commit_data_4.parquet\n",
      "   0 drwxr-xr-x 21 siddharth  672 Oct  3 00:52 searcher/\n"
     ]
    }
   ],
   "source": [
    "!ls -GFlash data/karpathy_llama2.c/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "df = pd.read_parquet('data/karpathy_llama2.c/karpathy_llama2.c_commit_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   owner                  85 non-null     string             \n",
      " 1   repo_name              85 non-null     string             \n",
      " 2   commit_date            85 non-null     datetime64[ns, UTC]\n",
      " 3   commit_id              85 non-null     string             \n",
      " 4   commit_message         85 non-null     string             \n",
      " 5   file_path              85 non-null     string             \n",
      " 6   previous_commit_id     85 non-null     string             \n",
      " 7   previous_file_content  82 non-null     string             \n",
      " 8   cur_file_content       79 non-null     string             \n",
      " 9   diff                   76 non-null     string             \n",
      " 10  status                 85 non-null     category           \n",
      " 11  is_merge_request       85 non-null     bool               \n",
      " 12  file_extension         85 non-null     category           \n",
      "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
      "memory usage: 7.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.41 MB\n"
     ]
    }
   ],
   "source": [
    " # print just the memory usage in human readable format (MB) to 2 decimal places\n",
    "print(f'{df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique commits stored (others excluded for not being code commits): 72\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique commits stored (others excluded for not being code commits):', df.commit_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = 'data/karpathy_llama2.c/'\n",
    "REPO_LIST = ['karpathy_llama2.c', 'siddharth-gandhi_refpred', 'facebook_react', 'apache_kafka', 'ggerganov_llama.cpp', 'nodejs_node']\n",
    "# REPO_LIST = ['karpathy_llama2.c', 'siddharth-gandhi_refpred', 'facebook_react', 'apache_kafka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_LIST = ['karpathy_llama2.c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_data_to_jsonl(data_dir, output_file):\n",
    "#     all_files = glob.glob(os.path.join(data_dir, '*.parquet'))\n",
    "#     all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "#     combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#     # replace NaN with empty string\n",
    "#     combined_df.fillna('', inplace=True)\n",
    "\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for index, row in combined_df.iterrows():\n",
    "#             doc = {\n",
    "#                 'id': row['commit_id'],\n",
    "#                 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "#                 # Optionally include source code\n",
    "#                 # 'source_code': row['cur_file_content']\n",
    "#             }\n",
    "#             f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_commits(repo_dir):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # number of unique commit_id columns\n",
    "    return combined_df.commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_commits = 0\n",
    "for repo in REPO_LIST:\n",
    "    total_commits += count_commits('data/' + repo + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of commits: 36731\n"
     ]
    }
   ],
   "source": [
    "print('Total number of commits:', total_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_repo_to_jsonl(repo_dir, output_file, use_tokenizer=False):\n",
    "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
    "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    # replace NaN with empty string in non-category columns\n",
    "    # combined_df.fillna('', inplace=True)\n",
    "\n",
    "    combined_df['commit_message'] = combined_df['commit_message'].fillna('')\n",
    "    combined_df['cur_file_content'] = combined_df['cur_file_content'].fillna('')\n",
    "\n",
    "    # print combined_df memory usage\n",
    "    # print(combined_df.info(memory_usage='deep'))\n",
    "    print(f'Combined Memory Usage: {combined_df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB for {len(combined_df)} rows')\n",
    "    print(output_file)\n",
    "    with open(output_file, 'x') as f:\n",
    "        for index, row in combined_df.iterrows():\n",
    "            doc = {\n",
    "                'id': row['commit_id'],\n",
    "                # 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "                # 'source_code': row['cur_file_content'],  # Optionally include source code\n",
    "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']),\n",
    "                'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']) if use_tokenizer else row['commit_message'] + '\\n' + row['cur_file_content'],\n",
    "                'repo_name': row['repo_name'],\n",
    "                'file_path': row['file_path'],\n",
    "            }\n",
    "            f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty data/jsonl if it has data\n",
    "# !rm -rf data/jsonl_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Memory Usage: 18.29 MB for 402 rows\n",
      "data/jsonl_6/karpathy_llama2.c.jsonl\n",
      "Combined Memory Usage: 0.94 MB for 108 rows\n",
      "data/jsonl_6/siddharth-gandhi_refpred.jsonl\n",
      "Combined Memory Usage: 2699.89 MB for 73551 rows\n",
      "data/jsonl_6/facebook_react.jsonl\n",
      "Combined Memory Usage: 3645.70 MB for 75870 rows\n",
      "data/jsonl_6/apache_kafka.jsonl\n",
      "Combined Memory Usage: 605.11 MB for 2111 rows\n",
      "data/jsonl_6/ggerganov_llama.cpp.jsonl\n",
      "Combined Memory Usage: 11010.96 MB for 208188 rows\n",
      "data/jsonl_6/nodejs_node.jsonl\n"
     ]
    }
   ],
   "source": [
    "jsonl_dir_name = 'jsonl_6'\n",
    "for repo_name in REPO_LIST:\n",
    "    repo_dir = os.path.join('data', repo_name)\n",
    "    # create data/jsonl directory if it doesn't exist\n",
    "    os.makedirs(os.path.join('data', jsonl_dir_name), exist_ok=True)\n",
    "\n",
    "    # store in data/jsonl\n",
    "    output_jsonl_file = os.path.join('data', jsonl_dir_name, f'{repo_name}.jsonl')\n",
    "    convert_repo_to_jsonl(repo_dir, output_jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# jsonl_file_path = f'{BASE_DIR}/jsonl/llama2.jsonl'\n",
    "# convert_data_to_jsonl(BASE_DIR, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of jsonl files which are present in data/repo_name/jsonl/repo_name.jsonl\n",
    "# jsonl_files = glob.glob('data/*/*/*.jsonl')\n",
    "# print(jsonl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normal untokenized\n",
    "- Parquet -> JSONL 22s\n",
    "- Index build 1m26s\n",
    "- 6 repos\n",
    "    Parquet -> JSONL 1m11s\n",
    "    Same mem usage as before, just lower time since no need for tokenization\n",
    "    Index Build 3m51s\n",
    "    Index Size 5Gb\n",
    "\n",
    "For tokenized\n",
    "- Parquet -> JSONL 8m3s\n",
    "- Index Build 2m12s\n",
    "- 6 repos:\n",
    "    Parquert -> JSONL 24m\n",
    "        Combined Memory Usage: 18.29 MB for 402 rows data/isonl_6/karpathy_llama2.c.jsonl \\\\\n",
    "        Combined Memory Usage: 0.94 MB for 108 rows data/json1_6/siddharth-gandhi_refpred.jsonl \\\\\n",
    "        Combined Memory Usage: 2699.89 MB for 73551 rows data/jsonl_6/facebook_react.jsonl \\\\\n",
    "        Combined Memory Usage: 3645.70 MB for 75870 rows data/jsonl_6/apache_kafka. jsonl \\\\\n",
    "        Combined Memory Usage: 605.11 MB for 2111 rows data/jsonl_6/ggerganov_llama.cpp.jsonl \\\\\n",
    "        Combined Memory Usage: 11010.96 MB for 208188 rows data/jsonl_6/nodejs_node.json\n",
    "        36731 total commits \n",
    "        Total ~360K rows\n",
    "        Interesting heuristic, on avg 10 files edited per commit?\n",
    "    Index build 6m42s\n",
    "    Index Size 10GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 976128\n",
      "     0 drwxr-xr-x@ 46 siddharth  staff   1.4K Oct  5 03:15 \u001b[1m\u001b[36m.\u001b[m\u001b[m/\n",
      "     0 drwxrwxrwx  25 siddharth  staff   800B Oct  4 18:13 \u001b[30m\u001b[43m..\u001b[m\u001b[m/\n",
      "    24 -rw-r--r--@  1 siddharth  staff    10K Oct  3 07:52 .DS_Store\n",
      "     8 -rw-r--r--@  1 siddharth  staff    62B Sep 17 22:59 .env\n",
      "     0 drwxr-xr-x@ 14 siddharth  staff   448B Oct  3 03:45 \u001b[1m\u001b[36m.git\u001b[m\u001b[m/\n",
      "     8 -rw-r--r--@  1 siddharth  staff   3.6K Oct  3 03:45 .gitignore\n",
      "     0 drwxr-xr-x@  9 siddharth  staff   288B Oct  3 00:54 \u001b[1m\u001b[36m.idea\u001b[m\u001b[m/\n",
      "     0 drwxr-xr-x@  4 siddharth  staff   128B Sep 23 17:54 \u001b[1m\u001b[36m.vscode\u001b[m\u001b[m/\n",
      "     8 -rw-r--r--@  1 siddharth  staff   542B Oct  2 17:45 README.md\n",
      "   160 -rw-r--r--@  1 siddharth  staff    77K Oct  5 03:15 bm25.ipynb\n",
      "     0 drwxr-xr-x@ 72 siddharth  staff   2.3K Oct  3 03:20 \u001b[1m\u001b[36mbm25_index\u001b[m\u001b[m/\n",
      "     0 drwxr-xr-x@ 72 siddharth  staff   2.3K Oct  3 14:40 \u001b[1m\u001b[36mbm25_index_6\u001b[m\u001b[m/\n",
      "     0 drwxr-xr-x@ 72 siddharth  staff   2.3K Oct  3 03:33 \u001b[1m\u001b[36mbm25_index_tiktoken\u001b[m\u001b[m/\n",
      "     0 drwxr-xr-x@ 72 siddharth  staff   2.3K Oct  3 14:48 \u001b[1m\u001b[36mbm25_index_tiktoken_6\u001b[m\u001b[m/\n",
      "     8 -rw-r--r--@  1 siddharth  staff   1.5K Oct  2 02:04 clone_repos.py\n",
      "     8 -rw-r--r--@  1 siddharth  staff   916B Oct  2 02:07 clone_repos.sh\n",
      "    16 -rw-r--r--@  1 siddharth  staff   6.2K Sep 24 19:39 code_extensions.json\n",
      "     0 drwxr-xr-x@ 17 siddharth  staff   544B Oct  5 03:14 \u001b[1m\u001b[36mdata\u001b[m\u001b[m/\n",
      "     0 drwxr-xr-x@  9 siddharth  staff   288B Oct  3 00:11 \u001b[1m\u001b[36mdata_api\u001b[m\u001b[m/\n",
      "     0 drwxr-xr-x@  6 siddharth  staff   192B Sep 25 03:14 \u001b[1m\u001b[36mdata_test\u001b[m\u001b[m/\n",
      "     8 -rw-r--r--@  1 siddharth  staff   1.5K Sep 24 19:20 diff_json.py\n",
      "   264 -rw-r--r--@  1 siddharth  staff   130K Oct  3 03:18 hs_err_pid11351.log\n",
      "   264 -rw-r--r--@  1 siddharth  staff   129K Oct  3 03:51 hs_err_pid11926.log\n",
      "   264 -rw-r--r--@  1 siddharth  staff   130K Oct  4 17:14 hs_err_pid34690.log\n",
      "   264 -rw-r--r--@  1 siddharth  staff   130K Oct  3 03:12 hs_err_pid73328.log\n",
      "     0 drwxr-xr-x@ 21 siddharth  staff   672B Oct  5 03:15 \u001b[1m\u001b[36midx_karpathy\u001b[m\u001b[m/\n",
      "     0 drwxr-xr-x@ 21 siddharth  staff   672B Oct  5 03:15 \u001b[1m\u001b[36midx_karpathy_double_token\u001b[m\u001b[m/\n",
      "   376 -rw-r--r--@  1 siddharth  staff   185K Oct  2 19:02 kafka_scrape_local.prof\n",
      "     8 -rw-r--r--@  1 siddharth  staff   3.0K Oct  3 03:50 karpathy_scrape_local.prof\n",
      "     0 drwxr-xr-x@ 11 siddharth  staff   352B Oct  3 13:36 \u001b[1m\u001b[36mmisc\u001b[m\u001b[m/\n",
      "     8 -rw-r--r--@  1 siddharth  staff   2.1K Sep 20 11:32 plot.py\n",
      "    16 -rw-r--r--@  1 siddharth  staff   4.2K Sep 24 19:44 programming_languages.json\n",
      "     0 drwxr-xr-x@ 12 siddharth  staff   384B Oct  3 03:50 \u001b[1m\u001b[36mrepos\u001b[m\u001b[m/\n",
      "     8 -rw-r--r--@  1 siddharth  staff   1.5K Sep 25 00:12 requirements.txt\n",
      "     8 -rw-r--r--@  1 siddharth  staff   137B Oct  2 02:06 run_scrape.sh\n",
      "     8 -rw-r--r--@  1 siddharth  staff   1.1K Oct  2 02:07 sbatch_scrape.sh\n",
      "    24 -rw-r--r--@  1 siddharth  staff   9.8K Sep 20 10:56 scrape.py\n",
      "    24 -rw-r--r--@  1 siddharth  staff    11K Oct  4 18:12 scrape_local.py\n",
      "     0 drwxr-xr-x@  2 siddharth  staff    64B Oct  2 18:38 \u001b[1m\u001b[36mscripts\u001b[m\u001b[m/\n",
      "    56 -rw-r--r--@  1 siddharth  staff    25K Oct  3 12:44 temp.ipynb\n",
      "     8 -rw-r--r--@  1 siddharth  staff   1.2K Sep 25 02:20 temp.py\n",
      "902064 -rw-r--r--@  1 siddharth  staff   440M Oct  3 11:40 test.json\n",
      " 72192 -rw-r--r--@  1 siddharth  staff    35M Oct  3 11:42 test.parquet\n",
      "     8 -rw-r--r--@  1 siddharth  staff    97B Sep 24 23:35 test_repos.txt\n",
      "     8 -rw-r--r--@  1 siddharth  staff    11B Oct  3 03:50 tmp.txt\n",
      "     8 -rw-r--r--@  1 siddharth  staff   2.0K Sep 25 02:22 top_repos.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -GFlash ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016ddd4000-0x000000016dde0000).\n",
      "[0.004s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-05 03:15:51,452 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-05 03:15:51,453 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-05 03:15:51,453 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-05 03:15:51,453 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/test_dir/\n",
      "2023-10-05 03:15:51,453 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-05 03:15:51,453 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-05 03:15:51,453 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-05 03:15:51,454 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-05 03:15:51,455 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-05 03:15:51,455 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-05 03:15:51,455 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-05 03:15:51,455 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
      "2023-10-05 03:15:51,455 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: ./idx_karpathy/\n",
      "2023-10-05 03:15:51,457 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-05 03:15:51,527 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-05 03:15:51,527 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/test_dir\n",
      "2023-10-05 03:15:51,528 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-05 03:15:51,528 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-10-05 03:15:52,232 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - test_dir/karpathy_llama2.c.jsonl: 402 docs added.\n",
      "2023-10-05 03:15:52,349 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 402 documents indexed\n",
      "2023-10-05 03:15:52,349 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-10-05 03:15:52,349 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:              402\n",
      "2023-10-05 03:15:52,349 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-10-05 03:15:52,349 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-10-05 03:15:52,349 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-10-05 03:15:52,349 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-10-05 03:15:52,352 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 402 documents indexed in 00:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Directory to store the index\n",
    "# index_dir=\"./bm25_index_6/\"\n",
    "# jsonl_dir_name=\"jsonl_6\"\n",
    "\n",
    "index_dir=\"./idx_karpathy/\"\n",
    "# jsonl_dir_name=\"jsonl_tiktoken_6\"\n",
    "jsonl_dir_name=\"test_dir\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "mkdir -p \"$index_dir\"\n",
    "\n",
    "# Remove any existing indexes\n",
    "rm -rf \"$index_dir/*\"\n",
    "\n",
    "# build the index from data/jsonl\n",
    "python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
    " -threads 4 -input data/\"$jsonl_dir_name\"/ -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pyserini.index.lucene \\\n",
    "#   --collection JsonCollection \\\n",
    "#   --input data/karpathy_llama2.c/jsonl/ \\\n",
    "#   --index data/karpathy_llama2.c/searcher/ \\\n",
    "#   --generator DefaultLuceneDocumentGenerator \\\n",
    "#   --threads 1 \\\n",
    "#   --storePositions --storeDocvectors --storeRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ce91b1b3b56ff7d43d894c204f965bfbf5d63c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2.c\n",
    "# query = 'nInference for Llama-2 Transformer model in pure C'\n",
    "\n",
    "# refpred\n",
    "# query = 'if is_arxiv:\\n return f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper_id}/references?fields=title,\n",
    "# abstract,url,venue,publicationVenue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess'\n",
    "\n",
    "# react\n",
    "# query = \"export {default} from './npm/Circle';\"\n",
    "\n",
    "# kafka\n",
    "# public class MockKafkaLog4jAppender extends KafkaLog4jAppender {\n",
    "#     private MockProducer<byte[], byte[]> mockProducer =\n",
    "#             new MockProducer<>(false, new MockSerializer(), new MockSerializer());\n",
    "\n",
    "#     private Properties producerProperties;\n",
    "\n",
    "#     @Override\n",
    "#     protected Producer<byte[], byte[]> getKafkaProducer(Properties props) {\n",
    "#         producerProperties = props;\n",
    "#         return mockProducer;\n",
    "#     }\n",
    "\n",
    "#     void setKafkaProducer(MockProducer<byte[], byte[]> producer) {\n",
    "#         this.mockProducer = producer;\n",
    "#     }\n",
    "# \"\"\"\n",
    "\n",
    "# Kakfa\n",
    "# query = \"\"\"\n",
    "# /**\n",
    "#  * Local file based quorum state store. It takes the JSON format of {@link QuorumStateData}\n",
    "#  * with an extra data version number as part of the data for easy deserialization.\n",
    "#  *\n",
    "#  * Example format:\n",
    "#  * <pre>\n",
    "#  * {\"clusterId\":\"\",\n",
    "#  *   \"leaderId\":1,\n",
    "#  *   \"leaderEpoch\":2,\n",
    "#  *   \"votedId\":-1,\n",
    "#  *   \"appliedOffset\":0,\n",
    "#  *   \"currentVoters\":[],\n",
    "#  *   \"data_version\":0}\n",
    "#  * </pre>\n",
    "#  * */\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# kakfa\n",
    "query = \"\"\"Convert coordinator retriable errors to a known producer…\n",
    "… response error (#14378)\n",
    "\n",
    "KIP-890 Part 1 tries to address hanging transactions on old clients. Thus, the produce version can not be bumped and no new errors can be added. Before we used the java client's notion of retriable and abortable errors -- retriable errors are defined as such by extending the retriable error class, fatal errors are defined explicitly, and abortable errors are the remaining. However, many other clients treat non specified errors as fatal and that means many retriable errors kill the application.\"\"\"\n",
    "\n",
    "# kakfa\n",
    "# query = \"\"\"Fix flaky TopicAdminTest::retryEndOffsetsShouldRetryWhenTopicNotFound test case\"\"\"\n",
    "\n",
    "# nodejs\n",
    "# query = \"\"\"bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
    "#   DebugSealHandleScope scope(isolate);\n",
    "#   Environment* env = Environment::GetCurrent(isolate);\n",
    "#   return env != nullptr &&\n",
    "#          (env->is_main_thread() || !env->is_stopping()) &&\n",
    "#          env->abort_on_uncaught_exception() &&\n",
    "#          env->should_abort_on_uncaught_toggle()[0] &&\n",
    "#          !env->inside_should_not_abort_on_uncaught_scope();\n",
    "# }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 99.51060 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 82.57980 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 81.72260 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 5aecd2825644728f68a26558c957f5dfd4643423 81.36090 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 5 ef09a2e3fc11a738f6681fd57fb84ad109593fd3 80.57710 kafka/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala\n",
      " 6 f5d5f654db359af077088685e29fbe5ea69616cf 79.69870 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 2b6365c78b6e659f8df0651a24013d028f39edd9 79.64400 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 78.68580 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 1a10c3445e157da1d2fd670c043f19c385465eb0 78.48480 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 69d2a177101eb1c29b59b4c64d8c22f6d5e3d281 78.27240 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "bm25searcher = LuceneSearcher('bm25_index_6/')\n",
    "hits = bm25searcher.search(query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 696778,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 6840}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [00:02<00:00, 190.05it/s]\n"
     ]
    }
   ],
   "source": [
    "index_reader.dump_documents_BM25('tmp/idx_karpathy_double.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 578447,\n",
       " 'documents': 402,\n",
       " 'non_empty_documents': 402,\n",
       " 'unique_terms': 3034}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader = IndexReader('idx_karpathy_double_token/')\n",
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 5aecd2825644728f68a26558c957f5dfd4643423 141.63670 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
      " 2 5aecd2825644728f68a26558c957f5dfd4643423 112.99820 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
      " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 111.59350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 4 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 111.57550 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 5 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 110.54000 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 6 ea0bb001262320bc9233221955a2be31c85993b9 109.68660 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 7 f5d5f654db359af077088685e29fbe5ea69616cf 109.62250 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 8 b937ec75677f8af13bf6fda686f07e9c62cdd20f 109.10350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      " 9 a81f35c1c8f9dc594aa585618c36f92ade0f86e2 109.03760 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
      "10 b49013b73efa25466652d8d8122974e60c927ec4 108.96060 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
     ]
    }
   ],
   "source": [
    "tiktoken_searcher = LuceneSearcher('bm25_index_tiktoken_6/')\n",
    "# get tokenized query with enc.encode\n",
    "tokeninzed_query = tokenize(query)\n",
    "hits = tiktoken_searcher.search(tokeninzed_query, k=10)\n",
    "# print(hits[0])\n",
    "for i in range(len(hits)):\n",
    "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "    # print with repo name and file name\n",
    "    obj = json.loads(hits[i].raw)\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 2698903862,\n",
       " 'documents': 360230,\n",
       " 'non_empty_documents': 360230,\n",
       " 'unique_terms': -1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_index_reader = IndexReader('bm25_index_tiktoken_6/')\n",
    "tiktoken_index_reader.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the document source code inside the first hit raw\n",
    "content = json.loads(hits[0].raw)['contents']\n",
    "\n",
    "# print the document source code inside the first hit raw by decoding the tokenized string with enc.decode (convert to array of int and then decode)\n",
    "# print(enc.decode(json.loads(hits[0].raw)['contents']))\n",
    "\n",
    "# convert content to array of int\n",
    "content_arr = [int(i) for i in content.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: fix --abort-on-uncaught-exception handling\n",
      "\n",
      "The `set_abort_on_uncaught_exception(false)` line was supposed to\n",
      "prevent aborting when running Workers in\n",
      "`--abort-on-uncaught-exception` mode, but it was incorrectly set\n",
      "and not checked properly in the should-abort callback.\n",
      "\n",
      "PR-URL: https://github.com/nodejs/node/pull/34724\n",
      "Reviewed-By: Colin Ihrig <cjihrig@gmail.com>\n",
      "Reviewed-By: Richard Lau <riclau@uk.ibm.com>\n",
      "Reviewed-By: James M Snell <jasnell@gmail.com>\n",
      "Reviewed-By: Mary Marchini <oss@mmarchini.me>\n",
      "\n",
      "#include \"node.h\"\n",
      "#include \"node_context_data.h\"\n",
      "#include \"node_errors.h\"\n",
      "#include \"node_internals.h\"\n",
      "#include \"node_native_module_env.h\"\n",
      "#include \"node_platform.h\"\n",
      "#include \"node_v8_platform-inl.h\"\n",
      "#include \"uv.h\"\n",
      "\n",
      "#if HAVE_INSPECTOR\n",
      "#include \"inspector/worker_inspector.h\"  // ParentInspectorHandle\n",
      "#endif\n",
      "\n",
      "namespace node {\n",
      "using errors::TryCatchScope;\n",
      "using v8::Array;\n",
      "using v8::Context;\n",
      "using v8::EscapableHandleScope;\n",
      "using v8::Function;\n",
      "using v8::FunctionCallbackInfo;\n",
      "using v8::HandleScope;\n",
      "using v8::Isolate;\n",
      "using v8::Local;\n",
      "using v8::MaybeLocal;\n",
      "using v8::Null;\n",
      "using v8::Object;\n",
      "using v8::ObjectTemplate;\n",
      "using v8::Private;\n",
      "using v8::PropertyDescriptor;\n",
      "using v8::SealHandleScope;\n",
      "using v8::String;\n",
      "using v8::Value;\n",
      "\n",
      "static bool AllowWasmCodeGenerationCallback(Local<Context> context,\n",
      "                                            Local<String>) {\n",
      "  Local<Value> wasm_code_gen =\n",
      "      context->GetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration);\n",
      "  return wasm_code_gen->IsUndefined() || wasm_code_gen->IsTrue();\n",
      "}\n",
      "\n",
      "static bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
      "  DebugSealHandleScope scope(isolate);\n",
      "  Environment* env = Environment::GetCurrent(isolate);\n",
      "  return env != nullptr &&\n",
      "         (env->is_main_thread() || !env->is_stopping()) &&\n",
      "         env->abort_on_uncaught_exception() &&\n",
      "         env->should_abort_on_uncaught_toggle()[0] &&\n",
      "         !env->inside_should_not_abort_on_uncaught_scope();\n",
      "}\n",
      "\n",
      "static MaybeLocal<Value> PrepareStackTraceCallback(Local<Context> context,\n",
      "                                      Local<Value> exception,\n",
      "                                      Local<Array> trace) {\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Function> prepare = env->prepare_stack_trace_callback();\n",
      "  if (prepare.IsEmpty()) {\n",
      "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
      "  }\n",
      "  Local<Value> args[] = {\n",
      "      context->Global(),\n",
      "      exception,\n",
      "      trace,\n",
      "  };\n",
      "  // This TryCatch + Rethrow is required by V8 due to details around exception\n",
      "  // handling there. For C++ callbacks, V8 expects a scheduled exception (which\n",
      "  // is what ReThrow gives us). Just returning the empty MaybeLocal would leave\n",
      "  // us with a pending exception.\n",
      "  TryCatchScope try_catch(env);\n",
      "  MaybeLocal<Value> result = prepare->Call(\n",
      "      context, Undefined(env->isolate()), arraysize(args), args);\n",
      "  if (try_catch.HasCaught() && !try_catch.HasTerminated()) {\n",
      "    try_catch.ReThrow();\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Allocate(size_t size) {\n",
      "  void* ret;\n",
      "  if (zero_fill_field_ || per_process::cli_options->zero_fill_all_buffers)\n",
      "    ret = UncheckedCalloc(size);\n",
      "  else\n",
      "    ret = UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  void* ret = node::UncheckedMalloc(size);\n",
      "  if (LIKELY(ret != nullptr))\n",
      "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void* NodeArrayBufferAllocator::Reallocate(\n",
      "    void* data, size_t old_size, size_t size) {\n",
      "  void* ret = UncheckedRealloc<char>(static_cast<char*>(data), size);\n",
      "  if (LIKELY(ret != nullptr) || UNLIKELY(size == 0))\n",
      "    total_mem_usage_.fetch_add(size - old_size, std::memory_order_relaxed);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void NodeArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  total_mem_usage_.fetch_sub(size, std::memory_order_relaxed);\n",
      "  free(data);\n",
      "}\n",
      "\n",
      "DebuggingArrayBufferAllocator::~DebuggingArrayBufferAllocator() {\n",
      "  CHECK(allocations_.empty());\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Allocate(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::Allocate(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* data = NodeArrayBufferAllocator::AllocateUninitialized(size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "  return data;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::Free(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "  NodeArrayBufferAllocator::Free(data, size);\n",
      "}\n",
      "\n",
      "void* DebuggingArrayBufferAllocator::Reallocate(void* data,\n",
      "                                                size_t old_size,\n",
      "                                                size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  void* ret = NodeArrayBufferAllocator::Reallocate(data, old_size, size);\n",
      "  if (ret == nullptr) {\n",
      "    if (size == 0)  // i.e. equivalent to free().\n",
      "      UnregisterPointerInternal(data, old_size);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  if (data != nullptr) {\n",
      "    auto it = allocations_.find(data);\n",
      "    CHECK_NE(it, allocations_.end());\n",
      "    allocations_.erase(it);\n",
      "  }\n",
      "\n",
      "  RegisterPointerInternal(ret, size);\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::RegisterPointer(data, size);\n",
      "  RegisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointer(void* data, size_t size) {\n",
      "  Mutex::ScopedLock lock(mutex_);\n",
      "  NodeArrayBufferAllocator::UnregisterPointer(data, size);\n",
      "  UnregisterPointerInternal(data, size);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::UnregisterPointerInternal(void* data,\n",
      "                                                              size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  auto it = allocations_.find(data);\n",
      "  CHECK_NE(it, allocations_.end());\n",
      "  if (size > 0) {\n",
      "    // We allow allocations with size 1 for 0-length buffers to avoid having\n",
      "    // to deal with nullptr values.\n",
      "    CHECK_EQ(it->second, size);\n",
      "  }\n",
      "  allocations_.erase(it);\n",
      "}\n",
      "\n",
      "void DebuggingArrayBufferAllocator::RegisterPointerInternal(void* data,\n",
      "                                                            size_t size) {\n",
      "  if (data == nullptr) return;\n",
      "  CHECK_EQ(allocations_.count(data), 0);\n",
      "  allocations_[data] = size;\n",
      "}\n",
      "\n",
      "std::unique_ptr<ArrayBufferAllocator> ArrayBufferAllocator::Create(bool debug) {\n",
      "  if (debug || per_process::cli_options->debug_arraybuffer_allocations)\n",
      "    return std::make_unique<DebuggingArrayBufferAllocator>();\n",
      "  else\n",
      "    return std::make_unique<NodeArrayBufferAllocator>();\n",
      "}\n",
      "\n",
      "ArrayBufferAllocator* CreateArrayBufferAllocator() {\n",
      "  return ArrayBufferAllocator::Create().release();\n",
      "}\n",
      "\n",
      "void FreeArrayBufferAllocator(ArrayBufferAllocator* allocator) {\n",
      "  delete allocator;\n",
      "}\n",
      "\n",
      "void SetIsolateCreateParamsForNode(Isolate::CreateParams* params) {\n",
      "  const uint64_t constrained_memory = uv_get_constrained_memory();\n",
      "  const uint64_t total_memory = constrained_memory > 0 ?\n",
      "      std::min(uv_get_total_memory(), constrained_memory) :\n",
      "      uv_get_total_memory();\n",
      "  if (total_memory > 0) {\n",
      "    // V8 defaults to 700MB or 1.4GB on 32 and 64 bit platforms respectively.\n",
      "    // This default is based on browser use-cases. Tell V8 to configure the\n",
      "    // heap based on the actual physical memory.\n",
      "    params->constraints.ConfigureDefaults(total_memory, 0);\n",
      "  }\n",
      "  params->embedder_wrapper_object_index = BaseObject::InternalFields::kSlot;\n",
      "  params->embedder_wrapper_type_index = std::numeric_limits<int>::max();\n",
      "}\n",
      "\n",
      "void SetIsolateErrorHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  if (s.flags & MESSAGE_LISTENER_WITH_ERROR_LEVEL)\n",
      "    isolate->AddMessageListenerWithErrorLevel(\n",
      "            errors::PerIsolateMessageListener,\n",
      "            Isolate::MessageErrorLevel::kMessageError |\n",
      "                Isolate::MessageErrorLevel::kMessageWarning);\n",
      "\n",
      "  auto* abort_callback = s.should_abort_on_uncaught_exception_callback ?\n",
      "      s.should_abort_on_uncaught_exception_callback :\n",
      "      ShouldAbortOnUncaughtException;\n",
      "  isolate->SetAbortOnUncaughtExceptionCallback(abort_callback);\n",
      "\n",
      "  auto* fatal_error_cb = s.fatal_error_callback ?\n",
      "      s.fatal_error_callback : OnFatalError;\n",
      "  isolate->SetFatalErrorHandler(fatal_error_cb);\n",
      "\n",
      "  auto* prepare_stack_trace_cb = s.prepare_stack_trace_callback ?\n",
      "      s.prepare_stack_trace_callback : PrepareStackTraceCallback;\n",
      "  isolate->SetPrepareStackTraceCallback(prepare_stack_trace_cb);\n",
      "}\n",
      "\n",
      "void SetIsolateMiscHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
      "  isolate->SetMicrotasksPolicy(s.policy);\n",
      "\n",
      "  auto* allow_wasm_codegen_cb = s.allow_wasm_code_generation_callback ?\n",
      "    s.allow_wasm_code_generation_callback : AllowWasmCodeGenerationCallback;\n",
      "  isolate->SetAllowWasmCodeGenerationCallback(allow_wasm_codegen_cb);\n",
      "\n",
      "  if ((s.flags & SHOULD_NOT_SET_PROMISE_REJECTION_CALLBACK) == 0) {\n",
      "    auto* promise_reject_cb = s.promise_reject_callback ?\n",
      "      s.promise_reject_callback : task_queue::PromiseRejectCallback;\n",
      "    isolate->SetPromiseRejectCallback(promise_reject_cb);\n",
      "  }\n",
      "\n",
      "  if (s.flags & DETAILED_SOURCE_POSITIONS_FOR_PROFILING)\n",
      "    v8::CpuProfiler::UseDetailedSourcePositionsForProfiling(isolate);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate,\n",
      "                         const IsolateSettings& settings) {\n",
      "  SetIsolateErrorHandlers(isolate, settings);\n",
      "  SetIsolateMiscHandlers(isolate, settings);\n",
      "}\n",
      "\n",
      "void SetIsolateUpForNode(v8::Isolate* isolate) {\n",
      "  IsolateSettings settings;\n",
      "  SetIsolateUpForNode(isolate, settings);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator, uv_loop_t* event_loop) {\n",
      "  return NewIsolate(allocator, event_loop, GetMainThreadMultiIsolatePlatform());\n",
      "}\n",
      "\n",
      "// TODO(joyeecheung): we may want to expose this, but then we need to be\n",
      "// careful about what we override in the params.\n",
      "Isolate* NewIsolate(Isolate::CreateParams* params,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate* isolate = Isolate::Allocate();\n",
      "  if (isolate == nullptr) return nullptr;\n",
      "\n",
      "  // Register the isolate on the platform before the isolate gets initialized,\n",
      "  // so that the isolate can access the platform during initialization.\n",
      "  platform->RegisterIsolate(isolate, event_loop);\n",
      "\n",
      "  SetIsolateCreateParamsForNode(params);\n",
      "  Isolate::Initialize(isolate, *params);\n",
      "  SetIsolateUpForNode(isolate);\n",
      "\n",
      "  return isolate;\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(ArrayBufferAllocator* allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator != nullptr) params.array_buffer_allocator = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "Isolate* NewIsolate(std::shared_ptr<ArrayBufferAllocator> allocator,\n",
      "                    uv_loop_t* event_loop,\n",
      "                    MultiIsolatePlatform* platform) {\n",
      "  Isolate::CreateParams params;\n",
      "  if (allocator) params.array_buffer_allocator_shared = allocator;\n",
      "  return NewIsolate(&params, event_loop, platform);\n",
      "}\n",
      "\n",
      "IsolateData* CreateIsolateData(Isolate* isolate,\n",
      "                               uv_loop_t* loop,\n",
      "                               MultiIsolatePlatform* platform,\n",
      "                               ArrayBufferAllocator* allocator) {\n",
      "  return new IsolateData(isolate, loop, platform, allocator);\n",
      "}\n",
      "\n",
      "void FreeIsolateData(IsolateData* isolate_data) {\n",
      "  delete isolate_data;\n",
      "}\n",
      "\n",
      "InspectorParentHandle::~InspectorParentHandle() {}\n",
      "\n",
      "// Hide the internal handle class from the public API.\n",
      "#if HAVE_INSPECTOR\n",
      "struct InspectorParentHandleImpl : public InspectorParentHandle {\n",
      "  std::unique_ptr<inspector::ParentInspectorHandle> impl;\n",
      "\n",
      "  explicit InspectorParentHandleImpl(\n",
      "      std::unique_ptr<inspector::ParentInspectorHandle>&& impl)\n",
      "    : impl(std::move(impl)) {}\n",
      "};\n",
      "#endif\n",
      "\n",
      "Environment* CreateEnvironment(IsolateData* isolate_data,\n",
      "                               Local<Context> context,\n",
      "                               int argc,\n",
      "                               const char* const* argv,\n",
      "                               int exec_argc,\n",
      "                               const char* const* exec_argv) {\n",
      "  return CreateEnvironment(\n",
      "      isolate_data, context,\n",
      "      std::vector<std::string>(argv, argv + argc),\n",
      "      std::vector<std::string>(exec_argv, exec_argv + exec_argc));\n",
      "}\n",
      "\n",
      "Environment* CreateEnvironment(\n",
      "    IsolateData* isolate_data,\n",
      "    Local<Context> context,\n",
      "    const std::vector<std::string>& args,\n",
      "    const std::vector<std::string>& exec_args,\n",
      "    EnvironmentFlags::Flags flags,\n",
      "    ThreadId thread_id,\n",
      "    std::unique_ptr<InspectorParentHandle> inspector_parent_handle) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Context::Scope context_scope(context);\n",
      "  // TODO(addaleax): This is a much better place for parsing per-Environment\n",
      "  // options than the global parse call.\n",
      "  Environment* env = new Environment(\n",
      "      isolate_data, context, args, exec_args, nullptr, flags, thread_id);\n",
      "#if HAVE_INSPECTOR\n",
      "  if (inspector_parent_handle) {\n",
      "    env->InitializeInspector(\n",
      "        std::move(static_cast<InspectorParentHandleImpl*>(\n",
      "            inspector_parent_handle.get())->impl));\n",
      "  } else {\n",
      "    env->InitializeInspector({});\n",
      "  }\n",
      "#endif\n",
      "\n",
      "  if (env->RunBootstrapping().IsEmpty()) {\n",
      "    FreeEnvironment(env);\n",
      "    return nullptr;\n",
      "  }\n",
      "\n",
      "  return env;\n",
      "}\n",
      "\n",
      "void FreeEnvironment(Environment* env) {\n",
      "  Isolate::DisallowJavascriptExecutionScope disallow_js(env->isolate(),\n",
      "      Isolate::DisallowJavascriptExecutionScope::THROW_ON_FAILURE);\n",
      "  {\n",
      "    HandleScope handle_scope(env->isolate());  // For env->context().\n",
      "    Context::Scope context_scope(env->context());\n",
      "    SealHandleScope seal_handle_scope(env->isolate());\n",
      "\n",
      "    env->set_stopping(true);\n",
      "    env->stop_sub_worker_contexts();\n",
      "    env->RunCleanup();\n",
      "    RunAtExit(env);\n",
      "  }\n",
      "\n",
      "  // This call needs to be made while the `Environment` is still alive\n",
      "  // because we assume that it is available for async tracking in the\n",
      "  // NodePlatform implementation.\n",
      "  MultiIsolatePlatform* platform = env->isolate_data()->platform();\n",
      "  if (platform != nullptr)\n",
      "    platform->DrainTasks(env->isolate());\n",
      "\n",
      "  delete env;\n",
      "}\n",
      "\n",
      "NODE_EXTERN std::unique_ptr<InspectorParentHandle> GetInspectorParentHandle(\n",
      "    Environment* env,\n",
      "    ThreadId thread_id,\n",
      "    const char* url) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  CHECK_NE(thread_id.id, static_cast<uint64_t>(-1));\n",
      "#if HAVE_INSPECTOR\n",
      "  return std::make_unique<InspectorParentHandleImpl>(\n",
      "      env->inspector_agent()->GetParentHandle(thread_id.id, url));\n",
      "#else\n",
      "  return {};\n",
      "#endif\n",
      "}\n",
      "\n",
      "void LoadEnvironment(Environment* env) {\n",
      "  USE(LoadEnvironment(env,\n",
      "                      StartExecutionCallback{},\n",
      "                      {}));\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    StartExecutionCallback cb,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  env->InitializeLibuv();\n",
      "  env->InitializeDiagnostics();\n",
      "\n",
      "  return StartExecution(env, cb);\n",
      "}\n",
      "\n",
      "MaybeLocal<Value> LoadEnvironment(\n",
      "    Environment* env,\n",
      "    const char* main_script_source_utf8,\n",
      "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
      "  CHECK_NOT_NULL(main_script_source_utf8);\n",
      "  return LoadEnvironment(\n",
      "      env,\n",
      "      [&](const StartExecutionCallbackInfo& info) -> MaybeLocal<Value> {\n",
      "        // This is a slightly hacky way to convert UTF-8 to UTF-16.\n",
      "        Local<String> str =\n",
      "            String::NewFromUtf8(env->isolate(),\n",
      "                                main_script_source_utf8).ToLocalChecked();\n",
      "        auto main_utf16 = std::make_unique<String::Value>(env->isolate(), str);\n",
      "\n",
      "        // TODO(addaleax): Avoid having a global table for all scripts.\n",
      "        std::string name = \"embedder_main_\" + std::to_string(env->thread_id());\n",
      "        native_module::NativeModuleEnv::Add(\n",
      "            name.c_str(),\n",
      "            UnionBytes(**main_utf16, main_utf16->length()));\n",
      "        env->set_main_utf16(std::move(main_utf16));\n",
      "        std::vector<Local<String>> params = {\n",
      "            env->process_string(),\n",
      "            env->require_string()};\n",
      "        std::vector<Local<Value>> args = {\n",
      "            env->process_object(),\n",
      "            env->native_module_require()};\n",
      "        return ExecuteBootstrapper(env, name.c_str(), &params, &args);\n",
      "      });\n",
      "}\n",
      "\n",
      "Environment* GetCurrentEnvironment(Local<Context> context) {\n",
      "  return Environment::GetCurrent(context);\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMainThreadMultiIsolatePlatform() {\n",
      "  return per_process::v8_platform.Platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(Environment* env) {\n",
      "  return GetMultiIsolatePlatform(env->isolate_data());\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* GetMultiIsolatePlatform(IsolateData* env) {\n",
      "  return env->platform();\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    node::tracing::TracingController* tracing_controller) {\n",
      "  return CreatePlatform(\n",
      "      thread_pool_size,\n",
      "      static_cast<v8::TracingController*>(tracing_controller));\n",
      "}\n",
      "\n",
      "MultiIsolatePlatform* CreatePlatform(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return MultiIsolatePlatform::Create(thread_pool_size, tracing_controller)\n",
      "      .release();\n",
      "}\n",
      "\n",
      "void FreePlatform(MultiIsolatePlatform* platform) {\n",
      "  delete platform;\n",
      "}\n",
      "\n",
      "std::unique_ptr<MultiIsolatePlatform> MultiIsolatePlatform::Create(\n",
      "    int thread_pool_size,\n",
      "    v8::TracingController* tracing_controller) {\n",
      "  return std::make_unique<NodePlatform>(thread_pool_size, tracing_controller);\n",
      "}\n",
      "\n",
      "MaybeLocal<Object> GetPerContextExports(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  EscapableHandleScope handle_scope(isolate);\n",
      "\n",
      "  Local<Object> global = context->Global();\n",
      "  Local<Private> key = Private::ForApi(isolate,\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"node:per_context_binding_exports\"));\n",
      "\n",
      "  Local<Value> existing_value;\n",
      "  if (!global->GetPrivate(context, key).ToLocal(&existing_value))\n",
      "    return MaybeLocal<Object>();\n",
      "  if (existing_value->IsObject())\n",
      "    return handle_scope.Escape(existing_value.As<Object>());\n",
      "\n",
      "  Local<Object> exports = Object::New(isolate);\n",
      "  if (context->Global()->SetPrivate(context, key, exports).IsNothing() ||\n",
      "      !InitializePrimordials(context))\n",
      "    return MaybeLocal<Object>();\n",
      "  return handle_scope.Escape(exports);\n",
      "}\n",
      "\n",
      "// Any initialization logic should be performed in\n",
      "// InitializeContext, because embedders don't necessarily\n",
      "// call NewContext and so they will experience breakages.\n",
      "Local<Context> NewContext(Isolate* isolate,\n",
      "                          Local<ObjectTemplate> object_template) {\n",
      "  auto context = Context::New(isolate, nullptr, object_template);\n",
      "  if (context.IsEmpty()) return context;\n",
      "\n",
      "  if (!InitializeContext(context)) {\n",
      "    return Local<Context>();\n",
      "  }\n",
      "\n",
      "  return context;\n",
      "}\n",
      "\n",
      "void ProtoThrower(const FunctionCallbackInfo<Value>& info) {\n",
      "  THROW_ERR_PROTO_ACCESS(info.GetIsolate());\n",
      "}\n",
      "\n",
      "// This runs at runtime, regardless of whether the context\n",
      "// is created from a snapshot.\n",
      "void InitializeContextRuntime(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  // Delete `Intl.v8BreakIterator`\n",
      "  // https://github.com/nodejs/node/issues/14909\n",
      "  Local<String> intl_string = FIXED_ONE_BYTE_STRING(isolate, \"Intl\");\n",
      "  Local<String> break_iter_string =\n",
      "    FIXED_ONE_BYTE_STRING(isolate, \"v8BreakIterator\");\n",
      "  Local<Value> intl_v;\n",
      "  if (context->Global()->Get(context, intl_string).ToLocal(&intl_v) &&\n",
      "      intl_v->IsObject()) {\n",
      "    Local<Object> intl = intl_v.As<Object>();\n",
      "    intl->Delete(context, break_iter_string).Check();\n",
      "  }\n",
      "\n",
      "  // Delete `Atomics.wake`\n",
      "  // https://github.com/nodejs/node/issues/21219\n",
      "  Local<String> atomics_string = FIXED_ONE_BYTE_STRING(isolate, \"Atomics\");\n",
      "  Local<String> wake_string = FIXED_ONE_BYTE_STRING(isolate, \"wake\");\n",
      "  Local<Value> atomics_v;\n",
      "  if (context->Global()->Get(context, atomics_string).ToLocal(&atomics_v) &&\n",
      "      atomics_v->IsObject()) {\n",
      "    Local<Object> atomics = atomics_v.As<Object>();\n",
      "    atomics->Delete(context, wake_string).Check();\n",
      "  }\n",
      "\n",
      "  // Remove __proto__\n",
      "  // https://github.com/nodejs/node/issues/31951\n",
      "  Local<String> object_string = FIXED_ONE_BYTE_STRING(isolate, \"Object\");\n",
      "  Local<String> prototype_string = FIXED_ONE_BYTE_STRING(isolate, \"prototype\");\n",
      "  Local<Object> prototype = context->Global()\n",
      "                                ->Get(context, object_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>()\n",
      "                                ->Get(context, prototype_string)\n",
      "                                .ToLocalChecked()\n",
      "                                .As<Object>();\n",
      "  Local<String> proto_string = FIXED_ONE_BYTE_STRING(isolate, \"__proto__\");\n",
      "  if (per_process::cli_options->disable_proto == \"delete\") {\n",
      "    prototype->Delete(context, proto_string).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto == \"throw\") {\n",
      "    Local<Value> thrower =\n",
      "        Function::New(context, ProtoThrower).ToLocalChecked();\n",
      "    PropertyDescriptor descriptor(thrower, thrower);\n",
      "    descriptor.set_enumerable(false);\n",
      "    descriptor.set_configurable(true);\n",
      "    prototype->DefineProperty(context, proto_string, descriptor).ToChecked();\n",
      "  } else if (per_process::cli_options->disable_proto != \"\") {\n",
      "    // Validated in ProcessGlobalArgs\n",
      "    FatalError(\"InitializeContextRuntime()\", \"invalid --disable-proto mode\");\n",
      "  }\n",
      "}\n",
      "\n",
      "bool InitializeContextForSnapshot(Local<Context> context) {\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  HandleScope handle_scope(isolate);\n",
      "\n",
      "  context->SetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration,\n",
      "                           True(isolate));\n",
      "  return InitializePrimordials(context);\n",
      "}\n",
      "\n",
      "bool InitializePrimordials(Local<Context> context) {\n",
      "  // Run per-context JS files.\n",
      "  Isolate* isolate = context->GetIsolate();\n",
      "  Context::Scope context_scope(context);\n",
      "  Local<Object> exports;\n",
      "\n",
      "  Local<String> primordials_string =\n",
      "      FIXED_ONE_BYTE_STRING(isolate, \"primordials\");\n",
      "  Local<String> global_string = FIXED_ONE_BYTE_STRING(isolate, \"global\");\n",
      "  Local<String> exports_string = FIXED_ONE_BYTE_STRING(isolate, \"exports\");\n",
      "\n",
      "  // Create primordials first and make it available to per-context scripts.\n",
      "  Local<Object> primordials = Object::New(isolate);\n",
      "  if (!primordials->SetPrototype(context, Null(isolate)).FromJust() ||\n",
      "      !GetPerContextExports(context).ToLocal(&exports) ||\n",
      "      !exports->Set(context, primordials_string, primordials).FromJust()) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  static const char* context_files[] = {\"internal/per_context/primordials\",\n",
      "                                        \"internal/per_context/domexception\",\n",
      "                                        \"internal/per_context/messageport\",\n",
      "                                        nullptr};\n",
      "\n",
      "  for (const char** module = context_files; *module != nullptr; module++) {\n",
      "    std::vector<Local<String>> parameters = {\n",
      "        global_string, exports_string, primordials_string};\n",
      "    Local<Value> arguments[] = {context->Global(), exports, primordials};\n",
      "    MaybeLocal<Function> maybe_fn =\n",
      "        native_module::NativeModuleEnv::LookupAndCompile(\n",
      "            context, *module, &parameters, nullptr);\n",
      "    Local<Function> fn;\n",
      "    if (!maybe_fn.ToLocal(&fn)) {\n",
      "      return false;\n",
      "    }\n",
      "    MaybeLocal<Value> result =\n",
      "        fn->Call(context, Undefined(isolate), arraysize(arguments), arguments);\n",
      "    // Execution failed during context creation.\n",
      "    // TODO(joyeecheung): deprecate this signature and return a MaybeLocal.\n",
      "    if (result.IsEmpty()) {\n",
      "      return false;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return true;\n",
      "}\n",
      "\n",
      "bool InitializeContext(Local<Context> context) {\n",
      "  if (!InitializeContextForSnapshot(context)) {\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  InitializeContextRuntime(context);\n",
      "  return true;\n",
      "}\n",
      "\n",
      "uv_loop_t* GetCurrentEventLoop(Isolate* isolate) {\n",
      "  HandleScope handle_scope(isolate);\n",
      "  Local<Context> context = isolate->GetCurrentContext();\n",
      "  if (context.IsEmpty()) return nullptr;\n",
      "  Environment* env = Environment::GetCurrent(context);\n",
      "  if (env == nullptr) return nullptr;\n",
      "  return env->event_loop();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env, const node_module& mod) {\n",
      "  CHECK_NOT_NULL(env);\n",
      "  Mutex::ScopedLock lock(env->extra_linked_bindings_mutex());\n",
      "\n",
      "  node_module* prev_head = env->extra_linked_bindings_head();\n",
      "  env->extra_linked_bindings()->push_back(mod);\n",
      "  if (prev_head != nullptr)\n",
      "    prev_head->nm_link = &env->extra_linked_bindings()->back();\n",
      "}\n",
      "\n",
      "void AddLinkedBinding(Environment* env,\n",
      "                      const char* name,\n",
      "                      addon_context_register_func fn,\n",
      "                      void* priv) {\n",
      "  node_module mod = {\n",
      "    NODE_MODULE_VERSION,\n",
      "    NM_F_LINKED,\n",
      "    nullptr,  // nm_dso_handle\n",
      "    nullptr,  // nm_filename\n",
      "    nullptr,  // nm_register_func\n",
      "    fn,\n",
      "    name,\n",
      "    priv,\n",
      "    nullptr   // nm_link\n",
      "  };\n",
      "  AddLinkedBinding(env, mod);\n",
      "}\n",
      "\n",
      "static std::atomic<uint64_t> next_thread_id{0};\n",
      "\n",
      "ThreadId AllocateEnvironmentThreadId() {\n",
      "  return ThreadId { next_thread_id++ };\n",
      "}\n",
      "\n",
      "void DefaultProcessExitHandler(Environment* env, int exit_code) {\n",
      "  env->set_can_call_into_js(false);\n",
      "  env->stop_sub_worker_contexts();\n",
      "  DisposePlatform();\n",
      "  exit(exit_code);\n",
      "}\n",
      "\n",
      "\n",
      "void SetProcessExitHandler(Environment* env,\n",
      "                           std::function<void(Environment*, int)>&& handler) {\n",
      "  env->set_process_exit_handler(std::move(handler));\n",
      "}\n",
      "\n",
      "}  // namespace node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(content_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = '''\n",
    "A crawler for the Semantic Scholar API.\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import logging.config\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "import httpx  # https://github.com/encode/httpx\n",
    "import requests\n",
    "from crawl_utils import get_batch_url, get_reference_url\n",
    "from db import MongoDBClient\n",
    "\n",
    "from config import S2_API_KEY, S2_RATE_LIMIT\n",
    "\n",
    "logging.config.fileConfig(fname=\"logging.conf\", disable_existing_loggers=False)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RateLimitExceededException(Exception):\n",
    "    \"\"\"Exception raised when rate limit is exceeded\"\"\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"RateLimitExceededException: {self.message}\"\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    \"\"\"Exception raised when a request times out\"\"\"\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"TimeoutException: {self.message}\"\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class Crawler:\n",
    "    \"\"\"A crawler for the Semantic Scholar API\"\"\"\n",
    "\n",
    "    client: httpx.AsyncClient = field(repr=False)\n",
    "    initial_papers: List[str] = field(default_factory=list)\n",
    "    num_workers: int = 10\n",
    "    max_papers: int = 100\n",
    "    mongodb_client: MongoDBClient = field(default_factory=MongoDBClient)\n",
    "    headers: dict = field(repr=False, default_factory=dict)\n",
    "    todo: asyncio.Queue = field(init=False, repr=False, default_factory=asyncio.Queue)\n",
    "    seen: Set[str]= field(init=False, default_factory=set)\n",
    "    done: Set[str] = field(init=False, default_factory=set)\n",
    "    retry: Dict[str, int] = field(init=False, default_factory=dict)\n",
    "    total: int = field(init=False, default=0)\n",
    "    MAX_RETRIES: int = field(init=False, default=3)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, settings: dict) -> \"Crawler\":\n",
    "        \"\"\"\n",
    "        Create a Crawler instance from a dict of settings\"\"\"\n",
    "        return cls(**settings)\n",
    "\n",
    "    async def run(self) -> None:\n",
    "        \"\"\"Run the crawler by creating workers until todo queue is empty\"\"\"\n",
    "        self.init_done()\n",
    "        await self.init_queue()\n",
    "        workers = [asyncio.create_task(self.worker()) for _ in range(self.num_workers)]\n",
    "        await self.todo.join()\n",
    "        for worker in workers:\n",
    "            worker.cancel()\n",
    "\n",
    "    async def init_queue(self) -> None:\n",
    "        \"\"\"Initialize the queue with the initial papers\"\"\"\n",
    "        batch_url = get_batch_url()\n",
    "        data = json.dumps({\"ids\": self.initial_papers})\n",
    "        response = requests.post(url=batch_url, data=data, headers=self.headers, timeout=10)\n",
    "        # initial_paper_id = self.initial_papers[0]\n",
    "        # initial_url = get_paper_url(initial_paper_id)\n",
    "        # response = requests.get(initial_url, headers=self.headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            logger.error(\"Error fetching initial papers\")\n",
    "            sys.exit(1)\n",
    "        logger.debug(f\"Fetching data for intial papers {self.initial_papers}\")\n",
    "        result_data = response.json()\n",
    "        # result_data[\"_id\"] = result_data[\"paperId\"]\n",
    "        for paper in result_data:\n",
    "            paper[\"_id\"] = paper[\"paperId\"]\n",
    "        # prime the queue\n",
    "        await self.on_found_papers(result_data, initial=True)\n",
    "\n",
    "    def init_done(self) -> None:\n",
    "        \"\"\"Initialize the seen set with already stored papers from DB\"\"\"\n",
    "        # self.seen = set(self.initial_papers)\n",
    "        self.done = self.mongodb_client.get_ids()\n",
    "        logger.info(f\"Already stored {len(self.done)} papers\")\n",
    "\n",
    "    async def worker(self) -> None:\n",
    "        \"\"\"One worker processes one paper at a time from the queue in a loop until cancelled\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                await self.process_one()\n",
    "            except asyncio.CancelledError:\n",
    "                return\n",
    "\n",
    "    async def retry_crawl(self, paper) -> None:\n",
    "        \"\"\"Retry crawling a paper in case of an exception\"\"\"\n",
    "        if paper[\"_id\"] in self.retry and self.retry[paper[\"_id\"]] > self.MAX_RETRIES:\n",
    "            logger.error(f\"Error processing {paper['_id']} even after retrying {self.MAX_RETRIES} times\")\n",
    "            return\n",
    "        # self.retry.add(paper[\"_id\"])\n",
    "        self.retry[paper[\"_id\"]] = self.retry.get(paper[\"_id\"], 0) + 1\n",
    "        logger.info(f\"Retry #{self.retry[paper['_id']]} for {paper['_id']}\")\n",
    "        # await self.todo.put_nowait(cur_paper)\n",
    "        await asyncio.sleep(1)\n",
    "        await self.crawl(paper)\n",
    "\n",
    "    async def process_one(self) -> None:\n",
    "        \"\"\"Gets one paper from the queue and processes it\"\"\"\n",
    "        # cur_paper is a dict\n",
    "        cur_paper = await self.todo.get()\n",
    "        try:\n",
    "            await self.crawl(cur_paper)\n",
    "        except TimeoutException as te:\n",
    "            # logger.warning(f\"Timeout for {cur_paper['_id']}\")\n",
    "            logger.warning(te)\n",
    "            await self.retry_crawl(cur_paper)\n",
    "        except RateLimitExceededException as rlee:\n",
    "            logger.critical(\"Rate limit exceeded, retrying in 2 second\")\n",
    "            logger.critical(rlee)\n",
    "            await asyncio.sleep(2)\n",
    "            await self.retry_crawl(cur_paper)\n",
    "        finally:\n",
    "            self.todo.task_done()\n",
    "\n",
    "    async def crawl(self, cur_paper: dict) -> None:\n",
    "        \"\"\"\n",
    "        Crawl a paper and its references, stores them in the database.\n",
    "        \"\"\"\n",
    "        # TODO proper rate limiting to 100 requests / second\n",
    "        # await asyncio.sleep(1 / self.num_workers)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "        cur_paper_id = cur_paper[\"paperId\"]\n",
    "        ref_url = get_reference_url(cur_paper_id)\n",
    "        cur_paper[\"_id\"] = cur_paper_id\n",
    "        if cur_paper[\"title\"] is None or cur_paper[\"abstract\"] is None:\n",
    "            logger.debug(f\"Skipping {cur_paper_id} as empty title or abstract\")\n",
    "            # I have no clue why this total -= 1 is here, it shouldn't be required, but crawler just prematurely stops\n",
    "            self.total -= 1\n",
    "            return\n",
    "        # async with self.semaphore:\n",
    "        # async with self.client.get(ref_url, headers=self.headers) as response:\n",
    "\n",
    "        response = await self.client.get(ref_url, headers=self.headers)\n",
    "\n",
    "        # if self.semaphore.locked():\n",
    "        #     logger.warning(f\"Semaphore locked for {cur_paper_id}\")\n",
    "        #     await asyncio.sleep(1)\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            # logger.critical(\n",
    "            #     f\"Rated limited for {cur_paper_id} - {response.status_code}\"\n",
    "            # )\n",
    "            # # await self.todo.put_nowait(cur_paper)\n",
    "            # await asyncio.sleep(1)\n",
    "            # await self.crawl(cur_paper)\n",
    "            raise RateLimitExceededException(\n",
    "                f\"Rated limited for {cur_paper_id} - {response.status_code}\"\n",
    "            )\n",
    "\n",
    "        if response.status_code == 504:\n",
    "            # raise asyncio.exceptions.TimeoutError(\n",
    "            #     f\"Timeout for {cur_paper_id} - {response.status_code}\"\n",
    "            # )\n",
    "            raise TimeoutException(f\"Timeout for {cur_paper_id} - {response.status_code}\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Error fetching references for {cur_paper_id} - {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        logger.debug(f\"Fetching references for {cur_paper_id} - {response.status_code}\")\n",
    "\n",
    "        result_data = response.json()\n",
    "        found_references = result_data[\"data\"]\n",
    "        found_references = [ref[\"citedPaper\"] for ref in found_references]\n",
    "        found_references = sorted(found_references, key=lambda x: x[\"citationCount\"] or 0, reverse=True)\n",
    "        ref_ids = [ref[\"paperId\"] for ref in found_references if ref[\"paperId\"] is not None]\n",
    "        cur_paper[\"references\"] = ref_ids\n",
    "        cur_paper[\"allReferencesStored\"] = True\n",
    "        if len(ref_ids) != cur_paper[\"referenceCount\"]:\n",
    "            cur_paper[\"allReferencesStored\"] = False\n",
    "\n",
    "        # self.collection.insert_one(cur_paper)\n",
    "        self.mongodb_client.insert_one(cur_paper)\n",
    "        self.done.add(cur_paper[\"paperId\"])\n",
    "        # self.stored += 1\n",
    "        # if self.stored % 100 == 0:\n",
    "        #     logger.info(f\"Stored {self.stored} papers\")\n",
    "\n",
    "        await self.on_found_papers(found_references)\n",
    "\n",
    "    # async def get_paper_references(self, base: str, text: str) -> set[str]:\n",
    "    #     parser = UrlParser(base, self.filter_url)\n",
    "    #     parser.feed(text)\n",
    "    #     return parser.found_references\n",
    "\n",
    "    async def on_found_papers(self, papers: List[dict], initial: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Called when new papers are found. Filters out papers that have already been seen and puts the new ones in the queue.\n",
    "        \"\"\"\n",
    "        if initial:\n",
    "            for paper in papers:\n",
    "                await self.put_todo(paper)\n",
    "            return\n",
    "        ids = {paper[\"paperId\"] for paper in papers if paper[\"paperId\"] is not None}\n",
    "        new = ids - self.seen\n",
    "        self.seen.update(new)\n",
    "\n",
    "        for paper in papers:\n",
    "            if paper[\"paperId\"] in new:\n",
    "                await self.put_todo(paper)\n",
    "\n",
    "    async def put_todo(self, paper: dict) -> None:\n",
    "        \"\"\"Put a paper in the queue\"\"\"\n",
    "        # paper is a dict with fields like paper_id, title, abstract, etc.\n",
    "        if self.total >= self.max_papers:\n",
    "            return\n",
    "        self.total += 1\n",
    "        await self.todo.put(paper)\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    \"\"\"Main function\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    headers={\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"x-api-key\": S2_API_KEY,\n",
    "    }\n",
    "    mongodb_client = MongoDBClient(mongo_url='mongodb://localhost:27017', db_name='refpred', collection_name='review3_demo', init_new=True)\n",
    "    timeout = httpx.Timeout(10, connect=10, read=None, write=10)\n",
    "    # based on https://towardsdatascience.com/top-10-research-papers-in-ai-1f02cf844e26\n",
    "    initial_papers = [\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"bee044c8e8903fb67523c1f8c105ab4718600cdb\", \"36eff562f65125511b5dfab68ce7f7a943c27478\", \"8388f1be26329fa45e5807e968a641ce170ea078\", \"846aedd869a00c09b40f1f1f35673cb22bc87490\", \"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\", \"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\", \"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\", \"4d376d6978dad0374edfa6709c9556b42d3594d3\", \"a6cb366736791bcccc5c8639de5a8f9636bf87e8\", \"df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"913f54b44dfb9202955fe296cf5586e1105565ea\", \"156d217b0a911af97fa1b5a71dc909ccef7a8028\", \"a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031\", \"5c5751d45e298cea054f32b392c12c61027d2fe7\", \"bc1586a2e74d6d1cf87b083c4cbd1eede2b09ea5\", \"921b2958cac4138d188fd5047aa12bbcf37ac867\", \"cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1\"]\n",
    "    MAX_PAPERS = 10000\n",
    "    async with httpx.AsyncClient(timeout=timeout) as client:\n",
    "        # starting with the famous paper 'Attention is all you need'\n",
    "        crawler = Crawler(\n",
    "            client=client,\n",
    "            initial_papers=initial_papers,\n",
    "            num_workers=S2_RATE_LIMIT,\n",
    "            max_papers=MAX_PAPERS,\n",
    "            mongodb_client=mongodb_client,\n",
    "            headers=headers,\n",
    "        )\n",
    "        await crawler.run()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    logger.info(\"Results:\")\n",
    "    logger.info(f\"Crawled: {len(crawler.done)} Papers\")\n",
    "    logger.info(f\"Found: {len(crawler.seen)} Papers\")\n",
    "    logger.info(f\"Done in {end - start:.2f}s\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n",
    "# TODO\n",
    "# 1. Batch processing of seed papers\n",
    "# 2. Initialize seen from dataset to avoid restarting over\n",
    "# 3. Null abstract papers need to be removed from the dataset ✅'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_msg = '''# A simple hello world program in python with docstring\n",
    "def hello_world():\n",
    "    \"\"\"A simple hello world program in python with docstring\"\"\"\n",
    "    print(\"Hello World!\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 362, 4382, 24748, 1917, 2068, 304, 10344, 449, 4733]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see tokenized output of the above code\n",
    "enc.encode(simple_msg)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 317, 2829, 23748, 995, 1430, 287, 21015, 351, 2205]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see tokenized output of the above code\n",
    "enc.encode(simple_msg)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
