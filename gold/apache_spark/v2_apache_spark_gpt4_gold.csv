commit_id,commit_date,commit_message,actual_files_modified,transformed_message_gpt4
3a94fb3dd92a05676a3c11cbcea314dd296ec059,1562806654,"[SPARK-28281][SQL][PYTHON][TESTS] Convert and port 'having.sql' into UDF test base

## What changes were proposed in this pull request?

This PR adds some tests converted from having.sql to test UDFs following the combination guide in [SPARK-27921](url)
<details><summary>Diff comparing to 'having.sql'</summary>
<p>

```diff
diff --git a/sql/core/src/test/resources/sql-tests/results/having.sql.out b/sql/core/src/test/resources/sql-tests/results/udf/udf-having.sql.out
index d87ee52216..7cea2e5128 100644
--- a/sql/core/src/test/resources/sql-tests/results/having.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/udf/udf-having.sql.out
 -16,34 +16,34  struct<>

 -- !query 1
-SELECT k, sum(v) FROM hav GROUP BY k HAVING sum(v) > 2
+SELECT udf(k) AS k, udf(sum(v)) FROM hav GROUP BY k HAVING udf(sum(v)) > 2
 -- !query 1 schema
-struct<k:string,sum(v):bigint>
+struct<k:string,udf(sum(cast(v as bigint))):string>
 -- !query 1 output
 one    6
 three  3

 -- !query 2
-SELECT count(k) FROM hav GROUP BY v + 1 HAVING v + 1 = 2
+SELECT udf(count(udf(k))) FROM hav GROUP BY v + 1 HAVING v + 1 = udf(2)
 -- !query 2 schema
-struct<count(k):bigint>
+struct<udf(count(udf(k))):string>
 -- !query 2 output
 1

 -- !query 3
-SELECT MIN(t.v) FROM (SELECT * FROM hav WHERE v > 0) t HAVING(COUNT(1) > 0)
+SELECT udf(MIN(t.v)) FROM (SELECT * FROM hav WHERE v > 0) t HAVING(udf(COUNT(udf(1))) > 0)
 -- !query 3 schema
-struct<min(v):int>
+struct<udf(min(v)):string>
 -- !query 3 output
 1

 -- !query 4
-SELECT a + b FROM VALUES (1L, 2), (3L, 4) AS T(a, b) GROUP BY a + b HAVING a + b > 1
+SELECT udf(a + b) FROM VALUES (1L, 2), (3L, 4) AS T(a, b) GROUP BY a + b HAVING a + b > udf(1)
 -- !query 4 schema
-struct<(a + CAST(b AS BIGINT)):bigint>
+struct<udf((a + cast(b as bigint))):string>
 -- !query 4 output
 3
 7

```

</p>
</details>

## How was this patch tested?

Tested as guided in SPARK-27921.

Closes #25093 from huaxingao/spark-28281.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['sql/core/src/test/resources/sql-tests/inputs/udf/udf-having.sql'],"In the 'having.sql' test suite, the applicable UDFs are not being tested properly which could miss potential bugs related to them."
19854371104451b5c4cb266aaee6b33a5049b145,1587470133,"[SPARK-31474][SQL] Consistency between dayofweek/dow in extract exprsession and dayofweek function

### What changes were proposed in this pull request?
```sql
spark-sql> SELECT extract(dayofweek from '2009-07-26');
1
spark-sql> SELECT extract(dow from '2009-07-26');
0
spark-sql> SELECT extract(isodow from '2009-07-26');
7
spark-sql> SELECT dayofweek('2009-07-26');
1
spark-sql> SELECT weekday('2009-07-26');
6
```
Currently, there are 4 types of day-of-week range:
1. the function `dayofweek`(2.3.0) and extracting `dayofweek`(2.4.0) result as of Sunday(1) to Saturday(7)
2. extracting `dow`(3.0.0) results as of Sunday(0) to Saturday(6)
3. extracting` isodow` (3.0.0) results as of Monday(1) to Sunday(7)
4. the function `weekday`(2.4.0) results as of Monday(0) to Sunday(6)

Actually, extracting `dayofweek` and `dow` are both derived from PostgreSQL but have different meanings.
https://issues.apache.org/jira/browse/SPARK-23903
https://issues.apache.org/jira/browse/SPARK-28623

In this PR, we make extracting `dow` as same as extracting `dayofweek` and the `dayofweek` function for historical reason and not breaking anything.

Also, add more documentation to the extracting function to make extract field more clear to understand.

### Why are the changes needed?

Consistency insurance

### Does this PR introduce any user-facing change?

yes, doc updated and extract `dow` is as same as `dayofweek`

### How was this patch tested?

1. modified ut
2. local SQL doc verification
#### before
![image](https://user-images.githubusercontent.com/8326978/79601949-3535b100-811c-11ea-957b-a33d68641181.png)

#### after
![image](https://user-images.githubusercontent.com/8326978/79601847-12a39800-811c-11ea-8ff6-aa329255d099.png)

Closes #28248 from yaooqinn/SPARK-31474.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala'],"Inconsistency in the day-of-week range when extracting `dayofweek` and `dow` from a date, and the associated functions `dayofweek` and `weekday`. Different extraction yields different results for the same date."
9c7aa90c771868da727073f9941b8b2c4b856946,1678692169,"[SPARK-42577][CORE] Add max attempts limitation for stages to avoid potential infinite retry

### What changes were proposed in this pull request?
Currently a stage will be resubmitted in a few scenarios:
1. Task failed with `FetchFailed` will trigger stage re-submit;
2. Barrier task failed;
3. Shuffle data loss due to executor/host decommissioned;

For the first 2 scenarios, there is a config `spark.stage.maxConsecutiveAttempts` to limit the retry times. While for the 3rd scenario, there'll be potential risks for inifinite retry if there are always executors hosting the shuffle data from successful tasks got killed/lost, the stage will be re-run again and again.

To avoid the potential risk, the proposal in this PR is to add a new config `spark.stage.maxConsecutiveAttempts` to limit the overall max attempts number for each stage, the stage will be aborted once the retry times beyond the limitation.

### Why are the changes needed?
To avoid the potential risks for stage infinite retry.

### Does this PR introduce _any_ user-facing change?
Added limitation for stage retry times, so jobs may fail if they need to retry for mutiplte times beyond the limitation.

### How was this patch tested?
Added new UT.

Closes #40286 from ivoson/SPARK-42577.

Authored-by: Tengfei Huang <tengfei.h@gmail.com>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala', 'core/src/main/scala/org/apache/spark/scheduler/Stage.scala', 'core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala']",There's a risk for infinite retry on a stage if executors hosting the shuffle data from successful tasks are consistently killed/lost.
00d169156d4b1c91d2bcfd788b254b03c509dc41,1516488589,"[SPARK-21786][SQL] The 'spark.sql.parquet.compression.codec' and 'spark.sql.orc.compression.codec' configuration doesn't take effect on hive table writing

[SPARK-21786][SQL] The 'spark.sql.parquet.compression.codec' and 'spark.sql.orc.compression.codec' configuration doesn't take effect on hive table writing

What changes were proposed in this pull request?

Pass ‘spark.sql.parquet.compression.codec’ value to ‘parquet.compression’.
Pass ‘spark.sql.orc.compression.codec’ value to ‘orc.compress’.

How was this patch tested?

Add test.

Note:
This is the same issue mentioned in #19218 . That branch was deleted mistakenly, so make a new pr instead.

gatorsmile maropu dongjoon-hyun discipleforteen

Author: fjh100456 <fu.jinhua6@zte.com.cn>
Author: Takeshi Yamamuro <yamamuro@apache.org>
Author: Wenchen Fan <wenchen@databricks.com>
Author: gatorsmile <gatorsmile@gmail.com>
Author: Yinan Li <liyinan926@gmail.com>
Author: Marcelo Vanzin <vanzin@cloudera.com>
Author: Juliusz Sompolski <julek@databricks.com>
Author: Felix Cheung <felixcheung_m@hotmail.com>
Author: jerryshao <sshao@hortonworks.com>
Author: Li Jin <ice.xelloss@gmail.com>
Author: Gera Shegalov <gera@apache.org>
Author: chetkhatri <ckhatrimanjal@gmail.com>
Author: Joseph K. Bradley <joseph@databricks.com>
Author: Bago Amirbekian <bago@databricks.com>
Author: Xianjin YE <advancedxy@gmail.com>
Author: Bruce Robbins <bersprockets@gmail.com>
Author: zuotingbing <zuo.tingbing9@zte.com.cn>
Author: Kent Yao <yaooqinn@hotmail.com>
Author: hyukjinkwon <gurwls223@gmail.com>
Author: Adrian Ionescu <adrian@databricks.com>

Closes #20087 from fjh100456/HiveTableWriting.
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveOptions.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/CompressionCodecSuite.scala']",The 'spark.sql.parquet.compression.codec' and 'spark.sql.orc.compression.codec' configurations are not affecting the compression of written hive tables.
e92b75482fd4e5e3533a7f76f205faca94c71a7b,1572328447,"[SPARK-29612][SQL] ALTER TABLE (RECOVER PARTITIONS) should look up catalog/table like v2 commands

### What changes were proposed in this pull request?
Add AlterTableRecoverPartitionsStatement and make ALTER TABLE ... RECOVER PARTITIONS go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC t // success and describe the table t from my_catalog
ALTER TABLE t RECOVER PARTITIONS  // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?
Yes. When running ALTER TABLE ... RECOVER PARTITIONS Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?
Unit tests.

Closes #26269 from huaxingao/spark-29612.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala']","The ALTER TABLE ... RECOVER PARTITIONS command doesn't consistently follow the v2 command's table resolution framework causing table not found error, leading to confusion for end-users."
21b13506cd822ed7db343bff4ca25d9555178f10,1605792670,"[SPARK-33442][SQL] Change Combine Limit to Eliminate limit using max row

### What changes were proposed in this pull request?

Change `CombineLimits` name to `EliminateLimits` and add check if `Limit` child max row <= limit.

### Why are the changes needed?

In Add-hoc scene, we always add limit for the query if user have no special limit value, but not all limit is nesessary.

A general negative example is
```
select count(*) from t limit 100000;
```

It will be great if we can eliminate limit at Spark side.

Also, we make a benchmark for this case
```
runBenchmark(""Sort and Limit"") {
  val N = 100000
  val benchmark = new Benchmark(""benchmark sort and limit"", N)

  benchmark.addCase(""TakeOrderedAndProject"", 3) { _ =>
    spark.range(N).toDF(""c"").repartition(200).sort(""c"").take(200000)
  }

  benchmark.addCase(""Sort And Limit"", 3) { _ =>
    withSQLConf(""spark.sql.execution.topKSortFallbackThreshold"" -> ""-1"") {
      spark.range(N).toDF(""c"").repartition(200).sort(""c"").take(200000)
    }
  }

  benchmark.addCase(""Sort"", 3) { _ =>
    spark.range(N).toDF(""c"").repartition(200).sort(""c"").collect()
  }
  benchmark.run()
}
```

and the result is
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12 on Mac OS X 10.15.6
Intel(R) Core(TM) i5-5257U CPU  2.70GHz
benchmark sort and limit:                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
TakeOrderedAndProject                              1833           2259         382          0.1       18327.1       1.0X
Sort And Limit                                     1417           1658         285          0.1       14167.5       1.3X
Sort                                               1324           1484         225          0.1       13238.3       1.4X
```

It shows that it makes sense to replace `TakeOrderedAndProjectExec` with `Sort + Project`.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Add test.

Closes #30368 from ulysses-you/SPARK-33442.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CombiningLimitsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala']",Unnecessary limits are not eliminated in Ad-hoc scene which negatively impacts queries with no specific limit value.
07ecbc4049aa7f8daa11e6a924c37c1db2f53c73,1633717812,"[SPARK-36913][SQL] Implement createIndex and IndexExists in DS V2 JDBC (MySQL dialect)

### What changes were proposed in this pull request?
Implementing `createIndex`/`IndexExists` in DS V2 JDBC

### Why are the changes needed?
This is a subtask of the V2 Index support. I am implementing index support for DS V2 JDBC so we can have a POC and an end to end testing. This PR implements `createIndex` and `IndexExists`. Next PR will implement `listIndexes` and `dropIndex`. I intentionally make the PR small so it's easier to review.

Index is not supported by h2 database and create/drop index are not standard SQL syntax. This PR only implements `createIndex` and `IndexExists` in `MySQL` dialect.

### Does this PR introduce _any_ user-facing change?
Yes, `createIndex`/`IndexExist` in DS V2 JDBC

### How was this patch tested?
new test

Closes #34164 from huaxingao/createIndexJDBC.

Authored-by: Huaxin Gao <huaxin_gao@apple.com>
Signed-off-by: Liang-Chi Hsieh <viirya@gmail.com>
","['external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/MySQLIntegrationSuite.scala', 'external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala', 'sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/index/SupportsIndex.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlreadyExistException.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTable.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTableCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala']","DS V2 JDBC lacks necessary support for creating and checking the existence of indexes, specifically in the MySQL dialect, limiting end-to-end testing and POC creation."
a28728a9afcff94194147573e07f6f4d0463687e,1505443990,"[SPARK-21513][SQL][FOLLOWUP] Allow UDF to_json support converting MapType to json for PySpark and SparkR

## What changes were proposed in this pull request?
In previous work SPARK-21513, we has allowed `MapType` and `ArrayType` of `MapType`s convert to a json string but only for Scala API. In this follow-up PR, we will make SparkSQL support it for PySpark and SparkR, too. We also fix some little bugs and comments of the previous work in this follow-up PR.

### For PySpark
```
>>> data = [(1, {""name"": ""Alice""})]
>>> df = spark.createDataFrame(data, (""key"", ""value""))
>>> df.select(to_json(df.value).alias(""json"")).collect()
[Row(json=u'{""name"":""Alice"")']
>>> data = [(1, [{""name"": ""Alice""}, {""name"": ""Bob""}])]
>>> df = spark.createDataFrame(data, (""key"", ""value""))
>>> df.select(to_json(df.value).alias(""json"")).collect()
[Row(json=u'[{""name"":""Alice""},{""name"":""Bob""}]')]
```
### For SparkR
```
# Converts a map into a JSON object
df2 <- sql(""SELECT map('name', 'Bob')) as people"")
df2 <- mutate(df2, people_json = to_json(df2$people))
# Converts an array of maps into a JSON array
df2 <- sql(""SELECT array(map('name', 'Bob'), map('name', 'Alice')) as people"")
df2 <- mutate(df2, people_json = to_json(df2$people))
```
## How was this patch tested?
Add unit test cases.

cc viirya HyukjinKwon

Author: goldmedal <liugs963@gmail.com>

Closes #19223 from goldmedal/SPARK-21513-fp-PySaprkAndSparkR.
","['python/pyspark/sql/functions.py', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/jsonExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala']","MapType and ArrayType of MapTypes converting to a JSON string only supported in the Scala API but not in PySpark and SparkR, causing inconsistencies across APIs."
ef1e8495ba9fae2f803d0b8653ced07baf4aab14,1570516745,"[SPARK-29366][SQL] Subqueries created for DPP are not printed in EXPLAIN FORMATTED

### What changes were proposed in this pull request?
The subquery expressions introduced by DPP are not printed in the newer explain command.
This PR fixes the code that computes the list of subqueries in the plan.

**SQL**
df1 and df2 are partitioned on k.
```
SELECT df1.id, df2.k
FROM df1 JOIN df2 ON df1.k = df2.k AND df2.id < 2
```

**Before**
```
|== Physical Plan ==
* Project (9)
+- * BroadcastHashJoin Inner BuildRight (8)
   :- * ColumnarToRow (2)
   :  +- Scan parquet default.df1 (1)
   +- BroadcastExchange (7)
      +- * Project (6)
         +- * Filter (5)
            +- * ColumnarToRow (4)
               +- Scan parquet default.df2 (3)

(1) Scan parquet default.df1
Output: [id#19L, k#20L]

(2) ColumnarToRow [codegen id : 2]
Input: [id#19L, k#20L]

(3) Scan parquet default.df2
Output: [id#21L, k#22L]

(4) ColumnarToRow [codegen id : 1]
Input: [id#21L, k#22L]

(5) Filter [codegen id : 1]
Input     : [id#21L, k#22L]
Condition : (isnotnull(id#21L) AND (id#21L < 2))

(6) Project [codegen id : 1]
Output    : [k#22L]
Input     : [id#21L, k#22L]

(7) BroadcastExchange
Input: [k#22L]

(8) BroadcastHashJoin [codegen id : 2]
Left keys: List(k#20L)
Right keys: List(k#22L)
Join condition: None

(9) Project [codegen id : 2]
Output    : [id#19L, k#22L]
Input     : [id#19L, k#20L, k#22L]
```
**After**
```
|== Physical Plan ==
* Project (9)
+- * BroadcastHashJoin Inner BuildRight (8)
   :- * ColumnarToRow (2)
   :  +- Scan parquet default.df1 (1)
   +- BroadcastExchange (7)
      +- * Project (6)
         +- * Filter (5)
            +- * ColumnarToRow (4)
               +- Scan parquet default.df2 (3)

(1) Scan parquet default.df1
Output: [id#19L, k#20L]

(2) ColumnarToRow [codegen id : 2]
Input: [id#19L, k#20L]

(3) Scan parquet default.df2
Output: [id#21L, k#22L]

(4) ColumnarToRow [codegen id : 1]
Input: [id#21L, k#22L]

(5) Filter [codegen id : 1]
Input     : [id#21L, k#22L]
Condition : (isnotnull(id#21L) AND (id#21L < 2))

(6) Project [codegen id : 1]
Output    : [k#22L]
Input     : [id#21L, k#22L]

(7) BroadcastExchange
Input: [k#22L]

(8) BroadcastHashJoin [codegen id : 2]
Left keys: List(k#20L)
Right keys: List(k#22L)
Join condition: None

(9) Project [codegen id : 2]
Output    : [id#19L, k#22L]
Input     : [id#19L, k#20L, k#22L]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = k#20L IN subquery25
* HashAggregate (16)
+- Exchange (15)
   +- * HashAggregate (14)
      +- * Project (13)
         +- * Filter (12)
            +- * ColumnarToRow (11)
               +- Scan parquet default.df2 (10)

(10) Scan parquet default.df2
Output: [id#21L, k#22L]

(11) ColumnarToRow [codegen id : 1]
Input: [id#21L, k#22L]

(12) Filter [codegen id : 1]
Input     : [id#21L, k#22L]
Condition : (isnotnull(id#21L) AND (id#21L < 2))

(13) Project [codegen id : 1]
Output    : [k#22L]
Input     : [id#21L, k#22L]

(14) HashAggregate [codegen id : 1]
Input: [k#22L]

(15) Exchange
Input: [k#22L]

(16) HashAggregate [codegen id : 2]
Input: [k#22L]
```
### Why are the changes needed?
Without the fix, the subqueries are not printed in the explain plan.

### Does this PR introduce any user-facing change?
Yes. the explain output will be different.

### How was this patch tested?
Added a test case in ExplainSuite.

Closes #26039 from dilipbiswal/explain_subquery_issue.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/ExplainUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala']",Subquery expressions created by Dynamic Partition Pruning (DPP) are not represented in the output of the EXPLAIN FORMATTED command.
c37b726bd09d34e1115a8af1969485e60dc02592,1638455582,"[SPARK-37442][SQL] InMemoryRelation statistics bug causing broadcast join failures with AQE enabled

### What changes were proposed in this pull request?
Immediately materialize underlying rdd cache (using .count) for an InMemoryRelation when `buildBuffers` is called.

### Why are the changes needed?

Currently, when `CachedRDDBuilder.buildBuffers` is called, `InMemoryRelation.computeStats` will try to read the accumulators to determine what the relation size is. However, the accumulators are not actually accurate until the cachedRDD is executed and finishes. While this has not happened, the accumulators will report a range from 0 bytes to the accumulator value when the cachedRDD finishes. In AQE, join planning can happen during this time and, if it reads the size as 0 bytes, will likely plan a broadcast join mistakenly believing the build side is very small. If the InMemoryRelation is actually very large in size, then this will cause many issues during execution such as job failure due to broadcasting over 8GB.

### Does this PR introduce _any_ user-facing change?

Yes. Before, cache materialization doesn't happen until the job starts to run. Now, it happens when trying to get the rdd representing an InMemoryRelation.

### How was this patch tested?

Tests added

Closes #34684 from ChenMichael/SPARK-37442-InMemoryRelation-statistics-inaccurate-during-join-planning.

Authored-by: Michael Chen <mike.chen@workday.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala']","InMemoryRelation statistics inaccuracies during join planning under AQE, leading to broadcast join failures when encountering large data sizes."
5415963d2caaf95604211419ffc4e29fff38e1d7,1509059628,"[SPARK-22131][MESOS] Mesos driver secrets

## Background

In #18837 , ArtRand added Mesos secrets support to the dispatcher. **This PR is to add the same secrets support to the drivers.** This means if the secret configs are set, the driver will launch executors that have access to either env or file-based secrets.

One use case for this is to support TLS in the driver <=> executor communication.

## What changes were proposed in this pull request?

Most of the changes are a refactor of the dispatcher secrets support (#18837) - moving it to a common place that can be used by both the dispatcher and drivers. The same goes for the unit tests.

## How was this patch tested?

There are four config combinations: [env or file-based] x [value or reference secret]. For each combination:
- Added a unit test.
- Tested in DC/OS.

Author: Susan X. Huynh <xhuynh@mesosphere.com>

Closes #19437 from susanxhuynh/sh-mesos-driver-secret.
","['resource-managers/mesos/src/main/scala/org/apache/spark/deploy/mesos/config.scala', 'resource-managers/mesos/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala', 'resource-managers/mesos/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosCoarseGrainedSchedulerBackend.scala', 'resource-managers/mesos/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosFineGrainedSchedulerBackend.scala', 'resource-managers/mesos/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackendUtil.scala', 'resource-managers/mesos/src/test/scala/org/apache/spark/scheduler/cluster/mesos/MesosClusterSchedulerSuite.scala', 'resource-managers/mesos/src/test/scala/org/apache/spark/scheduler/cluster/mesos/MesosCoarseGrainedSchedulerBackendSuite.scala', 'resource-managers/mesos/src/test/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackendUtilSuite.scala', 'resource-managers/mesos/src/test/scala/org/apache/spark/scheduler/cluster/mesos/Utils.scala']",Drivers do not have approvals for secret configurations which affects Mesos secret support and obstructs TLS communication between driver and executor.
cc88d7fad16e8b5cbf7b6b9bfe412908782b4a45,1528830608,"[SPARK-24216][SQL] Spark TypedAggregateExpression uses getSimpleName that is not safe in scala

## What changes were proposed in this pull request?

When user create a aggregator object in scala and pass the aggregator to Spark Dataset's agg() method, Spark's will initialize TypedAggregateExpression with the nodeName field as aggregator.getClass.getSimpleName. However, getSimpleName is not safe in scala environment, depending on how user creates the aggregator object. For example, if the aggregator class full qualified name is ""com.my.company.MyUtils$myAgg$2$"", the getSimpleName will throw java.lang.InternalError ""Malformed class name"". This has been reported in scalatest https://github.com/scalatest/scalatest/pull/1044 and discussed in many scala upstream jiras such as SI-8110, SI-5425.

To fix this issue, we follow the solution in https://github.com/scalatest/scalatest/pull/1044 to add safer version of getSimpleName as a util method, and TypedAggregateExpression will invoke this util method rather than getClass.getSimpleName.

## How was this patch tested?
added unit test

Author: Fangshi Li <fli@linkedin.com>

Closes #21276 from fangshil/SPARK-24216.
","['core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala', 'core/src/main/scala/org/apache/spark/util/Utils.scala', 'core/src/test/scala/org/apache/spark/util/UtilsSuite.scala', 'mllib/src/main/scala/org/apache/spark/ml/util/Instrumentation.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StringFormat.scala']","Passing a scala aggregator object to Spark Dataset's agg() method leads to a ""Malformed class name"" InternalError due to unsafe usage of getSimpleName for TypedAggregateExpression initialization."
3c66c11aa60f43f2cdfed221c55bc99e66653695,1623276783,"[SPARK-35601][PYTHON] Complete arithmetic operators involving bool literals, Series, and Index

### What changes were proposed in this pull request?

Completing arithmetic operators involving bool literals, Series, and Index consists of two main tasks:
- Support arithmetic operations against bool literals
- Support operators (+, *) between bool Series/Indexes.

### Why are the changes needed?

Arithmetic operators involving bool literals, Series, and Index are incomplete now.
We ought to match pandas' behaviors.

### Does this PR introduce _any_ user-facing change?

Yes.

Newly supported operations example:
```py
>>> ps.Series([1, 2, 3]) + True
0    2
1    3
2    4
dtype: int64
>>> ps.Series([1, 2, 3]) + False
0    1
1    2
2    3
dtype: int64
>>> ps.Series([True, False, True]) + True
0    True
1    True
2    True
dtype: bool
>>> ps.Series([True, False, True]) + False
0     True
1    False
2     True
dtype: bool
>>> ps.Series([True, False, True]) * True
0     True
1    False
2     True
dtype: bool
>>> ps.Series([True, False, True]) * False
0    False
1    False
2    False
dtype: bool
>>> ps.set_option('compute.ops_on_diff_frames', True)
>>> ps.Series([True, True, False]) + ps.Series([True, False, True])
0    True
1    True
2    True
dtype: bool
>>> ps.Series([True, True, False]) * ps.Series([True, False, True])
0     True
1    False
2    False
dtype: bool
```
Before the change, operations above are not supported, raising a TypeError such as
```py
>>> ps.Series([True, False, True]) + True
Traceback (most recent call last):
...
TypeError: Addition can not be applied to booleans and the given type.
>>> ps.Series([True, False, True]) + False
Traceback (most recent call last):
...
TypeError: Addition can not be applied to booleans and the given type.
```

### How was this patch tested?

Unit tests.

Closes #32785 from xinrong-databricks/datatypeops_arith_bool.

Authored-by: Xinrong Meng <xinrong.meng@databricks.com>
Signed-off-by: Takuya UESHIN <ueshin@databricks.com>
","['python/pyspark/pandas/data_type_ops/base.py', 'python/pyspark/pandas/data_type_ops/boolean_ops.py', 'python/pyspark/pandas/data_type_ops/num_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_num_ops.py', 'python/pyspark/pandas/tests/data_type_ops/testing_utils.py']","Arithmetic operations involving bool literals, Series, and Index in Python Spark are incomplete, causing TypeErrors when attempting unsupported operations such as addition or multiplication."
abe1998c5975a95eee0d73724301f7a7e3668878,1657958153,"[SPARK-39756][PS] Better error messages for missing pandas scalars

### What changes were proposed in this pull request?
pandas scalars are not reimplemented in pandas API on Spark intentionally, as part of the initial design principle.

Users can use pandas scalars in pandas API on Spark directly.

However, error messages are confusing when users mistakenly assume pandas scalars are reimplemented, for example, calling `ps.Timestamp` as below

```py
>>> ps.Series([ps.Timestamp(1994, 1, 31)])
Traceback (most recent call last):
...
AttributeError: module 'pyspark.pandas' has no attribute 'Timestamp'
```

Users may jump to the conclusion that a Series of timestamp data is not supported.

However, we do support that by using `pd.Timestamp` as below:

```py
>>> ps.Series([pd.Timestamp(1994, 1, 31)])
0   1994-01-31
dtype: datetime64[ns]
```

We should inform users to use pandas scalars instead.

In addition, `PandasNotImplementedError` should be raised rather than `AttributeError` for clarity.

### Why are the changes needed?
Better error messages should be clear and tell how to fix the errors.
That can enhance usability, debuggability, and furthermore, user adoption.

### Does this PR introduce _any_ user-facing change?
Yes. Error messages change. For example:

**Before**
```py
>>> ps.Series([ps.Timestamp(1994, 1, 31)])
Traceback (most recent call last):
...
AttributeError: module 'pyspark.pandas' has no attribute 'Timestamp'
```

**After**
```py
>>> ps.Series([ps.Timestamp(1994, 1, 31)])
Traceback (most recent call last):
...
pyspark.pandas.exceptions.PandasNotImplementedError: The scalar `ps.Timestamp` is not reimplemented in pyspark.pandas; use `pd.Timestamp`.
```
### How was this patch tested?
Unit tests.

Closes #37168 from xinrong-meng/ps_missing_scalar.

Authored-by: Xinrong Meng <xinrong.meng@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['dev/sparktestsupport/modules.py', 'python/pyspark/pandas/__init__.py', 'python/pyspark/pandas/exceptions.py', 'python/pyspark/pandas/missing/scalars.py', 'python/pyspark/pandas/supported_api_gen.py', 'python/pyspark/pandas/tests/test_scalars.py']",Error messages are unclear when users mistakenly assume pandas scalars are reimplemented in pandas API on Spark leading to confusion and misinterpretation of the system's capabilities.
ab4cf49a1ca5a2578ecd0441ad6db8e88593e648,1589869292,"[SPARK-31440][SQL] Improve SQL Rest API

### What changes were proposed in this pull request?
SQL Rest API exposes query execution metrics as Public API. This PR aims to apply following improvements on SQL Rest API by aligning Spark-UI.

**Proposed Improvements:**
1- Support Physical Operations and group metrics per physical operation by aligning Spark UI.
2- Support `wholeStageCodegenId` for Physical Operations
3- `nodeId` can be useful for grouping metrics and sorting physical operations (according to execution order) to differentiate same operators (if used multiple times during the same query execution) and their metrics.
4- Filter `empty` metrics by aligning with Spark UI - SQL Tab. Currently, Spark UI does not show empty metrics.
5- Remove line breakers(`\n`) from `metricValue`.
6- `planDescription` can be `optional` Http parameter to avoid network cost where there is specially complex jobs creating big-plans.
7- `metrics` attribute needs to be exposed at the bottom order as `nodes`. Specially, this can be useful for the user where `nodes` array size is high.
8- `edges` attribute is being exposed to show relationship between `nodes`.
9- Reverse order on `metricDetails` aims to match with Spark UI by supporting Physical Operators' execution order.

### Why are the changes needed?
Proposed improvements provides more useful (e.g: physical operations and metrics correlation, grouping) and clear (e.g: filtering blank metrics, removing line breakers) result for the end-user.

### Does this PR introduce any user-facing change?
Yes. Please find both current and improved versions of the results as attached for following SQL Rest Endpoint:
```
curl -X GET http://localhost:4040/api/v1/applications/$appId/sql/$executionId?details=true
```
**Current version:**
https://issues.apache.org/jira/secure/attachment/12999821/current_version.json

**Improved version:**
https://issues.apache.org/jira/secure/attachment/13000621/improved_version.json

### Backward Compatibility
SQL Rest API will be started to expose with `Spark 3.0` and `3.0.0-preview2` (released on 12/23/19) does not cover this API so if PR can catch 3.0 release, this will not have any backward compatibility issue.

### How was this patch tested?
1. New Unit tests are added.
2. Also, patch has been tested manually through both **Spark Core** and **History Server** Rest APIs.

Closes #28208 from erenavsarogullari/SPARK-31440.

Authored-by: Eren Avsarogullari <eren.avsarogullari@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala', 'sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/SqlResource.scala', 'sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/api.scala', 'sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceSuite.scala']","SQL Rest API has several limitations: it doesn't support physical operations, doesn't filter empty metrics, and exposes potentially unnecessary plan descriptions. Additionally, the metrics and nodes are not optimally organized, making it difficult to correlate and interpret."
2be1fe6abcbdd3ead89f10a22ddc3e5c5e07e41a,1572280880,"[SPARK-29521][SQL] LOAD DATA INTO TABLE should look up catalog/table like v2 commands

### What changes were proposed in this pull request?

Add LoadDataStatement and make LOAD DATA INTO TABLE go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.

```
USE my_catalog
DESC t // success and describe the table t from my_catalog
LOAD DATA INPATH 'filepath'  INTO TABLE t // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?

yes. When running LOAD DATA INTO TABLE, Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26178 from viirya/SPARK-29521.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala']","LOAD DATA INTO TABLE is not able to look up tables in the session catalog, causing ""table not found"" errors when using v2 commands and a v2 catalog is set as the current catalog."
7bb1b218f171aa7fcfd804d1eaedd2ff375fc035,1658801002,"[SPARK-33236][SHUFFLE] Enable Push-based shuffle service to store state in NM level DB for work preserving restart

### What changes were proposed in this pull request?
This PR adds the capability of storing the required information into LevelDB for push based shuffle.

### Why are the changes needed?
Without this PR, all the information is currently only stored in memory for push based shuffle in shuffle services. During NodeManager restarts, all these information will be lost. Either all the former merged shuffle data won't be able to serve the fetch requests, nor the shuffle services cannot merge any new push blocks from existing applications. After this patch, those information will be stored in LevelDB, and all the information will be recovered during NodeManager restarts.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Unit testing.
Deployed to clusters, and restart NMs in 3 different scenarios:
While a Spark shell application is running, restart NMs in magnet partition in three different scenarios:
1. After Spark-shell starts, but no scripts running, this will test the NM restart after application/executors register with NMs
2. While there is on-going shuffle push to shuffle services
3. While there is on-going merged shuffle fetch from shuffle services.
Results of the large shuffle testing scripts is identical to the case when there is no NM restart.

Closes #35906 from zhouyejoe/SPARK-33236.

Authored-by: Ye Zhou <yezhou@linkedin.com>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/MergedShuffleFileManager.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/NoOpMergedShuffleFileManager.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java', 'common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java', 'common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java', 'resource-managers/yarn/src/test/scala/org/apache/spark/network/shuffle/ShuffleTestAccessor.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/network/yarn/YarnShuffleServiceSuite.scala']","Push-based shuffle service loses all stored information during NodeManager restarts, preventing service of fetch requests and merge of new push blocks from existing applications."
1fd54f4bf58342c067adfa28f0705a4efef5e60a,1598051669,"[SPARK-32662][ML] CountVectorizerModel: Remove requirement for minimum Vocab size

### What changes were proposed in this pull request?

The strict requirement for the vocabulary to remain non-empty has been removed in this pull request.

Link to the discussion: http://apache-spark-user-list.1001560.n3.nabble.com/Ability-to-have-CountVectorizerModel-vocab-as-empty-td38396.html

### Why are the changes needed?

This soothens running it across the corner cases. Without this, the user has to manupulate the data in genuine case, which may be a perfectly fine valid use-case.

Question: Should we a log when empty vocabulary is found instead?

### Does this PR introduce _any_ user-facing change?

May be a slight change. If someone has put a try-catch to detect an empty vocab. Then that behavior would no longer stand still.

### How was this patch tested?

1. Added testcase to `fit` generating an empty vocabulary
2. Added testcase to `transform` with empty vocabulary

Request to review: srowen hhbyyh

Closes #29482 from purijatin/spark_32662.

Authored-by: Jatin Puri <purijatin@gmail.com>
Signed-off-by: Huaxin Gao <huaxing@us.ibm.com>
","['mllib/src/main/scala/org/apache/spark/ml/feature/CountVectorizer.scala', 'mllib/src/test/scala/org/apache/spark/ml/feature/CountVectorizerSuite.scala']","CountVectorizerModel does not handle cases of empty vocabulary properly, forcing users to manipulate their data even in valid use-cases."
31721baca4ecbd27e9dc7306476d259691f01654,1665996409,"[SPARK-40646][SQL] Fix returning partial results in JSON data source and JSON functions

### What changes were proposed in this pull request?

This PR is a follow-up for [SPARK-33134](https://issues.apache.org/jira/browse/SPARK-33134) (https://github.com/apache/spark/pull/30031).

I found another case when, depending on the order of columns, parsing one JSON field breaks all of the subsequent fields resulting in all nulls:

With a file like this:
```
{""a"": {""x"": 1, ""y"": true}, ""b"": {""x"": 1}}
{""a"": {""x"": 2}, ""b"": {""x"": 2}}
```

Reading the file results in column `b` as null even though it is a valid column.
```scala
val df = spark.read
  .schema(""a struct<x: int, y: struct<x: int>>, b struct<x: int>"")
  .json(""path"")

===

a	                b
null	                null
{""x"":2,""y"":null}	{""x"":2}
```

However, b column should be:
```
{""x"": 1}
{""x"": 2}
```

This particular example actually used to work in earlier Spark versions but it was affected by SPARK-33134 which fixed another bug with the incorrect parsing in `from_json`. Because this case was not tested, we missed it at the time.

In order to fix both SPARK-33134 and SPARK-40646, we need to process `PartialResultException` in `convertArray` method to handle any errors in child objects. Without the fix, the code would not wrap the row in the array for `from_json` resulting in a ClassCastException (SPARK-33134). Because of this handling, we don't need `isRoot` check anymore in `convertObject` thus unblocking SPARK-40646.

I updated the code to handle both cases. With these changes, we can correctly parse this case:
```scala
val df3 = Seq(""""""[{""c2"": [19], ""c1"": 123456}]"""""").toDF(""c0"")
checkAnswer(df3.select(from_json($""c0"", ArrayType(st))), Row(Array(Row(123456, null))))
```
which was previously returning `null` for the root row.

### Why are the changes needed?

Fixes a long-standing issue when parsing a JSON with an incorrect field that would break parsing of the entire record.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

I added unit tests for SPARK-40646 as well as SPARK-33134.

Closes #38090 from sadikovi/SPARK-40646.

Authored-by: Ivan Sadikov <ivan.sadikov@databricks.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala', 'sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala']","Parsing JSON fields in a specific order breaks subsequent fields, resulting in null values even when the column is valid. The problem replicates when an incorrect field breaks parsing of the entire record."
19cb8d7014e03d828794a637bc67d09fc84650ad,1678492315,"[SPARK-42721][CONNECT] RPC logging interceptor

### What changes were proposed in this pull request?

This adds an gRPC interceptor in spark-connect server. It logs all the incoming RPC requests and responses.

 - How to enable: Set interceptor config. e.g.

       ./sbin/start-connect-server.sh --conf spark.connect.grpc.interceptor.classes=org.apache.spark.sql.connect.service.LoggingInterceptor  --jars connector/connect/server/target/spark-connect_*-SNAPSHOT.jar

 - Sample output:

        23/03/08 10:54:37 INFO LoggingInterceptor: Received RPC Request spark.connect.SparkConnectService/ExecutePlan (id 1868663481):
        {
          ""client_id"": ""6844bc44-4411-4481-8109-a10e3a836f97"",
          ""user_context"": {
            ""user_id"": ""raghu""
          },
          ""plan"": {
            ""root"": {
              ""common"": {
                ""plan_id"": ""37""
              },
              ""show_string"": {
                ""input"": {
                  ""common"": {
                    ""plan_id"": ""36""
                  },
                  ""read"": {
                    ""data_source"": {
                      ""format"": ""csv"",
                      ""schema"": """",
                      ""paths"": [""file:///tmp/x-in""]
                    }
                  }
                },
                ""num_rows"": 20,
                ""truncate"": 20
              }
            }
          },
          ""client_type"": ""_SPARK_CONNECT_PYTHON""
        }

### Why are the changes needed?
This is useful in  development. It might be useful to debug some problems in production as well.

### Does this PR introduce _any_ user-facing change?
no

### How was this patch tested?
 - Manually in development
 - Unit test

Closes #40342 from rangadi/logging-interceptor.

Authored-by: Raghu Angadi <raghu.angadi@databricks.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
","['connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/LoggingInterceptor.scala', 'connector/connect/server/src/test/scala/org/apache/spark/sql/connect/service/InterceptorRegistrySuite.scala']","There is no mechanism to log incoming RPC requests and responses in the spark-connect server, hindering debugging and development processes."
b08cf6e8221cb13aab5be4bd2788bf51958c1776,1623806413,"[SPARK-35203][SQL] Improve Repartition statistics estimation

### What changes were proposed in this pull request?

This PR improves `Repartition` and `RepartitionByExpr` statistics estimation using child statistics.

### Why are the changes needed?

The current implementation will missing column stat. For example:
```sql
CREATE TABLE t1 USING parquet AS SELECT id % 10 AS key FROM range(100);
ANALYZE TABLE t1 COMPUTE STATISTICS FOR ALL COLUMNS;
set spark.sql.cbo.enabled=true;
EXPLAIN COST SELECT key FROM (SELECT key FROM t1 DISTRIBUTE BY key) t GROUP BY key;
```
Before this PR:
```
== Optimized Logical Plan ==
Aggregate [key#2950L], [key#2950L], Statistics(sizeInBytes=1600.0 B)
+- RepartitionByExpression [key#2950L], Statistics(sizeInBytes=1600.0 B, rowCount=100)
   +- Relation default.t1[key#2950L] parquet, Statistics(sizeInBytes=1600.0 B, rowCount=100)
```
After this PR:
```
== Optimized Logical Plan ==
Aggregate [key#2950L], [key#2950L], Statistics(sizeInBytes=160.0 B, rowCount=10)
+- RepartitionByExpression [key#2950L], Statistics(sizeInBytes=1600.0 B, rowCount=100)
   +- Relation default.t1[key#2950L] parquet, Statistics(sizeInBytes=1600.0 B, rowCount=100)
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #32309 from wangyum/SPARK-35203.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/BasicStatsPlanVisitor.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/SizeInBytesOnlyStatsPlanVisitor.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/statsEstimation/BasicStatsEstimationSuite.scala']","The `Repartition` and `RepartitionByExpr` statistics estimation is not properly using child statistics, resulting in inaccurate column statistics during SQL data repartitioning."
ffc8ac935e24c7d15a700034bd556c2f4b8271ee,1677043379,"[SPARK-36124][SQL] Support subqueries with correlation through INTERSECT/EXCEPT

## What changes were proposed in this pull request?

Adds support for subquery decorrelation with INTERSECT and EXCEPT operators on the correlation paths. For example:
```
SELECT t1a, (
  SELECT avg(b) FROM (
    SELECT t2b as b FROM t2 WHERE t2a = t1a
    INTERSECT
    SELECT t3b as b FROM t3 WHERE t3a = t1a
))
FROM t1
```

This uses the same logic as for UNION decorrelation added in https://github.com/apache/spark/pull/39375. The only real change is logic added to handle INTERSECT/EXCEPT DISTINCT, which are rewritten to semi/anti join and require extra logic in rewriteDomainJoins.

[This doc](https://docs.google.com/document/d/11b9ClCF2jYGU7vU2suOT7LRswYkg6tZ8_6xJbvxfh2I/edit#) describes how the decorrelation rewrite works for set operations and the code changes for it - see the INTERSECT/EXCEPT section in particular.

In this PR, we always add DomainJoins for correlation through INTERSECT/EXCEPT, and never do direct substitution of the outer refs. That can also be added as an optimization in a follow-up - it only affects performance, not surface area coverage.

### Why are the changes needed?
To improve subquery support in Spark.

### Does this PR introduce _any_ user-facing change?
Before this change, queries like this would return an error like: `Decorrelate inner query through Intersect is not supported.`

After this PR, this query can run successfully.

### How was this patch tested?
Unit tests and SQL query tests.

Moved the UNION decorrelation SQL tests to file scalar-subquery-set-op.sql and duplicated them to test each of [UNION/INTERSECT/EXCEPT] [ALL/DISTINCT]

Factors tested included:
- Subquery type:
  - Eligible for DecorrelateInnerQuery: Scalar, lateral join
  - Not supported: IN, EXISTS
- UNION inside and outside subquery
- Correlation in where, project, group by, aggregates, or no correlation
- Project, Aggregate, Window under the Union
- COUNT bug

Closes #39759 from jchen5/subq-intersect.

Authored-by: Jack Chen <jack.chen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/DecorrelateInnerQuery.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/DecorrelateInnerQuerySuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/join-lateral.sql', 'sql/core/src/test/resources/sql-tests/inputs/subquery/exists-subquery/exists-joins-and-set-ops.sql', 'sql/core/src/test/resources/sql-tests/inputs/subquery/in-subquery/in-set-operations.sql', 'sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql', 'sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-set-op.sql', 'sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala']","Decorrelation of subqueries with INTERSECT and EXCEPT operators on the correlation paths returns an error, limiting subquery support in Spark."
90a3a7e8055965dab4d5153e256fe622e8a7976e,1683122472,"[SPARK-43348][PYTHON] Support `Python 3.8` in PyPy3

### What changes were proposed in this pull request?

This PR aims two goals.
1. Make PySpark support Python 3.8+ with PyPy3
2. Upgrade PyPy3 to Python 3.8 in our GitHub Action Infra Image to enable test coverage

Note that there was one failure at `test_create_dataframe_from_pandas_with_day_time_interval` test case. This PR skips the test case and SPARK-43354 will recover it after further investigation.

### Why are the changes needed?

Previously, PySpark fails at PyPy3 `Python 3.8` environment.
```
pypy3 version is: Python 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:13)
[PyPy 7.3.11 with GCC 10.2.1 20210130 (Red Hat 10.2.1-11)]
Starting test(pypy3): pyspark.sql.tests.pandas.test_pandas_cogrouped_map (temp output: /__w/spark/spark/python/target/f1cacde7-d369-48cf-a8ea-724c42872020/pypy3__pyspark.sql.tests.pandas.test_pandas_cogrouped_map__rxih6dqu.log)
Traceback (most recent call last):
  File ""/usr/local/pypy/pypy3.8/lib/pypy3.8/runpy.py"", line 188, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""/usr/local/pypy/pypy3.8/lib/pypy3.8/runpy.py"", line 111, in _get_module_details
    __import__(pkg_name)
  File ""/__w/spark/spark/python/pyspark/__init__.py"", line 59, in <module>
    from pyspark.rdd import RDD, RDDBarrier
  File ""/__w/spark/spark/python/pyspark/rdd.py"", line 54, in <module>
    from pyspark.java_gateway import local_connect_and_auth
  File ""/__w/spark/spark/python/pyspark/java_gateway.py"", line 32, in <module>
    from pyspark.serializers import read_int, write_with_length, UTF8Deserializer
  File ""/__w/spark/spark/python/pyspark/serializers.py"", line 69, in <module>
    from pyspark import cloudpickle
  File ""/__w/spark/spark/python/pyspark/cloudpickle/__init__.py"", line 1, in <module>
    from pyspark.cloudpickle.cloudpickle import *  # noqa
  File ""/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py"", line 56, in <module>
    from .compat import pickle
  File ""/__w/spark/spark/python/pyspark/cloudpickle/compat.py"", line 13, in <module>
    from _pickle import Pickler  # noqa: F401
ModuleNotFoundError: No module named '_pickle'
```

To support Python 3.8 in PyPy3.
- From PyPy3.8, `_pickle` is removed.
  - https://github.com/cloudpipe/cloudpickle/issues/458
- We need this change.
  - https://github.com/cloudpipe/cloudpickle/pull/469

### Does this PR introduce _any_ user-facing change?

This is an additional support.

### How was this patch tested?

Pass the CIs.

Closes #41024 from dongjoon-hyun/SPARK-43348.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['python/pyspark/cloudpickle/compat.py', 'python/pyspark/sql/tests/test_dataframe.py']","PySpark is failing in the PyPy3 Python 3.8 environment due to ModuleNotFoundError for the '_pickle' module that was removed in PyPy3.8, affecting support for Python 3.8 in PyPy3."
fc65e0fe2c8a114feba47d8f7b63628a676dd24c,1560982886,"[SPARK-27839][SQL] Change UTF8String.replace() to operate on UTF8 bytes

## What changes were proposed in this pull request?

This PR significantly improves the performance of `UTF8String.replace()` by performing direct replacement over UTF8 bytes instead of decoding those bytes into Java Strings.

In cases where the search string is not found (i.e. no replacements are performed, a case which I expect to be common) this new implementation performs no object allocation or memory copying.

My implementation is modeled after `commons-lang3`'s `StringUtils.replace()` method. As part of my implementation, I needed a StringBuilder / resizable buffer, so I moved `UTF8StringBuilder` from the `catalyst` package to `unsafe`.

## How was this patch tested?

Copied tests from `StringExpressionSuite` to `UTF8StringSuite` and added a couple of new cases.

To evaluate performance, I did some quick local benchmarking by running the following code in `spark-shell` (with Java 1.8.0_191):

```scala
import org.apache.spark.unsafe.types.UTF8String

def benchmark(text: String, search: String, replace: String) {
  val utf8Text = UTF8String.fromString(text)
  val utf8Search = UTF8String.fromString(search)
  val utf8Replace = UTF8String.fromString(replace)

  val start = System.currentTimeMillis
  var i = 0
  while (i < 1000 * 1000 * 100) {
    utf8Text.replace(utf8Search, utf8Replace)
    i += 1
  }
  val end = System.currentTimeMillis

  println(end - start)
}

benchmark(""ABCDEFGH"", ""DEF"", ""ZZZZ"")  // replacement occurs
benchmark(""ABCDEFGH"", ""Z"", """")  // no replacement occurs
```

On my laptop this took ~54 / ~40 seconds seconds before this patch's changes and ~6.5 / ~3.8 seconds afterwards.

Closes #24707 from JoshRosen/faster-string-replace.

Authored-by: Josh Rosen <rosenville@gmail.com>
Signed-off-by: Josh Rosen <rosenville@gmail.com>
","['common/unsafe/src/main/java/org/apache/spark/unsafe/UTF8StringBuilder.java', 'common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java', 'common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala']",UTF8String.replace() function exhibits poor performance and high memory consumption due to unnecessary decoding of UTF8 bytes into Java Strings.
14b18363130656e2644bacd040f811a57fa83961,1624551690,"[SPARK-35290][SQL] Append new nested struct fields rather than sort for unionByName with null filling

### What changes were proposed in this pull request?

This PR changes the unionByName with null filling logic to append new nested struct fields from the right side of the union to the schema versus sorting fields alphabetically. It removes the need to use UpdateField expressions, and just directly projects new nested structs from each side of the union with the correct schema. This changes the union'd schema from being alphabetically sorted previously to now ""left dominant"", where the fields from the left side of the union are included and then the missing ones from the right are added in the same order found originally.

### Why are the changes needed?

Certain nested structs would cause unionByName with null filling to error out due to part of the logic for rewriting the expression tree to sort the structs.

### Does this PR introduce _any_ user-facing change?

Yes, nested struct fields will be in a different order after unionByName with null filling than before, though shouldn't cause much effective difference.

### How was this patch tested?

Updated existing tests based on the new StructField ordering and added a new test for the case that was broken originally.

Closes #33040 from Kimahriman/union-by-name-struct-order.

Authored-by: Adam Binford <adamq43@gmail.com>
Signed-off-by: Liang-Chi Hsieh <viirya@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala', 'sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala']","The unionByName operation with null filling is failing due to errors during the expression tree rewrite process for sorting nested structs, also the nested struct fields' order can be incorrect after operation."
ecaa495b1fe532c36e952ccac42f4715809476af,1544119648,"[SPARK-25274][PYTHON][SQL] In toPandas with Arrow send un-ordered record batches to improve performance

## What changes were proposed in this pull request?

When executing `toPandas` with Arrow enabled, partitions that arrive in the JVM out-of-order must be buffered before they can be send to Python. This causes an excess of memory to be used in the driver JVM and increases the time it takes to complete because data must sit in the JVM waiting for preceding partitions to come in.

This change sends un-ordered partitions to Python as soon as they arrive in the JVM, followed by a list of partition indices so that Python can assemble the data in the correct order. This way, data is not buffered at the JVM and there is no waiting on particular partitions so performance will be increased.

Followup to #21546

## How was this patch tested?

Added new test with a large number of batches per partition, and test that forces a small delay in the first partition. These test that partitions are collected out-of-order and then are are put in the correct order in Python.

## Performance Tests - toPandas

Tests run on a 4 node standalone cluster with 32 cores total, 14.04.1-Ubuntu and OpenJDK 8
measured wall clock time to execute `toPandas()` and took the average best time of 5 runs/5 loops each.

Test code
```python
df = spark.range(1 << 25, numPartitions=32).toDF(""id"").withColumn(""x1"", rand()).withColumn(""x2"", rand()).withColumn(""x3"", rand()).withColumn(""x4"", rand())
for i in range(5):
	start = time.time()
	_ = df.toPandas()
	elapsed = time.time() - start
```

Spark config
```
spark.driver.memory 5g
spark.executor.memory 5g
spark.driver.maxResultSize 2g
spark.sql.execution.arrow.enabled true
```

Current Master w/ Arrow stream | This PR
---------------------|------------
5.16207 | 4.342533
5.133671 | 4.399408
5.147513 | 4.468471
5.105243 | 4.36524
5.018685 | 4.373791

Avg Master | Avg This PR
------------------|--------------
5.1134364 | 4.3898886

Speedup of **1.164821449**

Closes #22275 from BryanCutler/arrow-toPandas-oo-batches-SPARK-25274.

Authored-by: Bryan Cutler <cutlerb@gmail.com>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>
","['python/pyspark/serializers.py', 'python/pyspark/sql/dataframe.py', 'python/pyspark/sql/tests/test_arrow.py', 'sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala']","`toPandas` with Arrow enabled causes excess memory use and delay in driver JVM as it waits for out-of-order partitions to arrive, impacting the overall performance."
06c40091a6d2218132b43e625c9d7acbc9affc9e,1620477819,"[SPARK-35327][SQL][TESTS] Filters out the TPC-DS queries that can cause flaky test results

### What changes were proposed in this pull request?

This PR proposes to filter out TPCDS v1.4 q6 and q75 in `TPCDSQueryTestSuite`.

I saw`TPCDSQueryTestSuite` failed nondeterministically because output row orders were different with those in the golden files. For example, the failure in the GA job, https://github.com/linhongliu-db/spark/runs/2507928605?check_suite_focus=true, happened because the `tpcds/q6.sql` query output rows were only sorted by `cnt`:

https://github.com/apache/spark/blob/a0c76a8755a148e2bd774edcda12fe20f2f38c75/sql/core/src/test/resources/tpcds/q6.sql#L20
Actually, `tpcds/q6.sql`  and `tpcds-v2.7.0/q6.sql` are almost the same and the only difference is that `tpcds-v2.7.0/q6.sql` sorts both `cnt` and `a.ca_state`:
https://github.com/apache/spark/blob/a0c76a8755a148e2bd774edcda12fe20f2f38c75/sql/core/src/test/resources/tpcds-v2.7.0/q6.sql#L22
So, I think it's okay just to test `tpcds-v2.7.0/q6.sql` in this case (q75 has the same issue).

### Why are the changes needed?

For stable testing.

### Does this PR introduce _any_ user-facing change?

No, dev-only.

### How was this patch tested?

GA passed.

Closes #32454 from maropu/CleanUpTpcdsQueries.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/core/src/test/scala/org/apache/spark/sql/TPCDSBase.scala', 'sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala']",The TPCDSQueryTestSuite is failing non-deterministically due to differences in output row orders compared to the golden files for the TPC-DS v1.4 q6 and q75 queries.
1fac870126c289a7ec75f45b6b61c93b9a4965d4,1659431148,"[SPARK-39932][SQL] WindowExec should clear the final partition buffer

### What changes were proposed in this pull request?

Explicitly clear final partition buffer if can not find next in `WindowExec`. The same fix in `WindowInPandasExec`

### Why are the changes needed?

We do a repartition after a window, then we need do a local sort after window due to RoundRobinPartitioning shuffle.

The error stack:
```java
ExternalAppendOnlyUnsafeRowArray INFO - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter

org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 65536 bytes of memory, got 0
	at org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:352)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:435)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:455)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:355)
```

`WindowExec` only clear buffer in `fetchNextPartition` so the final partition buffer miss to clear.

It is not a big problem since we have task completion listener.
```scala
taskContext.addTaskCompletionListener(context -> {
  cleanupResources();
});
```

This bug only affects if the window is not the last operator for this task and the follow operator like sort.

### Does this PR introduce _any_ user-facing change?

yes, bug fix

### How was this patch tested?

N/A

Closes #37358 from ulysses-you/window.

Authored-by: ulysses-you <ulyssesyou18@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala']",`WindowExec` does not properly clear final partition buffer leading to SparkOutOfMemoryError when there's a local sort after window due to RoundRobinPartitioning shuffle.
1125003b80305bd0d2c8c7cad06643f81cd1b09d,1634198071,"[SPARK-36946][PYTHON] Support time for ps.to_datetime

### What changes were proposed in this pull request?
Support time for ps.to_datetime

### Why are the changes needed?
ps.to_datetime does not support time elements
```python
# pandas
df_pan = pd.DataFrame({'year': [2015, 2016], 'month': [2, 3], 'day': [4, 5], 'hour': [2, 3], 'minute': [10, 30], 'second': [21,25]})
df_pan['datetime'] = pd.to_datetime(df_pan[['year', 'month', 'day', 'hour', 'minute']])
df_pan

   year  month  day  hour  minute  second            datetime
0  2015      2    4     2      10      21 2015-02-04 02:10:00
1  2016      3    5     3      30      25 2016-03-05 03:30:00

# pandas on spark
df_test = ps.DataFrame({'year': [2015, 2016], 'month': [2, 3], 'day': [4, 5], 'hour': [2, 3], 'minute': [10, 30], 'second': [21,25]})
df_test['datetime'] = ps.to_datetime(df_test[['year', 'month', 'day', 'hour', 'minute']])
df_test

   year  month  day  hour  minute  second   datetime
0  2015      2    4     2      10      21 2015-02-04
1  2016      3    5     3      30      25 2016-03-05
```

### Does this PR introduce _any_ user-facing change?
Yes.
After that
```python
# pandas on spark
df_test = ps.DataFrame({'year': [2015, 2016], 'month': [2, 3], 'day': [4, 5], 'hour': [2, 3], 'minute': [10, 30], 'second': [21,25]})
df_test['datetime'] = ps.to_datetime(df_test[['year', 'month', 'day', 'hour', 'minute']])
df_test

   year  month  day  hour  minute  second            datetime
0  2015      2    4     2      10      21 2015-02-04 02:10:00
1  2016      3    5     3      30      25 2016-03-05 03:30:00

```

### How was this patch tested?
Unit test

Closes #34211 from dchvn/SPARK-36946.

Authored-by: dch nguyen <dgd_contributor@viettel.com.vn>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/namespace.py', 'python/pyspark/pandas/tests/test_namespace.py']","The ps.to_datetime function does not correctly interpret time elements when converting to datetime, resulting in truncation of time details."
55dea9be62019d64d5d76619e1551956c8bb64d0,1585834218,"[SPARK-29153][CORE] Add ability to merge resource profiles within a stage with Stage Level Scheduling

### What changes were proposed in this pull request?

For the stage level scheduling feature, add the ability to optionally merged resource profiles if they were specified on multiple RDD within a stage.  There is a config to enable this feature, its off by default (spark.scheduler.resourceProfile.mergeConflicts). When the config is set to true, Spark will merge the profiles selecting the max value of each resource (cores, memory, gpu, etc).  further documentation will be added with SPARK-30322.

This also added in the ability to check if an equivalent resource profile already exists. This is so that if a user is running stages and combining the same profiles over and over again we don't get an explosion in the number of profiles.

### Why are the changes needed?

To allow users to specify resource on multiple RDD and not worry as much about if they go into the same stage and fail.

### Does this PR introduce any user-facing change?

Yes, when the config is turned on it now merges the profiles instead of errorring out.

### How was this patch tested?

Unit tests

Closes #28053 from tgravescs/SPARK-29153.

Lead-authored-by: Thomas Graves <tgraves@apache.org>
Co-authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>
","['core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala', 'core/src/main/scala/org/apache/spark/resource/ResourceProfileManager.scala', 'core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala', 'core/src/test/scala/org/apache/spark/resource/ResourceProfileManagerSuite.scala', 'core/src/test/scala/org/apache/spark/resource/ResourceProfileSuite.scala', 'core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala']",Failure occurs when users specify resources on multiple RDDs that go into the same stage. Lack of ability to merge resource profiles within a stage with Stage Level Scheduling.
7f153842041d66e9cf0465262f4458cfffda4f43,1671496158,"[SPARK-41535][SQL] Set null correctly for calendar interval fields in `InterpretedUnsafeProjection` and `InterpretedMutableProjection`

### What changes were proposed in this pull request?

In `InterpretedUnsafeProjection`, use `UnsafeWriter.write`, rather than `UnsafeWriter.setNullAt`, to set null for interval fields. Also, in `InterpretedMutableProjection`, use `InternalRow.setInterval`, rather than `InternalRow.setNullAt`, to set null for interval fields.

### Why are the changes needed?

This returns the wrong answer:
```
set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

select first(col1), last(col2) from values
(make_interval(0, 0, 0, 7, 0, 0, 0), make_interval(17, 0, 0, 2, 0, 0, 0))
as data(col1, col2);

+---------------+---------------+
|first(col1)    |last(col2)     |
+---------------+---------------+
|16 years 2 days|16 years 2 days|
+---------------+---------------+
```
In the above case, `TungstenAggregationIterator` uses `InterpretedUnsafeProjection` to create the aggregation buffer and to initialize all the fields to null. `InterpretedUnsafeProjection` incorrectly calls `UnsafeRowWriter#setNullAt`, rather than `unsafeRowWriter#write`, for the two calendar interval fields. As a result, the writer never allocates memory from the variable length region for the two intervals, and the pointers in the fixed region get left as zero. Later, when `InterpretedMutableProjection` attempts to update the first field, `UnsafeRow#setInterval` picks up the zero pointer and stores interval data on top of the null-tracking bit set. The call to UnsafeRow#setInterval for the second field also stomps the null-tracking bit set. Later updates to the null-tracking bit set (e.g., calls to `setNotNullAt`) further corrupt the interval data, turning `interval 7 years 2 days` into `interval 16 years 2 days`.

Even after one fixes the above bug in `InterpretedUnsafeProjection` so that the buffer is created correctly, `InterpretedMutableProjection` has a similar bug to SPARK-41395, except this time for calendar interval data:
```
set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

select first(col1), last(col2), max(col3) from values
(null, null, 1),
(make_interval(0, 0, 0, 7, 0, 0, 0), make_interval(17, 0, 0, 2, 0, 0, 0), 3)
as data(col1, col2, col3);

+---------------+---------------+---------+
|first(col1)    |last(col2)     |max(col3)|
+---------------+---------------+---------+
|16 years 2 days|16 years 2 days|3        |
+---------------+---------------+---------+
```
These two bugs could get exercised during codegen fallback.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New unit tests.

Closes #39117 from bersprockets/unsafe_interval_issue.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/codegen/UnsafeArrayWriter.java', 'sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/codegen/UnsafeWriter.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/InterpretedMutableProjection.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/InterpretedUnsafeProjection.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MutableProjectionSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/UnsafeRowConverterSuite.scala']","Null values are not being set correctly for calendar interval fields in 'InterpretedUnsafeProjection' and 'InterpretedMutableProjection', resulting in wrong results for certain SQL queries."
d22768a6be42bdd8af147112a01ec0910d8d0931,1569242845,"[SPARK-29036][SQL] SparkThriftServer cancel job after execute() thread interrupted

### What changes were proposed in this pull request?
Discuss in https://github.com/apache/spark/pull/25611

If cancel() and close() is called very quickly after the query is started, then they may both call cleanup() before Spark Jobs are started. Then sqlContext.sparkContext.cancelJobGroup(statementId) does nothing.
But then the execute thread can start the jobs, and only then get interrupted and exit through here. But then it will exit here, and no-one will cancel these jobs and they will keep running even though this execution has exited.

So  when execute() was interrupted by `cancel()`, when get into catch block, we should call canJobGroup again to make sure the job was canceled.

### Why are the changes needed?

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
MT

Closes #25743 from AngersZhuuuu/SPARK-29036.

Authored-by: angerszhu <angers.zhu@gmail.com>
Signed-off-by: Yuming Wang <wgyumg@gmail.com>
",['sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala'],"Quick consecutive calls to cancel() and close() after query initiation might not cancel Spark Jobs if they are started after these calls, leading to unnecessary running jobs."
33d1c16f5398ec6a7c681a771ded76ddc2c771ba,1614792236,"[SPARK-34590][TESTS] Allow JDWP debug for tests

### What changes were proposed in this pull request?

This PR proposes a new feature that allows developers to debug test code using JDWP with sbt an Maven.
More specifically, this PR introduces the following profile options.

* `jdwp-test-debug`: An profile which controls enable/disable JDWP debug
* `test.jdwp.address`: An option which corresponds to `address` option in JDWP
* `test.jdwp.suspend`: An option which corresponds to `suspend` option in JDWP
* `test.jdwp.server`: An option which corresponds to `server` option in JDWP
* `test.debug.suite`: An option which controls whether debug ScalaStyle suites (Maven only)

For `sbt`, this feature can be used like `build/sbt -Pjdwp-test-debug -Dtest.jdwp.address=localhost:9876 -Dtest.jdwp.suspend=y -Dtest.jdwp.server=y` and can be used for both JUnit tests and ScalaTest tests.

For `Maven`, this feature can be used like as follows:

(For JUnit tests) `build/mvn -Pjdwp-test-debug -Dtest.jdwp.address=localhost:9876 -Dtest.jdwp.suspend=y -Dtest.jdwp.server=y`
(For ScalaTest suites) `build/mvn -Pjdwp-test-debug -Dtest.debug.suite=true -Dtest.jdwp.address=localhost:9876 -Dtest.jdwp.suspend=y -Dtest.jdwp.server=y` (It might be useful to specify specific sub-modules like `-pl sql/core,sql/catalyst`).

### Why are the changes needed?

It's useful to debug test code.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

I confirmed the following things.

* `jdwp-tes-debug` can switch JDWP enabled/disabled
* `test.jdwp.address` can change address and port.
* `test.jdwp.suspend` can change the behavior that the target debugee suspends or not.
* `test.jdwp.server` can change the behavior that the JDWP debugger run as a server or client.
* ScalaTest suites can be debugged with Maven with setting `test.debug.suite` to `true`.

Closes #31706 from sarutak/sbt-jdwp.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['project/SparkBuild.scala'],Lack of JDWP debug support for tests in both sbt and Maven is causing difficulties in debugging test code.
cb0b51038b0ae17ba2a4a38082e322f5b6087e06,1657616350,"[SPARK-39748][SQL][SS] Include the origin logical plan for LogicalRDD if it comes from DataFrame

### What changes were proposed in this pull request?

This PR proposes to include the origin logical plan for LogicalRDD, if the LogicalRDD is built from DataFrame's RDD. Once the origin logical plan is available, LogicalRDD produces the stats from origin logical plan rather than default one.

Also, this PR applies the change to ForeachBatchSink, which seems to be the only case as of now in current codebase.

### Why are the changes needed?

The origin logical plan can be useful for several use cases, including:

1. wants to connect the two split logical plans into one (consider the case of foreachBatch sink: origin logical plan represents the plan for streaming query, and the logical plan for new Dataset represents the plan for batch query in user function)
2. inherits plan stats from origin logical plan

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New UT.

Closes #37161 from HeartSaVioR/SPARK-39748.

Authored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ForeachBatchSink.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/ForeachBatchSinkSuite.scala']",LogicalRDD built from DataFrame's RDD fails to include the origin logical plan causing incorrect stats to be produced. This issue is particularly evident in the ForeachBatchSink.
1aa665239876b32ccf81c9d170e17368c6b44c61,1642762816,"[SPARK-37972][PYTHON][MLLIB] Address typing incompatibilities with numpy==1.22.x

### What changes were proposed in this pull request?

This PR:

- Updates `Vector.norm` annotation to match numpy counterpart.
- Adds cast for numpy `dot` arguments.

### Why are the changes needed?

To resolve typing incompatibilities between `pyspark.mllib.linalg` and numpy 1.22.

```
python/pyspark/mllib/linalg/__init__.py:412: error: Argument 2 to ""norm"" has incompatible type ""Union[float, str]""; expected ""Union[None, float, Literal['fro'], Literal['nuc']]""  [arg-type]
python/pyspark/mllib/linalg/__init__.py:457: error: No overload variant of ""dot"" matches argument types ""ndarray[Any, Any]"", ""Iterable[float]""  [call-overload]
python/pyspark/mllib/linalg/__init__.py:457: note: Possible overload variant:
python/pyspark/mllib/linalg/__init__.py:457: note:     def dot(a: Union[_SupportsArray[dtype[Any]], _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float, complex, str, bytes, _NestedSequence[Union[bool, int, float, complex, str, bytes]]], b: Union[_SupportsArray[dtype[Any]], _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float, complex, str, bytes, _NestedSequence[Union[bool, int, float, complex, str, bytes]]], out: None = ...) -> Any
python/pyspark/mllib/linalg/__init__.py:457: note:     <1 more non-matching overload not shown>
python/pyspark/mllib/linalg/__init__.py:707: error: Argument 2 to ""norm"" has incompatible type ""Union[float, str]""; expected ""Union[None, float, Literal['fro'], Literal['nuc']]""  [arg-type]
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

`dev/lint-python`.

Closes #35261 from zero323/SPARK-37972.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: zero323 <mszymkiewicz@gmail.com>
",['python/pyspark/mllib/linalg/__init__.py'],"Typing incompatibilities between `pyspark.mllib.linalg` and numpy 1.22.x, particularly with `Vector.norm` annotation and arguments for numpy `dot`, resulting in argument type errors."
fa2bda5c4eabb23d5f5b3e14ccd055a2453f579f,1652251794,"[SPARK-37878][SQL][FOLLOWUP] V1Table should always carry the ""location"" property

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/35204 . https://github.com/apache/spark/pull/35204 introduced a potential regression: it removes the ""location"" table property from `V1Table` if the table is not external. The intention was to avoid putting the LOCATION clause for managed tables in `ShowCreateTableExec`. However, if we use the v2 DESCRIBE TABLE command by default in the future, this will bring a behavior change and v2 DESCRIBE TABLE command won't print the table location for managed tables.

This PR fixes this regression by using a different idea to fix the SHOW CREATE TABLE issue:
1. introduce a new reserved table property `is_managed_location`, to indicate that the location is managed by the catalog, not user given.
2. `ShowCreateTableExec` only generates the LOCATION clause if the ""location"" property is present and is not managed.

### Why are the changes needed?

avoid a potential regression

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

existing tests. We can add a test when we use v2 DESCRIBE TABLE command by default.

Closes #36498 from cloud-fan/regression.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala']","The ""location"" table property is being incorrectly removed from V1Table for non-external tables, potentially leading to a change in behaviour for the v2 DESCRIBE TABLE command."
186477c60e9cad71434b15fd9e08789740425d59,1621395467,"[SPARK-35263][TEST] Refactor ShuffleBlockFetcherIteratorSuite to reduce duplicated code

### What changes were proposed in this pull request?
Introduce new shared methods to `ShuffleBlockFetcherIteratorSuite` to replace copy-pasted code. Use modern, Scala-like Mockito `Answer` syntax.

### Why are the changes needed?
`ShuffleFetcherBlockIteratorSuite` has tons of duplicate code, like https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala#L172-L185 . It's challenging to tell what the interesting parts are vs. what is just being set to some default/unused value.

Similarly but not as bad, there are many calls like the following
```
verify(transfer, times(1)).fetchBlocks(any(), any(), any(), any(), any(), any())
when(transfer.fetchBlocks(any(), any(), any(), any(), any(), any())).thenAnswer ...
```

These changes result in about 10% reduction in both lines and characters in the file:
```bash
# Before
> wc core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala
    1063    3950   43201 core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala

# After
> wc core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala
     928    3609   39053 core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala
```

It also helps readability, e.g.:
```
    val iterator = createShuffleBlockIteratorWithDefaults(
      transfer,
      blocksByAddress,
      maxBytesInFlight = 1000L
    )
```
Now I can clearly tell that `maxBytesInFlight` is the main parameter we're interested in here.

### Does this PR introduce _any_ user-facing change?
No, test only. There aren't even any behavior changes, just refactoring.

### How was this patch tested?
Unit tests pass.

Closes #32389 from xkrogen/xkrogen-spark-35263-refactor-shuffleblockfetcheriteratorsuite.

Authored-by: Erik Krogen <xkrogen@apache.org>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
",['core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala'],"`ShuffleBlockFetcherIteratorSuite` has repeated blocks of code and redundant calls, hindering the readability and making it difficult to distinguish important parts."
7ab165b7061d9acc26523227076056e94354d204,1513905671,"[SPARK-22648][K8S] Spark on Kubernetes - Documentation

What changes were proposed in this pull request?

This PR contains documentation on the usage of Kubernetes scheduler in Spark 2.3, and a shell script to make it easier to build docker images required to use the integration. The changes detailed here are covered by https://github.com/apache/spark/pull/19717 and https://github.com/apache/spark/pull/19468 which have merged already.

How was this patch tested?
The script has been in use for releases on our fork. Rest is documentation.

cc rxin mateiz (shepherd)
k8s-big-data SIG members & contributors: foxish ash211 mccheah liyinan926 erikerlandson ssuchter varunkatta kimoonkim tnachen ifilonenko
reviewers: vanzin felixcheung jiangxb1987 mridulm

TODO:
- [x] Add dockerfiles directory to built distribution. (https://github.com/apache/spark/pull/20007)
- [x] Change references to docker to instead say ""container"" (https://github.com/apache/spark/pull/19995)
- [x] Update configuration table.
- [x] Modify spark.kubernetes.allocation.batch.delay to take time instead of int (#20032)

Author: foxish <ramanathana@google.com>

Closes #19946 from foxish/update-k8s-docs.
",['sbin/build-push-docker-images.sh'],Insufficient and outdated documentation for the usage of Kubernetes scheduler in Spark 2.3. It also lacks a streamlined process for building docker images required to use the integration.
99a979df0de95a966e1b0d780aa5329d4e62cbf7,1695012857,"[SPARK-45130][CONNECT][ML][PYTHON] Avoid Spark connect ML model to change input pandas dataframe

### What changes were proposed in this pull request?

Currently, to avoid data copy, Spark connect ML model directly changes input pandas dataframe for appending prediction columns. But we can use `pandas_df.copy(deep=False)` to shallow copy it and then append prediction columns in copied dataframe. This is easier for user to use it.

### Why are the changes needed?

This makes `pyspark.ml.connect` model `transform` method has more similar behavior with `pyspark.ml` model, i.e., the input dataframe is intact after `transform` is called. Otherwise user might be surprise at the new behavior and have to change more code to migrate their workload to `pyspark.ml.connect`

### Does this PR introduce _any_ user-facing change?

Yes.
Previous behavior:
In `pyspark.ml.connect`, `model.transform` will append new columns into input pandas dataframe, and return input dataframe object.

Changed behavior:
In `pyspark.ml.connect`, `model.transform` will shallow copy input pandas dataframe and append new columns into shallow copied pandas dataframe, then return copied pandas dataframe.

### How was this patch tested?

Unit tests.

### Was this patch authored or co-authored using generative AI tooling?

No.

Closes #42887 from WeichenXu123/spark-ml-connect-model-avoid-change-input-dataframe.

Authored-by: Weichen Xu <weichen.xu@databricks.com>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/ml/connect/base.py', 'python/pyspark/ml/connect/util.py', 'python/pyspark/ml/tests/connect/test_legacy_mode_classification.py', 'python/pyspark/ml/tests/connect/test_legacy_mode_feature.py', 'python/pyspark/ml/tests/connect/test_legacy_mode_pipeline.py']","The `transform` method in `pyspark.ml.connect` alters the input pandas dataframe by appending prediction columns, leading to unexpected modifications to the input data."
103d50b3f64b283e8fcf89f5e6c368535cf262bd,1567676121,"[SPARK-28272][SQL][PYTHON][TESTS] Convert and port 'pgSQL/aggregates_part3.sql' into UDF test base

### What changes were proposed in this pull request?

This PR proposes to port `pgSQL/aggregates_part3.sql` into UDF test base.

<details><summary>Diff comparing to 'pgSQL/aggregates_part3.sql'</summary>
<p>

```diff
diff --git a/sql/core/src/test/resources/sql-tests/results/pgSQL/aggregates_part3.sql.out b/sql/core/src/test/resources/sql-tests/results/udf/pgSQL/udf-aggregates_part3.sql.out
index f102383cb4d..eff33f280cf 100644
--- a/sql/core/src/test/resources/sql-tests/results/pgSQL/aggregates_part3.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/udf/pgSQL/udf-aggregates_part3.sql.out
 -3,7 +3,7

 -- !query 0
-select max(min(unique1)) from tenk1
+select udf(max(min(unique1))) from tenk1
 -- !query 0 schema
 struct<>
 -- !query 0 output
 -12,11 +12,11  It is not allowed to use an aggregate function in the argument of another aggreg

 -- !query 1
-select (select count(*)
-        from (values (1)) t0(inner_c))
+select udf((select udf(count(*))
+        from (values (1)) t0(inner_c))) as col
 from (values (2),(3)) t1(outer_c)
 -- !query 1 schema
-struct<scalarsubquery():bigint>
+struct<col:bigint>
 -- !query 1 output
 1
 1
```

</p>
</details>

### Why are the changes needed?

To improve test coverage in UDFs.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually tested via:

```bash
 build/sbt ""sql/test-only *SQLQueryTestSuite -- -z udf/pgSQL/udf-aggregates_part3.sql""
```

as guided in https://issues.apache.org/jira/browse/SPARK-27921

Closes #25676 from HyukjinKwon/SPARK-28272.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['sql/core/src/test/resources/sql-tests/inputs/udf/pgSQL/udf-aggregates_part3.sql'],'Lack of UDF test coverage for scenarios found in 'pgSQL/aggregates_part3.sql'.'
61035129a354d0b31c66908106238b12b1f2f7b0,1679362991,"[SPARK-42875][CONNECT][PYTHON] Fix toPandas to handle timezone and map types properly

### What changes were proposed in this pull request?

Fix `DataFrame.toPandas()` to handle timezone and map types properly.

### Why are the changes needed?

Currently `DataFrame.toPandas()` doesn't handle timezone for timestamp type, and map types properly.

For example:

```py
>>> schema = StructType().add(""ts"", TimestampType())
>>> spark.createDataFrame([(datetime(1969, 1, 1, 1, 1, 1),), (datetime(2012, 3, 3, 3, 3, 3),), (datetime(2100, 4, 4, 4, 4, 4),)], schema).toPandas()
                         ts
0 1969-01-01 01:01:01-08:00
1 2012-03-03 03:03:03-08:00
2 2100-04-04 03:04:04-08:00
```

which should be:

```py
                   ts
0 1969-01-01 01:01:01
1 2012-03-03 03:03:03
2 2100-04-04 04:04:04
```

### Does this PR introduce _any_ user-facing change?

The result of `DataFrame.toPandas()` with timestamp type and map type will be the same as PySpark.

### How was this patch tested?

Enabled the related tests.

Closes #40497 from ueshin/issues/SPARK-42875/timestamp.

Authored-by: Takuya UESHIN <ueshin@databricks.com>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/sql/connect/client.py', 'python/pyspark/sql/tests/connect/test_parity_dataframe.py', 'python/pyspark/sql/tests/test_dataframe.py']","The `DataFrame.toPandas()` method is not handling timezone for timestamp type and map types properly, leading to incorrect data being displayed."
60da25775ef46e8a58b3d5f74f2138d2b56afd25,1636695447,"[SPARK-37292][SQL] Removes outer join if it only has DISTINCT on streamed side with alias

### What changes were proposed in this pull request?

This pr enhance `EliminateOuterJoin` to remove outer join if it only has `DISTINCT` on streamed side with alias. For example:
```scala
spark.range(200L).selectExpr(""id AS a"").createTempView(""t1"")
spark.range(300L).selectExpr(""id AS b"").createTempView(""t2"")
spark.sql(""SELECT DISTINCT a AS newAlias FROM t1 LEFT JOIN t2 ON a = b"").explain(true)
```

Before this pr:
```
== Optimized Logical Plan ==
Aggregate [newAlias#8L], [newAlias#8L]
+- Project [a#2L AS newAlias#8L]
   +- Join LeftOuter, (a#2L = b#6L)
      :- Project [id#0L AS a#2L]
      :  +- Range (0, 200, step=1, splits=Some(2))
      +- Project [id#4L AS b#6L]
         +- Range (0, 300, step=1, splits=Some(2))
```

After this pr:
```
== Optimized Logical Plan ==
Aggregate [newAlias#8L], [newAlias#8L]
+- Project [id#0L AS newAlias#8L]
   +- Range (0, 200, step=1, splits=Some(2))
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #34557 from wangyum/SPARK-37292.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/AggregateOptimizeSuite.scala']",Outer join with `DISTINCT` on streamed side with alias leads to unoptimized logical plan impacting query performance.
761209c1f2af513a9db2e08c5937531cff7aeeed,1582640364,"[SPARK-30919][SQL] Make interval multiply and divide's overflow behavior consistent with other operations

### What changes were proposed in this pull request?

The current behavior of interval multiply and divide follows the ANSI SQL standard when overflow, it is compatible with other operations when `spark.sql.ansi.enabled` is true, but not compatible when `spark.sql.ansi.enabled` is false.

When `spark.sql.ansi.enabled` is false, as the factor is a double value, so it should use java's rounding or truncation behavior for casting double to integrals. when divided by zero, it returns `null`.  we also follow the natural rules for intervals as defined in the Gregorian calendar, so we do not add the month fraction to days but add days fraction to microseconds.

### Why are the changes needed?

Make interval multiply and divide's overflow behavior consistent with other interval operations

### Does this PR introduce any user-facing change?

no, these are new features in 3.0

### How was this patch tested?

add uts

Closes #27672 from yaooqinn/SPARK-30919.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/interval.sql']",Inconsistent overflow behavior with interval multiplication and division operations in SQL when 'spark.sql.ansi.enabled' is false.
32b8512912c211f7f12e717e7029e89645da9d3b,1631926095,"[SPARK-36762][PYTHON] Fix Series.isin when Series has NaN values

### What changes were proposed in this pull request?
Fix Series.isin when Series has NaN values

### Why are the changes needed?
Fix Series.isin when Series has NaN values
``` python
>>> pser = pd.Series([None, 5, None, 3, 2, 1, None, 0, 0])
>>> psser = ps.from_pandas(pser)
>>> pser.isin([1, 3, 5, None])
0    False
1     True
2    False
3     True
4    False
5     True
6    False
7    False
8    False
dtype: bool
>>> psser.isin([1, 3, 5, None])
0    None
1    True
2    None
3    True
4    None
5    True
6    None
7    None
8    None
dtype: object
```

### Does this PR introduce _any_ user-facing change?
After this PR
``` python
>>> pser = pd.Series([None, 5, None, 3, 2, 1, None, 0, 0])
>>> psser = ps.from_pandas(pser)
>>> psser.isin([1, 3, 5, None])
0    False
1     True
2    False
3     True
4    False
5     True
6    False
7    False
8    False
dtype: bool

```

### How was this patch tested?
unit tests

Closes #34005 from dgd-contributor/SPARK-36762_fix_series.isin_when_values_have_NaN.

Authored-by: dgd-contributor <dgd_contributor@viettel.com.vn>
Signed-off-by: Takuya UESHIN <ueshin@databricks.com>
","['python/pyspark/pandas/base.py', 'python/pyspark/pandas/tests/test_series.py']","The method Series.isin doesn't handle Series with NaN values correctly, resulting in inconsistent output compared to pandas' implementation."
a4601cb44e3709699f616f83d486351a1f459f8c,1569948111,"[SPARK-29055][CORE] Update driver/executors' storage memory when block is removed from BlockManager

### What changes were proposed in this pull request?

This patch proposes to fix the issue that storage memory is not decreasing even block is removed in BlockManager. Originally the issue is found while removed broadcast doesn't reflect the storage memory on driver/executors.

AppStatusListener expects the value of memory in events on block update as ""delta"" so that it adjusts driver/executors' storage memory based on delta, but when removing block BlockManager reports the delta as 0, so the storage memory is not decreased. `BlockManager.dropFromMemory` deals with this correctly, so some of path of freeing memory has been updated correctly.

### Why are the changes needed?

The storage memory in metrics in AppStatusListener is now out of sync which lets end users easy to confuse as memory leak is happening.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Modified UTs. Also manually tested via running simple query repeatedly and observe executor page of Spark UI to see the value of storage memory is decreasing as well.

Please refer the description of [SPARK-29055](https://issues.apache.org/jira/browse/SPARK-29055) to get simple reproducer.

Closes #25973 from HeartSaVioR/SPARK-29055.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['core/src/main/scala/org/apache/spark/storage/BlockManager.scala', 'core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala']","Storage memory metrics on AppStatusListener for both driver and executors are not decreasing when a block is removed from BlockManager, leading to inaccurate metrics and potential user confusion."
c5ebdc6ded0479f557541a08b1312097a9c4244f,1648043970,"[SPARK-18621][PYTHON] Make sql type reprs eval-able

### What changes were proposed in this pull request?

These changes update the `__repr__` methods of type classes in `pyspark.sql.types` to print string representations which are `eval`-able. In other words, any instance of a `DataType` will produce a repr which can be passed to `eval()` to create an identical instance.

Similar changes previously submitted: https://github.com/apache/spark/pull/25495

### Why are the changes needed?

This [bug](https://issues.apache.org/jira/browse/SPARK-18621) has been around for a while. The current implementation returns a string representation which is valid in scala rather than python. These changes fix the repr to be valid with python.

The [motivation](https://docs.python.org/3/library/functions.html#repr) is ""to return a string that would yield an object with the same value when passed to eval()"".

### Does this PR introduce _any_ user-facing change?

Example:

Current implementation:

```python
from pyspark.sql.types import *

struct = StructType([StructField('f1', StringType(), True)])
repr(struct)
# StructType(List(StructField(f1,StringType,true)))
new_struct = eval(repr(struct))
# Traceback (most recent call last):
#   File ""<input>"", line 1, in <module>
#   File ""<string>"", line 1, in <module>
# NameError: name 'List' is not defined

struct_field = StructField('f1', StringType(), True)
repr(struct_field)
# StructField(f1,StringType,true)
new_struct_field = eval(repr(struct_field))
# Traceback (most recent call last):
#   File ""<input>"", line 1, in <module>
#   File ""<string>"", line 1, in <module>
# NameError: name 'f1' is not defined
```

With changes:

```python
from pyspark.sql.types import *

struct = StructType([StructField('f1', StringType(), True)])
repr(struct)
# StructType([StructField('f1', StringType(), True)])
new_struct = eval(repr(struct))
struct == new_struct
# True

struct_field = StructField('f1', StringType(), True)
repr(struct_field)
# StructField('f1', StringType(), True)
new_struct_field = eval(repr(struct_field))
struct_field == new_struct_field
# True
```

### How was this patch tested?

The changes include a test which asserts that an instance of each type is equal to the `eval` of its `repr`, as in the above example.

Closes #34320 from crflynn/sql-types-repr.

Lead-authored-by: flynn <crf204@gmail.com>
Co-authored-by: Flynn <crflynn@users.noreply.github.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['python/pyspark/ml/functions.py', 'python/pyspark/pandas/extensions.py', 'python/pyspark/pandas/internal.py', 'python/pyspark/pandas/spark/utils.py', 'python/pyspark/pandas/tests/test_groupby.py', 'python/pyspark/pandas/tests/test_series.py', 'python/pyspark/pandas/typedef/typehints.py', 'python/pyspark/sql/dataframe.py', 'python/pyspark/sql/tests/test_dataframe.py', 'python/pyspark/sql/tests/test_types.py', 'python/pyspark/sql/types.py']","The `__repr__` methods of type classes in `pyspark.sql.types` produce a string representation that is valid in Scala instead of Python, causing failures when passed to `eval()`."
1a4c8f718f748d8c2a39f0a3b209ce606bffe958,1597443022,"[SPARK-32119][CORE] ExecutorPlugin doesn't work with Standalone Cluster and Kubernetes with --jars

### What changes were proposed in this pull request?

This PR changes Executor to load jars and files added by --jars and --files on Executor initialization.
To avoid downloading those jars/files twice, they are assosiated with `startTime` as their uploaded timestamp.

### Why are the changes needed?

ExecutorPlugin can't work with Standalone Cluster and Kubernetes
when a jar which contains plugins and files used by the plugins are added by --jars and --files option with spark-submit.

This is because jars and files added by --jars and --files are not loaded on Executor initialization.
I confirmed it works with YARN because jars/files are distributed as distributed cache.

### Does this PR introduce _any_ user-facing change?

Yes. jars/files added by --jars and --files are downloaded on each executor on initialization.

### How was this patch tested?

Added a new testcase.

Closes #28939 from sarutak/fix-plugin-issue.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['core/src/main/scala/org/apache/spark/SparkContext.scala', 'core/src/main/scala/org/apache/spark/executor/Executor.scala', 'core/src/test/java/test/org/apache/spark/JavaSparkContextSuite.java', 'core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala']","ExecutorPlugin doesn't operate as expected when used with Standalone Cluster and Kubernetes, if a jar containing plugins and files are included using the --jars and --files options with spark-submit."
924d794a6f5abb972fa07bf63adbb4ad544ef246,1565374730,"[SPARK-28656][SQL] Support `millennium`, `century` and `decade` at `extract()`

## What changes were proposed in this pull request?

In the PR, I propose new expressions `Millennium`, `Century` and `Decade`, and support additional parameters of `extract()` for feature parity with PostgreSQL (https://www.postgresql.org/docs/11/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT):

1. `millennium` - the current millennium for given date (or a timestamp implicitly casted to a date). For example, years in the 1900s are in the second millennium. The third millennium started _January 1, 2001_.
2. `century` - the current millennium for given date (or timestamp). The first century starts at 0001-01-01 AD.
3. `decade` - the current decade for given date (or timestamp). Actually, this is the year field divided by 10.

Here are examples:
```sql
spark-sql> SELECT EXTRACT(MILLENNIUM FROM DATE '1981-01-19');
2
spark-sql> SELECT EXTRACT(CENTURY FROM DATE '1981-01-19');
20
spark-sql> SELECT EXTRACT(DECADE FROM DATE '1981-01-19');
198
```

## How was this patch tested?

Added new tests to `DateExpressionsSuite` and uncommented existing tests in `pgSQL/date.sql`.

Closes #25388 from MaxGekk/extract-ext2.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/pgSQL/date.sql']","The `extract()` function lacks support for 'millennium', 'century', and 'decade' parameters, causing inconsistency with PostgreSQL's datetime functions."
d79aa36b12d9d6816679ba6348705fdd3bd0061e,1654285936,"[SPARK-39259][SQL][TEST][FOLLOWUP] Fix Scala 2.13 `ClassCastException` in `ComputeCurrentTimeSuite`

### What changes were proposed in this pull request?

Unfortunately, #36654 causes seven Scala 2.13 test failures in master/3.3 and Apache Spark 3.3 RC4.
This PR aims to fix Scala 2.13 ClassCastException in the test code.

### Why are the changes needed?

```
$ dev/change-scala-version.sh 2.13
$ build/sbt ""catalyst/testOnly *.ComputeCurrentTimeSuite"" -Pscala-2.13
...
[info] ComputeCurrentTimeSuite:
[info] - analyzer should replace current_timestamp with literals *** FAILED *** (1 second, 189 milliseconds)
[info]   java.lang.ClassCastException: scala.collection.mutable.ArrayBuffer cannot be cast to scala.collection.immutable.Seq
[info]   at org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite.literals(ComputeCurrentTimeSuite.scala:146)
[info]   at org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite.$anonfun$new$1(ComputeCurrentTimeSuite.scala:47)
...
[info] *** 7 TESTS FAILED ***
[error] Failed tests:
[error] 	org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite
[error] (catalyst / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 189 s (03:09), completed Jun 3, 2022 10:29:39 AM
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs and manually tests with Scala 2.13.

```
$ dev/change-scala-version.sh 2.13
$ build/sbt ""catalyst/testOnly *.ComputeCurrentTimeSuite"" -Pscala-2.13
...
[info] ComputeCurrentTimeSuite:
[info] - analyzer should replace current_timestamp with literals (545 milliseconds)
[info] - analyzer should replace current_date with literals (11 milliseconds)
[info] - SPARK-33469: Add current_timezone function (3 milliseconds)
[info] - analyzer should replace localtimestamp with literals (4 milliseconds)
[info] - analyzer should use equal timestamps across subqueries (182 milliseconds)
[info] - analyzer should use consistent timestamps for different timezones (13 milliseconds)
[info] - analyzer should use consistent timestamps for different timestamp functions (2 milliseconds)
[info] Run completed in 1 second, 579 milliseconds.
[info] Total number of tests run: 7
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 7, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[success] Total time: 12 s, completed Jun 3, 2022, 10:54:03 AM
```

Closes #36762 from dongjoon-hyun/SPARK-39259.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala'],Scala 2.13 test failures in master/3.3 and Apache Spark 3.3 RC4 due to ClassCastException from a mutable ArrayBuffer to an immutable Seq in ComputeCurrentTimeSuite.
958b85418034d3bdce56a7520c0728d666c79480,1686225941,"[SPARK-44006][CONNECT][PYTHON] Support cache artifacts

### What changes were proposed in this pull request?
In the PR, I propose to extend Artifact API of the Python connect client by two new methods similarly to https://github.com/apache/spark/pull/40827:
1. `is_cached_artifact()` checks the cache of the given hash presents at the server side.
2. `cache_artifact()` caches a blob in memory at the server side.

### Why are the changes needed?
To allow creating a dataframe from a large local collection. `spark.createDataFrame(...)` fails with the following error w/o the changes:
```python
pyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:
	status = StatusCode.RESOURCE_EXHAUSTED
	details = ""Sent message larger than max (629146388 vs. 134217728)""
	debug_error_string = ""UNKNOWN:Error received from peer localhost:58218 {grpc_message:""Sent message larger than max (629146388 vs. 134217728)"", grpc_status:8, created_time:""2023-06-05T18:35:50.912817+03:00""}""
```

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
By running new tests:
```
$ python/run-tests --parallelism=1 --testnames 'pyspark.sql.tests.connect.client.test_artifact ArtifactTests'
```

Closes #41465 from MaxGekk/streaming-createDataFrame-python-3.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['python/pyspark/sql/connect/client/artifact.py', 'python/pyspark/sql/tests/connect/client/test_artifact.py']",Creating a DataFrame from a large local collection in Python connect client results in RESOURCE_EXHAUSTED error due to sent message size exceeding maximum limit.
99f84892a56446b41cb36d32db7ff92460dc96da,1610439639,"[SPARK-34003][SQL][FOLLOWUP] Avoid pushing modified Char/Varchar sort attributes into aggregate for existing ones

### What changes were proposed in this pull request?

In https://github.com/apache/spark/commit/0f8e5dd445b03161a27893ba714db57919d8bcab, we partially fix the rule conflicts between `PaddingAndLengthCheckForCharVarchar` and `ResolveAggregateFunctions`, as error still exists in

sql like ```SELECT substr(v, 1, 2), sum(i) FROM t GROUP BY v ORDER BY substr(v, 1, 2)```

```sql
[info]   Failed to analyze query: org.apache.spark.sql.AnalysisException: expression 'spark_catalog.default.t.`v`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;
[info]   Project [substr(v, 1, 2)#100, sum(i)#101L]
[info]   +- Sort [aggOrder#102 ASC NULLS FIRST], true
[info]      +- !Aggregate [v#106], [substr(v#106, 1, 2) AS substr(v, 1, 2)#100, sum(cast(i#98 as bigint)) AS sum(i)#101L, substr(v#103, 1, 2) AS aggOrder#102
[info]         +- SubqueryAlias spark_catalog.default.t
[info]            +- Project [if ((length(v#97) <= 3)) v#97 else if ((length(rtrim(v#97, None)) > 3)) cast(raise_error(concat(input string of length , cast(length(v#97) as string),  exceeds varchar type length limitation: 3)) as string) else rpad(rtrim(v#97, None), 3,  ) AS v#106, i#98]
[info]               +- Relation[v#97,i#98] parquet
[info]
[info]   Project [substr(v, 1, 2)#100, sum(i)#101L]
[info]   +- Sort [aggOrder#102 ASC NULLS FIRST], true
[info]      +- !Aggregate [v#106], [substr(v#106, 1, 2) AS substr(v, 1, 2)#100, sum(cast(i#98 as bigint)) AS sum(i)#101L, substr(v#103, 1, 2) AS aggOrder#102
[info]         +- SubqueryAlias spark_catalog.default.t
[info]            +- Project [if ((length(v#97) <= 3)) v#97 else if ((length(rtrim(v#97, None)) > 3)) cast(raise_error(concat(input string of length , cast(length(v#97) as string),  exceeds varchar type length limitation: 3)) as string) else rpad(rtrim(v#97, None), 3,  ) AS v#106, i#98]
[info]               +- Relation[v#97,i#98] parquet

```
We need to look recursively into children to find char/varchars.

In this PR,  we try to resolve the full attributes including the original `Aggregate` expressions and the candidates in `SortOrder` together, then use the new re-resolved `Aggregate` expressions to determine which candidate in the `SortOrder` shall be pushed. This can avoid mismatch for the same attributes w/o this change, as the expressions returned by `executeSameContext` will change when `PaddingAndLengthCheckForCharVarchar` takes effects. W/ this change, the expressions can be matched correctly.

For those unmatched, w need to look recursively into children to find char/varchars instead of the expression itself only.

### Why are the changes needed?

bugfix

### Does this PR introduce _any_ user-facing change?

no
### How was this patch tested?

add new tests

Closes #31129 from yaooqinn/SPARK-34003-F.

Authored-by: Kent Yao <yao@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala']","Attempting to push modified Char/Varchar sort attributes into aggregate causes a conflict between 'PaddingAndLengthCheckForCharVarchar' and 'ResolveAggregateFunctions', leading to an AnalysisException error during SQL query analysis."
fe1da296da152528fc159c23e3026a0c19343780,1590725603,"[SPARK-31833][SQL][TEST-HIVE1.2] Set HiveThriftServer2 with actual port while configured 0

### What changes were proposed in this pull request?

When I was developing some stuff based on the `DeveloperAPI ` `org.apache.spark.sql.hive.thriftserver.HiveThriftServer2#startWithContext`, I need to use thrift port randomly to avoid race on ports. But the `org.apache.hive.service.cli.thrift.ThriftCLIService#getPortNumber` do not respond to me with the actual bound port but always 0.

And the server log is not right too, after starting the server, it's hard to form to the right JDBC connection string.

```
INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 0 with 5...500 worker threads
```

Indeed, the `53742` is the right port
```shell
lsof -nP -p `cat ./pid/spark-kentyao-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1.pid` | grep LISTEN
java    18990 kentyao  288u    IPv6 0x22858e3e60d6a0a7      0t0                 TCP 10.242.189.214:53723 (LISTEN)
java    18990 kentyao  290u    IPv6 0x22858e3e60d68827      0t0                 TCP *:4040 (LISTEN)
java    18990 kentyao  366u    IPv6 0x22858e3e60d66987      0t0                 TCP 10.242.189.214:53724 (LISTEN)
java    18990 kentyao  438u    IPv6 0x22858e3e60d65d47      0t0                 TCP *:53742 (LISTEN)
```

In the PR, when the port is configured 0, the `portNum` will be set to the real used port during the start process.

Also use 0 in thrift related tests to avoid potential flakiness.

### Why are the changes needed?

1 fix API bug
2 reduce test flakiness

### Does this PR introduce _any_ user-facing change?

yes, `org.apache.hive.service.cli.thrift.ThriftCLIService#getPortNumber` will always give you the actual port when it is configured 0.

### How was this patch tested?

modified unit tests

Closes #28651 from yaooqinn/SPARK-31833.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/SharedThriftServer.scala', 'sql/hive-thriftserver/v1.2/src/main/java/org/apache/hive/service/cli/thrift/ThriftBinaryCLIService.java', 'sql/hive-thriftserver/v1.2/src/main/java/org/apache/hive/service/cli/thrift/ThriftHttpCLIService.java', 'sql/hive-thriftserver/v2.3/src/main/java/org/apache/hive/service/cli/thrift/ThriftBinaryCLIService.java', 'sql/hive-thriftserver/v2.3/src/main/java/org/apache/hive/service/cli/thrift/ThriftHttpCLIService.java']","When the Thrift Server port is set to 0 for random allocation, the ThriftCLIService#getPortNumber method returns 0 instead of the actual used port. This also results in incorrect server logs."
3a840048ed3501e06260b7c5df18cc0bbdb1505c,1497201837,"Fixed typo in sql.functions

## What changes were proposed in this pull request?

I fixed a typo in the Scaladoc for the method `def struct(cols: Column*): Column`. 'retained' was misspelt as 'remained'.

## How was this patch tested?
Before:

Creates a new struct column.
   If the input column is a column in a `DataFrame`, or a derived column expression
   that is named (i.e. aliased), its name would be **remained** as the StructField's name,
   otherwise, the newly generated StructField's name would be auto generated as
   `col` with a suffix `index + 1`, i.e. col1, col2, col3, ...

After:

   Creates a new struct column.
   If the input column is a column in a `DataFrame`, or a derived column expression
   that is named (i.e. aliased), its name would be **retained** as the StructField's name,
   otherwise, the newly generated StructField's name would be auto generated as
   `col` with a suffix `index + 1`, i.e. col1, col2, col3, ...

Author: sujithjay <sujith@logistimo.com>

Closes #18254 from sujithjay/fix-typo.
",['sql/core/src/main/scala/org/apache/spark/sql/functions.scala'],"Typo in the Scaladoc for the `struct` method in sql.functions, 'retained' is misspelled as 'remained', causing confusion in understanding."
e14029b18df10db5094f8abf8b9874dbc9186b4e,1591774183,"[SPARK-26905][SQL] Add `TYPE` in the ANSI non-reserved list

### What changes were proposed in this pull request?

This PR intends to add `TYPE` in the ANSI non-reserved list because it is not reserved in the standard. See SPARK-26905 for a full set of the reserved/non-reserved keywords of `SQL:2016`.

Note: The current master behaviour is as follows;
```
scala> sql(""SET spark.sql.ansi.enabled=false"")
scala> sql(""create table t1 (type int)"")
res4: org.apache.spark.sql.DataFrame = []

scala> sql(""SET spark.sql.ansi.enabled=true"")
scala> sql(""create table t2 (type int)"")
org.apache.spark.sql.catalyst.parser.ParseException:
no viable alternative at input 'type'(line 1, pos 17)

== SQL ==
create table t2 (type int)
-----------------^^^
```

### Why are the changes needed?

To follow the ANSI/SQL standard.

### Does this PR introduce _any_ user-facing change?

Makes users use `TYPE` as identifiers.

### How was this patch tested?

Update the keyword lists in `TableIdentifierParserSuite`.

Closes #28773 from maropu/SPARK-26905.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableIdentifierParserSuite.scala']","In ANSI mode, using `TYPE` as an identifier in SQL leads to a parsing error."
9bc1cb9029482914d92de2b0557ad2c336061d0f,1664925081,"[SPARK-40585][SQL] Support double quoted identifiers

### What changes were proposed in this pull request?
In this PR I propose a new session config:
spark.sql.ansi.double_quoted_identifiers (true | false)

When true the parser will interpret a double quoted string not as a string-literal, but - in compliance with ANSI SQL  - as an identifier.
We do this by splitting the double-quoted  literal from the STRING token, onto its own BACKQUOTED_STRING token in the lexer.
in the grammar we replace all STRING references with a rule stringLit covering STRING and BACKQUOTED_STRING with the later being conditional on the config setting being false. (Note there already is a rule stringLiteral, hence the a tad quirky name).

Similarly quotedIdentifier is extended with BACKQUOTED_STRING conditional on the config being true.

Note that this is NOT PERFECT.
The escape logic for strings (backslash) is different from that of identifiers (double-doublequotes).
Unfortunately I do not know how to change this, since introducing a NEW token DOUBLE_QUOTED_IDENTIFIER has proven to break STRING - presumably due to the overlap in the pattern in the lexer.

At this point I consider this an edge-case.

### Why are the changes needed?

ANSI requires quotation of identifiers to use double quotes. We have seen customer requests for support especially around column aliases. But it makes sense to have a holistic fix rather than a context specific application.

### Does this PR introduce _any_ user-facing change?
Yes, this is a new config introducing a new feature. It is not a breaking change, though.

### How was this patch tested?
double_quoted_identifiers.sql was added to sqltests

Closes #38022 from srielau/SPARK-40585-double-quoted-identifier.

Lead-authored-by: Serge Rielau <serge.rielau@databricks.com>
Co-authored-by: Serge Rielau <srielau@users.noreply.github.com>
Co-authored-by: Gengliang Wang <ltnwgl@gmail.com>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4', 'sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParserUtils.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ParserUtilsSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala', 'sql/core/src/test/resources/sql-tests/inputs/double-quoted-identifiers.sql']","Identifiers wrapped in double quotes are mistakenly interpreted as string literals, defying ANSI SQL standards and causing inconsistency, especially around column aliases."
8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13,1577687837,"[SPARK-30348][SPARK-27510][CORE][TEST] Fix flaky test failure on ""MasterSuite.: Master should avoid ...""

### What changes were proposed in this pull request?

This patch fixes the flaky test failure on MasterSuite, ""SPARK-27510: Master should avoid dead loop while launching executor failed in Worker"".

The culprit of test failure was ironically the test ran too fast; the interval of `eventually` is by default ""15 ms"", but it took only ""8 ms"" from submitting driver to removing app from master.

```
19/12/23 15:45:06.533 dispatcher-event-loop-6 INFO Master: Registering worker localhost:9999 with 10 cores, 3.6 GiB RAM
19/12/23 15:45:06.534 dispatcher-event-loop-6 INFO Master: Driver submitted org.apache.spark.FakeClass
19/12/23 15:45:06.535 dispatcher-event-loop-6 INFO Master: Launching driver driver-20191223154506-0000 on worker 10001
19/12/23 15:45:06.536 dispatcher-event-loop-9 INFO Master: Registering app name
19/12/23 15:45:06.537 dispatcher-event-loop-9 INFO Master: Registered app name with ID app-20191223154506-0000
19/12/23 15:45:06.537 dispatcher-event-loop-9 INFO Master: Launching executor app-20191223154506-0000/0 on worker 10001
19/12/23 15:45:06.537 dispatcher-event-loop-10 INFO Master: Removing executor app-20191223154506-0000/0 because it is FAILED
...
19/12/23 15:45:06.542 dispatcher-event-loop-19 ERROR Master: Application name with ID app-20191223154506-0000 failed 10 times; removing it
```

Given the interval is already tiny, instead of lowering interval, the patch considers above case as well when verifying the status.

### Why are the changes needed?

We observed intermittent test failure in Jenkins build which should be fixed.
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115664/testReport/

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Modified UT.

Closes #27004 from HeartSaVioR/SPARK-30348.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala'],"Flaky test failure in MasterSuite due to test running too quickly, resulting in erroneous removal of the app from master."
913ab4b9fd2ac417822590b227ce4f7a9dd2ac04,1562160091,"[SPARK-28156][SQL] Self-join should not miss cached view

## What changes were proposed in this pull request?

The issue is when self-join a cached view, only one side of join uses cached relation. The cause is in `ResolveReferences` we do deduplicate for a view to have new output attributes. Then in `AliasViewChild`, the rule adds extra project under a view. So it breaks cache matching.

The fix is when dedup, we only dedup a view which has output different to its child plan. Otherwise, we dedup on the view's child plan.

```scala
val df = Seq.tabulate(5) { x => (x, x + 1, x + 2, x + 3) }.toDF(""a"", ""b"", ""c"", ""d"")
df.write.mode(""overwrite"").format(""orc"").saveAsTable(""table1"")

sql(""drop view if exists table1_vw"")
sql(""create view table1_vw as select * from table1"")

val cachedView = sql(""select a, b, c, d from table1_vw"")

cachedView.createOrReplaceTempView(""cachedview"")
cachedView.persist()

val queryDf = sql(
  s""""""select leftside.a, leftside.b
      |from cachedview leftside
      |join cachedview rightside
      |on leftside.a = rightside.a
    """""".stripMargin)
```

Query plan before this PR:
```scala
== Physical Plan ==
*(2) Project [a#12664, b#12665]
+- *(2) BroadcastHashJoin [a#12664], [a#12660], Inner, BuildRight
   :- *(2) Filter isnotnull(a#12664)
   :  +- *(2) InMemoryTableScan [a#12664, b#12665], [isnotnull(a#12664)]
   :        +- InMemoryRelation [a#12664, b#12665, c#12666, d#12667], StorageLevel(disk, memory, deserialized, 1 replicas)
   :              +- *(1) FileScan orc default.table1[a#12660,b#12661,c#12662,d#12663] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryF
ileIndex[file:/Users/viirya/repos/spark-1/sql/core/spark-warehouse/org.apache.spark.sql...., PartitionFilters: [], PushedFilters: [], ReadSchema: struc
t<a:int,b:int,c:int,d:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      +- *(1) Project [a#12660]
         +- *(1) Filter isnotnull(a#12660)
            +- *(1) FileScan orc default.table1[a#12660] Batched: true, DataFilters: [isnotnull(a#12660)], Format: ORC, Location: InMemoryFileIndex[fil
e:/Users/viirya/repos/spark-1/sql/core/spark-warehouse/org.apache.spark.sql...., PartitionFilters: [], PushedFilters: [IsNotNull(a)], ReadSchema: struc
t<a:int>
```

Query plan after this PR:
```scala
== Physical Plan ==
*(2) Project [a#12664, b#12665]
+- *(2) BroadcastHashJoin [a#12664], [a#12692], Inner, BuildRight
   :- *(2) Filter isnotnull(a#12664)
   :  +- *(2) InMemoryTableScan [a#12664, b#12665], [isnotnull(a#12664)]
   :        +- InMemoryRelation [a#12664, b#12665, c#12666, d#12667], StorageLevel(disk, memory, deserialized, 1 replicas)
   :              +- *(1) FileScan orc default.table1[a#12660,b#12661,c#12662,d#12663] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[file:/Users/viirya/repos/spark-1/sql/core/spark-warehouse/org.apache.spark.sql...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int,c:int,d:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
      +- *(1) Filter isnotnull(a#12692)
         +- *(1) InMemoryTableScan [a#12692], [isnotnull(a#12692)]
               +- InMemoryRelation [a#12692, b#12693, c#12694, d#12695], StorageLevel(disk, memory, deserialized, 1 replicas)
                     +- *(1) FileScan orc default.table1[a#12660,b#12661,c#12662,d#12663] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[file:/Users/viirya/repos/spark-1/sql/core/spark-warehouse/org.apache.spark.sql...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int,c:int,d:int>
```

## How was this patch tested?

Added test.

Closes #24960 from viirya/SPARK-28156.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/view.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala']","Self-join with a cached view only applies cache to one side of the join, due to an issue with 'ResolveReferences' deduplication and additional projection by 'AliasViewChild'."
a7d3fcd354289c1d0f5c80887b4f33beb3ad96a2,1609825046,"[SPARK-34000][CORE] Fix stageAttemptToNumSpeculativeTasks java.util.NoSuchElementException

### What changes were proposed in this pull request?
From below log, Stage 600 could be removed from `stageAttemptToNumSpeculativeTasks` by `onStageCompleted()`, but the speculative task 306.1 in stage 600 threw `NoSuchElementException` when it entered into `onTaskEnd()`.
```
21/01/04 03:00:32,259 WARN [task-result-getter-2] scheduler.TaskSetManager:69 : Lost task 306.1 in stage 600.0 (TID 283610, hdc49-mcc10-01-0510-4108-039-tess0097.stratus.rno.ebay.com, executor 27): TaskKilled (another attempt succeeded)
21/01/04 03:00:32,259 INFO [task-result-getter-2] scheduler.TaskSetManager:57 : Task 306.1 in stage 600.0 (TID 283610) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the
previous stage needs to be re-run, or because a different copy of the task has already succeeded).
21/01/04 03:00:32,259 INFO [task-result-getter-2] cluster.YarnClusterScheduler:57 : Removed TaskSet 600.0, whose tasks have all completed, from pool default
21/01/04 03:00:32,259 INFO [HiveServer2-Handler-Pool: Thread-5853] thriftserver.SparkExecuteStatementOperation:190 : Returning result set with 50 rows from offsets [5378600, 5378650) with 1fe245f8-a7f9-4ec0-bcb5-8cf324cbbb47
21/01/04 03:00:32,260 ERROR [spark-listener-group-executorManagement] scheduler.AsyncEventQueue:94 : Listener ExecutorAllocationListener threw an exception
java.util.NoSuchElementException: key not found: Stage 600 (Attempt 0)
        at scala.collection.MapLike.default(MapLike.scala:235)
        at scala.collection.MapLike.default$(MapLike.scala:234)
        at scala.collection.AbstractMap.default(Map.scala:63)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:69)
        at org.apache.spark.ExecutorAllocationManager$ExecutorAllocationListener.onTaskEnd(ExecutorAllocationManager.scala:621)
        at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:45)
        at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
        at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)
        at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)
        at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:115)
        at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:99)
        at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)
        at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
        at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:102)
        at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:97)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1320)
        at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:97)
```

### Why are the changes needed?
To avoid throwing the java.util.NoSuchElementException

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
This is a protective patch and it's not easy to reproduce in UT due to the event order is not fixed in a async queue.

Closes #31025 from LantaoJin/SPARK-34000.

Authored-by: LantaoJin <jinlantao@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala'],Removal of stage attempts from `stageAttemptToNumSpeculativeTasks` during `onStageCompleted()` leads to `NoSuchElementException` when a speculative task of the removed stage attempts to enter `onTaskEnd()`.
089c3b77e1771e42fee11ec52ef5275cca3202af,1616168873,"[SPARK-34793][SQL] Prohibit saving of day-time and year-month intervals

### What changes were proposed in this pull request?
For all built-in datasources, prohibit saving of year-month and day-time intervals that were introduced by SPARK-27793. We plan to support saving of such types at the milestone 2, see SPARK-27790.

### Why are the changes needed?
To improve user experience with Spark SQL, and print nicer error message. Current error message might confuse users:
```
scala> Seq(java.time.Period.ofMonths(1)).toDF.write.mode(""overwrite"").json(""/Users/maximgekk/tmp/123"")
21/03/18 22:44:35 ERROR FileFormatWriter: Aborting job 8de402d7-ab69-4dc0-aa8e-14ef06bd2d6b.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (192.168.1.66 executor driver): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:418)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:298)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:211)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1437)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Failed to convert value 1 (class of class java.lang.Integer}) with the type of YearMonthIntervalType to JSON.
	at scala.sys.package$.error(package.scala:30)
	at org.apache.spark.sql.catalyst.json.JacksonGenerator.$anonfun$makeWriter$23(JacksonGenerator.scala:179)
	at org.apache.spark.sql.catalyst.json.JacksonGenerator.$anonfun$makeWriter$23$adapted(JacksonGenerator.scala:176)
```

### Does this PR introduce _any_ user-facing change?
Yes. After the changes, the example above:
```
scala> Seq(java.time.Period.ofMonths(1)).toDF.write.mode(""overwrite"").json(""/Users/maximgekk/tmp/123"")
org.apache.spark.sql.AnalysisException: Cannot save interval data type into external storage.
```

### How was this patch tested?
1. Checked nested intervals:
```
scala> spark.range(1).selectExpr(""""""struct(timestamp'2021-01-02 00:01:02' - timestamp'2021-01-01 00:00:00')"""""").write.mode(""overwrite"").parquet(""/Users/maximgekk/tmp/123"")
org.apache.spark.sql.AnalysisException: Cannot save interval data type into external storage.
scala> Seq(Seq(java.time.Period.ofMonths(1))).toDF.write.mode(""overwrite"").json(""/Users/maximgekk/tmp/123"")
org.apache.spark.sql.AnalysisException: Cannot save interval data type into external storage.
```
2. By running existing test suites:
```
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *DataSourceV2DataFrameSuite""
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *DataSourceV2SQLSuite""
```

Closes #31884 from MaxGekk/ban-save-intervals.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala']","Saving year-month and day-time intervals with built-in data sources leads to job aborts and poor error messages, causing confusion for users."
d52c2de08b60930a129825d15e8f822c07e8bd31,1627355165,"[SPARK-36142][PYTHON] Follow Pandas when pow between fractional series with Na and bool literal

### What changes were proposed in this pull request?

Set the result to 1 when the exp with 0(or False).

### Why are the changes needed?
Currently, exponentiation between fractional series and bools is not consistent with pandas' behavior.
```
 >>> pser = pd.Series([1, 2, np.nan], dtype=float)
 >>> psser = ps.from_pandas(pser)
 >>> pser ** False
 0 1.0
 1 1.0
 2 1.0
 dtype: float64
 >>> psser ** False
 0 1.0
 1 1.0
 2 NaN
 dtype: float64
```
We ought to adjust that.

See more in [SPARK-36142](https://issues.apache.org/jira/browse/SPARK-36142)

### Does this PR introduce _any_ user-facing change?
Yes, it introduces a user-facing change, resulting in a different result for pow between fractional Series with missing values and bool literal, the results follow pandas behavior.

### How was this patch tested?
- Add test_pow_with_float_nan ut
- Exsiting test in test_pow

Closes #33521 from Yikun/SPARK-36142.

Authored-by: Yikun Jiang <yikunkero@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/data_type_ops/num_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_num_ops.py']","Exponentiation between fractional series and booleans result in inconsistency with Pandas' behavior, especially when dealing with NaN values in the series."
b5bc3e12a629e547e32e340ee0439bc53745d862,1578685588,"[SPARK-30312][SQL] Preserve path permission and acl when truncate table

### What changes were proposed in this pull request?

This patch proposes to preserve existing permission/acls of paths when truncate table/partition.

### Why are the changes needed?

When Spark SQL truncates table, it deletes the paths of table/partitions, then re-create new ones. If permission/acls were set on the paths, the existing permission/acls will be deleted.

We should preserve the permission/acls if possible.

### Does this PR introduce any user-facing change?

Yes. When truncate table/partition, Spark will keep permission/acls of paths.

### How was this patch tested?

Unit test.

Manual test:

1. Create a table.
2. Manually change it permission/acl
3. Truncate table
4. Check permission/acl

```scala
val df = Seq(1, 2, 3).toDF
df.write.mode(""overwrite"").saveAsTable(""test.test_truncate_table"")
val testTable = spark.table(""test.test_truncate_table"")
testTable.show()
+-----+
|value|
+-----+
|    1|
|    2|
|    3|
+-----+
// hdfs dfs -setfacl ...
// hdfs dfs -getfacl ...
sql(""truncate table test.test_truncate_table"")
// hdfs dfs -getfacl ...
val testTable2 = spark.table(""test.test_truncate_table"")
testTable2.show()
+-----+
|value|
+-----+
+-----+
```

![Screen Shot 2019-12-30 at 3 12 15 PM](https://user-images.githubusercontent.com/68855/71604577-c7875a00-2b17-11ea-913a-ba88096d20ab.jpg)

Closes #26956 from viirya/truncate-table-permission.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala']","Truncating a table or a partition in Spark SQL deletes and recreates the paths, resulting in the loss of previously assigned path permissions and ACLs."
9a539d5846814f5fd5317b9d0b7eb1a41299f092,1628501155,"[SPARK-36430][SQL] Adaptively calculate the target size when coalescing shuffle partitions in AQE

### What changes were proposed in this pull request?

This PR fixes a performance regression introduced in https://github.com/apache/spark/pull/33172

Before #33172 , the target size is adaptively calculated based on the default parallelism of the spark cluster. Sometimes it's very small and #33172 sets a min partition size to fix perf issues. Sometimes the calculated size is reasonable, such as dozens of MBs.

After #33172 , we no longer calculate the target size adaptively, and by default always coalesce the partitions into 1 MB. This can cause perf regression if the adaptively calculated size is reasonable.

This PR brings back the code that adaptively calculate the target size based on the default parallelism of the spark cluster.

### Why are the changes needed?

fix perf regression

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

existing tests

Closes #33655 from cloud-fan/minor.

Lead-authored-by: Wenchen Fan <wenchen@databricks.com>
Co-authored-by: Wenchen Fan <cloud0fan@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ShufflePartitionsUtil.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/internal/SQLConfSuite.scala']",The static target size calculated for coalescing shuffle partitions in AQE is causing performance regression compared to the adaptive target size based on default parallelism.
e804ed5e330c7dc6cd46812b520dd6b610a584b6,1574201700,"[SPARK-29691][ML][PYTHON] ensure Param objects are valid in fit, transform

modify Param._copyValues to check valid Param objects supplied as extra

### What changes were proposed in this pull request?

Estimator.fit() and Model.transform() accept a dictionary of extra parameters whose values are used to overwrite those supplied at initialization or by default.  Additionally, the ParamGridBuilder.addGrid accepts a parameter and list of values. The keys are presumed to be valid Param objects. This change adds a check that only Param objects are supplied as keys.

### Why are the changes needed?

Param objects are created by and bound to an instance of Params (Estimator, Model, or Transformer). They may be obtained from their parent as attributes, or by name through getParam.

The documentation does not state that keys must be valid Param objects, nor describe how one may be obtained. The current behavior is to silently ignore keys which are not valid Param objects.

### Does this PR introduce any user-facing change?

If the user does not pass in a Param object as required for keys in `extra` for Estimator.fit() and Model.transform(), and `param` for ParamGridBuilder.addGrid, an error will be raised indicating it is an invalid object.

### How was this patch tested?

Added method test_copy_param_extras_check to test_param.py.   Tested with Python 3.7

Closes #26527 from JohnHBauer/paramExtra.

Authored-by: John Bauer <john.h.bauer@gmail.com>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>
","['python/pyspark/ml/param/__init__.py', 'python/pyspark/ml/tests/test_param.py', 'python/pyspark/ml/tests/test_tuning.py', 'python/pyspark/ml/tuning.py']","Estimator.fit() and Model.transform() are not validating if extra parameters supplied through dictionaries are valid Param objects, leading to silent disregard of invalid keys."
f913a6e32edc57686b15611a39d9d19973f20e99,1667471968,"[SPARK-38270][BUILD][FOLLOW-UP] Exclude productElementName and productElementNames in Mima for Scala 2.13

### What changes were proposed in this pull request?
This PR is a followup of https://github.com/apache/spark/pull/35594 that recovers Mima compatibility test for Scala 2.13.

### Why are the changes needed?

To fix the Mima build broken (https://github.com/apache/spark/actions/runs/3380379538/jobs/5613108397)

```
[error] spark-core: Failed binary compatibility check against org.apache.spark:spark-core_2.13:3.3.0! Found 2 potential problems (filtered 945)
[error]  * method productElementName(Int)java.lang.String in object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages#Shutdown does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages#Shutdown.productElementName"")
[error]  * method productElementNames()scala.collection.Iterator in object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages#Shutdown does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages#Shutdown.productElementNames"")
```

### Does this PR introduce _any_ user-facing change?
No, dev-only.

### How was this patch tested?
CI in this PR should test it out. After that, scheduled jobs for Scala 2.13 will test this out

Closes #38492 from HyukjinKwon/SPARK-38270-followup.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['project/MimaExcludes.scala'],Mima compatibility test for Scala 2.13 fails due to missing methods 'productElementName' and 'productElementNames' in 'CoarseGrainedClusterMessages#Shutdown' object.
6c6626614e59b2e8d66ca853a74638d3d6267d73,1509860845,"[SPARK-22211][SQL] Remove incorrect FOJ limit pushdown

## What changes were proposed in this pull request?

It's not safe in all cases to push down a LIMIT below a FULL OUTER
JOIN. If the limit is pushed to one side of the FOJ, the physical
join operator can not tell if a row in the non-limited side would have a
match in the other side.

*If* the join operator guarantees that unmatched tuples from the limited
side are emitted before any unmatched tuples from the other side,
pushing down the limit is safe. But this is impractical for some join
implementations, e.g. SortMergeJoin.

For now, disable limit pushdown through a FULL OUTER JOIN, and we can
evaluate whether a more complicated solution is necessary in the future.

## How was this patch tested?

Ran org.apache.spark.sql.* tests. Altered full outer join tests in
LimitPushdownSuite.

Author: Henry Robinson <henry@cloudera.com>

Closes #19647 from henryr/spark-22211.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownSuite.scala']","Pushing down a LIMIT below a FULL OUTER JOIN can result in incorrect results for some join implementations, as the physical join operator cannot guarantee the order of unmatched tuples from each side."
4e84f33997302d3382a21ce5726ba2f10c91357e,1651051977,"[SPARK-39027][SQL] Output SQL statements in error messages in upper case and w/o double quotes

### What changes were proposed in this pull request?
In the PR, I propose to output any SQL statement in error messages in upper case, and apply new implementation of `QueryErrorsBase.toSQLStmt()` to all exceptions in `Query.*Errors` w/ error classes. Also this PR modifies all affected tests, see the list in the section ""How was this patch tested?"".

### Why are the changes needed?
To improve user experience with Spark SQL by highlighting SQL statements in error massage and make them more visible to users. Also this PR makes SQL statements in error messages consistent to the docs where such elements are showed in upper case w/ any quotes.

### Does this PR introduce _any_ user-facing change?
Yes. The changes might influence on error messages:

Before:
```sql
The operation ""DESC PARTITION"" is not allowed
```

After:
```sql
The operation DESC PARTITION is not allowed
```

### How was this patch tested?
By running affected test suites:
```
$ build/sbt ""sql/testOnly *QueryExecutionErrorsSuite""
$ build/sbt ""sql/testOnly *QueryParsingErrorsSuite""
$ build/sbt ""sql/testOnly *QueryCompilationErrorsSuite""
$ build/sbt ""test:testOnly *QueryCompilationErrorsDSv2Suite""
$ build/sbt ""test:testOnly *ExtractPythonUDFFromJoinConditionSuite""
$ build/sbt ""testOnly *PlanParserSuite""
$ build/sbt ""sql/testOnly *SQLQueryTestSuite -- -z transform.sql""
$ build/sbt ""sql/testOnly *SQLQueryTestSuite -- -z join-lateral.sql""
$ build/sbt ""sql/testOnly *SQLQueryTestSuite -- -z describe.sql""
$ build/sbt ""testOnly *DDLParserSuite""
```

Closes #36359 from MaxGekk/error-class-toSQLStmt-no-quotes.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['python/pyspark/sql/tests/test_udf.py', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala']","SQL error messages display statements inconsistently, using both upper and lower cases with double quotes, which decreases user visibility and deviates from documentation standards."
ee13f3e3dc3faa5152cefa91c22f8aaa8e425bb4,1505855953,"[SPARK-21969][SQL] CommandUtils.updateTableStats should call refreshTable

## What changes were proposed in this pull request?

Tables in the catalog cache are not invalidated once their statistics are updated. As a consequence, existing sessions will use the cached information even though it is not valid anymore. Consider and an example below.

```
// step 1
spark.range(100).write.saveAsTable(""tab1"")
// step 2
spark.sql(""analyze table tab1 compute statistics"")
// step 3
spark.sql(""explain cost select distinct * from tab1"").show(false)
// step 4
spark.range(100).write.mode(""append"").saveAsTable(""tab1"")
// step 5
spark.sql(""explain cost select distinct * from tab1"").show(false)
```

After step 3, the table will be present in the catalog relation cache. Step 4 will correctly update the metadata inside the catalog but will NOT invalidate the cache.

By the way, ``spark.sql(""analyze table tab1 compute statistics"")`` between step 3 and step 4 would also solve the problem.

## How was this patch tested?

Current and additional unit tests.

Author: aokolnychyi <anton.okolnychyi@sap.com>

Closes #19252 from aokolnychyi/spark-21969.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala', 'sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionTestBase.scala']","Table statistics in the catalog cache aren't invalidated after being updated, causing existing sessions to keep using outdated information, even after new data is appended to the table."
3a48ea1fe0fb85253f12d86caea01ffcb7e678d0,1584837683,"[SPARK-31184][SQL] Support getTablesByType API of Hive Client

### What changes were proposed in this pull request?
Hive 2.3+ supports `getTablesByType` API, which will provide an efficient way to get HiveTable with specific type. Now, we have following mappings when using `HiveExternalCatalog`.
```
CatalogTableType.EXTERNAL  =>  HiveTableType.EXTERNAL_TABLE
CatalogTableType.MANAGED => HiveTableType.MANAGED_TABLE
CatalogTableType.VIEW => HiveTableType.VIRTUAL_VIEW
```
Without this API, we need to achieve the goal by `getTables` + `getTablesByName` + `filter with type`.

This PR add `getTablesByType` in `HiveShim`. For those hive versions don't support this API, `UnsupportedOperationException` will be thrown. And the upper logic should catch the exception and fallback to the filter solution mentioned above.

Since the JDK11 related fix in `Hive` is not released yet, manual tests against hive 2.3.7-SNAPSHOT is done by following the instructions of SPARK-29245.

### Why are the changes needed?
This API will provide better usability and performance if we want to get a list of hiveTables with specific type. For example `HiveTableType.VIRTUAL_VIEW` corresponding to `CatalogTableType.VIEW`.

### Does this PR introduce any user-facing change?
No, this is a support function.

### How was this patch tested?
Add tests in VersionsSuite and manually run JDK11 test with following settings:

- Hive 2.3.6 Metastore on JDK8
- Hive 2.3.7-SNAPSHOT library build from source of Hive 2.3 branch
- Spark build with Hive 2.3.7-SNAPSHOT on jdk-11.0.6

Closes #27952 from Eric5553/GetTableByType.

Authored-by: Eric Wu <492960551@qq.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClient.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/client/VersionsSuite.scala']",Inefficient handling when trying to get HiveTable of a specific type due to lack of support for Hive's `getTablesByType` API. Fall-back filter solution is cumbersome and affects performance.
cc30ef8009b82c71a4b8e9caba82ed141761ab85,1514544527,"[SPARK-22916][SQL] shouldn't bias towards build right if user does not specify

## What changes were proposed in this pull request?

When there are no broadcast hints, the current spark strategies will prefer to building the right side, without considering the sizes of the two tables. This patch added the logic to consider the sizes of the two tables for the build side. To make the logic clear, the build side is determined by two steps:

1. If there are broadcast hints, the build side is determined by `broadcastSideByHints`;
2. If there are no broadcast hints, the build side is determined by `broadcastSideBySizes`;
3. If the broadcast is disabled by the config, it falls back to the next cases.

## How was this patch tested?

(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)

Please review http://spark.apache.org/contributing.html before opening a pull request.

Author: Feng Liu <fengliu@databricks.com>

Closes #20099 from liufengdb/fix-spark-strategies.
","['sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala']","When no broadcast hints are given, the current spark strategies lean towards building the right side, ignoring the size differences between the two tables."
6c047958f9fcf4cac848695915deea289c65ddc1,1610462057,"[SPARK-34084][SQL] Fix auto updating of table stats in `ALTER TABLE .. ADD PARTITION`

### What changes were proposed in this pull request?
Fix an issue in `ALTER TABLE .. ADD PARTITION` which happens when:
- A table doesn't have stats
- `spark.sql.statistics.size.autoUpdate.enabled` is `true`

In that case, `ALTER TABLE .. ADD PARTITION` does not update table stats automatically.

### Why are the changes needed?
The changes fix the issue demonstrated by the example:
```sql
spark-sql> create table tbl (col0 int, part int) partitioned by (part);
spark-sql> insert into tbl partition (part = 0) select 0;
spark-sql> set spark.sql.statistics.size.autoUpdate.enabled=true;
spark-sql> alter table tbl add partition (part = 1);
```
the `add partition` command should update table stats but it does not. There is no stats in the output of:
```
spark-sql> describe table extended tbl;
```

### Does this PR introduce _any_ user-facing change?
Yes. After the changes, `ALTER TABLE .. ADD PARTITION` updates stats even when a table does have them before the command:
```sql
spark-sql> alter table tbl add partition (part = 1);
spark-sql> describe table extended tbl;
col0	int	NULL
part	int	NULL
# Partition Information
# col_name	data_type	comment
part	int	NULL

# Detailed Table Information
...
Statistics	2 bytes
```

### How was this patch tested?
By running new UT and existing test suites:
```
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *.AlterTableAddPartitionSuite""
```

Closes #31149 from MaxGekk/fix-stats-in-add-partition.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLCommandTestUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/AlterTableAddPartitionSuite.scala']",`ALTER TABLE .. ADD PARTITION` does not correctly update table statistics when `spark.sql.statistics.size.autoUpdate.enabled` is set to true and the table doesn't have statistics.
8749c17c20062a81a0de91a0671b7bb198063406,1657691482,"[SPARK-39699][SQL] Make CollapseProject smarter about collection creation expressions

### What changes were proposed in this pull request?

The rule `CollapseProject` has been improved multiple times, to make sure it only collapses projects when there is no regresson. However, it still has a problem today. For example, if the project below has `struct(c1, expensive_expr as c2) as s`, and the project above has `s.c2 + s.c2`, then we should not collapse projects because it will duplicate the expensive expression.

This PR makes the rule smarter. If `CreateStruct` expression (or its friends) is referenced more than once, we can still collapse projects if the `CreateStruct` is only referenced by `GetStructField` and all the accesses to it are cheap. Cheap here means the result expression after we optimize `GetStructField` and `CreateStruct` is simple.
### Why are the changes needed?

To avoid bad optimized plan produced by `CollapseProject`.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

new tests

Closes #37165 from cloud-fan/qo.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala']","'CollapseProject' SQL optimization rule results in regression when collapsing projects that contain expensive expressions in struct creation, thereby leading to non-optimal plans."
0f4a9ac18f7af9238a46d6bc8a40f593556451a2,1661757727,"[SPARK-40224][SQL] Make ObjectHashAggregateExec release memory eagerly when fallback to sort-based

### What changes were proposed in this pull request?

Release the fetched key and aggregate buffer during iterating when the object hash-based dump to sort-basd.

### Why are the changes needed?

When the cardinality of grouping keys grow up over the fallbackCountThreshold, the ObjectHashAggregateExec will dump the hash map whose memory usage is not managed to sort-based. However, at this moment, we will keep the double memory overhead for both hash map and unsafe object, and it may cause OOM.

Unfortuntly, we encounter this issue. The error msg:
```
ObjectAggregationIterator INFO - Aggregation hash map size 128 reaches threshold capacity (128 entries), spilling and falling back to sort based aggregation. You may change the threshold by adjust option spark.sql.objectHashAggregate.sortBased.fallbackThreshold

#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError=""kill %p""
#   Executing /bin/sh -c ""kill 46725""...
```

### Does this PR introduce _any_ user-facing change?

To reduce the possibility of OOM issue when ObjectHashAggregateExec fallback to sort-based.

### How was this patch tested?

It's an obvious improvement that release unused object

Closes #37668 from ulysses-you/object-hash.

Authored-by: ulysses-you <ulyssesyou18@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationStoreSuite.scala']","When the cardinality of grouping keys exceeds the fallbackCountThreshold in ObjectHashAggregateExec, double memory overhead for both hash map and unsafe object can potentially lead to an out-of-memory error."
0aaf86b520606c5b8ca1b004bd8354c16d2cdee5,1631668413,"[SPARK-36709][PYTHON] Support new syntax for specifying index type and name in pandas API on Spark

### What changes were proposed in this pull request?

This PR proposes new syntax to specify the index type and name in pandas API on Spark. This is a base work for SPARK-36707.

More specifically, users now can use the type hints when typing as below:

```
pd.DataFrame[int, [int, int]]
pd.DataFrame[pdf.index.dtype, pdf.dtypes]
pd.DataFrame[(""index"", int), [(""id"", int), (""A"", int)]]
pd.DataFrame[(pdf.index.name, pdf.index.dtype), zip(pdf.columns, pdf.dtypes)]
```

Note that the types of `[(""id"", int), (""A"", int)]` or  `(""index"", int)` are matched to how you provide a compound NumPy type (see also https://numpy.org/doc/stable/user/basics.rec.html#introduction).

Therefore, the syntax will be:

**Without index:**

```
pd.DataFrame[type, type, ...]
pd.DataFrame[name: type, name: type, ...]
pd.DataFrame[dtypes instance]
pd.DataFrame[zip(names, types)]
```

(New) **With index:**

```
pd.DataFrame[index_type, [type, ...]]
pd.DataFrame[(index_name, index_type), [(name, type), ...]]
pd.DataFrame[dtype instance, dtypes instance]
pd.DataFrame[(index_name, index_type), zip(names, types)]
```

### Why are the changes needed?

Currently, there is no way to specify the type hint for index type - the type hints are converted to return type of pandas UDFs internally. Therefore, we always attach default index which degrade performance:

```python
>>> def transform(pdf) -> pd.DataFrame[int, int]:
...     pdf['A'] = pdf.id + 1
...     return pdf
...
>>> ks.range(5).koalas.apply_batch(transform)
```

```
   c0  c1
0   0   1
1   1   2
2   2   3
3   3   4
4   4   5
```

The [default index](https://koalas.readthedocs.io/en/latest/user_guide/options.html#default-index-type) (for the first column that looks unnamed) is attached when the type hint is specified. For better performance, we should have a way to work around, see also https://github.com/apache/spark/pull/33954#issuecomment-917742920 and [Specify the index column in conversion from Spark DataFrame to Koalas DataFrame](https://koalas.readthedocs.io/en/latest/user_guide/best_practices.html#specify-the-index-column-in-conversion-from-spark-dataframe-to-koalas-dataframe).

Note that this still remains as experimental because Python itself yet doesn't support such kind of typing out of the box. Once pandas completes typing support like NumPy did in `numpy.typing`, we should implement Koalas typing package, and migrate to it with leveraging pandas' typing way.

### Does this PR introduce _any_ user-facing change?

No, this PR does not yet affect any user-facing behavior in theory.

### How was this patch tested?

Unittests were added.

Closes #33954 from HyukjinKwon/SPARK-36709.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/frame.py', 'python/pyspark/pandas/series.py', 'python/pyspark/pandas/typedef/typehints.py']","The pandas API on Spark currently lacks a way to specify type hints for index types, thereby defaulting to attaching indices that degrade performance."
3884455780a214c620f309e00d5a083039746755,1585458316,"[SPARK-31087] [SQL] Add Back Multiple Removed APIs

### What changes were proposed in this pull request?

Based on the discussion in the mailing list [[Proposal] Modification to Spark's Semantic Versioning Policy](http://apache-spark-developers-list.1001551.n3.nabble.com/Proposal-Modification-to-Spark-s-Semantic-Versioning-Policy-td28938.html) , this PR is to add back the following APIs whose maintenance cost are relatively small.

- functions.toDegrees/toRadians
- functions.approxCountDistinct
- functions.monotonicallyIncreasingId
- Column.!==
- Dataset.explode
- Dataset.registerTempTable
- SQLContext.getOrCreate, setActive, clearActive, constructors

Below is the other removed APIs in the original PR, but not added back in this PR [https://issues.apache.org/jira/browse/SPARK-25908]:

- Remove some AccumulableInfo .apply() methods
- Remove non-label-specific multiclass precision/recall/fScore in favor of accuracy
- Remove unused Python StorageLevel constants
- Remove unused multiclass option in libsvm parsing
- Remove references to deprecated spark configs like spark.yarn.am.port
- Remove TaskContext.isRunningLocally
- Remove ShuffleMetrics.shuffle* methods
- Remove BaseReadWrite.context in favor of session

### Why are the changes needed?
Avoid breaking the APIs that are commonly used.

### Does this PR introduce any user-facing change?
Adding back the APIs that were removed in 3.0 branch does not introduce the user-facing changes, because Spark 3.0 has not been released.

### How was this patch tested?
Added a new test suite for these APIs.

Author: gatorsmile <gatorsmile@gmail.com>
Author: yi.wu <yi.wu@databricks.com>

Closes #27821 from gatorsmile/addAPIBackV2.
","['project/MimaExcludes.scala', 'python/pyspark/sql/dataframe.py', 'python/pyspark/sql/functions.py', 'sql/core/src/main/scala/org/apache/spark/sql/Column.scala', 'sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala', 'sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala', 'sql/core/src/main/scala/org/apache/spark/sql/functions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DeprecatedAPISuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SQLContextSuite.scala']","Some commonly used APIs, including functions.toDegrees/toRadians, functions.approxCountDistinct, functions.monotonicallyIncreasingId, Column.!==, Dataset.explode, Dataset.registerTempTable, and SQLContext methods were removed, potentially breaking users' code."
2e3ac4ffa332235f88aa8da523796617bc89db6f,1646984855,"[SPARK-38509][SQL] Unregister the `TIMESTAMPADD/DIFF` functions and remove `DATE_ADD/DIFF`

### What changes were proposed in this pull request?
1. Unregister the functions `timestampadd()` and `timestampdiff()` in `FunctionRegistry.expressions`.
2. Remove the aliases `date_add` for `timestampadd()` and `date_diff` for `timestampdiff()`.
3. Align tests (regenerate golden files) to the syntax rules
```
primaryExpression
    | name=(TIMESTAMPADD | DATEADD) LEFT_PAREN unit=identifier COMMA unitsAmount=valueExpression COMMA timestamp=valueExpression RIGHT_PAREN             #timestampadd
    | name=(TIMESTAMPDIFF | DATEDIFF) LEFT_PAREN unit=identifier COMMA startTimestamp=valueExpression COMMA endTimestamp=valueExpression RIGHT_PAREN    #timestampdiff
```
where the first parameter `unit` can have one of the identifiers:
   - YEAR
   - QUARTER
   - MONTH
   - WEEK
   - DAY, DAYOFYEAR (valid for timestampadd)
   - HOUR
   - MINUTE
   - SECOND
   - MILLISECOND
   - MICROSECOND

### Why are the changes needed?
1. The `timestampadd()`/`timestampdiff()` functions (and their aliases) with arbitrary string column as the first parameter is not require by any standard.
2. Remove the functions and aliases should reduce maintenance cost.

### Does this PR introduce _any_ user-facing change?
No, the functions and aliases haven't been released yet.

### How was this patch tested?
By running the affected test suites:
```
$ build/sbt ""test:testOnly *QueryExecutionErrorsSuite""
$ build/sbt ""test:testOnly *DateTimeUtilsSuite""
$ build/sbt ""sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite""
$ build/sbt ""sql/testOnly *ExpressionsSchemaSuite""
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z timestamp.sql""
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z timestamp-ansi.sql""
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z datetime-legacy.sql""
$ build/sbt ""test:testOnly *DateExpressionsSuite""
$ build/sbt ""test:testOnly *SQLKeywordSuite""
```

Closes #35805 from MaxGekk/unregister-timestampadd-func.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4', 'sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/date.sql', 'sql/core/src/test/resources/sql-tests/inputs/timestamp-ntz.sql', 'sql/core/src/test/resources/sql-tests/inputs/timestamp.sql', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala']","The functions `timestampadd()`, `timestampdiff()` and their aliases `date_add`, `date_diff` are not required by any standard and accept arbitrary string column as the first parameter, leading to potential misuse and maintenance issues."
fab563b9bd1581112462c0fc0b299ad6510b6564,1519832653,"[SPARK-23517][PYTHON] Make `pyspark.util._exception_message` produce the trace from Java side by Py4JJavaError

## What changes were proposed in this pull request?

This PR proposes for `pyspark.util._exception_message` to produce the trace from Java side by `Py4JJavaError`.

Currently, in Python 2, it uses `message` attribute which `Py4JJavaError` didn't happen to have:

```python
>>> from pyspark.util import _exception_message
>>> try:
...     sc._jvm.java.lang.String(None)
... except Exception as e:
...     pass
...
>>> e.message
''
```

Seems we should use `str` instead for now:

 https://github.com/bartdag/py4j/blob/aa6c53b59027925a426eb09b58c453de02c21b7c/py4j-python/src/py4j/protocol.py#L412

but this doesn't address the problem with non-ascii string from Java side -
 `https://github.com/bartdag/py4j/issues/306`

So, we could directly call `__str__()`:

```python
>>> e.__str__()
u'An error occurred while calling None.java.lang.String.\n: java.lang.NullPointerException\n\tat java.lang.String.<init>(String.java:588)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n'
```

which doesn't type coerce unicodes to `str` in Python 2.

This can be actually a problem:

```python
from pyspark.sql.functions import udf
spark.conf.set(""spark.sql.execution.arrow.enabled"", True)
spark.range(1).select(udf(lambda x: [[]])()).toPandas()
```

**Before**

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 2009, in toPandas
    raise RuntimeError(""%s\n%s"" % (_exception_message(e), msg))
RuntimeError:
Note: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true. Please set it to false to disable this.
```

**After**

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 2009, in toPandas
    raise RuntimeError(""%s\n%s"" % (_exception_message(e), msg))
RuntimeError: An error occurred while calling o47.collectAsArrowToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/.../spark/python/pyspark/worker.py"", line 245, in main
    process()
  File ""/.../spark/python/pyspark/worker.py"", line 240, in process
...
Note: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true. Please set it to false to disable this.
```

## How was this patch tested?

Manually tested and unit tests were added.

Author: hyukjinkwon <gurwls223@gmail.com>

Closes #20680 from HyukjinKwon/SPARK-23517.
","['python/pyspark/tests.py', 'python/pyspark/util.py']","The `pyspark.util._exception_message` function isn't returning the correct exception trace in Python 2 due to the lack of a `message` attribute in the `Py4JJavaError`, leading to crucial issues with non-ascii strings and potential type coercion problems."
038b185736fbc5bcdaf00ddcd59593571745f25f,1506369025,"[SPARK-22103] Move HashAggregateExec parent consume to a separate function in codegen

## What changes were proposed in this pull request?

HashAggregateExec codegen uses two paths for fast hash table and a generic one.
It generates code paths for iterating over both, and both code paths generate the consume code of the parent operator, resulting in that code being expanded twice.
This leads to a long generated function that might be an issue for the compiler (see e.g. SPARK-21603).
I propose to remove the double expansion by generating the consume code in a helper function that can just be called from both iterating loops.

An issue with separating the `consume` code to a helper function was that a number of places relied and assumed on being in the scope of an outside `produce` loop and e.g. use `continue` to jump out.
I replaced such code flows with nested scopes. It is code that should be handled the same by compiler, while getting rid of depending on assumptions that are outside of the `consume`'s own scope.

## How was this patch tested?

Existing test coverage.

Author: Juliusz Sompolski <julek@databricks.com>

Closes #19324 from juliuszsompolski/aggrconsumecodegen.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala']","'Double expansion in HashAggregateExec codegen results in a long generated function, potentially causing issues with the compiler. Code also overly reliant on being within scope of an outside `produce` loop.'"
aec79534abf819e7981babc73d13450ea8e49b08,1658286788,"[SPARK-39148][SQL] DS V2 aggregate push down can work with OFFSET or LIMIT

### What changes were proposed in this pull request?

This PR refactors the v2 agg pushdown code. The main change is, now we don't build the `Scan` immediately when pushing agg. We did it so before because we want to know the data schema with agg pushed, then we can add cast when rewriting the query plan after pushdown. But the problem is, we build `Scan` too early and can't push down any more operators, while it's common to see LIMIT/OFFSET after agg.

The idea of the refactor is, we don't need to know the data schema with agg pushed. We just give an expectation (the data type should be the same of Spark agg functions), use it to define the output of `ScanBuilderHolder`, and then rewrite the query plan. Later on, when we build the `Scan` and replace `ScanBuilderHolder` with `DataSourceV2ScanRelation`, we check the actual data schema and add a `Project` to do type cast if necessary.

### Why are the changes needed?

support pushing down LIMIT/OFFSET after agg.

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

updated tests

Closes #37195 from cloud-fan/agg.

Lead-authored-by: Wenchen Fan <wenchen@databricks.com>
Co-authored-by: Wenchen Fan <cloud0fan@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala', 'sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala']",Current implementation of aggregation pushdown in DS V2 doesn't support further operator pushdowns such as LIMIT/OFFSET due to early `Scan` building.
4818847e8765488cc4b8d43c367cd296b8dd840f,1614671547,"[SPARK-34578][SQL][TESTS][TEST-MAVEN] Refactor ORC encryption tests and ignore ORC shim loaded by old Hadoop library

### What changes were proposed in this pull request?

1. This PR aims to ignore ORC encryption tests when ORC shim is loaded by old Hadoop library by some other tests. The test coverage is preserved by Jenkins SBT runs and GitHub Action jobs. This PR only aims to recover Maven Jenkins jobs.
2. In addition, this PR simplifies SBT testing by refactor the test config to `SparkBuild.scala/pom.xml` and remove `DedicatedJVMTest`. This will remove one GitHub Action job which was recently added for `DedicatedJVMTest` tag.

### Why are the changes needed?

Currently, Maven test fails when it runs in a batch mode because `HadoopShimsPre2_3$NullKeyProvider` is loaded.

**MVN COMMAND**
```
$ mvn test -pl sql/core --am -Dtest=none -DwildcardSuites=org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite,org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite
```

**BEFORE**
```
- Write and read an encrypted table *** FAILED ***
...
  Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (localhost executor driver): java.lang.IllegalArgumentException: Unknown key pii
	at org.apache.orc.impl.HadoopShimsPre2_3$NullKeyProvider.getCurrentKeyVersion(HadoopShimsPre2_3.java:71)
	at org.apache.orc.impl.WriterImpl.getKey(WriterImpl.java:871)
```

**AFTER**
```
OrcV1QuerySuite
...
OrcEncryptionSuite:
- Write and read an encrypted file !!! CANCELED !!!
  [] was empty org.apache.orc.impl.HadoopShimsPre2_3$NullKeyProvider1b705f65 doesn't has the test keys. ORC shim is created with old Hadoop libraries (OrcEncryptionSuite.scala:39)
- Write and read an encrypted table !!! CANCELED !!!
  [] was empty org.apache.orc.impl.HadoopShimsPre2_3$NullKeyProvider22adeee1 doesn't has the test keys. ORC shim is created with old Hadoop libraries (OrcEncryptionSuite.scala:67)
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the Jenkins Maven tests.

For SBT command,
- the test suite required a dedicated JVM (Before)
- the test suite doesn't require a dedicated JVM (After)
```
$ build/sbt ""sql/testOnly *.OrcV1QuerySuite *.OrcEncryptionSuite""
...
[info] OrcV1QuerySuite
...
[info] - SPARK-20728 Make ORCFileFormat configurable between sql/hive and sql/core (26 milliseconds)
[info] OrcEncryptionSuite:
[info] - Write and read an encrypted file (431 milliseconds)
[info] - Write and read an encrypted table (359 milliseconds)
[info] All tests passed.
[info] Passed: Total 35, Failed 0, Errors 0, Passed 35
```

Closes #31697 from dongjoon-hyun/SPARK-34578-TEST.

Lead-authored-by: Dongjoon Hyun <dhyun@apple.com>
Co-authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['common/tags/src/test/java/org/apache/spark/tags/DedicatedJVMTest.java', 'project/SparkBuild.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcEncryptionSuite.scala']","ORC encryption tests fail when run in batch mode due to old Hadoop library-loaded ORC shim, causing `HadoopShimsPre2_3$NullKeyProvider` loading issues. Maven testing failure observed with `IllegalArgumentException: Unknown key pii`."
56264fb5d3ad1a488be5e08feb2e0304d1c2ed6a,1591848940,"[SPARK-31965][TESTS][PYTHON] Move doctests related to Java function registration to test conditionally

### What changes were proposed in this pull request?

This PR proposes to move the doctests in `registerJavaUDAF` and `registerJavaFunction` to the proper unittests that run conditionally when the test classes are present.

Both tests are dependent on the test classes in JVM side, `test.org.apache.spark.sql.JavaStringLength` and `test.org.apache.spark.sql.MyDoubleAvg`. So if you run the tests against the plain `sbt package`, it fails as below:

```
**********************************************************************
File ""/.../spark/python/pyspark/sql/udf.py"", line 366, in pyspark.sql.udf.UDFRegistration.registerJavaFunction
Failed example:
    spark.udf.registerJavaFunction(
        ""javaStringLength"", ""test.org.apache.spark.sql.JavaStringLength"", IntegerType())
Exception raised:
    Traceback (most recent call last):
   ...
test.org.apache.spark.sql.JavaStringLength, please make sure it is on the classpath;
...
   6 of   7 in pyspark.sql.udf.UDFRegistration.registerJavaFunction
   2 of   4 in pyspark.sql.udf.UDFRegistration.registerJavaUDAF
***Test Failed*** 8 failures.
```

### Why are the changes needed?

In order to support to run the tests against the plain SBT build. See also https://spark.apache.org/developer-tools.html

### Does this PR introduce _any_ user-facing change?

No, it's test-only.

### How was this patch tested?

Manually tested as below:

```bash
./build/sbt -DskipTests -Phive-thriftserver clean package
cd python
./run-tests --python-executable=python3 --testname=""pyspark.sql.udf UserDefinedFunction""
./run-tests --python-executable=python3 --testname=""pyspark.sql.tests.test_udf UDFTests""
```

```bash
./build/sbt -DskipTests -Phive-thriftserver clean test:package
cd python
./run-tests --python-executable=python3 --testname=""pyspark.sql.udf UserDefinedFunction""
./run-tests --python-executable=python3 --testname=""pyspark.sql.tests.test_udf UDFTests""
```

Closes #28795 from HyukjinKwon/SPARK-31965.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['python/pyspark/sql/tests/test_udf.py', 'python/pyspark/sql/udf.py']",Tests involving the use of registerJavaUDAF and registerJavaFunction fail when run against the plain `sbt package` due to dependency on JVM side test classes that may not be available on the classpath.
e0cb2eb104e02f672250fd114b81d63b8d1106b1,1661839342,"[SPARK-40135][PS] Support `data` mixed with `index` in DataFrame creation

### What changes were proposed in this pull request?

1. param `data` accepts `ps.DataFrame`, since `pd.DataFrame(...)` also accepts another `pd.DataFrame` as a `data` param;
2. when `data` is distributed (`InternalFrame`/`SparkDataFrame`/`ps.DataFrame`/`ps.Series`), apply `combine_frames` to combine `data` and `index`;
3. when `data` is a local data (`pd.DataFrame`/list/...), directly collect `index`, and apply `pd.DataFrame(...)` internally
- 3.1 `index` should be small, when `data` fits in memory
- 3.2 `combine_frames` can not work with a multi-index for now
- 3.3 most important, to make sure queries like `ps.DataFrame([1, 2], index=ps.Index([1, 2]))` returns same result with `pd.DataFrame([1, 2], index=pd.Index([1, 2]))`, if we apply `combine_frames` in this case, then the results will be different

### Why are the changes needed?
to support `index` in DataFrame creation

### Does this PR introduce _any_ user-facing change?
Yes, `ps.Index` is supported

### How was this patch tested?
added UT

Closes #37564 from zhengruifeng/ps_init_distributed.

Authored-by: Ruifeng Zheng <ruifengz@apache.org>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/pandas/frame.py', 'python/pyspark/pandas/tests/test_dataframe.py']",Issue with DataFrame creation not supporting mixed `data` with `index` inputs. Inconsistencies observed while comparing similar `ps.DataFrame` and `pd.DataFrame` creation queries.
8d3199d54144c39b46576a86ecd000094524e8cc,1696783084,"[SPARK-45462][CORE][WEBUI] Show `Duration` in `ApplicationPage`

### What changes were proposed in this pull request?

This PR aims to show `Duration` information in `ApplicationPage`.

### Why are the changes needed?

`Duration` is already exposed in `MasterPage` as `Duration` column in the list.

![Screenshot 2023-10-08 at 4 58 03 AM](https://github.com/apache/spark/assets/9700541/587bfba8-9ffb-4d83-8b4a-7c580c644cb4)

When we visit the Application detail page, we had better show it consistently. Otherwise, we need to go back to the list and search the app id. In the following images, `Duration` is added after `Submit Date` row.

- **Live App**
![Screenshot 2023-10-08 at 4 53 52 AM](https://github.com/apache/spark/assets/9700541/9b7bdd24-f9d4-4eab-b3a1-b36cc3e29600)

- **Completed App**
![Screenshot 2023-10-08 at 4 56 57 AM](https://github.com/apache/spark/assets/9700541/54897ba3-5fee-4df2-9486-9e62eb20e7ed)

### Does this PR introduce _any_ user-facing change?

No. This is a new addition to the UI.

### How was this patch tested?

Manual review.

### Was this patch authored or co-authored using generative AI tooling?

No.

Closes #43279 from dongjoon-hyun/SPARK-45462.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala'],"'Duration' information is missing in the 'ApplicationPage' of Spark's WEBUI, making it inconsistent with 'MasterPage' and hindering seamless user experience."
fda4070ea934cac081162f70d9ea7fe2e9a07cd4,1571280716,"[SPARK-29283][SQL] Error message is hidden when query from JDBC, especially enabled adaptive execution

### What changes were proposed in this pull request?
When adaptive execution is enabled, the Spark users who connected from JDBC always get adaptive execution error whatever the under root cause is. It's very confused. We have to check the driver log to find out why.
```shell
0: jdbc:hive2://localhost:10000> SELECT * FROM testData join testData2 ON key = v;
SELECT * FROM testData join testData2 ON key = v;
Error: Error running query: org.apache.spark.SparkException: Adaptive execution failed due to stage materialization failures. (state=,code=0)
0: jdbc:hive2://localhost:10000>
```

For example, a job queried from JDBC failed due to HDFS missing block. User still get the error message `Adaptive execution failed due to stage materialization failures`.

The easiest way to reproduce is changing the code of `AdaptiveSparkPlanExec`, to let it throws out  an exception when it faces `StageSuccess`.
```scala
  case class AdaptiveSparkPlanExec(
      events.drainTo(rem)
         (Seq(nextMsg) ++ rem.asScala).foreach {
           case StageSuccess(stage, res) =>
//            stage.resultOption = Some(res)
            val ex = new SparkException(""Wrapper Exception"",
              new IllegalArgumentException(""Root cause is IllegalArgumentException for Test""))
            errors.append(
              new SparkException(s""Failed to materialize query stage: ${stage.treeString}"", ex))
           case StageFailure(stage, ex) =>
             errors.append(
               new SparkException(s""Failed to materialize query stage: ${stage.treeString}"", ex))
```

### Why are the changes needed?
To make the error message more user-friend and more useful for query from JDBC.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Manually test query:
```shell
0: jdbc:hive2://localhost:10000> CREATE TEMPORARY VIEW testData (key, value) AS SELECT explode(array(1, 2, 3, 4)), cast(substring(rand(), 3, 4) as string);
CREATE TEMPORARY VIEW testData (key, value) AS SELECT explode(array(1, 2, 3, 4)), cast(substring(rand(), 3, 4) as string);
+---------+--+
| Result  |
+---------+--+
+---------+--+
No rows selected (0.225 seconds)
0: jdbc:hive2://localhost:10000> CREATE TEMPORARY VIEW testData2 (k, v) AS SELECT explode(array(1, 1, 2, 2)), cast(substring(rand(), 3, 4) as int);
CREATE TEMPORARY VIEW testData2 (k, v) AS SELECT explode(array(1, 1, 2, 2)), cast(substring(rand(), 3, 4) as int);
+---------+--+
| Result  |
+---------+--+
+---------+--+
No rows selected (0.043 seconds)
```
Before:
```shell
0: jdbc:hive2://localhost:10000> SELECT * FROM testData join testData2 ON key = v;
SELECT * FROM testData join testData2 ON key = v;
Error: Error running query: org.apache.spark.SparkException: Adaptive execution failed due to stage materialization failures. (state=,code=0)
0: jdbc:hive2://localhost:10000>
```
After:
```shell
0: jdbc:hive2://localhost:10000> SELECT * FROM testData join testData2 ON key = v;
SELECT * FROM testData join testData2 ON key = v;
Error: Error running query: java.lang.IllegalArgumentException: Root cause is IllegalArgumentException for Test (state=,code=0)
0: jdbc:hive2://localhost:10000>
```

Closes #25960 from LantaoJin/SPARK-29283.

Authored-by: lajin <lajin@ebay.com>
Signed-off-by: Yuming Wang <wgyumg@gmail.com>
","['sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetCatalogsOperation.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetColumnsOperation.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetFunctionsOperation.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetSchemasOperation.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetTableTypesOperation.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetTablesOperation.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetTypeInfoOperation.scala']","When running queries from JDBC with adaptive execution enabled, the error message is always related to adaptive execution failure, irrespective of the root cause, leading to user confusion."
115b8a180f41fe957341b0725c3f34499267bb92,1625797839,"[SPARK-36062][PYTHON] Try to capture faulthanlder when a Python worker crashes

### What changes were proposed in this pull request?

Try to capture the error message from the `faulthandler` when the Python worker crashes.

### Why are the changes needed?

Currently, we just see an error message saying `""exited unexpectedly (crashed)""` when the UDFs causes the Python worker to crash by like segmentation fault.
We should take advantage of [`faulthandler`](https://docs.python.org/3/library/faulthandler.html) and try to capture the error message from the `faulthandler`.

### Does this PR introduce _any_ user-facing change?

Yes, when a Spark config `spark.python.worker.faulthandler.enabled` is `true`, the stack trace will be seen in the error message when the Python worker crashes.

```py
>>> def f():
...   import ctypes
...   ctypes.string_at(0)
...
>>> sc.parallelize([1]).map(lambda x: f()).count()
```

```
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed): Fatal Python error: Segmentation fault

Current thread 0x000000010965b5c0 (most recent call first):
  File ""/.../ctypes/__init__.py"", line 525 in string_at
  File ""<stdin>"", line 3 in f
  File ""<stdin>"", line 1 in <lambda>
...
```

### How was this patch tested?

Added some tests, and manually.

Closes #33273 from ueshin/issues/SPARK-36062/faulthandler.

Authored-by: Takuya UESHIN <ueshin@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['core/src/main/scala/org/apache/spark/SparkEnv.scala', 'core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala', 'core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala', 'core/src/main/scala/org/apache/spark/internal/config/Python.scala', 'python/pyspark/tests/test_worker.py', 'python/pyspark/worker.py', 'sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonArrowOutput.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala']","UDF-induced Python worker crashes only output an unclear 'exited unexpectedly' error message, without providing substantial stack trace or crash details, hindering debugging efforts."
08fd5013ad09cdb11a021f9a652d40612d104ce9,1641295444,"[SPARK-37750][SQL] ANSI mode: optionally return null result if element not exists in array/map

### What changes were proposed in this pull request?

Add a new configuration `spark.sql.ansi.failOnElementNotExists` which controls whether throwing exceptions or returning null results when element not exists in the [] operator in array/map type

The default value of the new configuration is true.

### Why are the changes needed?

* Provide an alternative way for Spark SQL users who replies on null results when element not exists in array/map, e.g. `select .. where array[index] is not null` or `select .. where map[key] is not null`
* Map type is not part of the ANSI SQL type. There can be arguments that map[key] should return null if key not exist.

### Does this PR introduce _any_ user-facing change?

Yes, providing a new option `spark.sql.ansi.failOnElementNotExists` which can optionally return null result if element not exists in array/map. However, the default behavior is not changed.

### How was this patch tested?

Unit tests

Closes #35031 from gengliangwang/safeAccess.

Authored-by: Gengliang Wang <gengliang@apache.org>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/test/resources/sql-tests/inputs/ansi/array.sql', 'sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql']","In ANSI mode, accessing non-existing elements in array/map results in exceptions, no option for returning a null instead, which some users might prefer."
98f0a219915dc9ed696602b9bfad82d9cf6c4113,1603824848,"[SPARK-33231][SPARK-33262][CORE] Make pod allocation executor timeouts configurable & allow scheduling with pending pods

### What changes were proposed in this pull request?

Make pod allocation executor timeouts configurable. Keep all known pods in mind when allocating executors to avoid over-allocating if the pending time is much higher than the allocation interval.

This PR increases the default wait time to 600s from the current 60s.

Since nodes can now remain ""pending"" for long periods of time, we allow additional batches to be scheduled during pending allocation but keep the total number of pods in account.

### Why are the changes needed?
The current executor timeouts do not match that of all real world clusters especially under load. While this can be worked around by increasing the allocation batch delay, that will decrease the speed at which the total number of executors will be able to be requested.

The increase in default timeout is needed to handle real-world testing environments I've encountered on moderately busy clusters and K8s clusters with their own underlying dynamic scale-up of hardware (e.g. GKE, EKS, etc.)

### Does this PR introduce _any_ user-facing change?

Yes new configuration property

### How was this patch tested?

Updated existing test to use the timeout from the new configuration property. Verified test failed without the update.

Closes #30155 from holdenk/SPARK-33231-make-pod-creation-timeout-configurable.

Authored-by: Holden Karau <hkarau@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocatorSuite.scala']","Current executor timeouts do not align with real-world clusters under load conditions, leading to under-allocation of executors. Nodes remaining ""pending"" for long durations could result in over-scheduling."
614a5cc600b0ac01c5d03b1dc5fdf996ef18ac0e,1557252535,"[SPARK-27624][CORE] Fix CalenderInterval to show an empty interval correctly

## What changes were proposed in this pull request?

If the interval is `0`, it doesn't show both the value `0` and the unit at all. For example, this happens in the explain plans and Spark Web UI on `EventTimeWatermark` diagram.

**BEFORE**
```scala
scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""1 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#0: timestamp, interval 1 microseconds
+- StreamingRelation FileSource[/tmp/t], [ts#0]

scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""0 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#3: timestamp, interval
+- StreamingRelation FileSource[/tmp/t], [ts#3]
```

**AFTER**
```scala
scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""1 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#0: timestamp, interval 1 microseconds
+- StreamingRelation FileSource[/tmp/t], [ts#0]

scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""0 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#3: timestamp, interval 0 microseconds
+- StreamingRelation FileSource[/tmp/t], [ts#3]
```

## How was this patch tested?

Pass the Jenkins with the updated test case.

Closes #24516 from dongjoon-hyun/SPARK-27624.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['common/unsafe/src/main/java/org/apache/spark/unsafe/types/CalendarInterval.java', 'common/unsafe/src/test/java/org/apache/spark/unsafe/types/CalendarIntervalSuite.java']","Interval of '0' in EventTimeWatermark does not show the value '0' and the unit, leading to incorrect representation in the explain plans and Spark Web UI."
21413b7dd4e19f725b21b92cddfbe73d1b381a05,1604568077,"[SPARK-30294][SS] Explicitly defines read-only StateStore and optimize for HDFSBackedStateStore

### What changes were proposed in this pull request?

There's a concept of 'read-only' and 'read+write' state store in Spark which is defined ""implicitly"". Spark doesn't prevent write for 'read-only' state store; Spark just assumes read-only stateful operator will not modify the state store. Given it's not defined explicitly, the instance of state store has to be implemented as 'read+write' even it's being used as 'read-only', which sometimes brings confusion.

For example, abort() in HDFSBackedStateStore - https://github.com/apache/spark/blob/d38f8167483d4d79e8360f24a8c0bffd51460659/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L143-L155

The comment sounds as if statement works differently between 'read-only' and 'read+write', but that's not true as both state store has state initialized as UPDATING (no difference). So 'read-only' state also creates the temporary file, initializes output streams to write to temporary file, closes output streams, and finally deletes the temporary file. This unnecessary operations are being done per batch/partition.

This patch explicitly defines 'read-only' StateStore, and enables state store provider to create 'read-only' StateStore instance if requested. Relevant code paths are modified, as well as 'read-only' StateStore implementation for HDFSBackedStateStore is introduced. The new implementation gets rid of unnecessary operations explained above.

In point of backward-compatibility view, the only thing being changed in public API side is `StateStoreProvider`. The trait `StateStoreProvider` has to be changed to allow requesting 'read-only' StateStore; this patch adds default implementation which leverages 'read+write' StateStore but wrapping with 'write-protected' StateStore instance, so that custom providers don't need to change their code to reflect the change. But if the providers can optimize for read-only workload, they'll be happy to make a change.

Please note that this patch makes ReadOnlyStateStore extend StateStore and being referred as StateStore, as StateStore is being used in so many places and it's not easy to support both traits if we differentiate them. So unfortunately these write methods are still exposed for read-only state; it just throws UnsupportedOperationException.

### Why are the changes needed?

The new API opens the chance to optimize read-only state store instance compared with read+write state store instance. HDFSBackedStateStoreProvider is modified to provide read-only version of state store which doesn't deal with temporary file as well as state machine.

### Does this PR introduce any user-facing change?

Clearly ""no"" for most end users, and also ""no"" for custom state store providers as it doesn't touch trait `StateStore` as well as provides default implementation for added method in trait `StateStoreProvider`.

### How was this patch tested?

Modified UT. Existing UTs ensure the change doesn't break anything.

Closes #26935 from HeartSaVioR/SPARK-30294.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StreamingAggregationStateManager.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreSuite.scala']",The read-only concept in Spark's state store is not explicitly defined which leads to confusion and unnecessary operations when using HDFSBackedStateStore.
8ac06474f8cfa8e5619f817aaeea29a77ec8a2a4,1649637879,"[SPARK-38830][CORE] Warn on corrupted block messages

### What changes were proposed in this pull request?

This PR aims to warn when `NettyBlockRpcServer` received a corrupted block RPC message or under attack.

- `IllegalArgumentException`: When the type is unknown/invalid when decoding. This fails at Spark layer.
- `NegativeArraySizeException`: When the size read is negative. This fails at Spark layer during buffer creation.
- `IndexOutOfBoundsException`: When the data field isn't matched with the size. This fails at Netty later.

### Why are the changes needed?

When the RPC messages are corrupted or the servers are under attack, Spark shows `IndexOutOfBoundsException` due to the failure from `Decoder`. Instead of `Exception`, we had better ignore the message with a directional warning message.
```
java.lang.IndexOutOfBoundsException:
    readerIndex(5) + length(602416) exceeds writerIndex(172):
UnpooledUnsafeDirectByteBuf(ridx: 5, widx: 172, cap: 172/172)
    at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1477)
    at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1463)
    at io.netty.buffer.UnpooledDirectByteBuf.readBytes(UnpooledDirectByteBuf.java:316)
    at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:904)
    at org.apache.spark.network.protocol.Encoders$Strings.decode(Encoders.java:45)
    at org.apache.spark.network.shuffle.protocol.UploadBlock.decode(UploadBlock.java:112)
    at org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71)
    at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:53)
    at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:161)
    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)

```

### Does this PR introduce _any_ user-facing change?

Yes, but this clarify the log messages from exceptions, `IndexOutOfBoundsException`.

### How was this patch tested?

Pass the CIs with newly added test suite.

Closes #36116 from dongjoon-hyun/SPARK-38830.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala', 'core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala']","Corrupted block RPC messages or potential attacks are causing exceptions like `IllegalArgumentException`, `NegativeArraySizeException`, and `IndexOutOfBoundsException` in `NettyBlockRpcServer`."
fb80dfee70011d0ce344473515ff15b17d4c4935,1572321445,"[SPARK-28158][SQL][FOLLOWUP] HiveUserDefinedTypeSuite: don't use RandomDataGenerator to create row for UDT backed by ArrayType

### What changes were proposed in this pull request?

There're some issues observed in `HiveUserDefinedTypeSuite.""Support UDT in Hive UDF""`:

1) Neither function (TestUDF) nor test take ""nullable"" point column into account.
2) ExamplePointUDT. sqlType is ArrayType which doesn't provide information how many elements are expected. RandomDataGenerator may provide less elements than needed.

This patch fixes `HiveUserDefinedTypeSuite.""Support UDT in Hive UDF""` to change the type of ""point"" column to be non-nullable, as well as not use RandomDataGenerator to create row for UDT backed by ArrayType.

### Why are the changes needed?

CI builds are failing in high occurrences.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually tested by running tests locally multiple times.

Closes #26287 from HeartSaVioR/SPARK-28158-FOLLOWUP.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveUserDefinedTypeSuite.scala'],"The RandomDataGenerator, used in HiveUserDefinedTypeSuite.""Support UDT in Hive UDF"", doesn't account for ""nullable"" point column and may provide fewer elements than expected for ArrayType, leading to high occurrence of CI build failures."
8760032f4f7e1ef36fee6afc45923d3826ef14fc,1605078801,"[SPARK-33412][SQL] OverwriteByExpression should resolve its delete condition based on the table relation not the input query

### What changes were proposed in this pull request?

Make a special case in `ResolveReferences`, which resolves `OverwriteByExpression`'s condition expression based on the table relation instead of the input query.

### Why are the changes needed?

The condition expression is passed to the table implementation at the end, so we should resolve it using table schema. Previously it works because we have a hack in `ResolveReferences` to delay the resolution if `outputResolved == false`. However, this hack doesn't work for tables accepting any schema like https://github.com/delta-io/delta/pull/521 . We may wrongly resolve the delete condition using input query's outout columns which don't match the table column names.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

existing tests and updated test in v2 write.

Closes #30318 from cloud-fan/v2-write.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/DataSourceV2AnalysisSuite.scala']","`OverwriteByExpression` is not correctly resolving its delete condition based on the table relation, leading to potential misalignment with tables accepting any schema."
53783e706dde943adee978a8eeee95a6f60687bd,1603090492,"[SPARK-33179][TESTS] Switch default Hadoop profile in run-tests.py

### What changes were proposed in this pull request?

This PR aims to switch the default Hadoop profile from `hadoop2.7` to `hadoop3.2` in `dev/run-tests.py` when it's running in local or GitHub Action environments.

### Why are the changes needed?

The default Hadoop version is 3.2. We had better be consistent.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Manually.

**BEFORE**
```
% dev/run-tests.py
Cannot install SparkR as R was not found in PATH
[info] Using build tool sbt with Hadoop profile hadoop2.7 and Hive profile hive2.3 under environment local
```

**AFTER**
```
% dev/run-tests.py
Cannot install SparkR as R was not found in PATH
[info] Using build tool sbt with Hadoop profile hadoop3.2 and Hive profile hive2.3 under environment local
```

Closes #30090 from williamhyun/SPARK-33179.

Authored-by: William Hyun <williamhyun3@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['dev/run-tests.py'],"The default Hadoop profile in `dev/run-tests.py` is set to `hadoop2.7`, which is inconsistent with the overall system default of `hadoop3.2`."
4ff2718d5448f212c0ede885c4072790c43bf826,1582528597,"[SPARK-30924][SQL][3.0] Add additional checks to Merge Into

### What changes were proposed in this pull request?

Merge Into is currently missing additional validation around:

 1. The lack of any WHEN statements
 2. The first WHEN MATCHED statement needs to have a condition if there are two WHEN MATCHED statements.
 3. Single use of UPDATE/DELETE

This PR introduces these validations.
(1) is required, because otherwise the MERGE statement is useless.
(2) is required, because otherwise the second WHEN MATCHED condition becomes dead code
(3) is up for debate, but the idea there is that a single expression should be sufficient to specify when you would like to update or delete your records. We restrict it for now to reduce surface area and ambiguity.

### Why are the changes needed?

To ease DataSource developers when building implementations for MERGE

### Does this PR introduce any user-facing change?

Adds additional validation checks

### How was this patch tested?

Unit tests

Closes #27677 from brkyvz/mergeChecks.

Authored-by: Burak Yavuz <brkyvz@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala']","'Merge Into' SQL operation lacking proper validation for presence of 'WHEN' statements, order of 'WHEN MATCHED' statements and restriction on single usage of UPDATE/DELETE, leading to potentially ambiguous and non-functional queries."
7466031632c5f1771cad3f3131bc1a3e52be173a,1608604679,"[SPARK-32106][SQL] Implement script transform in sql/core

### What changes were proposed in this pull request?

 * Implement `SparkScriptTransformationExec` based on `BaseScriptTransformationExec`
 * Implement `SparkScriptTransformationWriterThread` based on `BaseScriptTransformationWriterThread` of writing data
 * Add rule `SparkScripts` to support convert script LogicalPlan to SparkPlan in Spark SQL (without hive mode)
 * Add `SparkScriptTransformationSuite` test spark spec case
 * add test in `SQLQueryTestSuite`

And we will close #29085 .

### Why are the changes needed?
Support user use Script Transform without Hive

### Does this PR introduce _any_ user-facing change?
User can use Script Transformation without hive in no serde mode.
Such as :
**default no serde **
```
SELECT TRANSFORM(a, b, c)
USING 'cat' AS (a int, b string, c long)
FROM testData
```
**no serde with spec ROW FORMAT DELIMITED**
```
SELECT TRANSFORM(a, b, c)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY '\u0002'
MAP KEYS TERMINATED BY '\u0003'
LINES TERMINATED BY '\n'
NULL DEFINED AS 'null'
USING 'cat' AS (a, b, c)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY '\u0004'
MAP KEYS TERMINATED BY '\u0005'
LINES TERMINATED BY '\n'
NULL DEFINED AS 'NULL'
FROM testData
```

### How was this patch tested?
Added UT

Closes #29414 from AngersZhuuuu/SPARK-32106-MINOR.

Authored-by: angerszhu <angers.zhu@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkScriptTransformationExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala', 'sql/core/src/test/resources/sql-tests/inputs/transform.sql', 'sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveScriptTransformationExec.scala']","Support for script transform in SQL core without Hive is currently missing, and users cannot utilize script transformation in no serde mode."
7ae2b85c485a2f7ceb32543c64f08f3b1f120464,1660197094,"[SPARK-40044][SQL] Fix the target interval type in cast overflow errors

### What changes were proposed in this pull request?
In the PR, I propose to fix the incorrect target type in the error message of the cast overflow. For example:
```sql
spark-sql> select CAST(9223372036854775807L AS INTERVAL YEAR TO MONTH);
org.apache.spark.SparkArithmeticException: [CAST_OVERFLOW] The value 9223372036854775807L of the type ""BIGINT"" cannot be cast to ""INTERVAL MONTH"" due to an overflow.
```
The target type is **""INTERVAL MONTH""** instead of **""INTERVAL YEAR TO MONTH""**.

### Why are the changes needed?
To improve user experience with Spark SQL, and do not confuse users by incorrect error messages.

### Does this PR introduce _any_ user-facing change?
Yes, it changes user-facing error messages.

After the changes, for the example above:
```sql
spark-sql> select CAST(9223372036854775807L AS INTERVAL YEAR TO MONTH);
org.apache.spark.SparkArithmeticException: [CAST_OVERFLOW] The value 9223372036854775807L of the type ""BIGINT"" cannot be cast to ""INTERVAL YEAR TO MONTH"" due to an overflow.
```

### How was this patch tested?
By running new test:
```
$  build/sbt ""test:testOnly *QueryExecutionErrorsSuite""
```

Closes #37470 from MaxGekk/fix-cast-target-decimal-on-overflow.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala']","Casting overflow error messages are displaying incorrect target types, leading to potential user confusion."
67cbc932638179925ebbeb76d6d6e6f25a3cb2e2,1628011820,"[SPARK-36349][SQL] Disallow ANSI intervals in file-based datasources

### What changes were proposed in this pull request?
In the PR, I propose to ban `YearMonthIntervalType` and `DayTimeIntervalType` at the analysis phase while creating a table using a built-in filed-based datasource or writing a dataset to such datasource. In particular, add the following case:
```scala
case _: DayTimeIntervalType | _: YearMonthIntervalType => false
```
to all methods that override either:
- V2 `FileTable.supportsDataType()`
- V1 `FileFormat.supportDataType()`

### Why are the changes needed?
To improve user experience with Spark SQL, and output a proper error message at the analysis phase.

### Does this PR introduce _any_ user-facing change?
Yes but ANSI interval types haven't released yet. So, for users this is new behavior.

### How was this patch tested?
By running the affected test suites:
```
$ build/sbt -Phive-2.3 ""test:testOnly *HiveOrcSourceSuite""
```

Closes #33580 from MaxGekk/interval-ban-in-ds.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVTable.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/json/JsonTable.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcTable.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetTable.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/CommonFileDataSourceSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala']","ANSI intervals (`YearMonthIntervalType` and `DayTimeIntervalType`) are not supported in built-in file-based data sources, causing issues during table creation or data writing."
dfb7790a9dad8e98bd27001a613b4e13a5eb9d51,1602308112,"[SPARK-33108][BUILD] Remove sbt-dependency-graph SBT plugin

### What changes were proposed in this pull request?

This PR aims to remove `sbt-dependency-graph` SBT plugin.

### Why are the changes needed?

`sbt-dependency-graph` officially doesn't support SBT 1.3.x and it's broken due to `NoSuchMethodError`. This cannot be fixed in `sbt-dependency-graph` side at SBT 1.3.x
- https://github.com/sbt/sbt-dependency-graph
    > Note: Under sbt >= 1.3.x some features might currently not work as expected or not at all (like dependencyLicenses).

```
$ build/sbt dependencyTree
Launching sbt from build/sbt-launch-1.3.13.jar
[info] welcome to sbt 1.3.13 (AdoptOpenJDK Java 1.8.0_252)
...
[error] java.lang.NoSuchMethodError: sbt.internal.LibraryManagement$.cachedUpdate(Lsbt/librarymanagement/DependencyResolution;Lsbt/librarymanagement/ModuleDescriptor;Lsbt/util/CacheStoreFactory;Ljava/lang/String;Lsbt/librarymanagement/UpdateConfiguration;Lscala/Function1;ZZZLsbt/librarymanagement/UnresolvedWarningConfiguration;Lsbt/librarymanagement/EvictionWarningOptions;ZLsbt/internal/librarymanagement/CompatibilityWarningOptions;Lsbt/util/Logger;)Lsbt/librarymanagement/UpdateReport;
```

**ALTERNATIVES**
- One alternative is `coursier`, but it requires `coursier-based sbt launcher` which is more intrusive.
  - https://get-coursier.io/docs/sbt-coursier.html#sbt-13x
    > you'll have to use the coursier-based sbt launcher, via its custom sbt-extras launcher for example.

- Another alternative is moving to `SBT 1.4.0` which uses `sbt-dependency-graph` as a built-in, but it's still new and will requires many change.

So, this PR aims to remove the broken plugin simply.

### Does this PR introduce _any_ user-facing change?

No. This is a dev-only change.

### How was this patch tested?

Manual.
```
$ build/sbt dependencyTree
...
[error] Not a valid command: dependencyTree
[error] Not a valid project ID: dependencyTree
[error] Not a valid key: dependencyTree (similar: dependencyOverrides, sbtDependency, dependencyResolution)
[error] dependencyTree
[error]               ^
```

Closes #29997 from dongjoon-hyun/remove_depedencyTree.

Lead-authored-by: Dongjoon Hyun <dongjoon@apache.org>
Co-authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['project/plugins.sbt'],The `sbt-dependency-graph` plugin is incompatible with SBT 1.3.x causing an 'NoSuchMethodError' to be thrown when running 'sbt dependencyTree'.
ded1a7495b443f4735057eb5520f31df5b9860d2,1564110790,"[SPARK-28365][ML] Fallback locale to en_US in StopWordsRemover if system default locale isn't in available locales in JVM

## What changes were proposed in this pull request?

Because the local default locale isn't in available locales at `Locale`, when I did some tests locally with python code, `StopWordsRemover` related python test hits some errors, like:

```
Traceback (most recent call last):
  File ""/spark-1/python/pyspark/ml/tests/test_feature.py"", line 87, in test_stopwordsremover
    stopWordRemover = StopWordsRemover(inputCol=""input"", outputCol=""output"")
  File ""/spark-1/python/pyspark/__init__.py"", line 111, in wrapper
    return func(self, **kwargs)
  File ""/spark-1/python/pyspark/ml/feature.py"", line 2646, in __init__
    self.uid)
  File ""/spark-1/python/pyspark/ml/wrapper.py"", line 67, in _new_java_obj
    return java_obj(*java_args)
  File /spark-1/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1554, in __call__
    answer, self._gateway_client, None, self._fqn)
  File ""/spark-1/python/pyspark/sql/utils.py"", line 93, in deco
    raise converted
pyspark.sql.utils.IllegalArgumentException: 'StopWordsRemover_4598673ee802 parameter locale given invalid value en_TW.'
```

As per HyukjinKwon's advice, instead of setting up locale to pass test, it is better to have a workable locale if system default locale can't be found in available locales in JVM. Otherwise, users have to manually change system locale or accessing a private property _jvm in PySpark.

## How was this patch tested?

Added test and manual test.

```
scala> val remover = new StopWordsRemover().setInputCol(""raw"").setOutputCol(""filtered"")
19/07/14 19:20:03 WARN StopWordsRemover: Default locale set was [en_TW]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.
```

Closes #25133 from viirya/pytest-default-locale.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['mllib/src/main/scala/org/apache/spark/ml/feature/StopWordsRemover.scala', 'mllib/src/test/scala/org/apache/spark/ml/feature/StopWordsRemoverSuite.scala']","'StopWordsRemover fails and throws IllegalArgumentException when the system default locale isn't available in JVM locales, requiring manual locale change or accessing a private property _jvm in PySpark.'"
c08021cd8734b3cc183e7f65312d14cdaa8541b7,1548822267,"[SPARK-26776][PYTHON] Reduce Py4J communication cost in PySpark's execution barrier check

## What changes were proposed in this pull request?

I am investigating flaky tests. I realised that:

```
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/rdd.py"", line 2512, in __init__
        self.is_barrier = prev._is_barrier() or isFromBarrier
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/rdd.py"", line 2412, in _is_barrier
        return self._jrdd.rdd().isBarrier()
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1286, in __call__
        answer, self.gateway_client, self.target_id, self.name)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py"", line 342, in get_return_value
        return OUTPUT_CONVERTER[type](answer[2:], gateway_client)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 2492, in <lambda>
        lambda target_id, gateway_client: JavaObject(target_id, gateway_client))
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1324, in __init__
        ThreadSafeFinalizer.add_finalizer(key, value)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/finalizer.py"", line 43, in add_finalizer
        cls.finalizers[id] = weak_ref
      File ""/usr/lib64/pypy-2.5.1/lib-python/2.7/threading.py"", line 216, in __exit__
        self.release()
      File ""/usr/lib64/pypy-2.5.1/lib-python/2.7/threading.py"", line 208, in release
        self.__block.release()
    error: release unlocked lock
```

I assume it might not be directly related with the test itself but I noticed that it `prev._is_barrier()` attempts to access via Py4J.

Accessing via Py4J is expensive. Therefore, this PR proposes to avoid Py4J access when `isFromBarrier` is `True`.

## How was this patch tested?

Unittests should cover this.

Closes #23690 from HyukjinKwon/minor-barrier.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['python/pyspark/rdd.py'],High Py4J communication cost in PySpark's execution barrier check is causing slow performance and release unlocked lock errors.
68ec207a320bd50ca61e820c9ff559f799c2ab0a,1535505949,"[SPARK-25260][SQL] Fix namespace handling in SchemaConverters.toAvroType

## What changes were proposed in this pull request?

`toAvroType` converts spark data type to avro schema. It always appends the record name to namespace so its impossible to have an Avro namespace independent of the record name.

When invoked with a spark data type like,

```java
val sparkSchema = StructType(Seq(
    StructField(""name"", StringType, nullable = false),
    StructField(""address"", StructType(Seq(
        StructField(""city"", StringType, nullable = false),
        StructField(""state"", StringType, nullable = false))),
    nullable = false)))

// map it to an avro schema with record name ""employee"" and top level namespace ""foo.bar"",
val avroSchema = SchemaConverters.toAvroType(sparkSchema,  false, ""employee"", ""foo.bar"")

// result is
// avroSchema.getName = employee
// avroSchema.getNamespace = foo.bar.employee
// avroSchema.getFullname = foo.bar.employee.employee
```
The patch proposes to fix this so that the result is

```
avroSchema.getName = employee
avroSchema.getNamespace = foo.bar
avroSchema.getFullname = foo.bar.employee
```
## How was this patch tested?

New and existing unit tests.

Please review http://spark.apache.org/contributing.html before opening a pull request.

Closes #22251 from arunmahadevan/avro-fix.

Authored-by: Arun Mahadevan <arunm@apache.org>
Signed-off-by: hyukjinkwon <gurwls223@apache.org>
","['external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala', 'external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala']","The SchemaConverters.toAvroType function always appends the record name to the namespace, causing incorrect full names and preventing Avro namespaces from being independent of record names."
