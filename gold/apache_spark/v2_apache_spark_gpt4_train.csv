commit_id,commit_date,commit_message,actual_files_modified,transformed_message_gpt4
51bee7aca13451167fa3e701fcd60f023eae5e61,1533609071,"[SPARK-25018][INFRA] Use `Co-authored-by` and `Signed-off-by` git trailer in `merge_spark_pr.py`

## What changes were proposed in this pull request?

In [Linux community](https://git.wiki.kernel.org/index.php/CommitMessageConventions), `Co-authored-by` and `Signed-off-by` git trailer have been used for awhile.

Until recently, Github adopted `Co-authored-by` to include the work of co-authors in the profile contributions graph and the repository's statistics. It's a convention for recognizing multiple authors, and can encourage people to collaborate in OSS communities.

Git provides a command line tools to read the metadata to know who commits the code to upstream, but it's not as easy as having `Signed-off-by` as part of the message so developers can find who is the relevant committers who can help with certain part of the codebase easier.

For a single author PR, I purpose to use `Authored-by` and `Signed-off-by`, so the message will look like

```
Authored-by: Author's name <authorexample.com>
Signed-off-by: Committer's name <committerexample.com>
```

For a multi-author PR, I purpose to use `Lead-authored-by:` and `Co-authored-by:` for the lead author and co-authors. The message will look like

```
Lead-authored-by: Lead Author's name <leadauthorexample.com>
Co-authored-by: CoAuthor's name <coauthorexample.com>
Signed-off-by: Committer's name <committerexample.com>
```

It's also useful to include `Reviewed-by:` to give credits to the people who participate on the code reviewing. We can add this in the next iteration.

Closes #21991 from dbtsai/script.

Lead-authored-by: DB Tsai <d_tsai@apple.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Brian Lindblom <blindblom@apple.com>
Co-authored-by: hyukjinkwon <gurwls223@apache.org>
Signed-off-by: hyukjinkwon <gurwls223@apache.org>
",['dev/merge_spark_pr.py'],"The current metadata included in commit messages does not adequately recognize or provide contact information for multiple authors/committers, making it harder to find who is responsible for certain parts of the codebase."
56c691caf05f84572f54939720e199e494af3d78,1674359118,"[SPARK-42146][CORE] Refactor `Utils#setStringField` to make maven build pass when sql module use this method

### What changes were proposed in this pull request?
This pr aims refactor  input parameter type of  `Utils#setStringField` function to make maven build pass when sql module use this functions.

### Why are the changes needed?
Make maven build pass when sql module use `Utils#setStringField` function

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

- Pass GitHub Actions
- Manual test:

Add the following code to sql module:

```scala
package org.apache.spark.status.protobuf.sql

import org.apache.spark.SparkFunSuite
import org.apache.spark.status.protobuf.StoreTypes
import org.apache.spark.status.protobuf.Utils.setStringField

class UtilsSuite extends SparkFunSuite {
  test(""maven build"") {
    val builder = StoreTypes.SQLExecutionUIData.newBuilder()
    setStringField(""1"", builder.setDetails)
  }
}
```

run `mvn  clean install  -DskipTests  -pl sql/core -am`

**Before**

```
[ERROR] [Error] /${basedir}/sql/core/src/test/scala/org/apache/spark/status/protobuf/sql/UtilsSuite.scala:27: type mismatch;
 found   : org.apache.spark.status.protobuf.StoreTypes.SQLExecutionUIData.Builder
 required: com.google.protobuf.MessageOrBuilder
[ERROR] one error found
```

**After**
BUILD SUCCESS

Closes #39688 from LuciferYang/SPARK-42146.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
",['core/src/main/scala/org/apache/spark/status/protobuf/Utils.scala'],"Maven build fails due to type mismatch when using `Utils#setStringField` within SQL module, causing issues with successful completing of build."
965f872500a3554142cab3078a7a4d513d2d2ee8,1650012347,"[SPARK-37575][SQL][FOLLOWUP] Add legacy flag for the breaking change of write null value in csv to unquoted empty string

### What changes were proposed in this pull request?

Add a legacy flag `spark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv` for the breaking change introduced in https://github.com/apache/spark/pull/34853 and https://github.com/apache/spark/pull/34905 (followup).

The flag is disabled by default, so the null values written as csv will output an unquoted empty string. When the legacy flag is enabled, the null will output quoted empty string.

### Why are the changes needed?
The original commit is a breaking change, and breaking changes should be encouraged to add a flag to turn it off for smooth migration between versions.

### Does this PR introduce _any_ user-facing change?
With the default value of the conf, there is no user-facing difference.
If users turn this conf off, they can restore the pre-change behavior.

### How was this patch tested?
Through unit tests.

Closes #36110 from anchovYu/flags-null-to-csv.

Authored-by: Xinyi Yu <xinyi.yu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala']",Writing null values as CSV outputs an unquoted empty string which introduces breaking change compared to previous versions.
43763629f1d1a220cd91e2aed89152d065dfba24,1533623294,"[SPARK-25010][SQL] Rand/Randn should produce different values for each execution in streaming query

## What changes were proposed in this pull request?

Like Uuid in SPARK-24896, Rand and Randn expressions now produce the same results for each execution in streaming query. It doesn't make too much sense for streaming queries. We should make them produce different results as Uuid.

In this change, similar to Uuid, we assign new random seeds to Rand/Randn when returning optimized plan from `IncrementalExecution`.

Note: Different to Uuid, Rand/Randn can be created with initial seed. Because we replace this initial seed at `IncrementalExecution`, it doesn't use the initial seed anymore. For now it seems to me not a big issue for streaming query. But need to confirm with others. cc zsxwing cloud-fan

## How was this patch tested?

Added test.

Closes #21980 from viirya/SPARK-25010.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/misc.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/randomExpressions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala']","Rand/Randn expressions in streaming query produce the same results for each execution rather than different random outcomes, losing their randomness and utility in a streaming context."
fa757ee1d41396ad8734a3f2dd045bb09bc82a2e,1496183586,"[SPARK-20883][SPARK-20376][SS] Refactored StateStore APIs and added conf to choose implementation

## What changes were proposed in this pull request?

A bunch of changes to the StateStore APIs and implementation.
Current state store API has a bunch of problems that causes too many transient objects causing memory pressure.

- `StateStore.get(): Option` forces creation of Some/None objects for every get. Changed this to return the row or null.
- `StateStore.iterator(): (UnsafeRow, UnsafeRow)` forces creation of new tuple for each record returned. Changed this to return a UnsafeRowTuple which can be reused across records.
- `StateStore.updates()` requires the implementation to keep track of updates, while this is used minimally (only by Append mode in streaming aggregations). Removed updates() and updated StateStoreSaveExec accordingly.
- `StateStore.filter(condition)` and `StateStore.remove(condition)` has been merge into a single API `getRange(start, end)` which allows a state store to do optimized range queries (i.e. avoid full scans). Stateful operators have been updated accordingly.
- Removed a lot of unnecessary row copies Each operator copied rows before calling StateStore.put() even if the implementation does not require it to be copied. It is left up to the implementation on whether to copy the row or not.

Additionally,
- Added a name to the StateStoreId so that each operator+partition can use multiple state stores (different names)
- Added a configuration that allows the user to specify which implementation to use.
- Added new metrics to understand the time taken to update keys, remove keys and commit all changes to the state store. These metrics will be visible on the plan diagram in the SQL tab of the UI.
- Refactored unit tests such that they can be reused to test any implementation of StateStore.

## How was this patch tested?
Old and new unit tests

Author: Tathagata Das <tathagata.das1565@gmail.com>

Closes #18107 from tdas/SPARK-20376.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDDSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala']","Current StateStore APIs have multiple issues leading to the creation of unnecessary transient objects, causing memory pressure. The implementations also cause redundant row copies and lack configuration to choose the implementation."
eb7adb1b000fc37e4315cf92e843985a739cb36a,1636147274,"[SPARK-35496][BUILD] Upgrade Scala to 2.13.7

### What changes were proposed in this pull request?
This pr aims to update from Scala 2.13.5 to Scala 2.13.7 for Apache Spark 3.3. The main change of this pr as follows:

1. For fix `weaker access privileges in overriding`, remove the access modifier different from the parent class declared in `SparkContext`, `DiskBlockObjectWriter`, `EpochTracker` and `SparkSessionExtensionSuite`

2. Add explicit `()` to return `Unit` type for `DateTimeFormatter.validatePatternString` and `TimestampFormatter.validatePatternString` to avoid `type mismatch`

3. Add 3 new compile args to `pom.xml` and `SparkBuild.scala` to suppress some compilation warnings that are converted to compilation error

### Why are the changes needed?
Scala 2.13.7 is a maintenance release for 2.13 line and the release notes as follows:

- [https://github.com/scala/scala/releases/tag/v2.13.6](https://github.com/scala/scala/releases/tag/v2.13.6)
- [https://github.com/scala/scala/releases/tag/v2.13.7](https://github.com/scala/scala/releases/tag/v2.13.7)

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

- Pass the GitHub Action Scala 2.13 job
- Manual test
```
dev/change-scala-version.sh 2.13

build/mvn clean install -DskipTests -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13

build/mvn test -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13

```
All tests passed.

Closes #32648 from LuciferYang/SPARK-35496.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/SparkContext.scala', 'core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala', 'project/SparkBuild.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateFormatter.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochTracker.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SparkSessionExtensionSuite.scala']","Apache Spark 3.3 is failing to upgrade from Scala 2.13.5 to Scala 2.13.7 due to issues related to access modifiers, type mismatch, and new compilation warnings."
dd4db21cb69a9a9c3715360673a76e6f150303d4,1684116531,"[SPARK-43492][SQL] Add 3-args function aliases `DATE_ADD` and `DATE_DIFF`

### What changes were proposed in this pull request?
In the PR, I propose to extend the rules of `primaryExpression` in `SqlBaseParser.g4`, and two more functions `DATE_ADD` and `DATE_DIFF` that accept 3-args in the same way as the existing expressions `DATEADD` and `DATEDIFF`.

### Why are the changes needed?
To do not confuse users and improve user experience with Spark SQL. There are the `DATE_ADD` and `DATE_DIFF` aliases for 2-args functions: https://github.com/apache/spark/blob/46332d985e52f76887c139f67d1471b82e85d5ca/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L606-L607

but Spark SQL doesn't provide 3-args `DATE_ADD` and `DATE_DIFF`. And Spark SQL outputs the confusing error:
```sql
spark-sql (default)> select date_add(MONTH, 1, date'2023-05-13');
[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `MONTH` cannot be resolved. ; line 1 pos 16;
```

### Does this PR introduce _any_ user-facing change?
No, only if user code depends on the error class. After the changes, Spark SQL outputs an result similar to `DATEADD`/`DATEDIFF`:
```sql
spark-sql (default)> select date_add(MONTH, 1, date'2023-05-13');
2023-06-13 00:00:00
```

### How was this patch tested?
By running the existing test suites:
```
$ PYSPARK_PYTHON=python3 build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite""
```

Closes #41157 from MaxGekk/date_add_diff.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4', 'sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4', 'sql/core/src/test/resources/sql-tests/inputs/date.sql', 'sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerWithSparkContextSuite.scala']","The 3-args functions `DATE_ADD` and `DATE_DIFF` are not recognized in Spark SQL, leading to unresolved column errors when they are called."
2e90574dd0e60ea960a33580dfb29654671b66f4,1554885568,"[SPARK-27414][SQL] make it clear that date type is timezone independent

## What changes were proposed in this pull request?

In SQL standard, date type is a union of the `year`, `month` and `day` fields. It's timezone independent, which means it does not represent a specific point in the timeline.

Spark SQL follows the SQL standard, this PR is to make it clear that date type is timezone independent
1. improve the doc to highlight that date is timezone independent.
2. when converting string to date,  uses the java time API that can directly parse a `LocalDate` from a string, instead of converting `LocalDate` to a `Instant` at UTC first.
3. when converting date to string, uses the java time API that can directly format a `LocalDate` to a string, instead of converting `LocalDate` to a `Instant` at UTC first.

2 and 3 should not introduce any behavior changes.

## How was this patch tested?

existing tests

Closes #24325 from cloud-fan/doc.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateFormatter.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala']","Misleading representations and operations on Spark SQL date type can create ambiguity, as it may not be clear that it's timezone independent."
1149c4efbc5ebe5b412d8f9c61558fef59179a9e,1535470687,"[SPARK-25005][SS] Support non-consecutive offsets for Kafka

## What changes were proposed in this pull request?

As the user uses Kafka transactions to write data, the offsets in Kafka will be non-consecutive. It will contains some transaction (commit or abort) markers. In addition, if the consumer's `isolation.level` is `read_committed`, `poll` will not return aborted messages either. Hence, we will see non-consecutive offsets in the date returned by `poll`. However, as `seekToEnd` may move the offset point to these missing offsets, there are 4 possible corner cases we need to support:

- The whole batch contains no data messages
- The first offset in a batch is not a committed data message
- The last offset in a batch is not a committed data message
- There is a gap in the middle of a batch

They are all covered by the new unit tests.

## How was this patch tested?

The new unit tests.

Closes #22042 from zsxwing/kafka-transaction-read.

Authored-by: Shixiong Zhu <zsxwing@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>
","['external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaDataConsumer.scala', 'external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaContinuousSourceSuite.scala', 'external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala', 'external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaRelationSuite.scala', 'external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala']","Kafka offsets are non-consecutive when user uses transactions to write data, with potential for missing offsets. This presents issues when `seekToEnd` moves the offset to these missing positions, causing four possible corner cases that need support."
0b892a543f9ea913f961eea95a4e45f1231b9a57,1666962409,"[SPARK-40932][CORE] Fix issue messages for allGather are overridden

### What changes were proposed in this pull request?

The messages returned by allGather may be overridden by the following barrier APIs, eg,

``` scala
      val messages: Array[String] = context.allGather(""ABC"")
      context.barrier()
```

the  `messages` may be like Array("""", """"), but we're expecting Array(""ABC"", ""ABC"")

The root cause of this issue is the [messages got by allGather](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala#L102) pointing to the [original message](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/BarrierCoordinator.scala#L107) in the local mode. So when the following barrier APIs changed the messages, then the allGather message will be changed accordingly.
Finally, users can't get the correct result.

This PR fixed this issue by sending back the cloned messages.

### Why are the changes needed?

The bug mentioned in this description may block some external SPARK ML libraries which heavily depend on the spark barrier API to do some synchronization. If the barrier mechanism can't guarantee the correctness of the barrier APIs, it will be a disaster for external SPARK ML libraries.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

I added a unit test, with this PR, the unit test can pass

Closes #38410 from wbo4958/allgather-issue.

Authored-by: Bobby Wang <wbo4958@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['core/src/main/scala/org/apache/spark/BarrierCoordinator.scala', 'core/src/test/scala/org/apache/spark/scheduler/BarrierTaskContextSuite.scala']","""allGather"" messages get overwritten by subsequent barrier API calls, preventing users from obtaining the correct results and interfering with external Spark ML libraries using Spark Barrier API for synchronization."
2643223e21b6e80ea150b41a99c040ef7eebd51a,1683622588,"[SPARK-43286][SQL] Updates aes_encrypt CBC mode to generate random IVs

### What changes were proposed in this pull request?

The current implementation of AES-CBC mode called via `aes_encrypt` and `aes_decrypt` uses a key derivation function (KDF) based on OpenSSL's [EVP_BytesToKey](https://www.openssl.org/docs/man3.0/man3/EVP_BytesToKey.html). This is intended for generating keys based on passwords and OpenSSL's documents discourage its use: ""Newer applications should use a more modern algorithm"".

`aes_encrypt` and `aes_decrypt` should use the key directly in CBC mode, as it does for both GCM and ECB mode. The output should then be the initialization vector (IV) prepended to the ciphertext – as is done with GCM mode:
`[16-byte randomly generated IV | AES-CBC encrypted ciphertext]`

### Why are the changes needed?

We want to have the ciphertext output similar across different modes. OpenSSL's EVP_BytesToKey is effectively deprecated and their own documentation says not to use it. Instead, CBC mode will generate a random vector.

### Does this PR introduce _any_ user-facing change?

AES-CBC output generated by the previous format will be incompatible with this change. That change was recently landed and we want to land this before CBC mode is used in practice.

### How was this patch tested?

A new unit test in `DataFrameFunctionsSuite` was added to test both GCM and CBC modes. Also, a new standalone unit test suite was added in `ExpressionImplUtilsSuite` to test all the modes and various key lengths.
```
build/sbt ""sql/test:testOnly org.apache.spark.sql.DataFrameFunctionsSuite""
build/sbt ""sql/test:testOnly org.apache.spark.sql.catalyst.expressions.ExpressionImplUtilsSuite""
```

CBC values can be verified with `openssl enc` using the following command:
```
echo -n ""[INPUT]"" | openssl enc -a -e -aes-256-cbc -iv [HEX IV] -K [HEX KEY]
echo -n ""Spark"" | openssl enc -a -e -aes-256-cbc -iv f8c832cc9c61bac6151960a58e4edf86 -K 6162636465666768696a6b6c6d6e6f7031323334353637384142434445464748
```

Closes #40969 from sweisdb/SPARK-43286.

Authored-by: Steve Weis <steve.weis@databricks.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/misc.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtilsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala']","The `aes_encrypt` and `aes_decrypt` in AES-CBC mode uses a deprecated key derivation function (KDF), OpenSSL's EVP_BytesToKey, causing inconsistency in ciphertext outputs across different modes."
cb869328ea7fcf95a4178a0db19a6fa821ce3f15,1672708955,"[SPARK-41804][SQL] Choose correct element size in `InterpretedUnsafeProjection` for array of UDTs

### What changes were proposed in this pull request?

Change `InterpretedUnsafeProjection#getElementSize` to choose the appropriate element size for the underlying SQL type of a UDT, rather than simply using the the default size of the underlying SQL type.

### Why are the changes needed?

Consider this query:
```
// create a file of vector data
import org.apache.spark.ml.linalg.{DenseVector, Vector}

case class TestRow(varr: Array[Vector])
val values = Array(0.1d, 0.2d, 0.3d)
val dv = new DenseVector(values).asInstanceOf[Vector]

val ds = Seq(TestRow(Array(dv, dv))).toDS
ds.coalesce(1).write.mode(""overwrite"").format(""parquet"").save(""vector_data"")

// this works
spark.read.format(""parquet"").load(""vector_data"").collect

sql(""set spark.sql.codegen.wholeStage=false"")
sql(""set spark.sql.codegen.factoryMode=NO_CODEGEN"")

// this will get an error
spark.read.format(""parquet"").load(""vector_data"").collect
```
The failures vary, incuding
* `VectorUDT` attempting to deserialize to a `SparseVector` (rather than a `DenseVector`)
* negative array size (for one of the nested arrays)
* JVM crash (SIGBUS error).

This is because `InterpretedUnsafeProjection` initializes the outer-most array writer with an element size of 17 (the size of the UDT's underlying struct), rather than an element size of 8, which would be appropriate for an array of structs.

When the outer-most array is later accessed, `UnsafeArrayData` assumes an element size of 8, so it picks up a garbage offset/size tuple for the second element.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New unit test.

Closes #39349 from bersprockets/udt_issue.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/InterpretedUnsafeProjection.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/UnsafeRowConverterSuite.scala']","`InterpretedUnsafeProjection` selects incorrect element size for array of UDTs, leading to failures when attempting to deserialize `VectorUDT`, to negative array sizes, or even JVM crashes."
61ebc8118665531b4c11a31f2b3d459dd201b097,1576276211,"[SPARK-30167][REPL] Log4j configuration for REPL can't override the root logger properly

### What changes were proposed in this pull request?

In the current implementation of `SparkShellLoggingFilter`, if the log level of the root logger and the log level of a message are different, whether a message should logged is decided based on log4j's configuration but whether the message should be output to the REPL's console is not cared.
So, if the log level of the root logger is `DEBUG`, the log level of REPL's logger is `WARN` and the log level of a message is `INFO`, the message will output to the REPL's console even though `INFO < WARN`.
https://github.com/apache/spark/pull/26798/files#diff-bfd5810d8aa78ad90150e806d830bb78L237

The ideal behavior should be like as follows and implemented them in this change.

1. If the log level of a message is greater than or equal to the log level of the root logger, the message should be logged but whether the message is output to the REPL's console should be decided based on whether the log level of the message is greater than or equal to the log level of the REPL's logger.

2. If a log level or custom appenders are explicitly defined for a category, whether a log message via the logger corresponding to the category is logged and output to the REPL's console should be decided baed on the log level of the category.
We can confirm whether a log level or appenders are explicitly set to a logger for a category by `Logger#getLevel` and `Logger#getAllAppenders.hasMoreElements`.

### Why are the changes needed?

This is a bug breaking a compatibility.

#9816 enabled REPL's log4j configuration to override root logger but #23675 seems to have broken the feature.
You can see one example when you modifies the default log4j configuration like as follows.
```
# Change the log level for rootCategory to DEBUG
log4j.rootCategory=DEBUG, console

...
# The log level for repl.Main remains WARN
log4j.logger.org.apache.spark.repl.Main=WARN
```
If you launch REPL with the configuration, INFO level logs appear even though the log level for REPL is WARN.
```
・・・

19/12/08 23:31:38 INFO Utils: Successfully started service 'sparkDriver' on port 33083.
19/12/08 23:31:38 INFO SparkEnv: Registering MapOutputTracker
19/12/08 23:31:38 INFO SparkEnv: Registering BlockManagerMaster
19/12/08 23:31:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/12/08 23:31:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/12/08 23:31:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat

・・・
```
Before #23675 was applied, those INFO level logs are not shown with the same log4j.properties.

### Does this PR introduce any user-facing change?

Yes. The logging behavior for REPL is fixed.

### How was this patch tested?

Manual test and newly added unit test.

Closes #26798 from sarutak/fix-spark-shell-loglevel.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['core/src/main/scala/org/apache/spark/internal/Logging.scala', 'core/src/test/scala/org/apache/spark/internal/LoggingSuite.scala', 'repl/src/test/scala/org/apache/spark/repl/ReplSuite.scala']","The log4j configuration for REPL is not overriding root logger properly, causing INFO level logs to appear even when the log level for REPL is set to WARN. This is a compatibility-breaking bug."
bdd8e1dbb1c526e5ce08e294b5b11ace752b2e2e,1621491198,"[SPARK-28551][SQL] CTAS with LOCATION should not allow to a non-empty directory

### What changes were proposed in this pull request?

CTAS with location clause acts as an insert overwrite. This can cause problems when there are subdirectories within a location directory.
This causes some users to accidentally wipe out directories with very important data. We should not allow CTAS with location to a non-empty directory.

### Why are the changes needed?

Hive already handled this scenario: HIVE-11319

Steps to reproduce:

```scala
sql(""""""create external table  `demo_CTAS`( `comment` string) PARTITIONED BY (`col1` string, `col2` string) STORED AS parquet location '/tmp/u1/demo_CTAS'"""""")
sql(""""""INSERT OVERWRITE TABLE demo_CTAS partition (col1='1',col2='1') VALUES ('abc')"""""")
sql(""select* from demo_CTAS"").show
sql(""""""create table ctas1 location '/tmp/u2/ctas1' as select * from demo_CTAS"""""")
sql(""select* from ctas1"").show
sql(""""""create table ctas2 location '/tmp/u2' as select * from demo_CTAS"""""")
```

Before the fix: Both create table operations will succeed. But values in table ctas1 will be replaced by ctas2 accidentally.

After the fix: `create table ctas2...` will throw `AnalysisException`:

```
org.apache.spark.sql.AnalysisException: CREATE-TABLE-AS-SELECT cannot create table with location to a non-empty directory /tmp/u2 . To allow overwriting the existing non-empty directory, set 'spark.sql.legacy.allowNonEmptyLocationInCTAS' to true.
```

### Does this PR introduce _any_ user-facing change?
Yes, if the location directory is not empty, CTAS with location will throw AnalysisException

```
sql(""""""create table ctas2 location '/tmp/u2' as select * from demo_CTAS"""""")
```
```
org.apache.spark.sql.AnalysisException: CREATE-TABLE-AS-SELECT cannot create table with location to a non-empty directory /tmp/u2 . To allow overwriting the existing non-empty directory, set 'spark.sql.legacy.allowNonEmptyLocationInCTAS' to true.
```

`CREATE TABLE AS SELECT` with non-empty `LOCATION` will throw `AnalysisException`. To restore the behavior before Spark 3.2, need to  set `spark.sql.legacy.allowNonEmptyLocationInCTAS` to `true`. , default value is `false`.
Updated SQL migration guide.

### How was this patch tested?
Test case added in SQLQuerySuite.scala

Closes #32411 from vinodkc/br_fixCTAS_nonempty_dir.

Authored-by: Vinod KC <vinod.kc.in@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/CreateTableAsSelectSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala']","CTAS with LOCATION clause behaves as an insert overwrite, which can unintentionally replace important data if there are subdirectories within the target location."
cdd694c52b53165acba6faabaf3a1fbaa925ac2e,1548987488,"[SPARK-7721][INFRA] Run and generate test coverage report from Python via Jenkins

## What changes were proposed in this pull request?

### Background

For the current status, the test script that generates coverage information was merged
into Spark, https://github.com/apache/spark/pull/20204

So, we can generate the coverage report and site by, for example:

```
run-tests-with-coverage --python-executables=python3 --modules=pyspark-sql
```

like `run-tests` script in `./python`.

### Proposed change

The next step is to host this coverage report via `github.io` automatically
by Jenkins (see https://spark-test.github.io/pyspark-coverage-site/).

This uses my testing account for Spark, spark-test, which is shared to Felix and Shivaram a long time ago for testing purpose including AppVeyor.

To cut this short, this PR targets to run the coverage in
[spark-master-test-sbt-hadoop-2.7](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7/)

In the specific job, it will clone the page, and rebase the up-to-date PySpark test coverage from the latest commit. For instance as below:

```bash
# Clone PySpark coverage site.
git clone https://github.com/spark-test/pyspark-coverage-site.git

# Remove existing HTMLs.
rm -fr pyspark-coverage-site/*

# Copy generated coverage HTMLs.
cp -r .../python/test_coverage/htmlcov/* pyspark-coverage-site/

# Check out to a temporary branch.
git symbolic-ref HEAD refs/heads/latest_branch

# Add all the files.
git add -A

# Commit current HTMLs.
git commit -am ""Coverage report at latest commit in Apache Spark""

# Delete the old branch.
git branch -D gh-pages

# Rename the temporary branch to master.
git branch -m gh-pages

# Finally, force update to our repository.
git push -f origin gh-pages
```

So, it is a one single up-to-date coverage can be shown in the `github-io` page. The commands above were manually tested.

### TODOs

- [x] Write a draft HyukjinKwon
- [x] `pip install coverage` to all python implementations (pypy, python2, python3) in Jenkins workers  - shaneknapp
- [x] Set hidden `SPARK_TEST_KEY` for spark-test's password in Jenkins via Jenkins's feature
 This should be set in both PR builder and `spark-master-test-sbt-hadoop-2.7` so that later other PRs can test and fix the bugs - shaneknapp
- [x] Set an environment variable that indicates `spark-master-test-sbt-hadoop-2.7` so that that specific build can report and update the coverage site - shaneknapp
- [x] Make PR builder's test passed HyukjinKwon
- [x] Fix flaky test related with coverage HyukjinKwon
  -  6 consecutive passes out of 7 runs

This PR will be co-authored with me and shaneknapp

## How was this patch tested?

It will be tested via Jenkins.

Closes #23117 from HyukjinKwon/SPARK-7721.

Lead-authored-by: Hyukjin Kwon <gurwls223@apache.org>
Co-authored-by: hyukjinkwon <gurwls223@apache.org>
Co-authored-by: shane knapp <incomplete@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['dev/run-tests.py', 'python/pyspark/streaming/tests/test_dstream.py']",Inability to automatically generate and update PySpark test coverage report using Jenkins and host it on `github.io`.
a824a6de89fdd2ecc119a9bb48bca64da5db72bd,1692863613,"[SPARK-44121][CONNECT][TESTS] Renable Arrow-based connect tests in Java 21

### What changes were proposed in this pull request?

This PR aims to re-enable Arrow-based connect tests in Java 21.
This depends on #42181.

### Why are the changes needed?

To have Java 21 test coverage.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

```
$ java -version
openjdk version ""21-ea"" 2023-09-19
OpenJDK Runtime Environment (build 21-ea+32-2482)
OpenJDK 64-Bit Server VM (build 21-ea+32-2482, mixed mode, sharing)

$ build/sbt ""connect/test"" -Phive
...
[info] Run completed in 14 seconds, 136 milliseconds.
[info] Total number of tests run: 858
[info] Suites: completed 20, aborted 0
[info] Tests: succeeded 858, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[success] Total time: 44 s, completed Aug 23, 2023, 9:42:53 PM

$ build/sbt ""connect-client-jvm/test"" -Phive
...
[info] Run completed in 1 minute, 24 seconds.
[info] Total number of tests run: 1220
[info] Suites: completed 24, aborted 0
[info] Tests: succeeded 1220, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 1222, Failed 0, Errors 0, Passed 1222
```

### Was this patch authored or co-authored using generative AI tooling?

No.

Closes #42643 from dongjoon-hyun/SPARK-44121.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/application/ReplE2ESuite.scala', 'connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala', 'connector/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectPlannerSuite.scala', 'connector/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectProtoSuite.scala', 'connector/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala']","Arrow-based connect tests are not currently run in Java 21 environment, leaving potential issues under Java 21 unchecked."
cab8aa1c4fe66c4cb1b69112094a203a04758f76,1648654001,"[SPARK-38652][K8S] `uploadFileUri` should preserve file scheme

### What changes were proposed in this pull request?

This PR replaces `new Path(fileUri.getPath)` with `new Path(fileUri)`.
By using `Path` class constructor with URI parameter, we can preserve file scheme.

### Why are the changes needed?

If we use, `Path` class constructor with `String` parameter, it loses file scheme information.
Although the original code works so far, it fails at Apache Hadoop 3.3.2 and breaks dependency upload feature which is covered by K8s Minikube integration tests.

```scala
test(""uploadFileUri"") {
   val fileUri = org.apache.spark.util.Utils.resolveURI(""/tmp/1.txt"")
   assert(new Path(fileUri).toString == ""file:/private/tmp/1.txt"")
   assert(new Path(fileUri.getPath).toString == ""/private/tmp/1.txt"")
}
```

### Does this PR introduce _any_ user-facing change?

No, this will prevent a regression at Apache Spark 3.3.0 instead.

### How was this patch tested?

Pass the CIs.

In addition, this PR and #36009 will recover K8s IT `DepsTestsSuite`.

Closes #36010 from dongjoon-hyun/SPARK-38652.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala'],"`uploadFileUri` fails at Apache Hadoop 3.3.2 due to utilizing a `Path` class constructor with a `String` parameter, ultimately leading to loss of file scheme information and breaking the dependency upload feature covered by K8s Minikube integration tests.
"
7c7266208a3be984ac1ce53747dc0c3640f4ecac,1505708411,"[SPARK-22043][PYTHON] Improves error message for show_profiles and dump_profiles

## What changes were proposed in this pull request?

This PR proposes to improve error message from:

```
>>> sc.show_profiles()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/context.py"", line 1000, in show_profiles
    self.profiler_collector.show_profiles()
AttributeError: 'NoneType' object has no attribute 'show_profiles'
>>> sc.dump_profiles(""/tmp/abc"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/context.py"", line 1005, in dump_profiles
    self.profiler_collector.dump_profiles(path)
AttributeError: 'NoneType' object has no attribute 'dump_profiles'
```

to

```
>>> sc.show_profiles()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/context.py"", line 1003, in show_profiles
    raise RuntimeError(""'spark.python.profile' configuration must be set ""
RuntimeError: 'spark.python.profile' configuration must be set to 'true' to enable Python profile.
>>> sc.dump_profiles(""/tmp/abc"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/context.py"", line 1012, in dump_profiles
    raise RuntimeError(""'spark.python.profile' configuration must be set ""
RuntimeError: 'spark.python.profile' configuration must be set to 'true' to enable Python profile.
```

## How was this patch tested?

Unit tests added in `python/pyspark/tests.py` and manual tests.

Author: hyukjinkwon <gurwls223@gmail.com>

Closes #19260 from HyukjinKwon/profile-errors.
","['python/pyspark/context.py', 'python/pyspark/tests.py']","Attempting to utilize show_profiles and dump_profiles functions when 'spark.python.profile' configuration isn't set to 'true', results in undescriptive AttributeError instead of a useful error message."
3286bff94286325920273a0d50434817158471cd,1554267906,"[SPARK-27255][SQL] Report error when illegal expressions are hosted by a plan operator.

## What changes were proposed in this pull request?
In the PR, we raise an AnalysisError when we detect the presense of aggregate expressions in where clause. Here is the problem description from the JIRA.

Aggregate functions should not be allowed in WHERE clause. But Spark SQL throws an exception when generating codes. It is supposed to throw an exception during parsing or analyzing.

Here is an example:
```
val df = spark.sql(""select * from t where sum(ta) > 0"")
df.explain(true)
df.show()
```
Resulting exception:
```
Exception in thread ""main"" java.lang.UnsupportedOperationException: Cannot generate code for expression: sum(cast(input[0, int, false] as bigint))
	at org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:291)
	at org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:290)
	at org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression.doGenCode(interfaces.scala:87)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:138)
	at scala.Option.getOrElse(Option.scala:138)
```
Checked the behaviour of other database and all of them return an exception:
**Postgress**
```
select * from foo where max(c1) > 0;
Error
ERROR: aggregate functions are not allowed in WHERE Position: 25
```
**DB2**
```
db2 => select * from foo where max(c1) > 0;
SQL0120N  Invalid use of an aggregate function or OLAP function.
```
**Oracle**
```
select * from foo where max(c1) > 0;
ORA-00934: group function is not allowed here
```
**MySql**
```
select * from foo where max(c1) > 0;
Invalid use of group function
```

**Update**
This PR has been enhanced to report error when expressions such as Aggregate, Window, Generate are hosted by operators where they are invalid.
## How was this patch tested?
Added tests in AnalysisErrorSuite and group-by.sql

Closes #24209 from dilipbiswal/SPARK-27255.

Authored-by: Dilip Biswal <dbiswal@us.ibm.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/PlanHelper.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/group-by.sql', 'sql/core/src/test/resources/sql-tests/inputs/subquery/negative-cases/invalid-correlation.sql']","Aggregate functions in WHERE clause don't trigger an error during parsing or analyzing but cause an exception during code generation, inconsistent with behaviors of other databases."
3192bbd29585607d43d0819c6c2d3ac00180261a,1684241941,"[SPARK-39281][SQL] Speed up Timestamp type inference with legacy format  in JSON/CSV data source

### What changes were proposed in this pull request?
Follow up https://github.com/apache/spark/pull/36562 , performance improvement when Timestamp type inference with legacy format.

In the current implementation of CSV/JSON data source, the schema inference with legacy format relies on methods that will throw exceptions if the fields can't convert as some data types .

Throwing and catching exceptions can be slow. We can improve it by creating methods that return optional results instead.

The optimization of DefaultTimestampFormatter has been implemented in https://github.com/apache/spark/pull/36562 , this PR adds the optimization of legacy format. The basic logic is to prevent the formatter from throwing exceptions, and then use catch to determine whether the parsing is successful.

### Why are the changes needed?

Performance improvement when Timestamp type inference with legacy format.

When use JSON datasource, the speed up `67%`. CSV datasource speed also up, but not obvious.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Add new test

Closes #41091 from Hisoka-X/SPARK-39281_legacy_format.

Lead-authored-by: Jia Fan <fanjiaeminem@qq.com>
Co-authored-by: Hisoka <fanjiaeminem@qq.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala']","Timestamp type inference with legacy format is slow in CSV/JSON data source due to reliance on methods throwing exceptions on failed conversions, impacting performance."
0f8e5dd445b03161a27893ba714db57919d8bcab,1610096722,"[SPARK-34003][SQL] Fix Rule conflicts between PaddingAndLengthCheckForCharVarchar and ResolveAggregateFunctions

### What changes were proposed in this pull request?

ResolveAggregateFunctions is a hacky rule and it calls `executeSameContext` to generate a `resolved agg` to determine which unresolved sort attribute should be pushed into the agg. However, after we add the PaddingAndLengthCheckForCharVarchar rule which will rewrite the query output, thus, the `resolved agg` cannot match original attributes anymore.

It causes some dissociative sort attribute to be pushed in and fails the query

``` logtalk
[info]   Failed to analyze query: org.apache.spark.sql.AnalysisException: expression 'testcat.t1.`v`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;
[info]   Project [v#14, sum(i)#11L]
[info]   +- Sort [aggOrder#12 ASC NULLS FIRST], true
[info]      +- !Aggregate [v#14], [v#14, sum(cast(i#7 as bigint)) AS sum(i)#11L, v#13 AS aggOrder#12]
[info]         +- SubqueryAlias testcat.t1
[info]            +- Project [if ((length(v#6) <= 3)) v#6 else if ((length(rtrim(v#6, None)) > 3)) cast(raise_error(concat(input string of length , cast(length(v#6) as string),  exceeds varchar type length limitation: 3)) as string) else rpad(rtrim(v#6, None), 3,  ) AS v#14, i#7]
[info]               +- RelationV2[v#6, i#7, index#15, _partition#16] testcat.t1
[info]
[info]   Project [v#14, sum(i)#11L]
[info]   +- Sort [aggOrder#12 ASC NULLS FIRST], true
[info]      +- !Aggregate [v#14], [v#14, sum(cast(i#7 as bigint)) AS sum(i)#11L, v#13 AS aggOrder#12]
[info]         +- SubqueryAlias testcat.t1
[info]            +- Project [if ((length(v#6) <= 3)) v#6 else if ((length(rtrim(v#6, None)) > 3)) cast(raise_error(concat(input string of length , cast(length(v#6) as string),  exceeds varchar type length limitation: 3)) as string) else rpad(rtrim(v#6, None), 3,  ) AS v#14, i#7]
[info]               +- RelationV2[v#6, i#7, index#15, _partition#16] testcat.t1
```

### Why are the changes needed?

bugfix
### Does this PR introduce _any_ user-facing change?

no
### How was this patch tested?

new tests

Closes #31027 from yaooqinn/SPARK-34003.

Authored-by: Kent Yao <yao@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala']","Conflict between the rules of ResolveAggregateFunctions and PaddingAndLengthCheckForCharVarchar, causing a mismatch in original attributes when a sort attribute is pushed into the aggregate causing query failure."
bfe1af9a720ed235937a0fdf665376ffff7cce54,1671093584,"[SPARK-41522][BUILD] Pin `versions-maven-plugin` to 2.13.0 to recover  `test-dependencies.sh`

### What changes were proposed in this pull request?
This pr aims to pin `versions-maven-plugin` to 2.13.0 to recover `test-dependencies.sh` and make GA pass , this pr should revert after we know how to use version 2.14.0.

### Why are the changes needed?
`dev/test-dependencies.sh` always use latest `versions-maven-plugin` version, and `versions-maven-plugin` 2.14.0 has not set the version of the sub-module.

Run:

```
build/mvn -q versions:set -DnewVersion=spark-928034 -DgenerateBackupPoms=false
```

**2.14.0**

```
+ git status
On branch test-ci
Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git restore <file>..."" to discard changes in working directory)
  modified:   assembly/pom.xml
  modified:   core/pom.xml
  modified:   examples/pom.xml
  modified:   graphx/pom.xml
  modified:   hadoop-cloud/pom.xml
  modified:   launcher/pom.xml
  modified:   mllib-local/pom.xml
  modified:   mllib/pom.xml
  modified:   pom.xml
  modified:   repl/pom.xml
  modified:   streaming/pom.xml
  modified:   tools/pom.xml

```

**2.13.0**

```
+ git status
On branch test-ci
Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git restore <file>..."" to discard changes in working directory)
  modified:   assembly/pom.xml
  modified:   common/kvstore/pom.xml
  modified:   common/network-common/pom.xml
  modified:   common/network-shuffle/pom.xml
  modified:   common/network-yarn/pom.xml
  modified:   common/sketch/pom.xml
  modified:   common/tags/pom.xml
  modified:   common/unsafe/pom.xml
  modified:   connector/avro/pom.xml
  modified:   connector/connect/common/pom.xml
  modified:   connector/connect/server/pom.xml
  modified:   connector/docker-integration-tests/pom.xml
  modified:   connector/kafka-0-10-assembly/pom.xml
  modified:   connector/kafka-0-10-sql/pom.xml
  modified:   connector/kafka-0-10-token-provider/pom.xml
  modified:   connector/kafka-0-10/pom.xml
  modified:   connector/kinesis-asl-assembly/pom.xml
  modified:   connector/kinesis-asl/pom.xml
  modified:   connector/protobuf/pom.xml
  modified:   connector/spark-ganglia-lgpl/pom.xml
  modified:   core/pom.xml
  modified:   dev/test-dependencies.sh
  modified:   examples/pom.xml
  modified:   graphx/pom.xml
  modified:   hadoop-cloud/pom.xml
  modified:   launcher/pom.xml
  modified:   mllib-local/pom.xml
  modified:   mllib/pom.xml
  modified:   pom.xml
  modified:   repl/pom.xml
  modified:   resource-managers/kubernetes/core/pom.xml
  modified:   resource-managers/kubernetes/integration-tests/pom.xml
  modified:   resource-managers/mesos/pom.xml
  modified:   resource-managers/yarn/pom.xml
  modified:   sql/catalyst/pom.xml
  modified:   sql/core/pom.xml
  modified:   sql/hive-thriftserver/pom.xml
  modified:   sql/hive/pom.xml
  modified:   streaming/pom.xml
  modified:   tools/pom.xml
```

Therefore, the following compilation error will occur when using 2.14.0.

```
2022-12-15T02:37:35.5536924Z [ERROR] [ERROR] Some problems were encountered while processing the POMs:
2022-12-15T02:37:35.5538469Z [FATAL] Non-resolvable parent POM for org.apache.spark:spark-sketch_2.12:3.4.0-SNAPSHOT: Could not find artifact org.apache.spark:spark-parent_2.12:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM  line 22, column 11
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Pass GitHub Actions

Closes #39067 from LuciferYang/test-ci.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['dev/test-dependencies.sh'],"`dev/test-dependencies.sh` fails with `versions-maven-plugin` 2.14.0 as it is not setting the correct version of the sub-module, leading to compilation errors."
f80041fdfddae66bead7a3950028ee04d1b60bd2,1654553256,"[SPARK-38987][SHUFFLE] Throw FetchFailedException when merged shuffle blocks are corrupted and spark.shuffle.detectCorrupt is set to true

### What changes were proposed in this pull request?
Adds the corruption exception handling for merged shuffle chunk when spark.shuffle.detectCorrupt is set to true(default value is true)

### Why are the changes needed?
Prior to Spark 3.0, spark.shuffle.detectCorrupt is set to true by default, and this configuration is one of the knob for early corruption detection. So the fallback can be triggered as expected.

After Spark 3.0, even though spark.shuffle.detectCorrupt is still set to true by default, but the early corruption detect knob is controlled with a new configuration spark.shuffle.detectCorrupt.useExtraMemory, and it set to false by default. Thus the default behavior, with only Magnet enabled after Spark 3.2.0(internal li-3.1.1), will disable the early corruption detection, thus no fallback will be triggered. And it will drop to throw an exception when start to read the corrupted blocks.

We handle the corrupted stream for merged blocks by throwing a FetchFailedException in this case. This will trigger a retry based on the values of spark.shuffle.detectCorrupt.useExtraMemory and spark.shuffle.detectCorrupt.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
- Tested on internal cluster
- Added UT

This is a PR to tackle some of the build weirdness found in PR 36601 (https://github.com/apache/spark/pull/36601).
It contains the exact same diff. Closed that one out and recreated it here.

Closes #36734 from akpatnam25/SPARK-38987.

Authored-by: Aravind Patnam <apatnam@linkedin.com>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala', 'core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala', 'core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala', 'core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala']","When spark.shuffle.detectCorrupt is set to true, corruption in merged shuffle blocks isn't triggering a FetchFailedException or any subsequent fallbacks, resulting in exceptions thrown during reads of corrupted blocks."
b09b7f7cc024d3054debd7bdb51caec3b53764d7,1626376917,"[SPARK-36034][SQL] Rebase datetime in pushed down filters to parquet

### What changes were proposed in this pull request?
In the PR, I propose to propagate either the SQL config `spark.sql.parquet.datetimeRebaseModeInRead` or/and Parquet option `datetimeRebaseMode` to `ParquetFilters`. The `ParquetFilters` class uses the settings in conversions of dates/timestamps instances from datasource filters to values pushed via `FilterApi` to the `parquet-column` lib.

Before the changes, date/timestamp values expressed as days/microseconds/milliseconds are interpreted as offsets in Proleptic Gregorian calendar, and pushed to the parquet library as is. That works fine if timestamp/dates values in parquet files were saved in the `CORRECTED` mode but in the `LEGACY` mode, filter's values could not match to actual values.

After the changes, timestamp/dates values of filters pushed down to parquet libs such as `FilterApi.eq(col1, -719162)` are rebased according the rebase settings. For the example, if the rebase mode is `CORRECTED`, **-719162** is pushed down as is but if the current rebase mode is `LEGACY`, the number of days is rebased to **-719164**. For more context, the PR description https://github.com/apache/spark/pull/28067 shows the diffs between two calendars.

### Why are the changes needed?
The changes fix the bug portrayed by the following example from SPARK-36034:
```scala
In [27]: spark.conf.set(""spark.sql.legacy.parquet.datetimeRebaseModeInWrite"", ""LEGACY"")
>>> spark.sql(""SELECT DATE '0001-01-01' AS date"").write.mode(""overwrite"").parquet(""date_written_by_spark3_legacy"")
>>> spark.read.parquet(""date_written_by_spark3_legacy"").where(""date = '0001-01-01'"").show()
+----+
|date|
+----+
+----+
```
The result must have the date value `0001-01-01`.

### Does this PR introduce _any_ user-facing change?
In some sense, yes. Query results can be different in some cases. For the example above:
```scala
scala> spark.conf.set(""spark.sql.parquet.datetimeRebaseModeInWrite"", ""LEGACY"")
scala> spark.sql(""SELECT DATE '0001-01-01' AS date"").write.mode(""overwrite"").parquet(""date_written_by_spark3_legacy"")
scala> spark.read.parquet(""date_written_by_spark3_legacy"").where(""date = '0001-01-01'"").show(false)
+----------+
|date      |
+----------+
|0001-01-01|
+----------+
```

### How was this patch tested?
By running the modified test suite `ParquetFilterSuite`:
```
$ build/sbt ""test:testOnly *ParquetV1FilterSuite""
$ build/sbt ""test:testOnly *ParquetV2FilterSuite""
```

Closes #33347 from MaxGekk/fix-parquet-ts-filter-pushdown.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala']","Filters pushed down to parquet in Spark SQL are not properly rebasing datetime values for 'LEGACY' files, leading to inaccurate query results."
8d28839689614b497be06743ef04a70f815ae0cb,1625030334,"[SPARK-35946][PYTHON] Respect Py4J server in InheritableThread API

### What changes were proposed in this pull request?

Currently ,we sets the environment variable `PYSPARK_PIN_THREAD` at the client side of `InhertiableThread` API for Py4J (`python/pyspark/util.py`). If the Py4J gateway is created somewhere else (e.g., Zeppelin, etc), it could introduce a breakage at:

```python
from pyspark import SparkContext
jvm = SparkContext._jvm
thread_connection = jvm._gateway_client.get_thread_connection()
# `AttributeError: 'GatewayClient' object has no attribute 'get_thread_connection'` (non-pinned thread mode)
# `get_thread_connection` is only in 'ClientServer' (pinned thread mode)
```

This PR proposes to check the given gateway created, and do the pinned thread mode behaviour accordingly so we can avoid any breakage when Py4J server/gateway is created separately from somewhere else without a pinned thread mode.

### Why are the changes needed?

To avoid any potential breakage.

### Does this PR introduce _any_ user-facing change?

No, the change happened only in the master (https://github.com/apache/spark/commit/fdd7ca5f4e35a906090f3c6b160bdba9ac9fd0ca).

### How was this patch tested?

This is actually a partial revert of https://github.com/apache/spark/commit/fdd7ca5f4e35a906090f3c6b160bdba9ac9fd0ca. As long as the existing tests pass, I guess we're all good.

I also manually tested to make doubly sure:

**Before**:

```python
>>> from pyspark import InheritableThread, inheritable_thread_target
>>> InheritableThread(lambda: 1).start()
>>> inheritable_thread_target(lambda: 1)()
Traceback (most recent call last):
  File ""/.../python3.8/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/.../python3.8/lib/python3.8/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""/.../spark/python/pyspark/util.py"", line 361, in copy_local_properties
    InheritableThread._clean_py4j_conn_for_current_thread()
  File ""/.../spark/python/pyspark/util.py"", line 381, in _clean_py4j_conn_for_current_thread
    thread_connection = jvm._gateway_client.get_thread_connection()
AttributeError: 'GatewayClient' object has no attribute 'get_thread_connection'

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/util.py"", line 324, in wrapped
    InheritableThread._clean_py4j_conn_for_current_thread()
  File ""/.../spark/python/pyspark/util.py"", line 381, in _clean_py4j_conn_for_current_thread
    thread_connection = jvm._gateway_client.get_thread_connection()
AttributeError: 'GatewayClient' object has no attribute 'get_thread_connection'
```

**After**:

```python
>>> from pyspark import InheritableThread, inheritable_thread_target
>>> InheritableThread(lambda: 1).start()
>>> inheritable_thread_target(lambda: 1)()
1
```

Closes #33147 from HyukjinKwon/SPARK-35946.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['python/pyspark/util.py'],"Use of 'InheritableThread' API for Py4J is causing breakage when the Py4J gateway is created independently, in a non-pinned thread mode. The 'get_thread_connection' attribute in 'GatewayClient' only exists in pinned thread mode."
925449283dcaef80e0f77e60aea6ef988bd697b4,1536173940,"[SPARK-22666][ML][SQL] Spark datasource for image format

## What changes were proposed in this pull request?

Implement an image schema datasource.

This image datasource support:
  - partition discovery (loading partitioned images)
  - dropImageFailures (the same behavior with `ImageSchema.readImage`)
  - path wildcard matching (the same behavior with `ImageSchema.readImage`)
  - loading recursively from directory (different from `ImageSchema.readImage`, but use such path: `/path/to/dir/**`)

This datasource **NOT** support:
  - specify `numPartitions` (it will be determined by datasource automatically)
  - sampling (you can use `df.sample` later but the sampling operator won't be pushdown to datasource)

## How was this patch tested?
Unit tests.

## Benchmark
I benchmark and compare the cost time between old `ImageSchema.read` API and my image datasource.

**cluster**: 4 nodes, each with 64GB memory, 8 cores CPU
**test dataset**: Flickr8k_Dataset (about 8091 images)

**time cost**:
- My image datasource time (automatically generate 258 partitions):  38.04s
- `ImageSchema.read` time (set 16 partitions): 68.4s
- `ImageSchema.read` time (set 258 partitions):  90.6s

**time cost when increase image number by double (clone Flickr8k_Dataset and loads double number images)**:
- My image datasource time (automatically generate 515 partitions):  95.4s
- `ImageSchema.read` (set 32 partitions): 109s
- `ImageSchema.read` (set 515 partitions):  105s

So we can see that my image datasource implementation (this PR) bring some performance improvement compared against old`ImageSchema.read` API.

Closes #22328 from WeichenXu123/image_datasource.

Authored-by: WeichenXu <weichen.xu@databricks.com>
Signed-off-by: Xiangrui Meng <meng@databricks.com>
","['mllib/src/main/scala/org/apache/spark/ml/source/image/ImageDataSource.scala', 'mllib/src/main/scala/org/apache/spark/ml/source/image/ImageFileFormat.scala', 'mllib/src/main/scala/org/apache/spark/ml/source/image/ImageOptions.scala', 'mllib/src/test/scala/org/apache/spark/ml/image/ImageSchemaSuite.scala', 'mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala', 'python/pyspark/ml/image.py', 'python/pyspark/ml/tests.py']","Lack of an image schema datasource inhibits functionalities like partition discovery, dropImageFailures, path wildcard matching, and recursive loading from directories, while also negatively impacting performance of image loading and scaling."
40ef5c91ade906b38169f959b3991ce8b0f45154,1600275205,"[SPARK-32816][SQL] Fix analyzer bug when aggregating multiple distinct DECIMAL columns

### What changes were proposed in this pull request?
This PR fixes a conflict between `RewriteDistinctAggregates` and `DecimalAggregates`.
In some cases, `DecimalAggregates` will wrap the decimal column to `UnscaledValue` using
different rules for different aggregates.

This means, same distinct column with different aggregates will change to different distinct columns
after `DecimalAggregates`. For example:
`avg(distinct decimal_col), sum(distinct decimal_col)` may change to
`avg(distinct UnscaledValue(decimal_col)), sum(distinct decimal_col)`

We assume after `RewriteDistinctAggregates`, there will be at most one distinct column in aggregates,
but `DecimalAggregates` breaks this assumption. To fix this, we have to switch the order of these two
rules.

### Why are the changes needed?
bug fix

### Does this PR introduce _any_ user-facing change?
no

### How was this patch tested?
added test cases

Closes #29673 from linhongliu-db/SPARK-32816.

Authored-by: Linhong Liu <linhong.liu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/core/src/test/resources/sql-tests/inputs/group-by.sql']","Aggregating multiple distinct DECIMAL columns results in a bug due to conflict between `RewriteDistinctAggregates` and `DecimalAggregates`, breaking assumption of having at most one distinct column in aggregates."
b01d81ef97a7b4e98558eca194900d337911c432,1648537093,"[SPARK-37641][SQL] Support ANSI Aggregate Function: regr_r2

### What changes were proposed in this pull request?
This PR used to support ANSI aggregate Function: `regr_r2`

**Syntax**: REGR_R2(y, x)
**Arguments**:
- **y**:The dependent variable. This must be an expression that can be evaluated to a numeric type.
- **x**:The independent variable. This must be an expression that can be evaluated to a numeric type.

**Examples**:
`select k, regr_r2(v, v2) from aggr group by k;`

| k | regr_r2(v, v2) |
|--|---------------|
| 1 |    [NULL]         |
| 2 | 0.9976905312   |

The mainstream database supports `regr_r2` show below:
**Teradata**
https://docs.teradata.com/r/756LNiPSFdY~4JcCCcR5Cw/exhFe2f_YyGqKFakYYUn2A
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_r2.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**DB2**
https://www.ibm.com/docs/en/db2/11.5?topic=af-regression-functions-regr-avgx-regr-avgy-regr-count
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_r2
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-r2-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`regr_r2` is very useful.

### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.

### How was this patch tested?
New tests.

Closes #34894 from beliefer/SPARK-37641.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpressionSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/group-by.sql', 'sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part1.sql', 'sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part1.sql']","The ANSI aggregate function 'regr_r2' is not supported in Spark SQL, which limits its functionality compared to other mainstream databases."
a74ec6d7bbfe185ba995dcb02d69e90a089c293e,1507578997,"[SPARK-22218] spark shuffle services fails to update secret on app re-attempts

This patch fixes application re-attempts when running spark on yarn using the external shuffle service with security on.  Currently executors will fail to launch on any application re-attempt when launched on a nodemanager that had an executor from the first attempt.  The reason for this is because we aren't updating the secret key after the first application attempt.  The fix here is to just remove the containskey check to see if it already exists. In this way, we always add it and make sure its the most recent secret.  Similarly remove the check for containsKey on the remove since its just adding extra check that isn't really needed.

Note this worked before spark 2.2 because the check used to be contains (which was looking for the value) rather then containsKey, so that never matched and it was just always adding the new secret.

Patch was tested on a 10 node cluster as well as added the unit test.
The test ran was a wordcount where the output directory already existed.  With the bug present the application attempt failed with max number of executor Failures which were all saslExceptions.  With the fix present the application re-attempts fail with directory already exists or when you remove the directory between attempts the re-attemps succeed.

Author: Thomas Graves <tgraves@unharmedunarmed.corp.ne1.yahoo.com>

Closes #19450 from tgravescs/SPARK-22218.
","['common/network-shuffle/src/main/java/org/apache/spark/network/sasl/ShuffleSecretManager.java', 'common/network-shuffle/src/test/java/org/apache/spark/network/sasl/ShuffleSecretManagerSuite.java']","Application re-attempts fail on Spark when running with Yarn using external shuffle service with security on, due to the non-updation of the secret key after the first attempt. This leads to executor launch failure on nodemanager that had an executor from the first attempt."
fde833c532630092204dc54299702676e1de8b74,1661452218,"[SPARK-40130][PYTHON] Support NumPy scalars in built-in functions

### What changes were proposed in this pull request?
Support NumPy scalars in built-in functions by introducing Py4J input converter `NumpyScalarConverter`.

Specifically,
- `np.int8, np.int16, np.int32, np.int64` are mapped to Spark `int/bigint`.
- `np.float32, np.float64` are mapped to Spark `double`.

Note that 2147483648 is the boundary between Spark `int` and `bigint`:
```py
>>> df.select(lit(np.int64(max_int + 1))).dtypes
[('2147483648', 'bigint')]
>>> df.select(lit(np.int64(max_int))).dtypes
[('2147483647', 'int')]
```

### Why are the changes needed?
As part of [SPARK-39405](https://issues.apache.org/jira/browse/SPARK-39405) for NumPy support in SQL.

### Does this PR introduce _any_ user-facing change?
Yes. NumPy scalars are supported in built-in functions when input parameter accepts scalars;
Influenced functions include `lit`, `when`, `array_contains`, `array_position`, `element_at`, `array_remove`.

Take `lit` for example,
```py
>>> df.select(lit(np.int8(1))).dtypes
[('1', 'int')]
>>> df.select(lit(np.float32(1))).dtypes
[('1.0', 'double')]
```

### How was this patch tested?
Unit tests.

Closes #37560 from xinrong-meng/builtin_np.

Authored-by: Xinrong Meng <xinrong@apache.org>
Signed-off-by: Xinrong Meng <xinrong@apache.org>
","['python/pyspark/sql/tests/test_functions.py', 'python/pyspark/sql/types.py', 'python/pyspark/sql/utils.py']","Built-in functions in Spark do not currently support NumPy scalars, causing type mismatches when using NumPy datatypes such as `np.int64` or `np.float32`."
c34140d8d744dc75d130af60080a2a8e25d501b1,1650225344,"[SPARK-38927][TESTS] Skip NumPy/Pandas tests in `test_rdd.py` if not available

### What changes were proposed in this pull request?
This PR aims to skip NumPy/Pandas tests in `test_rdd.py` if they are not available.

### Why are the changes needed?
Currently, the tests that involve NumPy or Pandas are failing because NumPy and Pandas are unavailable in underlying Python. The tests should be skipped instead instead of showing failure.

**BEFORE**
```
======================================================================
ERROR: test_take_on_jrdd_with_large_rows_should_not_cause_deadlock (pyspark.tests.test_rdd.RDDTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "".../test_rdd.py"", line 723, in test_take_on_jrdd_with_large_rows_should_not_cause_deadlock
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

----------------------------------------------------------------------
Ran 1 test in 1.990s

FAILED (errors=1)
```

**AFTER**
```
Finished test(python3.9): pyspark.tests.test_rdd RDDTests.test_take_on_jrdd_with_large_rows_should_not_cause_deadlock (1s) ... 1 tests were skipped
Tests passed in 1 seconds

Skipped tests in pyspark.tests.test_rdd RDDTests.test_take_on_jrdd_with_large_rows_should_not_cause_deadlock with python3.9:
    test_take_on_jrdd_with_large_rows_should_not_cause_deadlock (pyspark.tests.test_rdd.RDDTests) ... skipped 'NumPy or Pandas not installed'
```

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Pass the CIs.

Closes #36235 from williamhyun/skipnumpy.

Authored-by: William Hyun <william@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['python/pyspark/tests/test_rdd.py'],NumPy or Pandas unavailability in the Python environment cause tests in `test_rdd.py` to fail instead of skipping them.
823e3d309c51528e69893f71c4be0f5bc8552d99,1578812637,"[SPARK-30353][SQL] Add IsNotNull check in SimplifyBinaryComparison optimization

### What changes were proposed in this pull request?

Now Spark can propagate constraint during sql optimization when `spark.sql.constraintPropagation.enabled` is true, then `where c = 1` will convert to `where c = 1 and c is not null`. We also can use constraint in `SimplifyBinaryComparison`.

`SimplifyBinaryComparison` will simplify expression which is not nullable and semanticEquals. And we also can simplify if one expression is infered `IsNotNull`.

### Why are the changes needed?

Simplify SQL.
```
create table test (c1 string);

explain extended select c1 from test where c1 = c1 limit 10;
-- before
GlobalLimit 10
+- LocalLimit 10
   +- Filter (isnotnull(c1#20) AND (c1#20 = c1#20))
      +- Relation[c1#20]
-- after
GlobalLimit 10
+- LocalLimit 10
    +- Filter (isnotnull(c1#20)
        +- Relation[c1#20]

explain extended select c1 from test where c1 > c1 limit 10;
-- before
GlobalLimit 10
+- LocalLimit 10
   +- Filter (isnotnull(c1#20) && (c1#20 > c1#20))
      +- Relation[c1#20]
-- after
LocalRelation <empty>, [c1#20]
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Add UT.

Closes #27008 from ulysses-you/SPARK-30353.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/BinaryComparisonSimplificationSuite.scala']",SimplifyBinaryComparison optimization fails to simplify expressions with inferred IsNotNull constraints when spark.sql.constraintPropagation.enabled is set to true.
e000cb868ccb1a4f48a8356ccfc736e16ed1c1b5,1689369128,"[SPARK-44264][PYTHON][ML] FunctionPickler Class

### What changes were proposed in this pull request?
This PR introduces the FunctionPickler utility class that will be responsible for pickling class and their arguments, creating scripts that will run those functions and pickle their output, as well as extracting objects from a pickle file.

### Why are the changes needed?
This is used to abstract away the responsibility of pickling from the TorchDistributor, as that is relatively tangential to the actual distributed training. Additionally, for future distributors or anything that uses pickling to transmit objects, this class can prove useful with built-in functionalities.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Wrote unit tests.

Checklist:

- [x] Pickles a function and its arguments to a file.
- [x] Creates a file that will execute that, given a path to pickled functions and arguments, run the function and arguments and then pickle the output to another location.
- [x] Extracts output given a pickle file.
- [x] Unit tests for first feature.
- [x] Unit tests for second feature.
- [x] Unit tests for third feature.

Closes #41946 from mathewjacob1002/function_pickler.

Lead-authored-by: Mathew Jacob <mathew.jacob@databricks.com>
Co-authored-by: Mathew Jacob <134338709+mathewjacob1002@users.noreply.github.com>
Signed-off-by: Xinrong Meng <xinrong@apache.org>
","['python/pyspark/ml/dl_util.py', 'python/pyspark/ml/tests/test_dl_util.py']","The TorchDistributor currently handles the responsibility of pickling which is unrelated to its main function of distributed training, leading to potential inefficiencies and issues with future distributors or pickling-related tasks.
"
fe1f456b200075bef476b5d8eca2be505b486033,1573703701,"[SPARK-29837][SQL] PostgreSQL dialect: cast to boolean

### What changes were proposed in this pull request?

Make SparkSQL's `cast to boolean` behavior be consistent with PostgreSQL when
spark.sql.dialect is configured as PostgreSQL.

### Why are the changes needed?

SparkSQL and PostgreSQL have a lot different cast behavior between types by default. We should make SparkSQL's cast behavior be consistent with PostgreSQL when `spark.sql.dialect` is configured as PostgreSQL.

### Does this PR introduce any user-facing change?

Yes. If user switches to PostgreSQL dialect now, they will

* get an exception if they input a invalid string, e.g ""erut"", while they get `null` before;

* get an exception if they input `TimestampType`, `DateType`, `LongType`, `ShortType`, `ByteType`, `DecimalType`, `DoubleType`, `FloatType` values,  while they get `true` or `false` result before.

And here're evidences for those unsupported types from PostgreSQL:

timestamp:
```
postgres=# select cast(cast('2019-11-11' as timestamp) as boolean);
ERROR:  cannot cast type timestamp without time zone to boolean
```

date:
```
postgres=# select cast(cast('2019-11-11' as date) as boolean);
ERROR:  cannot cast type date to boolean
```

bigint:
```
postgres=# select cast(cast('20191111' as bigint) as boolean);
ERROR:  cannot cast type bigint to boolean
```

smallint:
```
postgres=# select cast(cast(2019 as smallint) as boolean);
ERROR:  cannot cast type smallint to boolean
```

bytea:
```
postgres=# select cast(cast('2019' as bytea) as boolean);
ERROR:  cannot cast type bytea to boolean
```

decimal:
```
postgres=# select cast(cast('2019' as decimal) as boolean);
ERROR:  cannot cast type numeric to boolean
```

float:
```
postgres=# select cast(cast('2019' as float) as boolean);
ERROR:  cannot cast type double precision to boolean
```

### How was this patch tested?

Added and tested manually.

Closes #26463 from Ngone51/dev-postgre-cast2bool.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/PostgreSQLDialect.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/postgreSQL/PostgreCastStringToBoolean.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/postgreSQL/PostgreCastToBoolean.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/postgreSQL/CastSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/PostgreSQLDialectQuerySuite.scala']",Inconsistency in casting to boolean between SparkSQL and PostgreSQL when the SQL dialect is set to PostgreSQL causing unexpected return types and exceptions.
a9d4e60a90d4d6765642e6bf7810da117af6437b,1598369158,"[SPARK-32614][SQL] Don't apply comment processing if 'comment' unset for CSV

### What changes were proposed in this pull request?

Spark's CSV source can optionally ignore lines starting with a comment char. Some code paths check to see if it's set before applying comment logic (i.e. not set to default of `\0`), but many do not, including the one that passes the option to Univocity. This means that rows beginning with a null char were being treated as comments even when 'disabled'.

### Why are the changes needed?

To avoid dropping rows that start with a null char when this is not requested or intended. See JIRA for an example.

### Does this PR introduce _any_ user-facing change?

Nothing beyond the effect of the bug fix.

### How was this patch tested?

Existing tests plus new test case.

Closes #29516 from srowen/SPARK-32614.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVExprUtils.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVOptions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala']","Rows that start with a null character are being treated as comments and dropped, regardless of whether comment processing is enabled or disabled in CSV source."
9948b860aca42bbc6478ddfbc0ff590adb00c2f3,1512595328,"[SPARK-22516][SQL] Bump up Univocity version to 2.5.9

## What changes were proposed in this pull request?

There was a bug in Univocity Parser that causes the issue in SPARK-22516. This was fixed by upgrading from 2.5.4 to 2.5.9 version of the library :

**Executing**
```
spark.read.option(""header"",""true"").option(""inferSchema"", ""true"").option(""multiLine"", ""true"").option(""comment"", ""g"").csv(""test_file_without_eof_char.csv"").show()
```
**Before**
```
ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)
com.univocity.parsers.common.TextParsingException: java.lang.IllegalArgumentException - Unable to skip 1 lines from line 2. End of input reached
...
Internal state when error was thrown: line=3, column=0, record=2, charIndex=31
	at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:339)
	at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:475)
	at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.next(UnivocityParser.scala:281)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
```
**After**
```
+-------+-------+
|column1|column2|
+-------+-------+
|    abc|    def|
+-------+-------+
```

## How was this patch tested?
The already existing `CSVSuite.commented lines in CSV data` test was extended to parse the file also in multiline mode. The test input file was modified to also include a comment in the last line.

Author: smurakozi <smurakozi@gmail.com>

Closes #19906 from smurakozi/SPARK-22516.
",['sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala'],"Univocity Parser bug causing failure in properly parsing CSV files with specific options (""header"", ""inferSchema"", ""multiLine"", ""comment""). Error thrown on trying to skip lines at end of input."
8b702e1e0aba1d3e4b0aa582f20cf99f80a44a09,1536814569,"[SPARK-25415][SQL] Make plan change log in RuleExecutor configurable by SQLConf

## What changes were proposed in this pull request?

In RuleExecutor, after applying a rule, if the plan has changed, the before and after plan will be logged using level ""trace"". At times, however, such information can be very helpful for debugging. Hence, making the log level configurable in SQLConf would allow users to turn on the plan change log independently and save the trouble of tweaking log4j settings. Meanwhile, filtering plan change log for specific rules can also be very useful.
So this PR adds two SQL configurations:
1. spark.sql.optimizer.planChangeLog.level - set a specific log level for logging plan changes after a rule is applied.
2. spark.sql.optimizer.planChangeLog.rules - enable plan change logging only for a set of specified rules, separated by commas.

## How was this patch tested?

Added UT.

Closes #22406 from maryannxue/spark-25415.

Authored-by: maryannxue <maryannxue@apache.org>
Signed-off-by: gatorsmile <gatorsmile@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleExecutor.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OptimizerLoggingSuite.scala']","The logging level of plan changes in RuleExecutor is not configurable, making debugging difficult without tweaking log4j settings. There's also no way to filter plan change logs for specific rules."
7046978949e5544d84909ea980523104be2aef7c,1661323975,"[SPARK-40180][SQL] Format error messages by `spark-sql`

### What changes were proposed in this pull request?
1. Respect the SQL config `spark.sql.error.messageFormat` introduced by https://github.com/apache/spark/pull/37520, and output error messages in the one of format: `PRETTY` (by default), `MINIMAL` or `STANDARD`.
2. In the `PRETTY` format, output the error message of the `AnalysisException` exception in the same way as for other exceptions, i. e. w/o `Error in query:`.
3. Take into account the `silent` CLI option, and output the call stack only when it is `false`.

In the `MINIMAL` and `STANDARD` formats don't print the call stack independently from the `silent` mode.

### Why are the changes needed?
To respect the SQL config `spark.sql.error.messageFormat` and to be consistent to error outputs of the Thrift Server.

### Does this PR introduce _any_ user-facing change?
Yes.

The PR changes the behavior for `AnalysisException`. In that case, `spark-sql` does not output the prefix: `Error in query:` by default (format is PRETTY).

Before:
```sql
spark-sql> DROP TABLE ajhkdha;
Error in query: Table or view not found: ajhkdha; line 1 pos 11;
```

After:
```sql
spark-sql> DROP TABLE ajhkdha;
Table or view not found: ajhkdha; line 1 pos 11;
```

### How was this patch tested?
By running the modified test suites:
```
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly org.apache.spark.sql.hive.thriftserver.CliSuite""
```

Closes #37590 from MaxGekk/spark-sql-error-json.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala', 'sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala']",The current error message handling in `spark-sql` is not respecting the SQL config `spark.sql.error.messageFormat` and is inconsistent in error outputs for different exceptions and formats.
f54b402021785e0b0ec976ec889de67d3b2fdc6e,1690916034,"[SPARK-29497][CONNECT] Throw error when UDF is not deserializable

### What changes were proposed in this pull request?
This PR adds a better error message when a JVM UDF cannot be deserialized.

### Why are the changes needed?
In some cases a UDF cannot be deserialized. The happens when a lambda references itself (typically through the capturing class). Java cannot deserialize such an object graph because SerializedLambda's are serialization proxies which need the full graph to be deserialized before they can be transformed into the actual lambda. This is not possible if there is such a cycle. This PR adds a more readable and understandable error when this happens, the original java one is a `ClassCastException` which does not explain or give any hints of what is actually going on.

### Does this PR introduce _any_ user-facing change?
Yes. It will throw an error on the client when a UDF is not deserializable. The error is better and more actionable then what we got before.

### How was this patch tested?
Added tests.

Closes #42245 from hvanhovell/SPARK-29497.

Lead-authored-by: Herman van Hovell <hvanhovell@databricks.com>
Co-authored-by: Herman van Hovell <herman@databricks.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
","['common/utils/src/main/scala/org/apache/spark/util/SparkSerDeUtils.scala', 'connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala', 'connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/UserDefinedFunctionSuite.scala', 'core/src/main/scala/org/apache/spark/util/Utils.scala']","Uninformative `ClassCastException` thrown when a self-referencing lambda UDF fails to deserialize, hindering understanding and issue resolution."
1e03db36a939aea5b4d55059967ccde96cb29564,1694508908,"[SPARK-44911][SQL] Create hive table with invalid column should return error class

### What changes were proposed in this pull request?

create hive table with invalid column should return error class.

run sql
```
create table test stored as parquet as select id, date'2018-01-01' + make_dt_interval(0, id)  from range(0, 10)
```

before this issue , error would be :

```
org.apache.spark.sql.AnalysisException: Cannot create a table having a column whose name contains commas in Hive metastore. Table: `spark_catalog`.`default`.`test`; Column: DATE '2018-01-01' + make_dt_interval(0, id, 0, 0.000000)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4(HiveExternalCatalog.scala:175)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4$adapted(HiveExternalCatalog.scala:171)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
```

after this issue
```
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: [INVALID_HIVE_COLUMN_NAME] Cannot create the table `spark_catalog`.`default`.`parquet_ds1` having the column `DATE '2018-01-01' + make_dt_interval(0, id, 0, 0`.`000000)` whose name contains invalid characters ',' in Hive metastore.
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4(HiveExternalCatalog.scala:180)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4$adapted(HiveExternalCatalog.scala:171)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
```

### Why are the changes needed?

as above

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

add UT

### Was this patch authored or co-authored using generative AI tooling?

no

Closes #42609 from zzzzming95/SPARK-44911.

Authored-by: zzzzming95 <505306252@qq.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala']","Creating a Hive table with an invalid column name containing commas does not return an appropriate class error.
"
0fd98abd859049dc3b200492487041eeeaa8f737,1589940058,"[SPARK-31750][SQL] Eliminate UpCast if child's dataType is DecimalType

### What changes were proposed in this pull request?

Eliminate the `UpCast` if it's child data type is already decimal type.

### Why are the changes needed?

While deserializing internal `Decimal` value to external `BigDecimal`(Java/Scala) value, Spark should also respect `Decimal`'s precision and scale, otherwise it will cause precision lost and look weird in some cases, e.g.:

```
sql(""select cast(11111111111111111111111111111111111111 as decimal(38, 0)) as d"")
  .write.mode(""overwrite"")
  .parquet(f.getAbsolutePath)

// can fail
spark.read.parquet(f.getAbsolutePath).as[BigDecimal]
```
```
[info]   org.apache.spark.sql.AnalysisException: Cannot up cast `d` from decimal(38,0) to decimal(38,18).
[info] The type path of the target object is:
[info] - root class: ""scala.math.BigDecimal""
[info] You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object;
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveUpCast$$fail(Analyzer.scala:3060)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$33$$anonfun$applyOrElse$174.applyOrElse(Analyzer.scala:3087)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$33$$anonfun$applyOrElse$174.applyOrElse(Analyzer.scala:3071)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)
```

### Does this PR introduce _any_ user-facing change?

Yes, for cases(cause precision lost) mentioned above will fail before this change but run successfully after this change.

### How was this patch tested?

Added tests.

Closes #28572 from Ngone51/fix_encoder.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/DeserializerBuildHelper.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala']","Deserializing internal Decimal value to external BigDecimal can cause precision loss, and an error can occur while upcasting `d` from decimal(38,0) to decimal(38,18)."
096552ae4d6fcef5e20c54384a2687db41ba2fa1,1550670578,"[SPARK-26859][SQL] Fix field writer index bug in non-vectorized ORC deserializer

## What changes were proposed in this pull request?

This happens in a schema evolution use case only when a user specifies the schema manually and use non-vectorized ORC deserializer code path.

There is a bug in `OrcDeserializer.scala` that results in `null`s being set at the wrong column position, and for state from previous records to remain uncleared in next records. There are more details for when exactly the bug gets triggered and what the outcome is in the [JIRA issue](https://jira.apache.org/jira/browse/SPARK-26859).

The high-level summary is that this bug results in severe data correctness issues, but fortunately the set of conditions to expose the bug are complicated and make the surface area somewhat small.

This change fixes the problem and adds a respective test.

## How was this patch tested?

Pass the Jenkins with the newly added test cases.

Closes #23766 from IvanVergiliev/fix-orc-deserializer.

Lead-authored-by: Ivan Vergiliev <ivan.vergiliev@gmail.com>
Co-authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/ReadSchemaSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/ReadSchemaTest.scala']","Schema evolution use cases in non-vectorized ORC deserializer have data integrity issues, often leading to setting `null`s in incorrect column positions and uncleared state in subsequent records."
df9d2f81954ff5c14a60c1dc3dad69fb5e6a8152,1656953772,"[SPARK-39653][SQL] Clean up `ColumnVectorUtils#populate(WritableColumnVector, InternalRow, int)` from `ColumnVectorUtils`

### What changes were proposed in this pull request?
After SPARK-39638 and SPARK-39231, `ColumnVectorUtils#populate(WritableColumnVector, InternalRow, int)` method in `ColumnVectorUtils` only used by  `ConstantColumnVectorBenchmark` and `ColumnVectorSuite`.  So this pr do following changes:

- Clean up `ColumnVectorUtils#populate(WritableColumnVector, InternalRow, int)` from `ColumnVectorUtils`
- Added a simplified version `populate` method for `ConstantColumnVectorBenchmark`
- Clean up `SPARK-38018: ColumnVectorUtils.populate to handle CalendarIntervalType correctly` from `ColumnVectorSuite` due this scenario no longer exists, and the similar scenarios using `ConstantColumnVector` have been covered by `fill calendar interval` in `ColumnVectorUtils`.

### Why are the changes needed?
Clean up useless code

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

- Pass GitHub Actions
- Execute `ConstantColumnVectorBenchmark` manually with [Benchmark GitHub Action](https://github.com/LuciferYang/spark/runs/7147111541), the result file can be produced successfully. Since the result has no obvious change, so not update in the current pr

Closes #37045 from LuciferYang/SPARK-39653.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java', 'sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/ConstantColumnVectorBenchmark.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnVectorSuite.scala']","'Redundant `ColumnVectorUtils#populate(WritableColumnVector, InternalRow, int)` method in `ColumnVectorUtils` only being used in `ConstantColumnVectorBenchmark` and `ColumnVectorSuite`. Also, previously fixed scenario `SPARK-38018: ColumnVectorUtils.populate to handle CalendarIntervalType correctly` no longer exists.'"
d0d4aab437843ce5adf5900d2d6088e79323f8d5,1690371087,"[SPARK-44154][SQL][FOLLOWUP] `BitmapCount` and `BitmapOrAgg` should use `DataTypeMismatch` to indicate unexpected input data type

### What changes were proposed in this pull request?

Change `BitmapCount` and `BitmapOrAgg` to use `DataTypeMismatch` rather than `TypeCheckResult.TypeCheckFailure` to indicate incorrect input types.

### Why are the changes needed?

It appears `TypeCheckResult.TypeCheckFailure` has been deprecated: No expressions except for the recently added `BitmapCount` and `BitmapOrAgg` are using it.

### Does this PR introduce _any_ user-facing change?

This PR changes an error message for two expressions that are not yet in any released version of Spark.

Before PR:
```
spark-sql (default)> select bitmap_count(12);
[DATATYPE_MISMATCH.TYPE_CHECK_FAILURE_WITH_HINT] Cannot resolve ""bitmap_count(12)"" due to data type mismatch: Bitmap must be a BinaryType.; line 1 pos 7;
'Project [unresolvedalias(bitmap_count(12), None)]
+- OneRowRelation

spark-sql (default)> select bitmap_or_agg(12);
[DATATYPE_MISMATCH.TYPE_CHECK_FAILURE_WITH_HINT] Cannot resolve ""bitmap_or_agg(12)"" due to data type mismatch: Bitmap must be a BinaryType.; line 1 pos 7;
'Aggregate [unresolvedalias(bitmap_or_agg(12, 0, 0), None)]
+- OneRowRelation
```
After PR:
```
spark-sql (default)> select bitmap_count(12);
[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve ""bitmap_count(12)"" due to data type mismatch: Parameter 0 requires the ""BINARY"" type, however ""12"" has the type ""INT"".; line 1 pos 7;
'Project [unresolvedalias(bitmap_count(12), None)]
+- OneRowRelation

spark-sql (default)> select bitmap_or_agg(12);
[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve ""bitmap_or_agg(12)"" due to data type mismatch: Parameter 0 requires the ""BINARY"" type, however ""12"" has the type ""INT"".; line 1 pos 7;
'Aggregate [unresolvedalias(bitmap_or_agg(12, 0, 0), None)]
+- OneRowRelation
```
### How was this patch tested?

New unit tests.

Closes #42139 from bersprockets/bitmap_type_check.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitmapExpressions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/BitmapExpressionsQuerySuite.scala']","`BitmapCount` and `BitmapOrAgg` make use of `TypeCheckResult.TypeCheckFailure` to highlight input mismatches, leading to unclear error messages when handling incorrect input types."
166cc6204c96665e7b568cfcc8ba243e79dbf837,1619299680,"[SPARK-34990][SQL][TESTS] Add ParquetEncryptionSuite

### What changes were proposed in this pull request?

A simple test that writes and reads an encrypted parquet and verifies that it's encrypted by checking its magic string (in encrypted footer mode).

### Why are the changes needed?

To provide a test coverage for Parquet encryption.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

- [x] [SBT / Hadoop 3.2 / Java8 (the default)](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/137785/testReport)
- [ ] ~SBT / Hadoop 3.2 / Java11 by adding [test-java11] to the PR title.~ (Jenkins Java11 build is broken due to missing JDK11 installation)
- [x] [SBT / Hadoop 2.7 / Java8 by adding [test-hadoop2.7] to the PR title.](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/137836/testReport)
- [x] Maven / Hadoop 3.2 / Java8 by adding [test-maven] to the PR title.
- [x] Maven / Hadoop 2.7 / Java8 by adding [test-maven][test-hadoop2.7] to the PR title.

Closes #32146 from andersonm-ibm/pme_testing.

Authored-by: Maya Anderson <mayaa@il.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['sql/hive/src/test/scala/org/apache/spark/sql/hive/ParquetEncryptionSuite.scala'],The Spark SQL Parquet encryption feature lacks test coverage to verify that Parquet files are correctly encrypted.
731aa2cdf8a78835621fbf3de2d3492b27711d1a,1652143459,"[SPARK-39107][SQL] Account for empty string input in regex replace

### What changes were proposed in this pull request?

When trying to perform a regex replace, account for the possibility of having empty strings as input.

### Why are the changes needed?

https://github.com/apache/spark/pull/29891 was merged to address https://issues.apache.org/jira/browse/SPARK-30796 and introduced a bug that would not allow regex matching on empty strings, as it would account for position within substring but not consider the case where input string has length 0 (empty string)

From https://issues.apache.org/jira/browse/SPARK-39107 there is a change in behavior between spark versions.
3.0.2
```
scala> val df = spark.sql(""SELECT '' AS col"")
df: org.apache.spark.sql.DataFrame = [col: string]

scala> df.withColumn(""replaced"", regexp_replace(col(""col""), ""^$"", ""<empty>"")).show
+---+--------+
|col|replaced|
+---+--------+
|   | <empty>|
+---+--------+
```
3.1.2
```
scala> val df = spark.sql(""SELECT '' AS col"")
df: org.apache.spark.sql.DataFrame = [col: string]

scala> df.withColumn(""replaced"", regexp_replace(col(""col""), ""^$"", ""<empty>"")).show
+---+--------+
|col|replaced|
+---+--------+
|   |        |
+---+--------+
```

The 3.0.2 outcome is the expected and correct one

### Does this PR introduce _any_ user-facing change?

Yes compared to spark 3.2.1, as it brings back the correct behavior when trying to regex match empty strings, as shown in the example above.

### How was this patch tested?

Added special casing test in `RegexpExpressionsSuite.RegexReplace` with empty string replacement.

Closes #36457 from LorenzoMartini/lmartini/fix-empty-string-replace.

Authored-by: Lorenzo Martini <lmartini@palantir.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala']","Regex replace doesn't work correctly with empty string inputs, displaying a change in behaviour between Spark versions 3.0.2 and 3.1.2, where the later version fails to match and replace empty strings."
e5e54a3614ffd2a9150921e84e5b813d5cbf285a,1600338940,"[SPARK-32900][CORE] Allow UnsafeExternalSorter to spill when there are nulls

### What changes were proposed in this pull request?

This PR changes the way `UnsafeExternalSorter.SpillableIterator` checks whether it has spilled already, by checking whether `inMemSorter` is null. It also allows it to spill other `UnsafeSorterIterator`s than `UnsafeInMemorySorter.SortedIterator`.

### Why are the changes needed?

Before this PR `UnsafeExternalSorter.SpillableIterator` could not spill when there are NULLs in the input and radix sorting is used. Currently, Spark determines whether UnsafeExternalSorter.SpillableIterator has not spilled yet by checking whether `upstream` is an instance of `UnsafeInMemorySorter.SortedIterator`. When radix sorting is used and there are NULLs in the input however, `upstream` will be an instance of `UnsafeExternalSorter.ChainedIterator` instead, and Spark will assume that the `SpillableIterator` iterator has spilled already, and therefore cannot spill again when it's supposed to spill.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

A test was added to `UnsafeExternalSorterSuite` (and therefore also to `UnsafeExternalSorterRadixSortSuite`). I manually confirmed that the test failed in `UnsafeExternalSorterRadixSortSuite` without this patch.

Closes #29772 from tomvanbussel/SPARK-32900.

Authored-by: Tom van Bussel <tom.vanbussel@databricks.com>
Signed-off-by: herman <herman@databricks.com>
","['core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java', 'core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java', 'core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java', 'core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java', 'core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java', 'core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java']","UnsafeExternalSorter.SpillableIterator does not spill when radix sorting is used and NULLs are present in the input, leading to falsely assuming that the SpillableIterator has already spilled."
29510821a0e3b1e09a7710ed02a0fa1caab506af,1609903000,"[SPARK-33029][CORE][WEBUI] Fix the UI executor page incorrectly marking the driver as excluded

### What changes were proposed in this pull request?
Filter out the driver entity when updating the exclusion status of live executors(including the driver), so the UI won't be marked as excluded in the UI even if the node that hosts the driver has been marked as excluded.

### Why are the changes needed?
Before this change, if we run spark with the standalone mode and with spark.blacklist.enabled=true. The driver will be marked as excluded when the host that hosts that driver has been marked as excluded. While it's incorrect because the exclude list feature will exclude executors only and the driver is still active.
![image](https://user-images.githubusercontent.com/26694233/103238740-35c05180-4911-11eb-99a2-c87c059ba0cf.png)
After the fix, the driver won't be marked as excluded.
![image](https://user-images.githubusercontent.com/26694233/103238806-6f915800-4911-11eb-80d5-3c99266cfd0a.png)

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Manual test. Reopen the UI and see the driver is no longer marked as excluded.

Closes #30954 from baohe-zhang/SPARK-33029.

Authored-by: Baohe Zhang <baohe.zhang@verizonmedia.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['core/src/main/scala/org/apache/spark/status/AppStatusListener.scala'],"In standalone mode with spark.blacklist.enabled=true, the UI incorrectly marks the driver as excluded when the host that hosts the driver is marked as excluded."
45a145eafe7fcb4303f20ce3360cbfd981f8b16c,1639635771,"[SPARK-37658][INFRA][PYTHON] Skip PIP packaging test in Jenkins

### What changes were proposed in this pull request?

This PR proposes to skip PIP packaging test in Jenkins. We're already running it in GitHub Actions, and Jenkins is expected to be dropped soon this month.

### Why are the changes needed?

```
...
Writing pyspark-3.3.0.dev0/setup.cfg
Creating tar archive
removing 'pyspark-3.3.0.dev0' (and everything under it)
Installing dist into virtual env
Obtaining file:///home/jenkins/workspace/SparkPullRequestBuilder%402/python
pyspark requires Python '>=3.7' but the running Python is 3.6.8
Cleaning up temporary directory - /tmp/tmp.CCragmNU1X
[error] running /home/jenkins/workspace/SparkPullRequestBuilder2/dev/run-pip-tests ; received return code 1
Attempting to post to GitHub...
```

After we drop Python 3.6 at SPARK-37632, PIP packaging test fails intermittently https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/146255/console. Apparently, different Python versions are installed in some of Jenkins machines. We should skip the test in these cases.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Jenkins build in this PR would test it out.

Closes #34917 from HyukjinKwon/SPARK-37658.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['dev/run-tests.py'],"PIP packaging tests are intermittently failing in Jenkins due to the presence of multiple Python versions, particularly Python 3.6, which is no longer supported in new builds."
7781c6fd7334979b6b4222d2271765219593f08e,1548832529,"[SPARK-26378][SQL] Restore performance of queries against wide CSV/JSON tables

## What changes were proposed in this pull request?

After [recent changes](https://github.com/apache/spark/commit/11e5f1bcd49eec8ab4225d6e68a051b5c6a21cb2) to CSV parsing to return partial results for bad CSV records, queries of wide CSV tables slowed considerably. That recent change resulted in every row being recreated, even when the associated input record had no parsing issues and the user specified no corrupt record field in his/her schema.

The change to FailureSafeParser.scala also impacted queries against wide JSON tables as well.

In this PR, I propose that a row should be recreated only if columns need to be shifted due to the existence of a corrupt column field in the user-supplied schema. Otherwise, the code should use the row as-is (For CSV input, it will have values for the columns that could be converted, and also null values for columns that could not be converted).

See benchmarks below. The CSV benchmark for 1000 columns went from 120144 ms to 89069 ms, a savings of 25% (this only brings the cost down to baseline levels. Again, see benchmarks below).

Similarly, the JSON benchmark for 1000 columns (added in this PR) went from 109621 ms to 80871 ms, also a savings of 25%.

Still, partial results functionality is preserved:

<pre>
bash-3.2$ cat test2.csv
""hello"",1999-08-01,""last""
""there"",""bad date"",""field""
""again"",""2017-11-22"",""in file""
bash-3.2$ bin/spark-shell
...etc...
scala> val df = spark.read.schema(""a string, b date, c string"").csv(""test2.csv"")
df: org.apache.spark.sql.DataFrame = [a: string, b: date ... 1 more field]
scala> df.show
+-----+----------+-------+
|    a|         b|      c|
+-----+----------+-------+
|hello|1999-08-01|   last|
|there|      null|  field|
|again|2017-11-22|in file|
+-----+----------+-------+
scala> val df = spark.read.schema(""badRecord string, a string, b date, c string"").
     | option(""columnNameOfCorruptRecord"", ""badRecord"").
     | csv(""test2.csv"")
df: org.apache.spark.sql.DataFrame = [badRecord: string, a: string ... 2 more fields]
scala> df.show
+--------------------+-----+----------+-------+
|           badRecord|    a|         b|      c|
+--------------------+-----+----------+-------+
|                null|hello|1999-08-01|   last|
|""there"",""bad date...|there|      null|  field|
|                null|again|2017-11-22|in file|
+--------------------+-----+----------+-------+
scala>
</pre>

### CSVBenchmark Benchmarks:

baseline = commit before partial results change
PR = this PR
master = master branch

[baseline_CSVBenchmark-results.txt](https://github.com/apache/spark/files/2697109/baseline_CSVBenchmark-results.txt)
[pr_CSVBenchmark-results.txt](https://github.com/apache/spark/files/2697110/pr_CSVBenchmark-results.txt)
[master_CSVBenchmark-results.txt](https://github.com/apache/spark/files/2697111/master_CSVBenchmark-results.txt)

### JSONBenchmark Benchmarks:

baseline = commit before partial results change
PR = this PR
master = master branch

[baseline_JSONBenchmark-results.txt](https://github.com/apache/spark/files/2711040/baseline_JSONBenchmark-results.txt)
[pr_JSONBenchmark-results.txt](https://github.com/apache/spark/files/2711041/pr_JSONBenchmark-results.txt)
[master_JSONBenchmark-results.txt](https://github.com/apache/spark/files/2711042/master_JSONBenchmark-results.txt)

## How was this patch tested?

- All SQL unit tests.
- Added 2 CSV benchmarks
- Python core and SQL tests

Closes #23336 from bersprockets/csv-wide-row-opt2.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/FailureSafeParser.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVBenchmark.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonBenchmark.scala']",Significant slowdown in queries against wide CSV/JSON tables since recent changes to CSV parsing aimed at handling bad CSV records by recreating every row.
3dda58af2b7f42beab736d856bf17b4d35c8866c,1544804608,"[SPARK-26370][SQL] Fix resolution of higher-order function for the same identifier.

## What changes were proposed in this pull request?

When using a higher-order function with the same variable name as the existing columns in `Filter` or something which uses `Analyzer.resolveExpressionBottomUp` during the resolution, e.g.,:

```scala
val df = Seq(
  (Seq(1, 9, 8, 7), 1, 2),
  (Seq(5, 9, 7), 2, 2),
  (Seq.empty, 3, 2),
  (null, 4, 2)
).toDF(""i"", ""x"", ""d"")

checkAnswer(df.filter(""exists(i, x -> x % d == 0)""),
  Seq(Row(Seq(1, 9, 8, 7), 1, 2)))
checkAnswer(df.select(""x"").filter(""exists(i, x -> x % d == 0)""),
  Seq(Row(1)))
```

the following exception happens:

```
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.BoundReference cannot be cast to org.apache.spark.sql.catalyst.expressions.NamedExpression
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:237)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.$anonfun$functionsForEval$1(higherOrderFunctions.scala:147)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:237)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval(higherOrderFunctions.scala:145)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval$(higherOrderFunctions.scala:145)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval$lzycompute(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval(higherOrderFunctions.scala:176)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval$(higherOrderFunctions.scala:176)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionForEval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.nullSafeEval(higherOrderFunctions.scala:387)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:190)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:185)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.eval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
  at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:216)
  at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:215)

...
```

because the `UnresolvedAttribute`s in `LambdaFunction` are unexpectedly resolved by the rule.

This pr modified to use a placeholder `UnresolvedNamedLambdaVariable` to prevent unexpected resolution.

## How was this patch tested?

Added a test and modified some tests.

Closes #23320 from ueshin/issues/SPARK-26370/hof_resolution.

Authored-by: Takuya UESHIN <ueshin@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala']",Using a higher-order function with the same variable name as existing columns in `Filter` or similar functions causes a ClassCastException during resolution.
d51d228048d519a9a666f48dc532625de13e7587,1572906797,"[SPARK-29397][CORE] Extend plugin interface to include the driver

Spark 2.4 added the ability for executor plugins to be loaded into
Spark (see SPARK-24918). That feature intentionally skipped the
driver to keep changes small, and also because it is possible to
load code into the Spark driver using listeners + configuration.

But that is a bit awkward, because the listener interface does not
provide hooks into a lot of Spark functionality. This change reworks
the executor plugin interface to also extend to the driver.

- there's a ""SparkPlugin"" main interface that provides APIs to
  load driver and executor components.
- custom metric support (added in SPARK-28091) can be used by
  plugins to register metrics both in the driver process and in
  executors.
- a communication channel now exists that allows the plugin's
  executor components to send messages to the plugin's driver
  component easily, using the existing Spark RPC system.

The latter was a feature intentionally left out of the original
plugin design (also because it didn't include a driver component).

To avoid polluting the ""org.apache.spark"" namespace, I added the new
interfaces to the ""org.apache.spark.api"" package, which seems like
a better place in any case. The actual implementation is kept in
an internal package.

The change includes unit tests for the new interface and features,
but I've also been running a custom plugin that extends the new
API in real applications.

Closes #26170 from vanzin/SPARK-29397.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['core/src/main/java/org/apache/spark/api/plugin/DriverPlugin.java', 'core/src/main/java/org/apache/spark/api/plugin/ExecutorPlugin.java', 'core/src/main/java/org/apache/spark/api/plugin/PluginContext.java', 'core/src/main/java/org/apache/spark/api/plugin/SparkPlugin.java', 'core/src/main/scala/org/apache/spark/SparkContext.scala', 'core/src/main/scala/org/apache/spark/executor/Executor.scala', 'core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/main/scala/org/apache/spark/internal/plugin/PluginContainer.scala', 'core/src/main/scala/org/apache/spark/internal/plugin/PluginContextImpl.scala', 'core/src/main/scala/org/apache/spark/internal/plugin/PluginEndpoint.scala', 'core/src/test/scala/org/apache/spark/internal/plugin/PluginContainerSuite.scala']",Inability to load driver plugins in Spark. The listener interface does not provide hooks into many Spark functionalities making it challenging to load code into the Spark driver using listeners + configuration.
e0d7665cec1e6954d640f422c79ebba4c273be7d,1511332306,"[SPARK-17920][SPARK-19580][SPARK-19878][SQL] Support writing to Hive table which uses Avro schema url 'avro.schema.url'

## What changes were proposed in this pull request?
SPARK-19580 Support for avro.schema.url while writing to hive table
SPARK-19878 Add hive configuration when initialize hive serde in InsertIntoHiveTable.scala
SPARK-17920 HiveWriterContainer passes null configuration to serde.initialize, causing NullPointerException in AvroSerde when using avro.schema.url

Support writing to Hive table which uses Avro schema url 'avro.schema.url'
For ex:
create external table avro_in (a string) stored as avro location '/avro-in/' tblproperties ('avro.schema.url'='/avro-schema/avro.avsc');

create external table avro_out (a string) stored as avro location '/avro-out/' tblproperties ('avro.schema.url'='/avro-schema/avro.avsc');

 insert overwrite table avro_out select * from avro_in;  // fails with java.lang.NullPointerException

 WARN AvroSerDe: Encountered exception determining schema. Returning signal schema to indicate problem
java.lang.NullPointerException
	at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:182)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:174)

## Changes proposed in this fix
Currently 'null' value is passed to serializer, which causes NPE during insert operation, instead pass Hadoop configuration object
## How was this patch tested?
Added new test case in VersionsSuite

Author: vinodkc <vinod.kc.in@gmail.com>

Closes #19779 from vinodkc/br_Fix_SPARK-17920.
","['sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveFileFormat.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/client/VersionsSuite.scala']","Null value is passed to the serializer while writing to a Hive table that uses 'avro.schema.url', causing a NullPointerException during the insert operation."
06e203b85682b63ee250b96520558fc79aae0a17,1574295570,"[SPARK-29911][SQL] Uncache cached tables when session closed

### What changes were proposed in this pull request?
The local temporary view is session-scoped. Its lifetime is the lifetime of the session that created it.  But now cache data is cross-session. Its lifetime is the lifetime of the Spark application. That's will cause the memory leak if cache a local temporary view in memory when the session closed.
In this PR, we uncache the cached data of local temporary view when session closed. This PR doesn't impact the cached data of global temp view and persisted view.

How to reproduce:
1. create a local temporary view v1
2. cache it in memory
3. close session without drop table v1.

The application will hold the memory forever. In a long running thrift server scenario. It's worse.
```shell
0: jdbc:hive2://localhost:10000> CACHE TABLE testCacheTable AS SELECT 1;
CACHE TABLE testCacheTable AS SELECT 1;
+---------+--+
| Result  |
+---------+--+
+---------+--+
No rows selected (1.498 seconds)
0: jdbc:hive2://localhost:10000> !close
!close
Closing: 0: jdbc:hive2://localhost:10000
0: jdbc:hive2://localhost:10000 (closed)> !connect 'jdbc:hive2://localhost:10000'
!connect 'jdbc:hive2://localhost:10000'
Connecting to jdbc:hive2://localhost:10000
Enter username for jdbc:hive2://localhost:10000:
lajin
Enter password for jdbc:hive2://localhost:10000:
***
Connected to: Spark SQL (version 3.0.0-SNAPSHOT)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
1: jdbc:hive2://localhost:10000> select * from testCacheTable;
select * from testCacheTable;
Error: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: testCacheTable; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [testCacheTable] (state=,code=0)
```
<img width=""1047"" alt=""Screen Shot 2019-11-15 at 2 03 49 PM"" src=""https://user-images.githubusercontent.com/1853780/68923527-7ca8c180-07b9-11ea-9cc7-74f276c46840.png"">

### Why are the changes needed?
Resolve memory leak for thrift server

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manual test in UI storage tab
And add an UT

Closes #26543 from LantaoJin/SPARK-29911.

Authored-by: LantaoJin <jinlantao@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala', 'sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerQueryTestSuite.scala']","Caching local temporary views in memory leads to permanent memory retention even when the session that created it is closed, causing a memory leak in long running thrift server scenarios."
189f56f3dcdad4d997248c01aa5490617f018bd0,1519134690,"[SPARK-23383][BUILD][MINOR] Make a distribution should exit with usage while detecting wrong options

## What changes were proposed in this pull request?
```shell
./dev/make-distribution.sh --name ne-1.0.0-SNAPSHOT xyz --tgz  -Phadoop-2.7
+++ dirname ./dev/make-distribution.sh
++ cd ./dev/..
++ pwd
+ SPARK_HOME=/Users/Kent/Documents/spark
+ DISTDIR=/Users/Kent/Documents/spark/dist
+ MAKE_TGZ=false
+ MAKE_PIP=false
+ MAKE_R=false
+ NAME=none
+ MVN=/Users/Kent/Documents/spark/build/mvn
+ ((  5  ))
+ case $1 in
+ NAME=ne-1.0.0-SNAPSHOT
+ shift
+ shift
+ ((  3  ))
+ case $1 in
+ break
+ '[' -z /Users/Kent/.jenv/candidates/java/current ']'
+ '[' -z /Users/Kent/.jenv/candidates/java/current ']'
++ command -v git
+ '[' /usr/local/bin/git ']'
++ git rev-parse --short HEAD
+ GITREV=98ea6a7
+ '[' '!' -z 98ea6a7 ']'
+ GITREVSTRING=' (git revision 98ea6a7)'
+ unset GITREV
++ command -v /Users/Kent/Documents/spark/build/mvn
+ '[' '!' /Users/Kent/Documents/spark/build/mvn ']'
++ /Users/Kent/Documents/spark/build/mvn help:evaluate -Dexpression=project.version xyz --tgz -Phadoop-2.7
++ grep -v INFO
++ tail -n 1
+ VERSION=' -X,--debug                             Produce execution debug output'
```
It is better to declare the mistakes and exit with usage than `break`

## How was this patch tested?

manually

cc srowen

Author: Kent Yao <yaooqinn@hotmail.com>

Closes #20571 from yaooqinn/SPARK-23383.
",['dev/make-distribution.sh'],`make-distributions.sh` script breaks when encountering unexpected arguments instead of exiting and declaring respective error messages.
2ce3ebb0e099ed5a4ddb998c40d35996494aa490,1641891681,"[SPARK-37841][SQL] BasicWriteTaskStatsTracker should not try get status for a skipped file

### What changes were proposed in this pull request?

In the comment https://github.com/apache/spark/pull/35117#issuecomment-1007171965 in #35117 and other details, we noticed that the BasicWriteTaskStatsTracker keeps complaining that some of the written files are missing. This is truely worrisome.

```scala
scala> spark.range(0, 3, 1, 4).write.option(""compression"", ""zstd"").orc(""./spark-warehouse/zstd_1"")
22/01/07 14:22:48 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.
```

This happens for ORC format and the root cause is because, in OrcOutputWriter, the `recordWriter` and the `file` are lazily generated once have a record to write. So this step is skipped when the records iterator is empty. In the `commit` part, we will use the `file` to update stats assuming it exists. This causes `FileNotFoundException` and results not stats update. Then, `numSubmittedFiles` and `numFiles` become inconsistent and issue a false warning like above.

In this PR, we make the hive.OrcOutputWriter and OrcFileFormat behave the same as other data sources. That said, it will always create a file even when there are no records at all.

### Why are the changes needed?

fix a regression
- the warning is wrong
- make builtin data sources behave same

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

added ut

locally

```log
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0-SNAPSHOT
      /_/

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.12)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.range(3).write.option(""compression"", ""zstd"").orc(""./spark-warehouse/zstdtest_23"")

scala> spark.range(3).write.option(""compression"", ""zstd"").orc(""./spark-warehouse/zstdtest_24"")

scala>
```

Closes #35132 from yaooqinn/SPARK-37841.

Authored-by: Kent Yao <yao@apache.org>
Signed-off-by: Kent Yao <yao@apache.org>
","['sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcSourceSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcQuerySuite.scala']",The BasicWriteTaskStatsTracker throws a false warning regarding missing files when writing in ORC format with an empty records iterator.
060a2b8c1ebe2f9950b124009d746ce836a75752,1675865879,"[SPARK-42131][SQL] Extract the function that construct the select statement for JDBC dialect

### What changes were proposed in this pull request?
Currently, JDBCRDD uses fixed format for SELECT statement.
```
val sqlText = options.prepareQuery +
      s""SELECT $columnList FROM ${options.tableOrQuery} $myTableSampleClause"" +
      s"" $myWhereClause $getGroupByClause $getOrderByClause $myLimitClause $myOffsetClause""
```

But some databases have different syntax. For example, MS SQL Server uses keyword TOP to describe LIMIT clause or Top N.
The LIMIT clause of MS SQL Server show below.
```
SELECT TOP(1) Model, Color, Price
      FROM dbo.Cars
      WHERE Color = 'blue'
```
The Top N of MS SQL Server show below.
```
SELECT TOP(1) Model, Color, Price
FROM dbo.Cars
WHERE Color = 'blue'
ORDER BY Price ASC
```

This PR lets JDBC dialect could define their own syntax.

### Why are the changes needed?
Extract the function that construct the select statement for JDBC dialect.

### Does this PR introduce _any_ user-facing change?
'No'.
New feature.

### How was this patch tested?
N/A

Closes #39667 from beliefer/SPARK-42131.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/DB2IntegrationSuite.scala', 'connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/MsSqlServerIntegrationSuite.scala', 'connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/MySQLIntegrationSuite.scala', 'connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/OracleIntegrationSuite.scala', 'connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcSQLQueryBuilder.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala']","JDBCRDD's fixed format for creating SELECT statements does not work efficiently with databases with different syntax, such as MS SQL Server, which uses the TOP keyword for LIMIT clause or Top N."
6e56cfeaca884b1ccfaa8524c70f12f118bc840c,1688064366,"[SPARK-44150][PYTHON][CONNECT] Explicit Arrow casting for mismatched return type in Arrow Python UDF

### What changes were proposed in this pull request?
Explicit Arrow casting for the mismatched return type of Arrow Python UDF.

### Why are the changes needed?
A more standardized and coherent type coercion.

Please refer to https://github.com/apache/spark/pull/41706 for a comprehensive comparison between type coercion rules of Arrow and Pickle(used by the default Python UDF) separately.

See more at [[Design] Type-coercion in Arrow Python UDFs](https://docs.google.com/document/d/e/2PACX-1vTEGElOZfhl9NfgbBw4CTrlm-8F_xQCAKNOXouz-7mg5vYobS7lCGUsGkDZxPY0wV5YkgoZmkYlxccU/pub).

### Does this PR introduce _any_ user-facing change?
Yes.

FROM
```py
>>> df = spark.createDataFrame(['1', '2'], schema='string')
df.select(pandas_udf(lambda x: x, 'int')('value')).show()
>>> df.select(pandas_udf(lambda x: x, 'int')('value')).show()
...
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
...
pyarrow.lib.ArrowInvalid: Could not convert '1' with type str: tried to convert to int32
```

TO
```py
>>> df = spark.createDataFrame(['1', '2'], schema='string')
>>> df.select(pandas_udf(lambda x: x, 'int')('value')).show()
+---------------+
|<lambda>(value)|
+---------------+
|              1|
|              2|
+---------------+
```
### How was this patch tested?
Unit tests.

Closes #41503 from xinrong-meng/type_coersion.

Authored-by: Xinrong Meng <xinrong@apache.org>
Signed-off-by: Xinrong Meng <xinrong@apache.org>
","['python/pyspark/sql/pandas/serializers.py', 'python/pyspark/sql/tests/test_arrow_python_udf.py', 'python/pyspark/worker.py']","Mismatched return types in Arrow Python UDFs lead to conversion errors, especially when trying to convert string types to integers."
538c81bb7afea3db56ad7de1bc11d32c10c0688e,1646915078,"[SPARK-38481][SQL] Substitute Java overflow exception from `TIMESTAMPADD` by Spark exception

### What changes were proposed in this pull request?
In the PR, I propose to throw `SparkArithmeticException` from the datetime function: `timestampadd()` and from its aliases `date_add()`/`dateadd()` with the error class `DATETIME_OVERFLOW` in the case when internal arithmetic or datetime overflow occurs.  The new error classes are added to `error-classes.json`.

### Why are the changes needed?
Porting the functions to new error framework should improve user experience with Spark SQL.

Before the changes:
```sql
spark-sql> select timestampadd(YEAR, 1000000, timestamp'2022-03-09 01:02:03');
java.lang.ArithmeticException: long overflow
	at java.lang.Math.multiplyExact(Math.java:892) ~[?:1.8.0_292]
```

After:
```sql
spark-sql> select timestampadd(YEAR, 1000000, timestamp'2022-03-09 01:02:03');
org.apache.spark.SparkArithmeticException: The 'timestampadd' function overflows the input '2022-03-08T22:02:03Z' timestamp by 1000000 YEAR.
```

### Does this PR introduce _any_ user-facing change?
Yes, but the datetime functions `timestampadd()` and its aliases `dateadd()`/`date_add()` haven't released yet.

### How was this patch tested?
By running the affected test suites:
```
$ build/sbt ""test:testOnly *SparkThrowableSuite""
```
and new test:
```
$ build/sbt ""test:testOnly *QueryExecutionErrorsSuite""
```

Closes #35787 from MaxGekk/timestamp_add_diff-overflow.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala']","`timestampadd()` function and its aliases (`date_add()`, `dateadd()`) display standard Java ArithmeticException on encountering datetime or arithmetic overflow instead of a more descriptive SparkArithmeticException."
b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6,1575618313,"[SPARK-30001][SQL] ResolveRelations should handle both V1 and V2 tables

### What changes were proposed in this pull request?

This PR makes `Analyzer.ResolveRelations` responsible for looking up both v1 and v2 tables from the session catalog and create an appropriate relation.

### Why are the changes needed?

Currently there are two issues:
1. As described in [SPARK-29966](https://issues.apache.org/jira/browse/SPARK-29966), the logic for resolving relation can load a table twice, which is a perf regression (e.g., Hive metastore can be accessed twice).
2. As described in [SPARK-30001](https://issues.apache.org/jira/browse/SPARK-30001), if a catalog name is specified for v1 tables, the query fails:
```
scala> sql(""create table t using csv as select 1 as i"")
res2: org.apache.spark.sql.DataFrame = []

scala> sql(""select * from t"").show
+---+
|  i|
+---+
|  1|
+---+

scala> sql(""select * from spark_catalog.t"").show
org.apache.spark.sql.AnalysisException: Table or view not found: spark_catalog.t; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [spark_catalog, t]
```

### Does this PR introduce any user-facing change?

Yes. Now the catalog name is resolved correctly:
```
scala> sql(""create table t using csv as select 1 as i"")
res0: org.apache.spark.sql.DataFrame = []

scala> sql(""select * from t"").show
+---+
|  i|
+---+
|  1|
+---+

scala> sql(""select * from spark_catalog.t"").show
+---+
|  i|
+---+
|  1|
+---+
```

### How was this patch tested?

Added new tests.

Closes #26684 from imback82/resolve_relation.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/LookupCatalog.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala']",The Analyzer.ResolveRelations for Spark SQL isn't properly looking up both v1 and v2 tables from the session catalog causing performance issues and failure when a catalog name is specified for v1 tables.
1df69f7e324aa799c05f6158e433371c5eeed8ce,1605737271,"[SPARK-31255][SQL] Add SupportsMetadataColumns to DSv2

### What changes were proposed in this pull request?

This adds support for metadata columns to DataSourceV2. If a source implements `SupportsMetadataColumns` it must also implement `SupportsPushDownRequiredColumns` to support projecting those columns.

The analyzer is updated to resolve metadata columns from `LogicalPlan.metadataOutput`, and this adds a rule that will add metadata columns to the output of `DataSourceV2Relation` if one is used.

### Why are the changes needed?

This is the solution discussed for exposing additional data in the Kafka source. It is also needed for a generic `MERGE INTO` plan.

### Does this PR introduce any user-facing change?

Yes. Users can project additional columns from sources that implement the new API. This also updates `DescribeTableExec` to show metadata columns.

### How was this patch tested?

Will include new unit tests.

Closes #28027 from rdblue/add-dsv2-metadata-columns.

Authored-by: Ryan Blue <blue@apache.org>
Signed-off-by: Burak Yavuz <brkyvz@gmail.com>
","['sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/MetadataColumn.java', 'sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsMetadataColumns.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryTable.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeTableExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala']","Metadata columns aren't supported in DataSourceV2, making it impossible to project additional data from sources that use the new API."
5e6173ebefcbf3d85ba10234b35fe043f8497d32,1599054549,"[SPARK-31670][SQL] Trim unnecessary Struct field alias in Aggregate/GroupingSets

### What changes were proposed in this pull request?
Struct field both in GROUP BY and Aggregate Expresison with CUBE/ROLLUP/GROUPING SET will failed when analysis.

```
test(""SPARK-31670"") {
  withTable(""t1"") {
      sql(
        """"""
          |CREATE TEMPORARY VIEW t(a, b, c) AS
          |SELECT * FROM VALUES
          |('A', 1, NAMED_STRUCT('row_id', 1, 'json_string', '{""i"": 1}')),
          |('A', 2, NAMED_STRUCT('row_id', 2, 'json_string', '{""i"": 1}')),
          |('A', 2, NAMED_STRUCT('row_id', 2, 'json_string', '{""i"": 2}')),
          |('B', 1, NAMED_STRUCT('row_id', 3, 'json_string', '{""i"": 1}')),
          |('C', 3, NAMED_STRUCT('row_id', 4, 'json_string', '{""i"": 1}'))
        """""".stripMargin)

      checkAnswer(
        sql(
          """"""
            |SELECT a, c.json_string, SUM(b)
            |FROM t
            |GROUP BY a, c.json_string
            |WITH CUBE
            |"""""".stripMargin),
        Row(""A"", ""{\""i\"": 1}"", 3) :: Row(""A"", ""{\""i\"": 2}"", 2) :: Row(""A"", null, 5) ::
          Row(""B"", ""{\""i\"": 1}"", 1) :: Row(""B"", null, 1) ::
          Row(""C"", ""{\""i\"": 1}"", 3) :: Row(""C"", null, 3) ::
          Row(null, ""{\""i\"": 1}"", 7) :: Row(null, ""{\""i\"": 2}"", 2) :: Row(null, null, 9) :: Nil)

  }
}
```
Error 
```
[info] - SPARK-31670 *** FAILED *** (2 seconds, 857 milliseconds)
[info]   Failed to analyze query: org.apache.spark.sql.AnalysisException: expression 't.`c`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;
[info]   Aggregate [a#247, json_string#248, spark_grouping_id#246L], [a#247, c#223.json_string AS json_string#241, sum(cast(b#222 as bigint)) AS sum(b)#243L]
[info]   +- Expand [List(a#221, b#222, c#223, a#244, json_string#245, 0), List(a#221, b#222, c#223, a#244, null, 1), List(a#221, b#222, c#223, null, json_string#245, 2), List(a#221, b#222, c#223, null, null, 3)], [a#221, b#222, c#223, a#247, json_string#248, spark_grouping_id#246L]
[info]      +- Project [a#221, b#222, c#223, a#221 AS a#244, c#223.json_string AS json_string#245]
[info]         +- SubqueryAlias t
[info]            +- Project [col1#218 AS a#221, col2#219 AS b#222, col3#220 AS c#223]
[info]               +- Project [col1#218, col2#219, col3#220]
[info]                  +- LocalRelation [col1#218, col2#219, col3#220]
[info]
```
For Struct type Field, when we resolve it, it will construct with Alias. When struct field in GROUP BY with CUBE/ROLLUP etc,  struct field in groupByExpression and aggregateExpression will be resolved with different exprId as below
```
'Aggregate [cube(a#221, c#223.json_string AS json_string#240)], [a#221, c#223.json_string AS json_string#241, sum(cast(b#222 as bigint)) AS sum(b)#243L]
+- SubqueryAlias t
   +- Project [col1#218 AS a#221, col2#219 AS b#222, col3#220 AS c#223]
      +- Project [col1#218, col2#219, col3#220]
         +- LocalRelation [col1#218, col2#219, col3#220]
```
This makes `ResolveGroupingAnalytics.constructAggregateExprs()` failed to replace aggreagteExpression use expand groupByExpression attribute since there exprId is not same. then error happened.

### Why are the changes needed?
Fix analyze bug

### Does this PR introduce _any_ user-facing change?
NO

### How was this patch tested?
Added UT

Closes #28490 from AngersZhuuuu/SPARK-31670.

Lead-authored-by: angerszhu <angers.zhu@gmail.com>
Co-authored-by: AngersZhuuuu <angers.zhu@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala']","When using Struct fields in both GROUP BY and Aggregate Expression with CUBE/ROLLUP/GROUPING SET, analysis fails due to differing exprId between groupByExpression and aggregateExpression."
11bbb130df7b083f42acf0207531efe3912d89eb,1603716116,"[SPARK-33204][UI] The 'Event Timeline' area cannot be opened when a spark application has some failed jobs

### What changes were proposed in this pull request?
The page returned by /jobs in Spark UI will  store the detail information of each job in javascript like this:
```javascript
{
  'className': 'executor added',
  'group': 'executors',
  'start': new Date(1602834008978),
  'content': '<div class=""executor-event-content""' +
    'data-toggle=""tooltip"" data-placement=""top""' +
    'data-title=""Executor 3<br>' +
    'Added at 2020/10/16 15:40:08""' +
    'data-html=""true"">Executor 3 added</div>'
}
```
if an application has a failed job, the failure reason corresponding to the job will be stored in the ` content`  field in the javascript . if the failure  reason contains the character: **'**,   the  javascript code will throw an exception to cause the `event timeline url` had no response ， The following is an example of error json:
```javascript
{
  'className': 'executor removed',
  'group': 'executors',
  'start': new Date(1602925908654),
  'content': '<div class=""executor-event-content""' +
    'data-toggle=""tooltip"" data-placement=""top""' +
    'data-title=""Executor 2<br>' +
    'Removed at 2020/10/17 17:11:48' +
    '<br>Reason: Container from a bad node: ...   20/10/17 16:00:42 WARN ShutdownHookManager: ShutdownHook **'$anon$2'** timeout...""' +
    'data-html=""true"">Executor 2 removed</div>'
}
```

So we need to considier this special case , if the returned job info contains the character:**'**, just remove it

### Why are the changes needed?

Ensure that the UI page can function normally

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

This pr only  fixes an exception in a special case, manual test result as blows:

![fixed](https://user-images.githubusercontent.com/52202080/96711638-74490580-13d0-11eb-93e0-b44d9ed5da5c.gif)

Closes #30119 from akiyamaneko/timeline_view_cannot_open.

Authored-by: neko <echohlne@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala', 'core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala']","Spark UI's 'Event Timeline' area fails to open if a Spark application has jobs that failed due to specific JavaScript exceptions, particularly if failure reasons contain single quotation mark (')."
26c1b959cf29b8552beb715cc5d39288d5298bdc,1539462871,"[SPARK-25711][CORE] Improve start-history-server.sh: show usage User-Friendly and remove deprecated options

## What changes were proposed in this pull request?

Currently, if we try run
```
./start-history-server.sh -h
```
We will get such error
```
java.io.FileNotFoundException: File -h does not exist
```

1. This is not User-Friendly.  For option `-h` or `--help`, it should be parsed correctly and show the usage of the class/script.
2. We can remove deprecated options for setting event log directory through command line options.

After fix, we can get following output:
```
Usage: ./sbin/start-history-server.sh [options]

Options:
  --properties-file FILE      Path to a custom Spark properties file.
                              Default is conf/spark-defaults.conf.

Configuration options can be set by setting the corresponding JVM system property.
History Server options are always available; additional options depend on the provider.

History Server options:

  spark.history.ui.port              Port where server will listen for connections
                                     (default 18080)
  spark.history.acls.enable          Whether to enable view acls for all applications
                                     (default false)
  spark.history.provider             Name of history provider class (defaults to
                                     file system-based provider)
  spark.history.retainedApplications Max number of application UIs to keep loaded in memory
                                     (default 50)
FsHistoryProvider options:

  spark.history.fs.logDirectory      Directory where app logs are stored
                                     (default: file:/tmp/spark-events)
  spark.history.fs.updateInterval    How often to reload log data from storage
                                     (in seconds, default: 10)

```

## How was this patch tested?

Manual test

Closes #22699 from gengliangwang/refactorSHSUsage.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala', 'core/src/test/scala/org/apache/spark/deploy/history/HistoryServerArgumentsSuite.scala', 'sbin/start-history-server.sh']","Running the `./start-history-server.sh -h` command returns an error `java.io.FileNotFoundException: File -h does not exist` instead of showing the usage of the script. Moreover, deprecated event log directory setting options are still in use."
9e6f2dd72686a9ac44fd4573b5a408f8a8e55fe1,1654233802,"[SPARK-39320][SQL] Support aggregate function `MEDIAN`

### What changes were proposed in this pull request?
Many mainstream database supports aggregate function `MEDIAN`.

**Syntax：**
Aggregate function
`MEDIAN( <expr> )`

Window function
`MEDIAN( <expr> ) OVER ( [ PARTITION BY <expr2> ] )`
**Arguments：**
expr: The expression must evaluate to a numeric data type (INTEGER, FLOAT, DECIMAL, or equivalent).

**Examples**：
```
select k, median(v) from aggr group by k order by k;
+---+-----------+
| K | MEDIAN(V) |
|---+-----------|
| 1 |  20.00000 |
| 2 |  22.50000 |
| 3 |      NULL |
+---+-----------+
```

### Why are the changes needed?
The mainstream database supports `MEDIAN` show below:
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/median.html

**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/MEDIAN.html#GUID-DE15705A-AC18-4416-8487-B9E1D70CE01A

**ClickHouse**
https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/median

**Redshift**
https://docs.aws.amazon.com/redshift/latest/dg/r_MEDIAN.html

**Teradata**
https://docs.teradata.com/r/Teradata-VantageTM-SQL-Functions-Expressions-and-Predicates/March-2019/Ordered-Analytical/Window-Aggregate-Functions/MEDIAN

**DB2**
https://www.ibm.com/docs/en/db2/11.5?topic=functions-median

**Vertica**
https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Analytic/MEDIANAnalytic.htm?tocpath=SQL%20Reference%20Manual%7CSQL%20Functions%7CAnalytic%20Functions%7C_____20

**H2**
http://www.h2database.com/html/functions-aggregate.html#median

**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc01776.1601/doc/html/san1278453109663.html

**Exasol**
https://docs.exasol.com/db/latest/sql_references/functions/alphabeticallistfunctions/median.htm

**Yellowbrick**
https://www.yellowbrick.com/docs/5.2/ybd_sqlref/median.html

**Mariadb**
https://mariadb.com/kb/en/median/

**Singlestore**
https://docs.singlestore.com/db/v7.6/en/reference/sql-reference/aggregate-functions/median.html

**InfluxDB**
https://docs.influxdata.com/flux/v0.x/stdlib/universe/median/

### Does this PR introduce _any_ user-facing change?
'No'.
New feature.

### How was this patch tested?
New tests.

Closes #36714 from beliefer/SPARK-39320.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala', 'sql/core/src/test/resources/sql-tests/inputs/group-by.sql', 'sql/core/src/test/resources/sql-tests/inputs/percentiles.sql', 'sql/core/src/test/resources/sql-tests/inputs/window.sql']","Spark SQL does not support the aggregate function `MEDIAN`, despite it being widely supported by many mainstream databases."
0963fcd848f62b4f2231dfcf67f9beabf927c21e,1601559427,"[SPARK-33024][SQL] Fix CodeGen fallback issue of UDFSuite in Scala 2.13

### What changes were proposed in this pull request?
After `SPARK-32851` set `CODEGEN_FACTORY_MODE` to `CODEGEN_ONLY` of `sparkConf` in `SharedSparkSessionBase`  to construction `SparkSession`  in test, the test suite `SPARK-32459: UDF should not fail on WrappedArray` in s.sql.UDFSuite exposed a codegen fallback issue in Scala 2.13 as follow:

```
- SPARK-32459: UDF should not fail on WrappedArray *** FAILED ***
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 99: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 99: No applicable constructor/method found for zero actual parameters; candidates are: ""public scala.collection.mutable.Builder scala.collection.mutable.ArraySeq$.newBuilder(java.lang.Object)"", ""public scala.collection.mutable.Builder scala.collection.mutable.ArraySeq$.newBuilder(scala.reflect.ClassTag)"", ""public abstract scala.collection.mutable.Builder scala.collection.EvidenceIterableFactory.newBuilder(java.lang.Object)""
```

The root cause is `WrappedArray` represent `mutable.ArraySeq`  in Scala 2.13 and has a different constructor of `newBuilder` method.

The main change of is pr is add Scala 2.13 only code part to deal with  `case match WrappedArray` in Scala 2.13.

### Why are the changes needed?
We need to support a Scala 2.13 build

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
- Scala 2.12: Pass the Jenkins or GitHub Action

- Scala 2.13: All tests passed.

Do the following:

```
dev/change-scala-version.sh 2.13
mvn clean install -DskipTests  -pl sql/core -Pscala-2.13 -am
mvn test -pl sql/core -Pscala-2.13
```

**Before**
```
Tests: succeeded 8540, failed 1, canceled 1, ignored 52, pending 0
*** 1 TEST FAILED ***

```

**After**

```
Tests: succeeded 8541, failed 0, canceled 1, ignored 52, pending 0
All tests passed.
```

Closes #29903 from LuciferYang/fix-udfsuite.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala'],'UDFSuite test fails on WrappedArray due to codegen fallback issue specifically in Scala 2.13 version.'
1bd897cbc4fe30eb8b7740c7232aae87081e8e33,1606166180,"[SPARK-32918][SHUFFLE] RPC implementation to support control plane coordination for push-based shuffle

### What changes were proposed in this pull request?
This is one of the patches for SPIP SPARK-30602 which is needed for push-based shuffle.
Summary of changes:
This PR introduces a new RPC to be called within Driver. When the expected shuffle push wait time reaches, Driver will call this RPC to facilitate coordination of shuffle map/reduce stages and notify external shuffle services to finalize shuffle block merge for a given shuffle. Shuffle services also respond back the metadata about a merged shuffle partition back to the caller.

### Why are the changes needed?
Refer to the SPIP in SPARK-30602.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
This code snippets won't be called by any existing code and will be tested after the coordinated driver changes gets merged in SPARK-32920.

Lead-authored-by: Min Shen mshenlinkedin.com

Closes #30163 from zhouyejoe/SPARK-32918.

Lead-authored-by: Ye Zhou <yezhou@linkedin.com>
Co-authored-by: Min Shen <mshen@linkedin.com>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/BlockStoreClient.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockStoreClient.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/MergeFinalizerListener.java']","The existing shuffle mechanism lacks a control plane for coordination between shuffle map/reduce stages, causing inefficiency in finalizing shuffle block merge for a given shuffle."
6958d7e62959313fb6a1288527aaa611aebee087,1572391534,"[SPARK-28746][SQL] Add partitionby hint  for sql queries

## What changes were proposed in this pull request?

Now, `RepartitionByExpression` is allowed at Dataset method `Dataset.repartition()`. But in spark sql,  we do not have an equivalent functionality.
In hive, we can use `distribute by`, so it's worth to add a hint to support such function.
Similar jira [SPARK-24940](https://issues.apache.org/jira/browse/SPARK-24940)

## Why are the changes needed?

Make repartition hints consistent with repartition api .

## Does this PR introduce any user-facing change?
This pr intends to support quries below;
```
// SQL cases
 - sql(""SELECT /*+ REPARTITION(c) */ * FROM t"")
 - sql(""SELECT /*+ REPARTITION(1, c) */ * FROM t"")
 - sql(""SELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t"")
 - sql(""SELECT /*+ REPARTITION_BY_RANGE(1, c) */ * FROM t"")
```

## How was this patch tested?
UT

Closes #25464 from ulysses-you/SPARK-28746.

Lead-authored-by: ulysses <youxiduo@weidian.com>
Co-authored-by: ulysses <646303253@qq.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveHints.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveHintsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameHintSuite.scala']","Lack of a repartition hint in spark SQL, making the functionality inconsistent with the Dataset API's repartition method."
20cd47e82d7d84516455df960ededbd647694aa5,1593436422,"[SPARK-32030][SQL] Support unlimited MATCHED and NOT MATCHED clauses in MERGE INTO

### What changes were proposed in this pull request?
This PR add unlimited MATCHED and NOT MATCHED clauses in MERGE INTO statement.

### Why are the changes needed?
Now the MERGE INTO syntax is,
```
MERGE INTO [db_name.]target_table [AS target_alias]
 USING [db_name.]source_table [<time_travel_version>] [AS source_alias]
 ON <merge_condition>
 [ WHEN MATCHED [ AND <condition> ] THEN <matched_action> ]
 [ WHEN MATCHED [ AND <condition> ] THEN <matched_action> ]
 [ WHEN NOT MATCHED [ AND <condition> ] THEN <not_matched_action> ]
```
It would be nice if we support unlimited MATCHED and NOT MATCHED clauses in MERGE INTO statement, because users may want to deal with different ""AND <condition>""s, the result of which just like a series of ""CASE WHEN""s. The expected syntax looks like
```
MERGE INTO [db_name.]target_table [AS target_alias]
 USING [db_name.]source_table [<time_travel_version>] [AS source_alias]
 ON <merge_condition>
 [when_matched_clause [, ...]]
 [when_not_matched_clause [, ...]]
```
where when_matched_clause is
```
WHEN MATCHED [ AND <condition> ] THEN <matched_action>
```
and when_not_matched_clause is
```
WHEN NOT MATCHED [ AND <condition> ] THEN <not_matched_action>
 ```
matched_action can be one of
```
DELETE
UPDATE SET * or
UPDATE SET col1 = value1 [, col2 = value2, ...]
```
and not_matched_action can be one of
```
INSERT *
INSERT (col1 [, col2, ...]) VALUES (value1 [, value2, ...])
```
### Does this PR introduce _any_ user-facing change?
Yes. The SQL command changes, but it is backward compatible.

### How was this patch tested?
New tests added.

Closes #28875 from xianyinxin/SPARK-32030.

Authored-by: xy_xin <xianyin.xxy@alibaba-inc.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala']",The existing syntax for MERGE INTO statement supports only two MATCHED and NOT MATCHED clauses. This limits users to deal with different conditions in an expressive sequential manner.
32a87f03da7eef41161a5a7a3aba4a48e0421912,1693830219,"[SPARK-44846][SQL] Convert the lower redundant Aggregate to Project in RemoveRedundantAggregates

### What changes were proposed in this pull request?
This PR provides a safe way to remove a redundant `Aggregate` in rule `RemoveRedundantAggregates`. Just convert the lower redundant `Aggregate` to `Project`.

### Why are the changes needed?
The aggregate contains complex grouping expressions after `RemoveRedundantAggregates`, if `aggregateExpressions` has (if / case) branches, it is possible that `groupingExpressions` is no longer a subexpression of `aggregateExpressions` after execute `PushFoldableIntoBranches` rule, Then cause `boundReference` error.
For example
```
SELECT c * 2 AS d
FROM (
         SELECT if(b > 1, 1, b) AS c
         FROM (
                  SELECT if(a < 0, 0, a) AS b
                  FROM VALUES (-1), (1), (2) AS t1(a)
              ) t2
         GROUP BY b
     ) t3
GROUP BY c
```
Before pr
```
== Optimized Logical Plan ==
Aggregate [if ((b#0 > 1)) 1 else b#0], [if ((b#0 > 1)) 2 else (b#0 * 2) AS d#2]
+- Project [if ((a#3 < 0)) 0 else a#3 AS b#0]
   +- LocalRelation [a#3]
```
```
== Error ==
Couldn't find b#0 in [if ((b#0 > 1)) 1 else b#0#7]
java.lang.IllegalStateException: Couldn't find b#0 in [if ((b#0 > 1)) 1 else b#0#7]
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1241)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1240)
	at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:653)
        ......
```
After pr
```
== Optimized Logical Plan ==
Aggregate [c#1], [(c#1 * 2) AS d#2]
+- Project [if ((b#0 > 1)) 1 else b#0 AS c#1]
   +- Project [if ((a#3 < 0)) 0 else a#3 AS b#0]
      +- LocalRelation [a#3]
```
### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
UT

Closes #42633 from zml1206/SPARK-44846-2.

Authored-by: zml1206 <zhuml1206@gmail.com>
Signed-off-by: Yuming Wang <yumwang@ebay.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregates.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregatesSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/group-by.sql']",Redundant `Aggregate` in `RemoveRedundantAggregates` rule can lead to `boundReference` errors when `groupingExpressions` is no longer a subexpression of `aggregateExpressions` in cases like complex if/case branches within aggregate expressions.
2011ab5cfb9a878714763296149c6b6f3b46c584,1636952094,"[SPARK-32567][SQL][FOLLOWUP] Fix CompileException when code generate for FULL OUTER shuffled hash join

### What changes were proposed in this pull request?
Add `throws IOException` to `consumeFullOuterJoinRow` in ShuffledHashJoinExec

### Why are the changes needed?
pr #34444 add code-gen for full outer shuffled hash join. If we don't have this patch,   when the dataframes are `fullter outer` shuffled hash joined, and then aggregate the results.  the dataframes will throw a `CompileException`.
For example:
```scala
    val df1 = spark.range(5).select($""id"".as(""k1""))
    val df2 = spark.range(10).select($""id"".as(""k2""))
    df1.join(df2.hint(""SHUFFLE_HASH""), $""k1"" === $""k2"", ""full_outer"").count()
```
The dataframe will throw an Exception which is as follows:
```
23:28:19.079 ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 175, Column 1: Thrown exception of type ""java.io.IOException"" is neither caught by a ""try...catch"" block nor declared in the ""throws"" clause of the declaring function
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 175, Column 1: Thrown exception of type ""java.io.IOException"" is neither caught by a ""try...catch"" block nor declared in the ""throws"" clause of the declaring function
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12021)
	at org.codehaus.janino.UnitCompiler.checkThrownException(UnitCompiler.java:9801)
	at org.codehaus.janino.UnitCompiler.checkThrownExceptions(UnitCompiler.java:9720)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9163)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5055)
```
### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Add a new UT

Closes #34589 from jerqi/SPARK-32567.

Authored-by: RoryQi <1242949407@qq.com>
Signed-off-by: Liang-Chi Hsieh <viirya@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala']",'Code-generation for full outer shuffled hash join is throwing a CompileException due to an unhandled IOException in consumeFullOuterJoinRow of ShuffledHashJoinExec.'
e402de5fd030cdc4150fda0755c7c636cad9619e,1552998587,"[SPARK-26176][SQL] Verify column names for CTAS with `STORED AS`

 ## What changes were proposed in this pull request?
Currently, users meet job abortions while creating a table using the Hive serde ""STORED AS"" with invalid column names. We had better prevent this by raising **AnalysisException** with a guide to use aliases instead like Paquet data source tables.
thus making compatible with error message shown while creating Parquet/ORC native table.

**BEFORE**
```scala
scala> sql(""set spark.sql.hive.convertMetastoreParquet=false"")
scala> sql(""CREATE TABLE a STORED AS PARQUET AS SELECT 1 AS `COUNT(ID)`"")
Caused by: java.lang.IllegalArgumentException: No enum constant parquet.schema.OriginalType.col1
```

**AFTER**
```scala
scala> sql(""CREATE TABLE a STORED AS PARQUET AS SELECT 1 AS `COUNT(ID)`"")
 Please use alias to rename it.;eption: Attribute name ""count(ID)"" contains invalid character(s) among "" ,;{}()\n\t="".
```

## How was this patch tested?
Pass the Jenkins with the newly added test case.

Closes #24075 from sujith71955/master_serde.

Authored-by: s71955 <sujithchacko.2010@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala']","Creating a table using Hive serde 'STORED AS' with invalid column names leads to job abortion, instead of throwing an AnalysisException."
3c3ad5f7c00f6f68bc659d4cf7020fa944b7bc69,1603867223,"[SPARK-32934][SQL] Improve the performance for NTH_VALUE and reactor the OffsetWindowFunction

### What changes were proposed in this pull request?
Spark SQL supports some window function like `NTH_VALUE`.
If we specify window frame like `UNBOUNDED PRECEDING AND CURRENT ROW` or `UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`, we can elimate some calculations.
For example: if we execute the SQL show below:
```
SELECT NTH_VALUE(col,
         2) OVER(ORDER BY rank UNBOUNDED PRECEDING
        AND CURRENT ROW)
FROM tab;
```
The output for row number greater than 1, return the fixed value. otherwise, return null. So we just calculate the value once and notice whether the row number less than 2.
`UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING` is simpler.

### Why are the changes needed?
Improve the performance for `NTH_VALUE`, `FIRST_VALUE` and `LAST_VALUE`.

### Does this PR introduce _any_ user-facing change?
 'No'.

### How was this patch tested?
Jenkins test.

Closes #29800 from beliefer/optimize-nth_value.

Lead-authored-by: gengjiaan <gengjiaan@360.cn>
Co-authored-by: beliefer <beliefer@163.com>
Co-authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/windowExpressions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala', 'sql/core/src/test/resources/sql-tests/inputs/window.sql', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala']","Performance issues with window function like `NTH_VALUE`, `FIRST_VALUE`, and `LAST_VALUE` when using window frames like `UNBOUNDED PRECEDING AND CURRENT ROW` or `UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`."
f7cc6958084111cc6180c572519082ec235eb189,1569050963,"[SPARK-29140][SQL] Handle parameters having ""array"" of javaType properly in splitAggregateExpressions

### What changes were proposed in this pull request?

This patch fixes the issue brought by [SPARK-21870](http://issues.apache.org/jira/browse/SPARK-21870): when generating code for parameter type, it doesn't consider array type in javaType. At least we have one, Spark should generate code for BinaryType as `byte[]`, but Spark create the code for BinaryType as `[B` and generated code fails compilation.

Below is the generated code which failed compilation (Line 380):

```
/* 380 */   private void agg_doAggregate_count_0([B agg_expr_1_1, boolean agg_exprIsNull_1_1, org.apache.spark.sql.catalyst.InternalRow agg_unsafeRowAggBuffer_1) throws java.io.IOException {
/* 381 */     // evaluate aggregate function for count
/* 382 */     boolean agg_isNull_26 = false;
/* 383 */     long agg_value_28 = -1L;
/* 384 */     if (!false && agg_exprIsNull_1_1) {
/* 385 */       long agg_value_31 = agg_unsafeRowAggBuffer_1.getLong(1);
/* 386 */       agg_isNull_26 = false;
/* 387 */       agg_value_28 = agg_value_31;
/* 388 */     } else {
/* 389 */       long agg_value_33 = agg_unsafeRowAggBuffer_1.getLong(1);
/* 390 */
/* 391 */       long agg_value_32 = -1L;
/* 392 */
/* 393 */       agg_value_32 = agg_value_33 + 1L;
/* 394 */       agg_isNull_26 = false;
/* 395 */       agg_value_28 = agg_value_32;
/* 396 */     }
/* 397 */     // update unsafe row buffer
/* 398 */     agg_unsafeRowAggBuffer_1.setLong(1, agg_value_28);
/* 399 */   }
```

There wasn't any test for HashAggregateExec specifically testing this, but randomized test in ObjectHashAggregateSuite could encounter this and that's why ObjectHashAggregateSuite is flaky.

### Why are the changes needed?

Without the fix, generated code from HashAggregateExec may fail compilation.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Added new UT. Without the fix, newly added UT fails.

Closes #25830 from HeartSaVioR/SPARK-29140.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala']","Code generation for parameter types doesn't properly account for array types in javaType, leading to compilation failures in cases like BinaryType being generated as `[B` instead of `byte[]`."
ee1de66fe4e05754ea3f33b75b83c54772b00112,1588295296,"[SPARK-31549][PYSPARK] Add a develop API invoking collect on Python RDD with user-specified job group

### What changes were proposed in this pull request?
I add a new API in pyspark RDD class:

def collectWithJobGroup(self, groupId, description, interruptOnCancel=False)

This API do the same thing with `rdd.collect`, but it can specify the job group when do collect.
The purpose of adding this API is, if we use:

```
sc.setJobGroup(""group-id..."")
rdd.collect()
```
The `setJobGroup` API in pyspark won't work correctly. This related to a bug discussed in
https://issues.apache.org/jira/browse/SPARK-31549

Note:

This PR is a rather temporary workaround for `PYSPARK_PIN_THREAD`, and as a step to migrate to  `PYSPARK_PIN_THREAD` smoothly. It targets Spark 3.0.

- `PYSPARK_PIN_THREAD` is unstable at this moment that affects whole PySpark applications.
- It is impossible to make it runtime configuration as it has to be set before JVM is launched.
- There is a thread leak issue between Python and JVM. We should address but it's not a release blocker for Spark 3.0 since the feature is experimental. I plan to handle this after Spark 3.0 due to stability.

Once `PYSPARK_PIN_THREAD` is enabled by default, we should remove this API out ideally. I will target to deprecate this API in Spark 3.1.

### Why are the changes needed?
Fix bug.

### Does this PR introduce any user-facing change?
A develop API in pyspark: `pyspark.RDD. collectWithJobGroup`

### How was this patch tested?
Unit test.

Closes #28395 from WeichenXu123/collect_with_job_group.

Authored-by: Weichen Xu <weichen.xu@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala', 'python/pyspark/rdd.py', 'python/pyspark/tests/test_rdd.py']","The `setJobGroup` API in PySpark does not function correctly during usage with `rdd.collect()`, necessitating an alternative approach for specifying job groups."
22baf05a9ec6fffe53bd34d35c122de776464dd0,1605162991,"[SPARK-33408][SPARK-32354][K8S][R] Use R 3.6.3 in K8s R image and re-enable RTestsSuite

### What changes were proposed in this pull request?

This PR aims to use R 3.6.3 in K8s R image and re-enable `RTestsSuite`.

### Why are the changes needed?

Jenkins Server is using `R 3.6.3`.
```
+ SPARK_HOME=/home/jenkins/workspace/SparkPullRequestBuilder-K8s
+ /usr/bin/R CMD check --as-cran --no-tests SparkR_3.1.0.tar.gz
* using log directory ‘/home/jenkins/workspace/SparkPullRequestBuilder-K8s/R/SparkR.Rcheck’
* using R version 3.6.3 (2020-02-29)
```

OpenJDK docker image is using `R 3.5.2 (2018-12-20)` which is old and currently `spark-3.0.1` fails to run SparkR.
```
$ cd spark-3.0.1-bin-hadoop3.2

$ bin/docker-image-tool.sh -R kubernetes/dockerfiles/spark/bindings/R/Dockerfile -n build
...
	 exit code: 1
	 termination reason: Error
...

$ bin/spark-submit --master k8s://https://192.168.64.49:8443 --deploy-mode cluster --conf spark.kubernetes.container.image=spark-r:latest local:///opt/spark/examples/src/main/r/dataframe.R

$ k logs dataframe-r-b1c14b75b0c09eeb-driver
...
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=172.17.0.4 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.RRunner local:///opt/spark/examples/src/main/r/dataframe.R
20/11/10 06:03:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Error: package or namespace load failed for ‘SparkR’ in rbind(info, getNamespaceInfo(env, ""S3methods"")):
 number of columns of matrices must match (see arg 2)
In addition: Warning message:
package ‘SparkR’ was built under R version 4.0.2
Execution halted
```

In addition, this PR aims to recover the test coverage.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass K8S IT Jenkins job.

Closes #30130 from dongjoon-hyun/SPARK-32354.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala'],"The older version of R (3.5.2) in the K8s R image is not compatible with SparkR, causing package load failures, and preventing `RTestsSuite` from being executed successfully."
555aece74d2a22d312e815ec07f5553800e14b9d,1695353483,"[SPARK-45093][CONNECT][PYTHON] Error reporting for addArtifacts query

### What changes were proposed in this pull request?

Add error logging into `addArtifact`  (see example in ""How this is tested""). The logging code is moved into separate file to avoid circular dependency.

### Why are the changes needed?

Currently, in case `addArtifact` is executed with the file which doesn't exist, the user gets cryptic error

```grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception iterating requests!""
        debug_error_string = ""None""
>
```

Which is hard to debug without deep digging into the subject.

This happens because addArtifact is implemented as client-side streaming and the actual error happens during grpc consuming iterator generating requests. Unfortunately grpc doesn't print any debug information for user to understand the problem.

### Does this PR introduce _any_ user-facing change?

Additional logging which is opt-in same way as before with `SPARK_CONNECT_LOG_LEVEL` environment variable.

### How was this patch tested?

```
>>> s.addArtifact(""XYZ"", file=True)
[New:] 2023-09-15 17:06:40,078 11789 ERROR _create_requests Failed to execute addArtifact: [Errno 2] No such file or directory: '/Users/alice.sayutina/apache_spark/python/XYZ'
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/alice.sayutina/apache_spark/python/pyspark/sql/connect/session.py"", line 743, in addArtifacts
    self._client.add_artifacts(*path, pyfile=pyfile, archive=archive, file=file)

[....]

  File ""/Users/alice.sayutina/oss-venv/lib/python3.11/site-packages/grpc/_channel.py"", line 910, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception iterating requests!""
        debug_error_string = ""None""
>

```

Closes #42949 from cdkrot/SPARK-45093.

Lead-authored-by: Alice Sayutina <alice.sayutina@databricks.com>
Co-authored-by: Alice Sayutina <cdkrot0@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/sql/connect/client/__init__.py', 'python/pyspark/sql/connect/client/artifact.py', 'python/pyspark/sql/connect/client/core.py', 'python/pyspark/sql/connect/client/logging.py']",'addArtifact' executed with non-existing file results in a cryptic error message and difficult debugging due to lack of detailed exception information.
d0470d639412ecbe6e126f8d8abf5a5819b9e278,1573925161,"[MINOR][TESTS] Ignore GitHub Action and AppVeyor file changes in testing

### What changes were proposed in this pull request?

This PR aims to ignore `GitHub Action` and `AppVeyor` file changes. When we touch these files, Jenkins job should not trigger a full testing.

### Why are the changes needed?

Currently, these files are categorized to `root` and trigger the full testing and ends up wasting the Jenkins resources.
- https://github.com/apache/spark/pull/26555
```
[info] Using build tool sbt with Hadoop profile hadoop2.7 under environment amplab_jenkins
From https://github.com/apache/spark
 * [new branch]      master     -> master
[info] Found the following changed modules: sparkr, root
[info] Setup the following environment variables for tests:
```

### Does this PR introduce any user-facing change?

No. (Jenkins testing only).

### How was this patch tested?

Manually.
```
$ dev/run-tests.py -h -v
...
Trying:
    [x.name for x in determine_modules_for_files(["".github/workflows/master.yml"", ""appveyor.xml""])]
Expecting:
    []
...
```

Closes #26556 from dongjoon-hyun/SPARK-IGNORE-APPVEYOR.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['dev/run-tests.py'],"Modifications to GitHub Action and AppVeyor files are triggering unnecessary full testing in Jenkins, leading to wasted resources."
11260310f65e1a30f6b00b380350e414609c5fd4,1675823978,"[SPARK-42371][CONNECT] Add scripts to start and stop Spark Connect server

### What changes were proposed in this pull request?

This PR proposes to scripts to start and stop the Spark Connect server.

### Why are the changes needed?

Currently, there is no proper way to start and stop the Spark Connect server. Now it requires you to start it with, for example, a Spark shell:

```bash
# For development,
./bin/spark-shell \
   --jars `ls connector/connect/server/target/**/spark-connect*SNAPSHOT.jar` \
  --conf spark.plugins=org.apache.spark.sql.connect.SparkConnectPlugin
```

```bash
# For released Spark versions
./bin/spark-shell \
  --packages org.apache.spark:spark-connect_2.12:3.4.0 \
  --conf spark.plugins=org.apache.spark.sql.connect.SparkConnectPlugin
```

which is awkward.

### Does this PR introduce _any_ user-facing change?

Yes, it adds new scripts to start and stop Spark Connect server.

### How was this patch tested?

Manually tested:

```bash
# For released Spark versions,
#`sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.4.0`
sbin/start-connect-server.sh --jars `ls connector/connect/server/target/**/spark-connect*SNAPSHOT.jar`
```

```bash
bin/pyspark --remote sc://localhost:15002
...
```

```bash
sbin/stop-connect-server.sh
ps -fe | grep Spark
```

Closes #39928 from HyukjinKwon/exec-script.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectServer.scala', 'connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectService.scala', 'core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala', 'launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java', 'sbin/start-connect-server.sh', 'sbin/stop-connect-server.sh']","There are no straightforward scripts to start or stop the Spark Connect server, complicating the current process and making it awkward to use."
59365703c51a36efca5390b7535489a839f51659,1663550982,"[SPARK-40424][CORE][TESTS] Refactor `ChromeUIHistoryServerSuite` to add UTs for RocksDB

### What changes were proposed in this pull request?
`ChromeUIHistoryServerSuite` only test LevelDB backend now, this pr refactor the UTs of `ChromeUIHistoryServerSuite` to add UTs for RocksDB

### Why are the changes needed?
Add UTs related to RocksDB.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?

- Pass GA
- Manual test on Apple Silicon environment：

```
build/sbt -Dguava.version=31.1-jre -Dspark.test.webdriver.chrome.driver=/path/to/chromedriver -Dtest.default.exclude.tags="""" ""core/testOnly org.apache.spark.deploy.history.RocksBackendChromeUIHistoryServerSuite""
```

```
[info] RocksBackendChromeUIHistoryServerSuite:
Starting ChromeDriver 105.0.5195.52 (412c95e518836d8a7d97250d62b29c2ae6a26a85-refs/branch-heads/5195{#853}) on port 54402
Only local connections are allowed.
Please see https://chromedriver.chromium.org/security-considerations for suggestions on keeping ChromeDriver safe.
ChromeDriver was started successfully.
[info] - ajax rendered relative links are prefixed with uiRoot (spark.ui.proxyBase) (5 seconds, 387 milliseconds)
[info] Run completed in 20 seconds, 838 milliseconds.
[info] Total number of tests run: 1
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[success] Total time: 118 s (01:58), completed 2022-9-15 10:30:53
```

Closes #37878 from LuciferYang/SPARK-40424.

Lead-authored-by: yangjie01 <yangjie01@baidu.com>
Co-authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/test/scala/org/apache/spark/deploy/history/ChromeUIHistoryServerSuite.scala', 'core/src/test/scala/org/apache/spark/deploy/history/RealBrowserUIHistoryServerSuite.scala']","'ChromeUIHistoryServerSuite' only tests LevelDB backend. There are no UTs related to RocksDB, limiting the suite's capability to handle different database backends."
423ba5a16038c1cb28d0973e18518645e69d5ff1,1605305783,"[SPARK-32916][SHUFFLE][TEST-MAVEN][TEST-HADOOP2.7] Remove the newly added YarnShuffleServiceSuite.java

### What changes were proposed in this pull request?
This is a follow-up fix for the failing tests in `YarnShuffleServiceSuite.java`. This java class was introduced in https://github.com/apache/spark/pull/30062. The tests in the class fail when run with hadoop-2.7 profile:
```
[ERROR] testCreateDefaultMergedShuffleFileManagerInstance(org.apache.spark.network.yarn.YarnShuffleServiceSuite)  Time elapsed: 0.627 s  <<< ERROR!
java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory
	at org.apache.spark.network.yarn.YarnShuffleServiceSuite.testCreateDefaultMergedShuffleFileManagerInstance(YarnShuffleServiceSuite.java:37)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.logging.LogFactory
	at org.apache.spark.network.yarn.YarnShuffleServiceSuite.testCreateDefaultMergedShuffleFileManagerInstance(YarnShuffleServiceSuite.java:37)

[ERROR] testCreateRemoteBlockPushResolverInstance(org.apache.spark.network.yarn.YarnShuffleServiceSuite)  Time elapsed: 0 s  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.network.yarn.YarnShuffleService
	at org.apache.spark.network.yarn.YarnShuffleServiceSuite.testCreateRemoteBlockPushResolverInstance(YarnShuffleServiceSuite.java:47)

[ERROR] testInvalidClassNameOfMergeManagerWillUseNoOpInstance(org.apache.spark.network.yarn.YarnShuffleServiceSuite)  Time elapsed: 0.001 s  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.network.yarn.YarnShuffleService
	at org.apache.spark.network.yarn.YarnShuffleServiceSuite.testInvalidClassNameOfMergeManagerWillUseNoOpInstance(YarnShuffleServiceSuite.java:57)
```
A test suit for `YarnShuffleService` did exist here:
`resource-managers/yarn/src/test/scala/org/apache/spark/network/yarn/YarnShuffleServiceSuite.scala`
I missed this when I created `common/network-yarn/src/test/java/org/apache/spark/network/yarn/YarnShuffleServiceSuite.java`. Moving all the new tests to the earlier test suite fixes the failures with hadoop-2.7 even though why this happened is not clear.

### Why are the changes needed?
The newly added tests are failing when run with hadoop profile 2.7

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Ran the unit tests with the default profile as well as hadoop 2.7 profile.
`build/mvn test -Dtest=none -DwildcardSuites=org.apache.spark.network.yarn.YarnShuffleServiceSuite -Phadoop-2.7 -Pyarn`
```
Run starting. Expected test count is: 11
YarnShuffleServiceSuite:
- executor state kept across NM restart
- removed applications should not be in registered executor file
- shuffle service should be robust to corrupt registered executor file
- get correct recovery path
- moving recovery file from NM local dir to recovery path
- service throws error if cannot start
- recovery db should not be created if NM recovery is not enabled
- SPARK-31646: metrics should be registered into Node Manager's metrics system
- create default merged shuffle file manager instance
- create remote block push resolver instance
- invalid class name of merge manager will use noop instance
Run completed in 2 seconds, 572 milliseconds.
Total number of tests run: 11
Suites: completed 2, aborted 0
Tests: succeeded 11, failed 0, canceled 0, ignored 0, pending 0
All tests passed.
```

Closes #30349 from otterc/SPARK-32916-followup.

Authored-by: Chandni Singh <singh.chandni@gmail.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>
","['common/network-yarn/src/test/java/org/apache/spark/network/yarn/YarnShuffleServiceSuite.java', 'resource-managers/yarn/src/test/scala/org/apache/spark/network/yarn/YarnShuffleServiceSuite.scala']","Newly added tests in `YarnShuffleServiceSuite.java` fail when run with the Hadoop 2.7 profile due to a `java.lang.NoClassDefFoundError`, indicating an issue with class initialization or missing class dependencies."
83f753e4e1412a896243f4016600552c0110c1b0,1618943757,"[SPARK-34472][YARN] Ship ivySettings file to driver in cluster mode

### What changes were proposed in this pull request?

In YARN, ship the `spark.jars.ivySettings` file to the driver when using `cluster` deploy mode so that `addJar` is able to find it in order to resolve ivy paths.

### Why are the changes needed?

SPARK-33084 introduced support for Ivy paths in `sc.addJar` or Spark SQL `ADD JAR`. If we use a custom ivySettings file using `spark.jars.ivySettings`, it is loaded at https://github.com/apache/spark/blob/b26e7b510bbaee63c4095ab47e75ff2a70e377d7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L1280. However, this file is only accessible on the client machine. In YARN cluster mode, this file is not available on the driver and so `addJar` fails to find it.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Added unit tests to verify that the `ivySettings` file is localized by the YARN client and that a YARN cluster mode application is able to find to load the `ivySettings` file.

Closes #31591 from shardulm94/SPARK-34472.

Authored-by: Shardul Mahadik <smahadik@linkedin.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>
","['core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala', 'resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala']","In YARN cluster mode, custom `ivySettings` file for `spark.jars.ivySettings` isn't accessible on the driver, causing `addJar` to fail when resolving Ivy paths."
b2f06608b785f577999318c00f2c315f39d90889,1567799214,"[SPARK-29002][SQL] Avoid changing SMJ to BHJ if the build side has a high ratio of empty partitions

### What changes were proposed in this pull request?
This PR aims to avoid AQE regressions by avoiding changing a sort merge join to a broadcast hash join when the expected build plan has a high ratio of empty partitions, in which case sort merge join can actually perform faster. This PR achieves this by adding an internal join hint in order to let the planner know which side has this high ratio of empty partitions and it should avoid planning it as a build plan of a BHJ. Still, it won't affect the other side if the other side qualifies for a build plan of a BHJ.

### Why are the changes needed?
It is a performance improvement for AQE.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Added UT.

Closes #25703 from maryannxue/aqe-demote-bhj.

Authored-by: maryannxue <maryannxue@apache.org>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/hints.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/DemoteBroadcastHashJoin.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala']",Sort Merge Join (SMJ) changing to Broadcast Hash Join (BHJ) with a high ratio of empty partitions causes Adaptive Query Execution (AQE) performance regression.
5b4816cfc8e290d6f56e57227cb397eebf6a030e,1624546765,"[SPARK-34320][SQL] Migrate ALTER TABLE DROP COLUMNS commands to use UnresolvedTable to resolve the identifier

### What changes were proposed in this pull request?

This PR proposes to migrate the following `ALTER TABLE ... DROP COLUMNS` command to use `UnresolvedTable` as a `child` to resolve the table identifier. This allows consistent resolution rules (temp view first, etc.) to be applied for both v1/v2 commands. More info about the consistent resolution rule proposal can be found in [JIRA](https://issues.apache.org/jira/browse/SPARK-29900) or [proposal doc](https://docs.google.com/document/d/1hvLjGA8y_W_hhilpngXVub1Ebv8RsMap986nENCFnrg/edit?usp=sharing).

### Why are the changes needed?

This is a part of effort to make the relation lookup behavior consistent: [SPARK-29900](https://issues.apache.org/jira/browse/SPARK-29900).

### Does this PR introduce _any_ user-facing change?

After this PR, the above `ALTER TABLE ... DROP COLUMNS` commands will have a consistent resolution behavior.

### How was this patch tested?

Updated existing tests.

Closes #32854 from imback82/alter_alternative.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTableCatalogSuite.scala']","The current ALTER TABLE DROP COLUMNS command does not apply consistent resolution rules across v1/v2 commands, leading to inconsistencies in table identifier resolution."
78b0cbe265c4e8cc3d4d8bf5d734f2998c04d376,1571386014,"[SPARK-29444] Add configuration to support JacksonGenrator to keep fields with null values

### Why are the changes needed?
As mentioned in jira, sometimes we need to be able to support the retention of null columns when writing JSON.
For example, sparkmagic(used widely in jupyter with livy) will generate sql query results based on DataSet.toJSON and parse JSON to pandas DataFrame to display. If there is a null column, it is easy to have some column missing or even the query result is empty. The loss of the null column in the first row, may cause parsing exceptions or loss of entire column data.

### Does this PR introduce any user-facing change?
Example in spark-shell.
scala> spark.sql(""select null as a, 1 as b"").toJSON.collect.foreach(println)
{""b"":1}

scala> spark.sql(""set spark.sql.jsonGenerator.struct.ignore.null=false"")
res2: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> spark.sql(""select null as a, 1 as b"").toJSON.collect.foreach(println)
{""a"":null,""b"":1}

### How was this patch tested?
Add new test to JacksonGeneratorSuite

Closes #26098 from stczwd/json.

Lead-authored-by: stczwd <qcsd2011@163.com>
Co-authored-by: Jackey Lee <qcsd2011@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonGeneratorSuite.scala']","JacksonGenerator drops fields with null values when writing JSON, leading to potential column missing or parsing exceptions when consuming the output."
71aea02e9ffb0c6f7c72c91054c2a4653e22e801,1596258566,"[SPARK-32467][UI] Avoid encoding URL twice on https redirect

### What changes were proposed in this pull request?

When https is enabled for Spark UI, an HTTP request will be redirected as an encoded HTTPS URL: https://github.com/apache/spark/pull/10238/files#diff-f79a5ead735b3d0b34b6b94486918e1cR312

When we create the redirect url, we will call getRequestURI and getQueryString. Both two methods may return an encoded string. However, we pass them directly to the following URI constructor
```
URI(String scheme, String authority, String path, String query, String fragment)
```
As this URI constructor assumes both path and query parameters are decoded strings, it will encode them again. This makes the redirect URL encoded twice.

This problem is on stage page with HTTPS enabled. The URL of ""/taskTable"" contains query parameter `order%5B0%5D%5Bcolumn%5D`. After encoded it becomes  `order%255B0%255D%255Bcolumn%255D` and it will be decoded as `order%5B0%5D%5Bcolumn%5D` instead of `order[0][dir]`.  When the parameter `order[0][dir]` is missing, there will be an excetpion from:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala#L176
and the stage page fail to load.

To fix the problem, we can try decoding the query parameters before encoding it. This is to make sure we encode the URL

### Why are the changes needed?

Fix a UI issue when HTTPS is enabled

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

A new Unit test + manually test on a cluster

Closes #29271 from gengliangwang/urlEncode.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['core/src/main/scala/org/apache/spark/ui/JettyUtils.scala', 'core/src/test/scala/org/apache/spark/ui/UISuite.scala']","When HTTPS is enabled for Spark UI, the HTTP request redirection is leading to double URL encoding, which causes failure in loading the stage page due to missing query parameters."
8fb799d47bbd5d5ce9db35283d08ab1a31dc37b9,1692381809,"[SPARK-44813][INFRA] The Jira Python misses our assignee when it searches users again

### What changes were proposed in this pull request?

This PR creates an alternative to the assign_issue function in jira.client.JIRA.

The original one has an issue that it will search users again and only choose the assignee from 20 candidates. If it's unmatched, it picks the head blindly.

For example,

```python
>>> assignee = asf_jira.user(""yao"")
>>> ""SPARK-44801""
'SPARK-44801'
>>> asf_jira.assign_issue(issue.key, assignee.name)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'issue' is not defined
>>> asf_jira.assign_issue(""SPARK-44801"", assignee.name)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/hzyaoqin/python/lib/python3.11/site-packages/jira/client.py"", line 123, in wrapper
    result = func(*arg_list, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/hzyaoqin/python/lib/python3.11/site-packages/jira/client.py"", line 1891, in assign_issue
    self._session.put(url, data=json.dumps(payload))
  File ""/Users/hzyaoqin/python/lib/python3.11/site-packages/requests/sessions.py"", line 649, in put
    return self.request(""PUT"", url, data=data, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/hzyaoqin/python/lib/python3.11/site-packages/jira/resilientsession.py"", line 246, in request
    elif raise_on_error(response, **processed_kwargs):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/hzyaoqin/python/lib/python3.11/site-packages/jira/resilientsession.py"", line 71, in raise_on_error
    raise JIRAError(
jira.exceptions.JIRAError: JiraError HTTP 400 url: https://issues.apache.org/jira/rest/api/latest/issue/SPARK-44801/assignee
	response text = {""errorMessages"":[],""errors"":{""assignee"":""User 'airhot' cannot be assigned issues.""}}
```

The Jira userid 'yao' fails to return my JIRA profile as a candidate(20 in total) to match. So, 'airhot' from the head replaces me as an assignee.

### Why are the changes needed?

bugfix for merge_spark_pr

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

test locally

```python
>>> def assign_issue(client: jira.client.JIRA, issue: int, assignee: str) -> bool:
...     """"""Assign an issue to a user.
...
...     Args:
...         issue (Union[int, str]): the issue ID or key to assign
...         assignee (str): the user to assign the issue to. None will set it to unassigned. -1 will set it to Automatic.
...
...     Returns:
...         bool
...     """"""
...     url = getattr(client, ""_get_latest_url"")(f""issue/{issue}/assignee"")
...     payload = {""name"": assignee}
...     getattr(client, ""_session"").put(url, data=json.dumps(payload))
...     return True
...

>>>
>>> assign_issue(asf_jira, ""SPARK-44801"", ""yao"")
True
```

Closes #42496 from yaooqinn/SPARK-44813.

Authored-by: Kent Yao <yao@apache.org>
Signed-off-by: Kent Yao <yao@apache.org>
",['dev/merge_spark_pr.py'],"The Jira Python library is failing to assign the correct user to an issue, instead picking from the first 20 users returned from a search and replacing the intended assignee."
42c0b175ce6ee4bf1104b6a8cef6bb6477693781,1604507710,"[SPARK-33338][SQL] GROUP BY using literal map should not fail

### What changes were proposed in this pull request?

This PR aims to fix `semanticEquals` works correctly on `GetMapValue` expressions having literal maps with `ArrayBasedMapData` and `GenericArrayData`.

### Why are the changes needed?

This is a regression from Apache Spark 1.6.x.
```scala
scala> sc.version
res1: String = 1.6.3

scala> sqlContext.sql(""SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]"").show
+---+
|_c0|
+---+
| v1|
+---+
```

Apache Spark 2.x ~ 3.0.1 raise`RuntimeException` for the following queries.
```sql
CREATE TABLE t USING ORC AS SELECT map('k1', 'v1') m, 'k1' k
SELECT map('k1', 'v1')[k] FROM t GROUP BY 1
SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]
SELECT map('k1', 'v1')[k] a FROM t GROUP BY a
```

**BEFORE**
```scala
Caused by: java.lang.RuntimeException: Couldn't find k#3 in [keys: [k1], values: [v1][k#3]#6]
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:85)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:79)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
```

**AFTER**
```sql
spark-sql> SELECT map('k1', 'v1')[k] FROM t GROUP BY 1;
v1
Time taken: 1.278 seconds, Fetched 1 row(s)
spark-sql> SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k];
v1
Time taken: 0.313 seconds, Fetched 1 row(s)
spark-sql> SELECT map('k1', 'v1')[k] a FROM t GROUP BY a;
v1
Time taken: 0.265 seconds, Fetched 1 row(s)
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs with the newly added test case.

Closes #30246 from dongjoon-hyun/SPARK-33338.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala']","SQL queries involving grouping by literal maps results in a RuntimeException, marking a regression from previous functioning versions of Apache Spark."
94c1e3c38cfcc7444aad6efa38a7d2c3ed9f9f4a,1625152500,"[SPARK-35969][K8S] Make the pod prefix more readable and tallied with K8S DNS Label Names

### What changes were proposed in this pull request?

By default, the executor pod prefix is generated by the app name. It handles characters that match [^a-z0-9\\-] differently. The '.' and all whitespaces will be converted to '-', but other ones to empty string. Especially,  characters like '_', '|' are commonly used as a word separator in many languages.

According to the K8S DNS Label Names, see https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names, we can convert all special characters to `-`.

 
For example,

```
scala> ""xyz_abc_i_am_a_app_name_w/_some_abbrs"".replaceAll(""[^a-z0-9\\-]"", ""-"").replaceAll(""-+"", ""-"")
res11: String = xyz-abc-i-am-a-app-name-w-some-abbrs

scala> ""xyz_abc_i_am_a_app_name_w/_some_abbrs"".replaceAll(""\\s+"", ""-"").replaceAll(""\\."", ""-"").replaceAll(""[^a-z0-9\\-]"", """").replaceAll(""-+"", ""-"")
res12: String = xyzabciamaappnamewsomeabbrs
```

```scala
scala> ""time.is%the￥most$valuable_——————thing,it's about time."".replaceAll(""[^a-z0-9\\-]"", ""-"").replaceAll(""-+"", ""-"")
res9: String = time-is-the-most-valuable-thing-it-s-about-time-

scala> ""time.is%the￥most$valuable_——————thing,it's about time."".replaceAll(""\\s+"", ""-"").replaceAll(""\\."", ""-"").replaceAll(""[^a-z0-9\\-]"", """").replaceAll(""-+"", ""-"")
res10: String = time-isthemostvaluablethingits-about-time-

```

### Why are the changes needed?

For better UX

### Does this PR introduce _any_ user-facing change?

yes, the executor pod name might look better
### How was this patch tested?

add new ones

Closes #33171 from yaooqinn/SPARK-35969.

Authored-by: Kent Yao <yao@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala']","Default executor pod prefix generated by app name handles certain special characters incorrectly, leading to inconsistencies with Kubernetes DNS label names."
1b5603ba683122854fd41d821225f5014c204bbc,1680267085,"[SPARK-42918] Generalize handling of metadata attributes in FileSourceStrategy

### What changes were proposed in this pull request?

This change improves the handling of constant and generated metadata fields in `FileSourceStrategy`: instead of relying on hard-wired logic to categorize constant and generated metadata fields and process them, this change embeds the required information when creating them in `FileFormat` and uses this information in `FileSourceStrategy` to process arbitrary metadata fields.

### Why are the changes needed?

This change is a first step towards allowing file format implementations to inject their own metadata fields into plans.  The second step will be to change `FileScanRdd`/`FileFormat` to be able to populate values of arbitrary constant and generated metadata columns.
Once this is done, each file format will be able to declare and populate its own metadata field, e.g. `ParquetFileFormat` can provide the Parquet row index metadata field `ROW_INDEX` without polluting the `FileFormat`.

### Does this PR introduce _any_ user-facing change?
No, this is strictly a refactor without any functional change.

### How was this patch tested?
This change is covered by existing tests, e.p. FileMetadataStructSuite.

Closes #40545 from johanl-db/SPARK-42918-refactor-metadata-attribute.

Authored-by: Johan Lasperas <johan.lasperas@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala']","'FileSourceStrategy' currently relies on hardwired logic to process constant and generated metadata fields, which blocks file formats from injecting their own metadata fields into plans."
50256bde9bdf217413545a6d2945d6c61bf4cfff,1644499938,"[SPARK-38054][SQL] Supports list namespaces in JDBC v2 MySQL dialect

### What changes were proposed in this pull request?
Currently, `JDBCTableCatalog.scala` query namespaces show below.
```
      val schemaBuilder = ArrayBuilder.make[Array[String]]
      val rs = conn.getMetaData.getSchemas()
      while (rs.next()) {
        schemaBuilder += Array(rs.getString(1))
      }
      schemaBuilder.result
```

But the code cannot get any information when using MySQL JDBC driver.
This PR uses `SHOW SCHEMAS` to query namespaces of MySQL.
This PR also fix other issues below:

- Release the docker tests in `MySQLNamespaceSuite.scala`.
- Because MySQL doesn't support create comment of schema, let's throws `SQLFeatureNotSupportedException`.
- Because MySQL doesn't support `DROP SCHEMA` in `RESTRICT` mode, let's throws `SQLFeatureNotSupportedException`.
- Reactor `JdbcUtils.executeQuery` to avoid `java.sql.SQLException: Operation not allowed after ResultSet closed`.

### Why are the changes needed?
MySQL dialect supports query namespaces.

### Does this PR introduce _any_ user-facing change?
'Yes'.
Some API changed.

### How was this patch tested?
New tests.

Closes #35355 from beliefer/SPARK-38054.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/MySQLNamespaceSuite.scala', 'external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCNamespaceTest.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTableCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala']","The current implementation of JDBC MySQL dialect fails to fetch namespace information, alongside several related issues including unsupported schema comments and issues with 'DROP SCHEMA' in 'RESTRICT' mode. Operation issues also arise post ResultSet closure."
fb5b44f46b35562330e5e89133a0bca8e0bee36b,1676461110,"[SPARK-42401][SQL][FOLLOWUP] Always set `containsNull=true` for `array_insert`

### What changes were proposed in this pull request?

Always set `containsNull=true` in the data type returned by `ArrayInsert#dataType`.

### Why are the changes needed?

PR #39970 fixed an issue where the data type for `array_insert` did not always have `containsNull=true` when the user was explicitly inserting a nullable value into the array. However, that fix does not handle the case where `array_insert` implicitly inserts null values into the array (e.g., when the insertion position is out-of-range):
```
spark-sql> select array_insert(array('1', '2', '3', '4'), -6, '5');
23/02/14 16:10:19 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
```
Because we can't know at planning time whether the insertion position will be out of range, we should always set `containsNull=true` on the data type for `array_insert`.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New unit tests.

Closes #40026 from bersprockets/array_insert_null_anytime.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala']",The `array_insert` function throws a NullPointerException when the insertion position is out-of-range which leads to implicit null value insertion.
5b96bd5cf8f44eee7a16cd027d37dec552ed5a6a,1657721421,"[SPARK-39758][SQL] Fix NPE from the regexp functions on invalid patterns

### What changes were proposed in this pull request?
In the PR, I propose to catch `PatternSyntaxException` while compiling the regexp pattern by the `regexp_extract`, `regexp_extract_all` and `regexp_instr`, and substitute the exception by Spark's exception w/ the error class `INVALID_PARAMETER_VALUE`. In this way, Spark SQL will output the error in the form:
```sql
org.apache.spark.SparkRuntimeException
[INVALID_PARAMETER_VALUE] The value of parameter(s) 'regexp' in `regexp_instr` is invalid: ) ?
```
instead of (on Spark 3.3.0):
```java
java.lang.NullPointerException: null
```
Also I propose to set `lastRegex` only after the compilation of the regexp pattern completes successfully.

### Why are the changes needed?
The changes fix NPE portrayed by the code on Spark 3.3.0:
```sql
spark-sql> SELECT regexp_extract('1a 2b 14m', '(?l)');
22/07/12 19:07:21 ERROR SparkSQLDriver: Failed in [SELECT regexp_extract('1a 2b 14m', '(?l)')]
java.lang.NullPointerException: null
	at org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.getLastMatcher(regexpExpressions.scala:768) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
```
This should improve user experience with Spark SQL.

### Does this PR introduce _any_ user-facing change?
No. In regular cases, the behavior is the same but users will observe different exceptions (error messages) after the changes.

### How was this patch tested?
By running new tests:
```
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z regexp-functions.sql""
$ build/sbt ""test:testOnly *.RegexpExpressionsSuite""
$ build/sbt ""sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite""
```

Closes #37171 from MaxGekk/pattern-syntax-exception.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql']","Regexp functions such as 'regexp_extract', 'regexp_extract_all', and 'regexp_instr' throw a NullPointerException (NPE) on encountering invalid patterns."
f4772fd26f32b11ae54e7721924b5cf6eb27298a,1533083064,"[SPARK-24976][PYTHON] Allow None for Decimal type conversion (specific to PyArrow 0.9.0)

## What changes were proposed in this pull request?

See [ARROW-2432](https://jira.apache.org/jira/browse/ARROW-2432). Seems using `from_pandas` to convert decimals fails if encounters a value of `None`:

```python
import pyarrow as pa
import pandas as pd
from decimal import Decimal

pa.Array.from_pandas(pd.Series([Decimal('3.14'), None]), type=pa.decimal128(3, 2))
```

**Arrow 0.8.0**

```
<pyarrow.lib.Decimal128Array object at 0x10a572c58>
[
  Decimal('3.14'),
  NA
]
```

**Arrow 0.9.0**

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""array.pxi"", line 383, in pyarrow.lib.Array.from_pandas
  File ""array.pxi"", line 177, in pyarrow.lib.array
  File ""error.pxi"", line 77, in pyarrow.lib.check_status
  File ""error.pxi"", line 77, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Error converting from Python objects to Decimal: Got Python object of type NoneType but can only handle these types: decimal.Decimal
```

This PR propose to work around this via Decimal NaN:

```python
pa.Array.from_pandas(pd.Series([Decimal('3.14'), Decimal('NaN')]), type=pa.decimal128(3, 2))
```

```
<pyarrow.lib.Decimal128Array object at 0x10ffd2e68>
[
  Decimal('3.14'),
  NA
]
```

## How was this patch tested?

Manually tested:

```bash
SPARK_TESTING=1 ./bin/pyspark pyspark.sql.tests ScalarPandasUDFTests
```

**Before**

```
Traceback (most recent call last):
  File ""/.../spark/python/pyspark/sql/tests.py"", line 4672, in test_vectorized_udf_null_decimal
    self.assertEquals(df.collect(), res.collect())
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 533, in collect
    sock_info = self._jdf.collectToPython()
  File ""/.../spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/.../spark/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/.../spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
Py4JJavaError: An error occurred while calling o51.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/.../spark/python/pyspark/worker.py"", line 320, in main
    process()
  File ""/.../spark/python/pyspark/worker.py"", line 315, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/.../spark/python/pyspark/serializers.py"", line 274, in dump_stream
    batch = _create_batch(series, self._timezone)
  File ""/.../spark/python/pyspark/serializers.py"", line 243, in _create_batch
    arrs = [create_array(s, t) for s, t in series]
  File ""/.../spark/python/pyspark/serializers.py"", line 241, in create_array
    return pa.Array.from_pandas(s, mask=mask, type=t)
  File ""array.pxi"", line 383, in pyarrow.lib.Array.from_pandas
  File ""array.pxi"", line 177, in pyarrow.lib.array
  File ""error.pxi"", line 77, in pyarrow.lib.check_status
  File ""error.pxi"", line 77, in pyarrow.lib.check_status
ArrowInvalid: Error converting from Python objects to Decimal: Got Python object of type NoneType but can only handle these types: decimal.Decimal
```

**After**

```
Running tests...
----------------------------------------------------------------------
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
.......S.............................
----------------------------------------------------------------------
Ran 37 tests in 21.980s
```

Author: hyukjinkwon <gurwls223@apache.org>

Closes #21928 from HyukjinKwon/SPARK-24976.
",['python/pyspark/serializers.py'],"Conversion of decimals to None in PyArrow 0.9.0 fails and raises a TypeError, while it worked seamlessly in previous versions like 0.8.0."
79ba2890f51c5f676b9cd6e3a6682c7969462999,1657603243,"[SPARK-39647][CORE] Register the executor with ESS before registering the BlockManager

### What changes were proposed in this pull request?
Currently the executors register with the ESS after the `BlockManager` registration with the `BlockManagerMaster`.  This order creates a problem with the push-based shuffle. A registered BlockManager node is picked up by the driver as a merger but the shuffle service on that node is not yet ready to merge the data which causes block pushes to fail until the local executor registers with it. This fix is to reverse the order, that is, register with the ESS before registering the `BlockManager`

### Why are the changes needed?
They are needed to fix the issue which causes block pushes to fail.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added a UT.

Closes #37052 from otterc/SPARK-39647.

Authored-by: Chandni Singh <singh.chandni@gmail.com>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['core/src/main/scala/org/apache/spark/storage/BlockManager.scala', 'core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala']",The order of registering with ESS and BlockManager causes a failure in block pushes as the shuffle service on a node is not ready to merge the data when picked up by the driver.
878527d9fae8945d087ec871bb0a5f49b6341939,1622754446,"[SPARK-35612][SQL] Support LZ4 compression in ORC data source

### What changes were proposed in this pull request?

This PR aims to support LZ4 compression in the ORC data source.

### Why are the changes needed?

Apache ORC supports LZ4 compression, but we cannot set LZ4 compression in the ORC data source

**BEFORE**

```scala
scala> spark.range(10).write.option(""compression"", ""lz4"").orc(""/tmp/lz4"")
java.lang.IllegalArgumentException: Codec [lz4] is not available. Available codecs are uncompressed, lzo, snappy, zlib, none, zstd.
```

**AFTER**

```scala
scala> spark.range(10).write.option(""compression"", ""lz4"").orc(""/tmp/lz4"")
```
```bash
$ orc-tools meta /tmp/lz4
Processing data file file:/tmp/lz4/part-00000-6a244eee-b092-4c79-a977-fb8a69dde2eb-c000.lz4.orc [length: 222]
Structure for file:/tmp/lz4/part-00000-6a244eee-b092-4c79-a977-fb8a69dde2eb-c000.lz4.orc
File Version: 0.12 with ORC_517
Rows: 10
Compression: LZ4
Compression size: 262144
Type: struct<id:bigint>

Stripe Statistics:
  Stripe 1:
    Column 0: count: 10 hasNull: false
    Column 1: count: 10 hasNull: false bytesOnDisk: 7 min: 0 max: 9 sum: 45

File Statistics:
  Column 0: count: 10 hasNull: false
  Column 1: count: 10 hasNull: false bytesOnDisk: 7 min: 0 max: 9 sum: 45

Stripes:
  Stripe: offset: 3 data: 7 rows: 10 tail: 35 index: 35
    Stream: column 0 section ROW_INDEX start: 3 length 11
    Stream: column 1 section ROW_INDEX start: 14 length 24
    Stream: column 1 section DATA start: 38 length 7
    Encoding column 0: DIRECT
    Encoding column 1: DIRECT_V2

File length: 222 bytes
Padding length: 0 bytes
Padding ratio: 0%

User Metadata:
  org.apache.spark.version=3.2.0
```

### Does this PR introduce _any_ user-facing change?

Yes.

### How was this patch tested?

Pass the newly added test case.

Closes #32751 from fornaix/spark-35612.

Authored-by: fornaix <foxnaix@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcSourceSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala']",Setting LZ4 compression option in ORC data source throws IllegalArgumentException stating LZ4 codec is unavailable.
0a58029d5251fd8ef3687c73264e4ffc51c8ea09,1616125828,"[SPARK-31897][SQL] Enable codegen for GenerateExec

### What changes were proposed in this pull request?
Enabling codegen for GenerateExec

### Why are the changes needed?
To leverage code generation for Generators

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
- UT tests added

### Benchmark
```
case class Data(value1: Float, value2: Map[String, String], value3: String)
val path = ""<path>""

val numRecords = Seq(10000000, 100000000)
numRecords.map {
  recordCount =>
    import java.util.concurrent.TimeUnit.NANOSECONDS

    val srcDF = spark.range(recordCount).map {
      x => Data(x.toFloat, Map(x.toString -> x.toString ), s""value3$x"")
    }.select($""value1"", explode($""value2""), $""value3"")
    val start = System.nanoTime()
    srcDF
      .write
      .mode(""overwrite"")
      .parquet(s""$path/$recordCount"")
    val end = System.nanoTime()
    val diff = end - start
    (recordCount, NANOSECONDS.toMillis(diff))
}
```
**With codegen**:
```
res0: Seq[(Int, Long)] = List((10000000,13989), (100000000,129625))
```
**Without codegen**:
```
res0: Seq[(Int, Long)] = List((10000000,15736), (100000000,150399))
```

Closes #28715 from karuppayya/SPARK-31897.

Lead-authored-by: Karuppayya Rajendran <karuppayya1990@gmail.com>
Co-authored-by: Karuppayya Rajendran <karuppayya.rajendran@apple.com>
Signed-off-by: Liang-Chi Hsieh <viirya@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/GenerateExecBenchmark.scala']","Code generation for GenerateExec is currently not enabled, potentially leading to less optimal performance."
77a4aa05720982464d987182d58b9341e9c4dbad,1688971194,"[SPARK-44337][PROTOBUF] Any fields set to 'Any.getDefaultInstance' cause parse errors

### What changes were proposed in this pull request?

While building the Protobuf descriptor from FileDescriptorSet, this PR uses bundled Java descriptor for 'google/protobuf/any.proto'. This works around a bug in Protobuf's JsonFormat while parsing Any fields initialized with `Any.getDefaultInstance()`.

An existing test for Any fields is updated to test this case. Without the fix, the test passes with Java classes, but fails with FileDescriptorSet bytes. The latter is the common use case.

The bug in JsonFormat is related to how it checks for Any values at runtime that were originally initialized with `Any.getDefaultInstance()`. Specifically the conditional [here](https://github.com/protocolbuffers/protobuf/blob/51f79d4415338cfe068a39d7ce466870df223245/java/util/src/main/java/com/google/protobuf/util/JsonFormat.java#L860)  in JsonFormat.java fails since it tries to compare with actual default instance in Java from Java descriptor. If we build the same descriptor separately, JsonFormat does not recognize it.

### Why are the changes needed?

Without this fix, input that includes fields initialized to `Any.getDefaultInstance` can not be parsed.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

An existing unit test is updated to test this case.

Closes #41897 from rangadi/any-default.

Authored-by: Raghu Angadi <raghu.angadi@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['connector/protobuf/src/main/scala/org/apache/spark/sql/protobuf/utils/ProtobufUtils.scala', 'connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufFunctionsSuite.scala']",Parsing errors occur when fields in Protobuf descriptor from FileDescriptorSet are set to 'Any.getDefaultInstance'.
dff5c2f2e9ce233e270e0e5cde0a40f682ba9534,1660272753,"[SPARK-39976][SQL] ArrayIntersect should handle null in left expression correctly

### What changes were proposed in this pull request?
`ArrayInterscet` miss judge if null contains in right expression's hash set.

```
>>> a = [1, 2, 3]
>>> b = [3, None, 5]
>>> df = spark.sparkContext.parallelize(data).toDF([""a"",""b""])
>>> df.show()
+---------+------------+
|        a|           b|
+---------+------------+
|[1, 2, 3]|[3, null, 5]|
+---------+------------+

>>> df.selectExpr(""array_intersect(a,b)"").show()
+---------------------+
|array_intersect(a, b)|
+---------------------+
|                  [3]|
+---------------------+

>>> df.selectExpr(""array_intersect(b,a)"").show()
+---------------------+
|array_intersect(b, a)|
+---------------------+
|            [3, null]|
+---------------------+
```

In origin code gen's code path, when handle `ArrayIntersect`'s array1, it use the below code
```
        def withArray1NullAssignment(body: String) =
          if (left.dataType.asInstanceOf[ArrayType].containsNull) {
            if (right.dataType.asInstanceOf[ArrayType].containsNull) {
              s""""""
                 |if ($array1.isNullAt($i)) {
                 |  if ($foundNullElement) {
                 |    $nullElementIndex = $size;
                 |    $foundNullElement = false;
                 |    $size++;
                 |    $builder.$$plus$$eq($nullValueHolder);
                 |  }
                 |} else {
                 |  $body
                 |}
               """""".stripMargin
            } else {
              s""""""
                 |if (!$array1.isNullAt($i)) {
                 |  $body
                 |}
               """""".stripMargin
            }
          } else {
            body
          }
```
We have a flag `foundNullElement` to indicate if array2 really contains a null value. But when implement https://issues.apache.org/jira/browse/SPARK-36829, misunderstand the meaning of `ArrayType.containsNull`,
so when implement  `SQLOpenHashSet.withNullCheckCode()`
```
  def withNullCheckCode(
      arrayContainsNull: Boolean,
      setContainsNull: Boolean,
      array: String,
      index: String,
      hashSet: String,
      handleNotNull: (String, String) => String,
      handleNull: String): String = {
    if (arrayContainsNull) {
      if (setContainsNull) {
        s""""""
           |if ($array.isNullAt($index)) {
           |  if (!$hashSet.containsNull()) {
           |    $hashSet.addNull();
           |    $handleNull
           |  }
           |} else {
           |  ${handleNotNull(array, index)}
           |}
         """""".stripMargin
      } else {
        s""""""
           |if (!$array.isNullAt($index)) {
           | ${handleNotNull(array, index)}
           |}
         """""".stripMargin
      }
    } else {
      handleNotNull(array, index)
    }
  }
```
The code path of `  if (arrayContainsNull && setContainsNull) `  is misinterpreted that array's openHashSet really have a null value.

In this pr we add a new parameter `additionalCondition ` to complements the previous implementation of `foundNullElement`. Also refactor the method's parameter name.

### Why are the changes needed?
Fix data correct issue

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added UT

Closes #37436 from AngersZhuuuu/SPARK-39776-FOLLOW_UP.

Lead-authored-by: Angerszhuuuu <angers.zhu@gmail.com>
Co-authored-by: AngersZhuuuu <angers.zhu@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala']","'ArrayIntersect' function incorrectly evaluates null values in array when left and right expressions contain null, leading to inconsistencies in results."
559b899aceb160fcec3a57109c0b60a0ae40daeb,1536181849,"[SPARK-25231] Fix synchronization of executor heartbeat receiver in TaskSchedulerImpl

Running a large Spark job with speculation turned on was causing executor heartbeats to time out on the driver end after sometime and eventually, after hitting the max number of executor failures, the job would fail.

## What changes were proposed in this pull request?

The main reason for the heartbeat timeouts was that the heartbeat-receiver-event-loop-thread was blocked waiting on the TaskSchedulerImpl object which was being held by one of the dispatcher-event-loop threads executing the method dequeueSpeculativeTasks() in TaskSetManager.scala. On further analysis of the heartbeat receiver method executorHeartbeatReceived() in TaskSchedulerImpl class, we found out that instead of waiting to acquire the lock on the TaskSchedulerImpl object, we can remove that lock and make the operations to the global variables inside the code block to be atomic. The block of code in that method only uses  one global HashMap taskIdToTaskSetManager. Making that map a ConcurrentHashMap, we are ensuring atomicity of operations and speeding up the heartbeat receiver thread operation.

## How was this patch tested?

Screenshots of the thread dump have been attached below:
**heartbeat-receiver-event-loop-thread:**

<img width=""1409"" alt=""screen shot 2018-08-24 at 9 19 57 am"" src=""https://user-images.githubusercontent.com/22228190/44593413-e25df780-a788-11e8-9520-176a18401a59.png"">

**dispatcher-event-loop-thread:**

<img width=""1409"" alt=""screen shot 2018-08-24 at 9 21 56 am"" src=""https://user-images.githubusercontent.com/22228190/44593484-13d6c300-a789-11e8-8d88-34b1d51d4541.png"">

Closes #22221 from pgandhi999/SPARK-25231.

Authored-by: pgandhi <pgandhi@oath.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>
","['core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala', 'core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala', 'core/src/test/scala/org/apache/spark/scheduler/SchedulerIntegrationSuite.scala', 'core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala']","Running large Spark job with speculation results in executor heartbeat timeouts due to blocked heartbeat-receiver-event-loop-thread, ultimately leading to job failure."
271aa331b338efd50dc67669faa29765a3452b26,1618461755,"[MINOR][SQL] Refactor the comments in HiveClientImpl.withHiveState

### What changes were proposed in this pull request?

This PR refactors three parts of the comments in `HiveClientImpl.withHiveState`

One is about the following comment.
```
// The classloader in clientLoader could be changed after addJar, always use the latest
// classloader.
```
The comment was added in SPARK-10810 (#8909) because `IsolatedClientLoader.classLoader` was declared as `var`.
But the field is now `val` and cannot be changed after instanciation.
So, the comment can confuse developers.

One is about the following code and comment.
```
// classloader. We explicitly set the context class loader since ""conf.setClassLoader"" does
// not do that, and the Hive client libraries may need to load classes defined by the client's
// class loader.
Thread.currentThread().setContextClassLoader(clientLoader.classLoader)
```
It's not trivial why this part is necessary and it's difficult when we can remove this code in the future.
So, I revised the comment by adding the reference of the related JIRA.

And the last one is about the following code and comment.
```
// Replace conf in the thread local Hive with current conf
Hive.get(conf)
```
It's also not trivial why this part is necessary.
I revised the comment by adding the reference of the related discussion.

### Why are the changes needed?

To make code more readable.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

It's just a comment refactoring so I add no new test.

Closes #32162 from sarutak/refactor-HiveClientImpl.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala'],"Comments in `HiveClientImpl.withHiveState` are misleading and unclear, making it difficult for developers to understand the code's purpose and necessity."
7e9b88bfceb86d3b32e82a86b672aab3c74def8c,1670948046,"[SPARK-27561][SQL] Support implicit lateral column alias resolution on Project

### What changes were proposed in this pull request?
This PR implements a new feature: Implicit lateral column alias  on `Project` case, controlled by `spark.sql.lateralColumnAlias.enableImplicitResolution` temporarily (default false now, but will turn on this conf once the feature is completely merged).

#### Lateral column alias
View https://issues.apache.org/jira/browse/SPARK-27561 for more details on lateral column alias.
There are two main cases to support: LCA in Project, and LCA in Aggregate.
```sql
-- LCA in Project. The base_salary references an attribute defined by a previous alias
SELECT salary AS base_salary, base_salary + bonus AS total_salary
FROM employee

-- LCA in Aggregate. The avg_salary references an attribute defined by a previous alias
SELECT dept, average(salary) AS avg_salary, avg_salary + average(bonus)
FROM employee
GROUP BY dept
```
This **implicit** lateral column alias (no explicit keyword, e.g. `lateral.base_salary`) should be supported.

#### High level design
This PR defines a new Resolution rule, `ResolveLateralColumnAlias` to resolve the implicit lateral column alias, covering the `Project` case.
It introduces a new leaf node NamedExpression, `LateralColumnAliasReference`, as a placeholder used to hold a referenced that has been temporarily resolved as the reference to a lateral column alias.

The whole process is generally divided into two phases:
1) recognize **resolved** lateral alias, wrap the attributes referencing them with `LateralColumnAliasReference`.
 2) when the whole operator is resolved, unwrap `LateralColumnAliasReference`. For Project, it further resolves the attributes and push down the referenced lateral aliases to the new Project.

For example:
```
// Before
Project [age AS a, 'a + 1]
+- Child

// After phase 1
Project [age AS a, lateralalias(a) + 1]
+- Child

// After phase 2
Project [a, a + 1]
+- Project [child output, age AS a]
   +- Child
```

#### Resolution order
Given this new rule, the name resolution order will be (higher -> lower):
```
local table column > local metadata attribute > local lateral column alias > all others (outer reference of subquery, parameters of SQL UDF, ..)
```

There is a recent refactor that moves the creation of `OuterReference` in the Resolution batch: https://github.com/apache/spark/pull/38851.
Because lateral column alias has higher resolution priority than outer reference, it will try to resolve an `OuterReference` using lateral column alias, similar as an `UnresolvedAttribute`. If success, it strips `OuterReference` and also wraps it with `LateralColumnAliasReference`.

### Why are the changes needed?
The lateral column alias is a popular feature wanted for a long time. It is supported by lots of other database vendors (Redshift, snowflake, etc) and provides a better user experience.

### Does this PR introduce _any_ user-facing change?
Yes, as shown in the above example, it will be able to resolve lateral column alias. I will write the migration guide or release note when most PRs of this feature are merged.

### How was this patch tested?
Existing tests and newly added tests.

Closes #38776 from anchovYu/SPARK-27561-refactor.

Authored-by: Xinyi Yu <xinyi.yu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala-2.12/org/apache/spark/sql/catalyst/expressions/AttributeMap.scala', 'sql/catalyst/src/main/scala-2.13/org/apache/spark/sql/catalyst/expressions/AttributeMap.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/test/scala/org/apache/spark/sql/LateralColumnAliasSuite.scala']","Lateral column alias on 'Project' cannot be implicitly resolved, causing difficulty in referencing aliased attributes in calculations or subsequent operations."
6b33796d0df152d4acd52c1237de69d59b94d1fc,1673608391,"[SPARK-42046][TESTS] Add `connect-client-jvm` to `connect` module and fix port failure

### What changes were proposed in this pull request?

This PR aims to add `connect-client-jvm` module to `connect` module.

### Why are the changes needed?

Currently, `connect-client-jvm` are not tested in CI because we only invoke the following for `connect` module.

https://github.com/apache/spark/blob/5e0a82e590d1c3c3c5fa7f347dddf450fabbf772/.github/workflows/build_and_test.yml#L155

And, it fails in GitHub Action environment.
```
[info] - Check URI: sc://localhost:123/, isCorrect: true *** FAILED *** (6 milliseconds)
[info]   java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:123
...
Cause: java.net.SocketException: Permission denied
[info]   at sun.nio.ch.Net.bind0(Native Method)
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs and check `SparkConnectClientSuite` from the log.

```
$ dev/run-tests.py -m connect
...
[info] SparkConnectClientSuite:
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[info] - Placeholder test: Create SparkConnectClient (100 milliseconds)
[info] - Test connection (114 milliseconds)
[info] - Test connection string (6 milliseconds)
[info] - Check URI: sc://host, isCorrect: true (0 milliseconds)
[info] - Check URI: sc://localhost/, isCorrect: true (4 milliseconds)
[info] - Check URI: sc://localhost:123/, isCorrect: true (12 milliseconds)
[info] - Check URI: sc://localhost/;, isCorrect: true (4 milliseconds)
[info] - Check URI: sc://host:123, isCorrect: true (1 millisecond)
[info] - Check URI: sc://host:123/;user_id=a94, isCorrect: true (1 millisecond)
[info] - Check URI: scc://host:12, isCorrect: false (1 millisecond)
[info] - Check URI: http://host, isCorrect: false (0 milliseconds)
[info] - Check URI: sc:/host:1234/path, isCorrect: false (0 milliseconds)
[info] - Check URI: sc://host/path, isCorrect: false (1 millisecond)
[info] - Check URI: sc://host/;parm1;param2, isCorrect: false (0 milliseconds)
[info] - Check URI: sc://host:123;user_id=a94, isCorrect: false (0 milliseconds)
[info] - Check URI: sc:///user_id=123, isCorrect: false (0 milliseconds)
[info] - Check URI: sc://host:-4, isCorrect: false (0 milliseconds)
[info] - Check URI: sc://:123/, isCorrect: false (0 milliseconds)
[info] - Non user-id parameters throw unsupported errors (1 millisecond)
[info] Run completed in 1 second, 75 milliseconds.
[info] Total number of tests run: 19
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 19, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[success] Total time: 1 s, completed Jan 12, 2023, 10:10:49 PM
```

Closes #39549 from dongjoon-hyun/SPARK-42046.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/SparkConnectClientSuite.scala', 'dev/sparktestsupport/modules.py']",`connect-client-jvm` module isn't tested in CI for `connect` causing failures in GitHub Action environment due to permission issues while trying to bind to address 0.0.0.0/0.0.0.0:123.
69dab94b139a4f1034407d0a562c2b94984b35de,1548353515,"[SPARK-26687][K8S] Fix handling of custom Dockerfile paths

## What changes were proposed in this pull request?

With the changes from vanzin's PR #23019 (SPARK-26025) we use a pared down temporary Docker build context which significantly improves build times.  However the way this is implemented leads to non-intuitive behaviour when supplying custom Docker file paths.  This is because of the following code snippets:

```
(cd $(img_ctx_dir base) && docker build $NOCACHEARG ""${BUILD_ARGS[]}"" \
    -t $(image_ref spark) \
    -f ""$BASEDOCKERFILE"" .)
```

Since the script changes to the temporary build context directory and then runs `docker build` there any path given for the Docker file is taken as relative to the temporary build context directory rather than to the directory where the user invoked the script.  This is rather unintuitive and produces somewhat unhelpful errors e.g.

```
> ./bin/docker-image-tool.sh -r rvesse -t badpath -p resource-managers/kubernetes/docker/src/main/dockerfiles/spark/bindings/python/Dockerfile build
Sending build context to Docker daemon  218.4MB
Step 1/15 : FROM openjdk:8-alpine
 ---> 5801f7d008e5
Step 2/15 : ARG spark_uid=185
 ---> Using cache
 ---> 5fd63df1ca39
...
Successfully tagged rvesse/spark:badpath
unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /Users/rvesse/Documents/Work/Code/spark/target/tmp/docker/pyspark/resource-managers: no such file or directory
Failed to build PySpark Docker image, please refer to Docker build output for details.
```

Here we can see that the relative path that was valid where the user typed the command was not valid inside the build context directory.

To resolve this we need to ensure that we are resolving relative paths to Docker files appropriately which we do by adding a `resolve_file` function to the script and invoking that on the supplied Docker file paths

## How was this patch tested?

Validated that relative paths now work as expected:

```
> ./bin/docker-image-tool.sh -r rvesse -t badpath -p resource-managers/kubernetes/docker/src/main/dockerfiles/spark/bindings/python/Dockerfile build
Sending build context to Docker daemon  218.4MB
Step 1/15 : FROM openjdk:8-alpine
 ---> 5801f7d008e5
Step 2/15 : ARG spark_uid=185
 ---> Using cache
 ---> 5fd63df1ca39
Step 3/15 : RUN set -ex &&     apk upgrade --no-cache &&     apk add --no-cache bash tini libc6-compat linux-pam krb5 krb5-libs &&     mkdir -p /opt/spark &&     mkdir -p /opt/spark/examples &&     mkdir -p /opt/spark/work-dir &&     touch /opt/spark/RELEASE &&     rm /bin/sh &&     ln -sv /bin/bash /bin/sh &&     echo ""auth required pam_wheel.so use_uid"" >> /etc/pam.d/su &&     chgrp root /etc/passwd && chmod ug+rw /etc/passwd
 ---> Using cache
 ---> eb0a568e032f
Step 4/15 : COPY jars /opt/spark/jars
...
Successfully tagged rvesse/spark:badpath
Sending build context to Docker daemon  6.599MB
Step 1/13 : ARG base_img
Step 2/13 : ARG spark_uid=185
Step 3/13 : FROM $base_img
 ---> 8f4fff16f903
Step 4/13 : WORKDIR /
 ---> Running in 25466e66f27f
Removing intermediate container 25466e66f27f
 ---> 1470b6efae61
Step 5/13 : USER 0
 ---> Running in b094b739df37
Removing intermediate container b094b739df37
 ---> 6a27eb4acad3
Step 6/13 : RUN mkdir ${SPARK_HOME}/python
 ---> Running in bc8002c5b17c
Removing intermediate container bc8002c5b17c
 ---> 19bb12f4286a
Step 7/13 : RUN apk add --no-cache python &&     apk add --no-cache python3 &&     python -m ensurepip &&     python3 -m ensurepip &&     rm -r /usr/lib/python*/ensurepip &&     pip install --upgrade pip setuptools &&     rm -r /root/.cache
 ---> Running in 12dcba5e527f
...
Successfully tagged rvesse/spark-py:badpath
```

Closes #23613 from rvesse/SPARK-26687.

Authored-by: Rob Vesse <rvesse@dotnetrdf.org>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
",['bin/docker-image-tool.sh'],"Custom Dockerfile paths are getting interpreted as relative to the temporary build context directory, not the directory where the user invoked the script, leading to unhelpful errors."
1144df3b5dc8280ae4a07678cb439f0b44cb17b0,1543514412,"[SPARK-26015][K8S] Set a default UID for Spark on K8S Images

Adds USER directives to the Dockerfiles which is configurable via build argument (`spark_uid`) for easy customisation.  A `-u` flag is added to `bin/docker-image-tool.sh` to make it easy to customise this e.g.

```
> bin/docker-image-tool.sh -r rvesse -t uid -u 185 build
> bin/docker-image-tool.sh -r rvesse -t uid push
```

If no UID is explicitly specified it defaults to `185` - this is per skonto's suggestion to align with the OpenShift standard reserved UID for Java apps (
https://lists.openshift.redhat.com/openshift-archives/users/2016-March/msg00283.html)

Notes:
- We have to make the `WORKDIR` writable by the root group or otherwise jobs will fail with `AccessDeniedException`

To Do:
- [x] Debug and resolve issue with client mode test
- [x] Consider whether to always propagate `SPARK_USER_NAME` to environment of driver and executor pods so `entrypoint.sh` can insert that into `/etc/passwd` entry
- [x] Rebase once PR #23013 is merged and update documentation accordingly

Built the Docker images with the new Dockerfiles that include the `USER` directives.  Ran the Spark on K8S integration tests against the new images.  All pass except client mode which I am currently debugging further.

Also manually dropped myself into the resulting container images via `docker run` and checked `id -u` output to see that UID is as expected.

Tried customising the UID from the default via the new `-u` argument to `docker-image-tool.sh` and again checked the resulting image for the correct runtime UID.

cc felixcheung skonto vanzin

Closes #23017 from rvesse/SPARK-26015.

Authored-by: Rob Vesse <rvesse@dotnetrdf.org>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['bin/docker-image-tool.sh', 'resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh', 'resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/ClientModeTestsSuite.scala']","Spark on K8S images lack a default USER directive in Dockerfiles, causing issues with permissions in the `WORKDIR`, and failures due to `AccessDeniedException`."
d193248205b6c197d5e1106f012b482b726f39ad,1570927480,"[SPARK-29368][SQL][TEST] Port interval.sql

### What changes were proposed in this pull request?
This PR is to port interval.sql from PostgreSQL regression tests: https://raw.githubusercontent.com/postgres/postgres/REL_12_STABLE/src/test/regress/sql/interval.sql

The expected results can be found in the link: https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/expected/interval.out

When porting the test cases, found PostgreSQL specific features below that do not exist in Spark SQL:
- [SPARK-29369](https://issues.apache.org/jira/browse/SPARK-29369): Accept strings without `interval` prefix in casting to intervals
- [SPARK-29370](https://issues.apache.org/jira/browse/SPARK-29370): Interval strings without explicit unit markings
- [SPARK-29371](https://issues.apache.org/jira/browse/SPARK-29371): Support interval field values with fractional parts
- [SPARK-29382](https://issues.apache.org/jira/browse/SPARK-29382): Support the `INTERVAL` type by Parquet datasource
- [SPARK-29383](https://issues.apache.org/jira/browse/SPARK-29383): Support the optional prefix `` in interval strings
- [SPARK-29384](https://issues.apache.org/jira/browse/SPARK-29384): Support `ago` in interval strings
- [SPARK-29385](https://issues.apache.org/jira/browse/SPARK-29385): Make `INTERVAL` values comparable
- [SPARK-29386](https://issues.apache.org/jira/browse/SPARK-29386): Copy data between a file and a table
- [SPARK-29387](https://issues.apache.org/jira/browse/SPARK-29387): Support `*` and `\` operators for intervals
- [SPARK-29388](https://issues.apache.org/jira/browse/SPARK-29388): Construct intervals from the `millenniums`, `centuries` or `decades` units
- [SPARK-29389](https://issues.apache.org/jira/browse/SPARK-29389): Support synonyms for interval units
- [SPARK-29390](https://issues.apache.org/jira/browse/SPARK-29390): Add the justify_days(), justify_hours() and justify_interval() functions
- [SPARK-29391](https://issues.apache.org/jira/browse/SPARK-29391): Default year-month units
- [SPARK-29393](https://issues.apache.org/jira/browse/SPARK-29393): Add the make_interval() function
- [SPARK-29394](https://issues.apache.org/jira/browse/SPARK-29394): Support ISO 8601 format for intervals
- [SPARK-29395](https://issues.apache.org/jira/browse/SPARK-29395): Precision of the interval type
- [SPARK-29406](https://issues.apache.org/jira/browse/SPARK-29406): Interval output styles
- [SPARK-29407](https://issues.apache.org/jira/browse/SPARK-29407): Support syntax for zero interval
- [SPARK-29408](https://issues.apache.org/jira/browse/SPARK-29408): Support interval literal with negative sign `-`

### Why are the changes needed?
To improve the test coverage, see https://issues.apache.org/jira/browse/SPARK-27763

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By manually comparing Spark results with PostgreSQL

Closes #26055 from MaxGekk/port-interval-sql.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['sql/core/src/test/resources/sql-tests/inputs/postgreSQL/interval.sql'],"Spark SQL is lacking PostgreSQL specific features when porting interval.sql, including various interval string handling, interval type support, interval units and functions, and supporting different interval formats."
c211abe970d9e88fd25cd859ea729e630d9491a7,1659312760,"[SPARK-39857][SQL][TESTS][FOLLOW-UP] Make ""translate complex expression"" pass with ANSI mode on

### What changes were proposed in this pull request?

This PR fixes `translate complex expression` to pass with ANSI mode on. We do push `Abs` with ANSI mode on (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala#L93):

```
[info] - translate complex expression *** FAILED *** (22 milliseconds)
[info]   Expected None, but got Some((ABS(cint) - 2) <= 1) (DataSourceV2StrategySuite.scala:325)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1563)
[info]   at org.scalatest.Assertions.assertResult(Assertions.scala:867)
[info]   at org.scalatest.Assertions.assertResult$(Assertions.scala:863)
[info]   at org.scalatest.funsuite.AnyFunSuite.assertResult(AnyFunSuite.scala:1563)
[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.testTranslateFilter(DataSourceV2StrategySuite.scala:325)
[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.$anonfun$new$4(DataSourceV2StrategySuite.scala:176)
[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.$anonfun$new$4$adapted(DataSourceV2StrategySuite.scala:170)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.$anonfun$new$3(DataSourceV2StrategySuite.scala:170)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:204)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:65)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:65)
```

https://github.com/apache/spark/runs/7595362617?check_suite_focus=true

### Why are the changes needed?

To make the build pass with ANSI mode on.

### Does this PR introduce _any_ user-facing change?

No, test-only.

### How was this patch tested?

Manually ran the unittest with ANSI mode on.

Closes #37349 from HyukjinKwon/SPARK-39857-followup.

Lead-authored-by: Hyukjin Kwon <gurwls223@apache.org>
Co-authored-by: Hyukjin Kwon <gurwls223@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala'],"'Translate complex expression' test in DataSourceV2StrategySuite fails when run with ANSI mode on, potentially causing build failures."
a7c61c100b6e4380e8d0e588969dd7f2fd58d40c,1497226984,"[SPARK-21031][SQL] Add `alterTableStats` to store spark's stats and let `alterTable` keep existing stats

## What changes were proposed in this pull request?

Currently, hive's stats are read into `CatalogStatistics`, while spark's stats are also persisted through `CatalogStatistics`. As a result, hive's stats can be unexpectedly propagated into spark' stats.

For example, for a catalog table, we read stats from hive, e.g. ""totalSize"" and put it into `CatalogStatistics`. Then, by using ""ALTER TABLE"" command, we will store the stats in `CatalogStatistics` into metastore as spark's stats (because we don't know whether it's from spark or not). But spark's stats should be only generated by ""ANALYZE"" command. This is unexpected from this command.

Secondly, now that we have spark's stats in metastore, after inserting new data, although hive updated ""totalSize"" in metastore, we still cannot get the right `sizeInBytes` in `CatalogStatistics`, because we respect spark's stats (should not exist) over hive's stats.

A running example is shown in [JIRA](https://issues.apache.org/jira/browse/SPARK-21031).

To fix this, we add a new method `alterTableStats` to store spark's stats, and let `alterTable` keep existing stats.

## How was this patch tested?

Added new tests.

Author: Zhenhua Wang <wzh_zju@163.com>

Closes #18248 from wzhfy/separateHiveStats.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalog.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/InMemoryCatalog.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala']",Hive's statistics are being unintentionally propagated into Spark's stats due to indistinguishable storage in `CatalogStatistics`; post data insert manifests faulty `sizeInBytes` in `CatalogStatistics` due to precedence of Spark's stats (which should not exist) over Hive's stats.
5b5eb23c0f3c709595f15686ae3701d576c82561,1666868050,"[SPARK-40922][PYTHON] Document multiple path support in `pyspark.pandas.read_csv`

### What changes were proposed in this pull request?

as discussed in https://issues.apache.org/jira/browse/SPARK-40922:

> The path argument of `pyspark.pandas.read_csv(path, ...)` currently has type annotation `str` and is documented as
>
>       path : str
>           The path string storing the CSV file to be read.
>The implementation however uses `pyspark.sql.DataFrameReader.csv(path, ...)` which does support multiple paths:
>
>        path : str or list
>            string, or list of strings, for input path(s),
>            or RDD of Strings storing CSV rows.
>

This PR updates the type annotation and documentation of `path` argument of `pyspark.pandas.read_csv`

### Why are the changes needed?

Loading multiple CSV files at once is a useful feature to have and should be documented

### Does this PR introduce _any_ user-facing change?
it documents and existing feature

### How was this patch tested?
No need for tests (so far): only type annotations and docblocks were changed

Closes #38399 from soxofaan/SPARK-40922-pyspark-pandas-read-csv-multiple-paths.

Authored-by: Stefaan Lippens <stefaan.lippens@vito.be>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['python/pyspark/pandas/namespace.py'],"The `pyspark.pandas.read_csv(path, ...)` function is documented and typed to only accept a single path, but the underlying function actually supports multiple paths."
09bebc8bdef49a9c35bd5e5edf9d513dfa6c9557,1626919956,"[SPARK-35912][SQL] Fix nullability of `spark.read.json/spark.read.csv`

### What changes were proposed in this pull request?

Rework [PR](https://github.com/apache/spark/pull/33212) with suggestions.

This PR make `spark.read.json()` has the same behavior with Datasource API `spark.read.format(""json"").load(""path"")`. Spark should turn a non-nullable schema into nullable when using API `spark.read.json()` by default.

Here is an example:

```scala
  val schema = StructType(Seq(StructField(""value"",
    StructType(Seq(
      StructField(""x"", IntegerType, nullable = false),
      StructField(""y"", IntegerType, nullable = false)
    )),
    nullable = true
  )))

  val testDS = Seq(""""""{""value"":{""x"":1}}"""""").toDS
  spark.read
    .schema(schema)
    .json(testDS)
    .printSchema()

  spark.read
    .schema(schema)
    .format(""json"")
    .load(""/tmp/json/t1"")
    .printSchema()
  // root
  //  |-- value: struct (nullable = true)
  //  |    |-- x: integer (nullable = true)
  //  |    |-- y: integer (nullable = true)
```

Before this pr:
```
// output of spark.read.json()
root
 |-- value: struct (nullable = true)
 |    |-- x: integer (nullable = false)
 |    |-- y: integer (nullable = false)
```

After this pr:
```
// output of spark.read.json()
root
 |-- value: struct (nullable = true)
 |    |-- x: integer (nullable = true)
 |    |-- y: integer (nullable = true)
```

- `spark.read.csv()` also has the same problem.
- Datasource API `spark.read.format(""json"").load(""path"")` do this logical when resolve relation.

https://github.com/apache/spark/blob/c77acf0bbc25341de2636649fdd76f9bb4bdf4ed/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L415-L421

### Does this PR introduce _any_ user-facing change?

Yes, `spark.read.json()` and `spark.read.csv()` not respect the user-given schema and always turn it into a nullable schema by default.

### How was this patch tested?

New test.

Closes #33436 from cfmcgrady/SPARK-35912-v3.

Authored-by: Fu Chen <cfmcgrady@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala']",Discrepancy in the nullability of schemas when using `spark.read.json()` and `spark.read.csv()`. The given non-nullable schema is incorrectly being transformed to a nullable one.
3022fd4ccfed676d4ba194afbfde2dd5ec1d348f,1645149136,"[SPARK-38197][CORE] Improve error message of BlockManager.fetchRemoteManagedBuffer

### What changes were proposed in this pull request?
When locations's size is 1, and fetch failed, it only will print a error message like
```
22/02/13 18:58:11 WARN BlockManager: Failed to fetch block after 1 fetch failures. Most recent failure cause:
java.lang.IllegalStateException: Empty buffer received for non empty block
	at org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1063)
	at org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1005)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1005)
	at org.apache.spark.storage.BlockManager.getRemoteValues(BlockManager.scala:951)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:1168)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1230)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```

We don't know the target nm ip and block id. This pr improve the error message to show necessary information

### Why are the changes needed?
Improve error message

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Not need

Closes #35505 from AngersZhuuuu/SPARK-38197.

Authored-by: Angerszhuuuu <angers.zhu@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
",['core/src/main/scala/org/apache/spark/storage/BlockManager.scala'],"When fetch operation fails, the current error logging does not provide sufficient information such as target node manager IP and block ID, making troubleshooting difficult."
89666d44a39c48df841a0102ff6f54eaeb4c6140,1672975848,"[SPARK-41912][SQL] Subquery should not validate CTE

### What changes were proposed in this pull request?

The commit https://github.com/apache/spark/pull/38029 actually intended to do the right thing: it checks CTE more aggressively even if a CTE is not used, which is ok. However, it triggers an existing issue where a subquery checks itself but in the CTE case if the subquery contains a CTE which is defined outside of the subquery, the check will fail as CTE not found (e.g. key not found).

So it is:

the commit checks more thus in the repro examples, every CTE is checked now (in the past only used CTE is checked).

One of the CTE that is checked after the commit in the example contains subquery.

The subquery contains another CTE which is defined outside of the subquery.

The subquery checks itself thus fail due to CTE not found.

This PR fixes the issue by removing the subquery self-validation on CTE case.

### Why are the changes needed?

This fixed a regression that
```
    val df = sql(""""""
                   |    WITH
                   |    cte1 as (SELECT 1 col1),
                   |    cte2 as (SELECT (SELECT MAX(col1) FROM cte1))
                   |    SELECT * FROM cte1
                   |"""""".stripMargin
    )
    checkAnswer(df, Row(1) :: Nil)
```

cannot pass analyzer anymore.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

UT

Closes #39414 from amaliujia/fix_subquery_validate.

Authored-by: Rui Wang <rui.wang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala']",Subquery validation issue: Subquery self-validates and fails when it contains a CTE (Common Table Expression) that is defined outside of the subquery.
71183b283343a99c6fa99a41268dae412598067f,1546881350,"[SPARK-24489][ML] Check for invalid input type of weight data in ml.PowerIterationClustering

## What changes were proposed in this pull request?
The test case will result the following failure. currently in ml.PIC, there is no check for the data type of weight column.
 ```
 test(""invalid input types for weight"") {
    val invalidWeightData = spark.createDataFrame(Seq(
      (0L, 1L, ""a""),
      (2L, 3L, ""b"")
    )).toDF(""src"", ""dst"", ""weight"")

    val pic = new PowerIterationClustering()
      .setWeightCol(""weight"")

    val result = pic.assignClusters(invalidWeightData)
  }
```
```
Job aborted due to stage failure: Task 0 in stage 8077.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8077.0 (TID 882, localhost, executor driver): scala.MatchError: [0,1,null] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)
	at org.apache.spark.ml.clustering.PowerIterationClustering$$anonfun$3.apply(PowerIterationClustering.scala:178)
	at org.apache.spark.ml.clustering.PowerIterationClustering$$anonfun$3.apply(PowerIterationClustering.scala:178)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:107)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847)
```
In this PR, added check types for weight column.
## How was this patch tested?
UT added

Please review http://spark.apache.org/contributing.html before opening a pull request.

Closes #21509 from shahidki31/testCasePic.

Authored-by: Shahid <shahidki31@gmail.com>
Signed-off-by: Holden Karau <holden@pigscanfly.ca>
","['mllib/src/main/scala/org/apache/spark/ml/clustering/PowerIterationClustering.scala', 'mllib/src/test/scala/org/apache/spark/ml/clustering/PowerIterationClusteringSuite.scala']","The PowerIterationClustering in Spark ML doesn't validate the data type of the weight column, causing a MatchError when the type is invalid."
1f1d79649027a4c03e48dea2bcef280dca53767a,1655746522,"[SPARK-39497][SQL] Improve the analysis exception of missing map key column

### What changes were proposed in this pull request?

Sometimes users forgot to add single quotes on the map key string literal, for example `map_col[a]`. In such a case, the Analyzer will throw an exception:
```
[MISSING_COLUMN] Column 'struct.a' does not exist. Did you mean one of the following? ...
```
We can improve this message by saying that the user should append single quotes if the map key is a string literal.

```
[UNRESOLVED_MAP_KEY] Cannot resolve column 'a' as a map key. If the key is a string literal, please add single quotes around it. Otherwise, did you mean one of the following column(s)? ...
```

### Why are the changes needed?

Error message improvement

### Does this PR introduce _any_ user-facing change?

Yes but trivial, an improvement on the error message of unresolved map key column

### How was this patch tested?

New UT

Closes #36896 from gengliangwang/unreslovedMapKey.

Authored-by: Gengliang Wang <gengliang@apache.org>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala']","When users forget to add single quotes on a map key string literal (i.e., map_col[a]) in Spark SQL, an unclear analysis exception is thrown indicating missing column."
dfd7b026dc7c3c38bef9afab82852aff902a25d2,1624388641,"[SPARK-35800][SS] Improving GroupState testability by introducing TestGroupState

### What changes were proposed in this pull request?
Proposed changes in this pull request:

1. Introducing the `TestGroupState` interface which is inherited from `GroupState` so that testing related getters can be exposed in a controlled manner
2. Changing `GroupStateImpl` to inherit from `TestGroupState` interface, instead of directly from `GroupState`
3. Implementing `TestGroupState` object with `create()` method to forward inputs to the private `GroupStateImpl` constructor
4. User input validations have been added into `GroupStateImpl`'s `createForStreaming()` method to prevent users from creating invalid GroupState objects.
5. Replacing existing `GroupStateImpl` usages in sql pkg internal unit tests with the newly added `TestGroupState` to give user best practice about `TestGroupState` usage.

With the changes in this PR, the class hierarchy is changed from `GroupStateImpl` -> `GroupState` to `GroupStateImpl` -> `TestGroupState` -> `GroupState` (-> means inherits from)

### Why are the changes needed?
The internal `GroupStateImpl` implementation for the `GroupState` interface has no public constructors accessible outside of the sql pkg. However, the user-provided state transition function for `[map|flatMap]GroupsWithState` requires a `GroupState` object as the prevState input.

Currently, users are calling the Structured Streaming engine in their unit tests in order to instantiate such `GroupState` instances, which makes UTs cumbersome.

The proposed `TestGroupState` interface is to give users controlled access to the `GroupStateImpl` internal implementation to largely improve testability of Structured Streaming state transition functions.

**Usage Example**
```
import org.apache.spark.sql.streaming.TestGroupState

test(“Structured Streaming state update function”) {
  var prevState = TestGroupState.create[UserStatus](
    optionalState = Optional.empty[UserStatus],
    timeoutConf = EventTimeTimeout,
    batchProcessingTimeMs = 1L,
    eventTimeWatermarkMs = Optional.of(1L),
    hasTimedOut = false)

  val userId: String = ...
  val actions: Iterator[UserAction] = ...

  assert(!prevState.hasUpdated)

  updateState(userId, actions, prevState)

  assert(prevState.hasUpdated)
}
```

### Does this PR introduce _any_ user-facing change?
Yes, the `TestGroupState` interface and its corresponding `create()` factory function in its companion object are introduced in this pull request for users to use in unit tests.

### How was this patch tested?
- New unit tests are added
- Existing GroupState unit tests are updated

Closes #32938 from lizhangdatabricks/improve-group-state-testability.

Authored-by: Li Zhang <li.zhang@databricks.com>
Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/FlatMapGroupsWithStateExecHelper.scala', 'sql/core/src/main/scala/org/apache/spark/sql/streaming/TestGroupState.scala', 'sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala']","Inability to create `GroupState` instances for user-provided state transition functions in unit tests due to missing public constructors in `GroupStateImpl`, resulting in cumbersome testing process."
0975019cd475f990585b1b436d25373501cfadf7,1496310162,"[SPARK-20109][MLLIB] Rewrote toBlockMatrix method on IndexedRowMatrix

## What changes were proposed in this pull request?

- ~~I added the method `toBlockMatrixDense` to the IndexedRowMatrix class. The current implementation of `toBlockMatrix` is insufficient for users with relatively dense IndexedRowMatrix objects, since it assumes sparsity.~~

EDIT: Ended up deciding that there should be just a single `toBlockMatrix` method, which creates a BlockMatrix whose blocks may be dense or sparse depending on the sparsity of the rows. This method will work better on any current use case of `toBlockMatrix` and doesn't go through `CoordinateMatrix` like the old method.

## How was this patch tested?

~~I used the same tests already written for `toBlockMatrix()` to test this method. I also added a new additional unit test for an edge case that was not adequately tested by current test suite.~~

I ran the original `IndexedRowMatrix` tests, plus wrote more to better handle edge cases ignored by original tests.

Author: John Compitello <johnc@broadinstitute.org>

Closes #17459 from johnc1231/johnc-fix-ir-to-block.
","['mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/CoordinateMatrix.scala', 'mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/IndexedRowMatrix.scala', 'mllib/src/test/scala/org/apache/spark/mllib/linalg/distributed/IndexedRowMatrixSuite.scala']","Current `toBlockMatrix` method on IndexedRowMatrix is inefficient for relatively dense matrices, as it's optimized for sparse data and it unnecessarily goes through `CoordinateMatrix`."
710120a499d6082bcec6b65ad1f8dbe4789f4bd9,1638352642,"[SPARK-37508][SQL] Add CONTAINS() string function

### What changes were proposed in this pull request?
Add `CONTAINS` string function.

| function| arguments | Returns |
|-------|-------|-------|
| CONTAINS( left , right) | left: String, right: String | Returns a BOOLEAN. The value is True if right is found inside left. Returns NULL if either input expression is NULL. Otherwise, returns False.|

### Why are the changes needed?
contains() is a common convenient function supported by a number of database systems:

- https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#contains_substr
- https://docs.snowflake.com/en/sql-reference/functions/contains.html

Support of the function can make the migration from other systems to Spark SQL easier.

### Does this PR introduce _any_ user-facing change?
User can use `contains(left, right)`:

| Left   |      Right      |  Return |
|----------|:-------------:|------:|
| null |  ""Spark SQL"" | null |
| ""Spark SQL"" |  null | null |
| null |  null | null |
| ""Spark SQL"" |  ""Spark"" | true |
| ""Spark SQL"" |  ""k SQL"" | true |
| ""Spark SQL"" | ""SPARK"" | false |

### How was this patch tested?
Added UT

Closes #34761 from AngersZhuuuu/SPARK-37508.

Authored-by: Angerszhuuuu <angers.zhu@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/string-functions.sql']","Spark SQL lacks a `CONTAINS()` string function which is a common feature supported by many other database systems, making migration to Spark SQL difficult."
6484992535767ae8dc93df1c79efc66420728155,1664574760,"[SPARK-40612][CORE] Fixing the principal used for delegation token renewal on non-YARN resource managers

### What changes were proposed in this pull request?

When the delegation token is fetched at the first time (see the `fetchDelegationTokens()` call at `HadoopFSDelegationTokenProvider#getTokenRenewalInterval()`) the principal is the current user but at the subsequent token renewals (see `obtainDelegationTokens()` where `getTokenRenewer()` is used to identify the principal) are using a MapReduce/Yarn specific principal even on resource managers different from YARN.

This PR fixes `getTokenRenewer()` to use the current user instead of `org.apache.hadoop.mapred.Master.getMasterPrincipal(hadoopConf)` when the resource manager is not YARN.
The condition `(master != null && master.contains(""yarn""))` is the very same what we already have in `hadoopFSsToAccess()`.

I would like to say thank you for squito who have done the investigation regarding of the problem which lead to this PR.

### Why are the changes needed?

To avoid `org.apache.hadoop.security.AccessControlException: Permission denied.` for long running applications.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Manually.

Closes #38048 from attilapiros/SPARK-40612.

Authored-by: attilapiros <piros.attila.zsolt@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala'],"Non-YARN resource managers are utilizing a MapReduce/Yarn specific principal for delegation token renewal rather than the current user, causing 'Permission denied' AccessControlExceptions for long-running applications."
7e7bc940dcbbf918c7d571e1d27c7654ad387817,1670820547,"[SPARK-41187][CORE] LiveExecutor MemoryLeak in AppStatusListener when ExecutorLost happen

### What changes were proposed in this pull request?
Ignore the SparkListenerTaskEnd with Reason ""Resubmitted"" in AppStatusListener to avoid memory leak

### Why are the changes needed?
For a long running spark thriftserver,  LiveExecutor will be accumulated in the deadExecutors HashMap and cause message event queue processing slowly.
For a every task, actually always sent out a `SparkListenerTaskStart` event and a `SparkListenerTaskEnd` event, they are always pairs. But in a executor lost situation, it send out event like following steps.

a) There was a pair of task start and task end event which were fired for the task (let us call it Tr)
b) When executor which ran Tr was lost, while stage is still running, a task end event with reason `Resubmitted`  is fired for Tr.
c) Subsequently, a new task start and task end will be fired for the retry of Tr.

The processing of the `Resubmitted` task end event in AppStatusListener can lead to negative `LiveStage.activeTasks` since there's no corresponding `SparkListenerTaskStart` event for each of them. The negative activeTasks will make the stage always remains in the live stage list as it can never meet the condition activeTasks == 0. This in turn causes the dead executor to never be cleaned up if that live stage's submissionTime is less than the dead executor's removeTime( see isExecutorActiveForLiveStages). Since this kind of `SparkListenerTaskEnd` is useless here, we simply ignore it.

Check  [SPARK-41187](https://issues.apache.org/jira/browse/SPARK-41187) for evidences.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
New UT Added
Test in thriftserver env

### The way to reproduce
I try to reproduce it in spark shell, but it is a little bit handy
1.  start spark-shell , set spark.dynamicAllocation.maxExecutors=2 for convient
` bin/spark-shell  --driver-java-options ""-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8006""`
2. run a job with shuffle
 `sc.parallelize(1 to 1000, 10).map { x => Thread.sleep(1000) ; (x % 3, x) }.reduceByKey((a, b) => a + b).collect()`
3. After some ShuffleMapTask finished, kill one or two executor to let tasks resubmitted
4. check by heap dump or debug or log

Closes #38702 from wineternity/SPARK-41187.

Authored-by: yuanyimeng <yuanyimeng@youzan.com>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['core/src/main/scala/org/apache/spark/status/AppStatusListener.scala', 'core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala']","In LiveExecutor, a memory leak happens in AppStatusListener when executor is lost, causing the dead executor to not be cleaned up and the message event queue processing to slow down."
1e197b5bc2258f9c0657cf9d792005c540ccb7f4,1686103609,"[SPARK-43356][K8S] Migrate deprecated createOrReplace to serverSideApply

### What changes were proposed in this pull request?

The deprecation message of `createOrReplace` indicates that we should change `createOrReplace` to `serverSideApply` instead.

```
deprecated please use {link ServerSideApplicable#serverSideApply()} or attempt a create and edit/patch operation.
```

The change is not fully equivalent, but I believe it's reasonable.

> With the caveat that the user may choose not to use forcing if they want to know when there are conflicting changes.
>
> Also unlike createOrReplace if the resourceVersion is set on the resource and a replace is attempted, it will be optimistically locked.

See more details at https://github.com/fabric8io/kubernetes-client/pull/5073

### Why are the changes needed?

Remove usage of deprecated API.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass GA.

Closes #41136 from pan3793/SPARK-43356.

Authored-by: Cheng Pan <chengpan@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala']",The use of deprecated `createOrReplace` method in K8S might lead to issues due to optimistic locking when the resourceVersion is set on the resource and a replace is attempted.
61abae36eaccfa0ccb6bd2916e28164a96207e34,1642569795,"[SPARK-37917][SQL] Push down limit 1 for right side of left semi/anti join if join condition is empty

### What changes were proposed in this pull request?

It is safe to push down the limit 1 for the right side of left semi/anti join if the join condition is empty, since we only care if the right side is empty. For example:
```scala
val numRows = 1024 * 1024 * 40

spark.sql(s""CREATE TABLE t1 using parquet AS SELECT id AS a, id AS b, id AS c FROM range(1, ${numRows}L, 1, 5)"")
spark.sql(s""CREATE TABLE t2 using parquet AS SELECT id AS a, id AS b, id AS c FROM range(1, ${numRows}L, 1, 5)"")

spark.sql(""SELECT * FROM t1 LEFT SEMI JOIN t2 LIMIT 5"").explain(true)
```

Before this pr:
```
== Optimized Logical Plan ==
GlobalLimit 5
+- LocalLimit 5
   +- Join LeftSemi
      :- LocalLimit 5
      :  +- Relation default.t1[a#8L,b#9L,c#10L] parquet
      +- Project
         +- Relation default.t2[a#11L,b#12L,c#13L] parquet
```

After this pr:
```
== Optimized Logical Plan ==
GlobalLimit 5
+- LocalLimit 5
   +- Join LeftSemi
      :- LocalLimit 5
      :  +- Relation default.t1[a#8L,b#9L,c#10L] parquet
      +- LocalLimit 1
         +- Project
            +- Relation default.t2[a#11L,b#12L,c#13L] parquet
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #35216 from wangyum/SPARK-37917.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownSuite.scala']","Pushing down limit 1 for the right side of left semi/anti join with an empty join condition doesn't work properly, leading to a decrease in query performance."
bd37b10c7568b0fe07c98da5a532c5ff84d702f6,1670580372,"[SPARK-41377][BUILD] Fix spark-version-info.properties not found on Windows

### What changes were proposed in this pull request?

This PR enhances the Maven build configuration to automatically detect and switch between using Powershell for Windows and Bash for non-Windows OS to generate `spark-version-info.properties` file.

### Why are the changes needed?

While building Spark, the `spark-version-info.properties` file [is generated using bash](https://github.com/apache/spark/blob/d62c18b7497997188ec587e1eb62e75c979c1c93/core/pom.xml#L560-L564). In Windows environment, if Windows Subsystem for Linux (WSL) is installed, it somehow overrides the other bash executables in the PATH, as noted in SPARK-40739. The bash in WSL has a different mounting configuration and thus, [the target location specified for spark-version-info.properties](https://github.com/apache/spark/blob/d62c18b7497997188ec587e1eb62e75c979c1c93/core/pom.xml#L561-L562) won't be the expected location. Ultimately, this leads to `spark-version-info.properties` to get excluded from the spark-core jar, thus causing the SparkContext initialization to fail with the above depicted error message.

This PR fixes the issue by directing the build system to use the right shell according to the platform.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

I tested this by building on a Windows 10 PC.

```psh
mvn -Pyarn '-Dhadoop.version=3.3.0' -DskipTests clean package
```

Once the build finished, I verified that `spark-version-info.properties` file was included in the spark-core jar.

![image](https://user-images.githubusercontent.com/10280768/205497898-80e53617-c991-460e-b04a-a3bdd4f298ae.png)

I also ran the SparkPi application and verified that it ran successfully without any errors.

![image](https://user-images.githubusercontent.com/10280768/205499567-f6e8e10a-dcbb-45fb-b282-fc29ba58adee.png)

Closes #38903 from GauthamBanasandra/spark-version-info-ps.

Authored-by: Gautham Banasandra <gautham.bangalore@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['build/spark-build-info.ps1'],"The `spark-version-info.properties` file doesn't get correctly generated on the Windows environment due to the path conflicts with Windows Subsystem for Linux (WSL), causing SparkContext initialization to fail."
04f04e0ea74fd26a7bd39e833752ce277997f4dc,1586845380,"[SPARK-31420][WEBUI] Infinite timeline redraw in job details page

### What changes were proposed in this pull request?

Upgrade vis.js to fix an infinite re-drawing issue.

As reported here, old releases of vis.js have that issue.
Fortunately, the latest version seems to resolve the issue.

With the latest release of vis.js, there are some performance issues with the original `timeline-view.js` and `timeline-view.css` so I also changed them.

### Why are the changes needed?

For better UX.

### Does this PR introduce any user-facing change?

No. Appearance and functionalities are not changed.

### How was this patch tested?

I confirmed infinite redrawing doesn't happen with a JobPage which I had reproduced the issue.

With the original version of vis.js, I reproduced the issue with the following conditions.

* Use history server and load core/src/test/resources/spark-events.
* Visit the JobPage for job2 in application_1553914137147_0018.
* Zoom out to 80% on Safari / Chrome / Firefox.

Maybe, it depends on OS and the version of browsers.

Closes #28192 from sarutak/upgrade-visjs.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['core/src/main/resources/org/apache/spark/ui/static/timeline-view.js', 'core/src/main/resources/org/apache/spark/ui/static/vis-timeline-graph2d.min.js', 'core/src/main/resources/org/apache/spark/ui/static/vis.min.js', 'core/src/main/scala/org/apache/spark/ui/UIUtils.scala', 'core/src/test/scala/org/apache/spark/ui/UISeleniumSuite.scala']","The Job Details page experiences an issue with infinite timeline redraws when zoomed out to 80% on Safari/Chrome/Firefox in certain operating systems, affecting user interaction."
57d27e900f79e6c5699b9a23db236aae98e761ad,1584471846,"[SPARK-31125][K8S] Terminating pods have a deletion timestamp but they are not yet dead

### What changes were proposed in this pull request?

Change what we consider a deleted pod to not include ""Terminating""

### Why are the changes needed?

If we get a new snapshot while a pod is in the process of being cleaned up we shouldn't delete the executor until it is fully terminated.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

This should be covered by the decommissioning tests in that they currently are flaky because we sometimes delete the executor instead of allowing it to decommission all the way.

I also ran this in a loop locally ~80 times with the only failures being the PV suite because of unrelated minikube mount issues.

Closes #27905 from holdenk/SPARK-31125-Processing-state-snapshots-incorrect.

Authored-by: Holden Karau <hkarau@apple.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodStates.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsSnapshot.scala']","When a new snapshot is obtained while a pod is being cleaned up, the executor may incorrectly get deleted before it's fully terminated."
3722ed430ddef4a4c73f8bae73b21f2097fdc33a,1597864294,"[SPARK-32655][K8S] Support appId/execId placeholder in K8s SPARK_EXECUTOR_DIRS

### What changes were proposed in this pull request?

This PR aims to support replacements of `SPARK_APPLICATION_ID`/`SPARK_EXECUTOR_ID` in `SPARK_EXECUTOR_DIRS ` executor environment.

### Why are the changes needed?

This PR provides users additional controllability.

**HOW TO RUN**
```
bin/spark-submit --master k8s://https://kubernetes.docker.internal:6443 --deploy-mode cluster \
-c spark.kubernetes.container.image=spark:SPARK-32655 \
-c spark.kubernetes.driver.pod.name=pi \
-c spark.kubernetes.executor.podNamePrefix=pi \
-c spark.kubernetes.executor.volumes.nfs.data.mount.path=/efs \
-c spark.kubernetes.executor.volumes.nfs.data.mount.readOnly=false \
-c spark.kubernetes.executor.volumes.nfs.data.options.server=efs-server-ip \
-c spark.kubernetes.executor.volumes.nfs.data.options.path=/ \
-c spark.executorEnv.SPARK_EXECUTOR_DIRS=/efs/SPARK_APPLICATION_ID/SPARK_EXECUTOR_ID \
--class org.apache.spark.examples.SparkPi \
local:///opt/spark/examples/jars/spark-examples_2.12-3.1.0-SNAPSHOT.jar 20000
```

**EFS Layout**
```
/efs
├── spark-f45039b13b0b4fd4baf80fed561a2228
│   ├── 1
│   │   ├── blockmgr-bbe76578-8ff2-4c2d-ab4f-37671d886f56
│   │   │   ├── 0e
│   │   │   └── 11
│   │   └── spark-e41aeb41-00fc-49e1-a77d-093b6df5958a
│   │       ├── -18375678081597852666997_cache
│   │       └── -18375678081597852666997_lock
│   └── 2
│       ├── blockmgr-765bfb50-ab13-4b2b-9350-356fed0169e3
│       │   ├── 0e
│       │   └── 11
│       └── spark-737671fc-1697-4367-9daf-2b1575f92aba
│           ├── -18375678081597852666997_cache
│           └── -18375678081597852666997_lock
```

### Does this PR introduce _any_ user-facing change?

- Yes because this is a new feature.
- This will not affect the existing jobs because users don't use the string pattern `SPARK_APPLICATION_ID` or `SPARK_EXECUTOR_ID` inside `SPARK_EXECUTOR_DIRS` environment variable.

### How was this patch tested?

Pass the newly added test case.

Closes #29472 from dongjoon-hyun/SPARK-32655.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala']","In Spark submitted over Kubernetes, `SPARK_APPLICATION_ID` or `SPARK_EXECUTOR_ID` are not supported within `SPARK_EXECUTOR_DIRS` executor environment variable, limiting user control."
960af63913613ed7104cd76e477e325bd3020163,1533710760,"[SPARK-25036][SQL] avoid match may not be exhaustive in Scala-2.12

## What changes were proposed in this pull request?

The PR remove the following compilation error using scala-2.12 with sbt by adding a default case to `match`.

```
/home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/ValueInterval.scala:63: match may not be exhaustive.
[error] It would fail on the following inputs: (NumericValueInterval(_, _), _), (_, NumericValueInterval(_, _)), (_, _)
[error] [warn]   def isIntersected(r1: ValueInterval, r2: ValueInterval): Boolean = (r1, r2) match {
[error] [warn]
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/ValueInterval.scala:79: match may not be exhaustive.
[error] It would fail on the following inputs: (NumericValueInterval(_, _), _), (_, NumericValueInterval(_, _)), (_, _)
[error] [warn]     (r1, r2) match {
[error] [warn]
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproxCountDistinctForIntervals.scala:67: match may not be exhaustive.
[error] It would fail on the following inputs: (ArrayType(_, _), _), (_, ArrayData()), (_, _)
[error] [warn]     (endpointsExpression.dataType, endpointsExpression.eval()) match {
[error] [warn]
[error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala:470: match may not be exhaustive.
[error] It would fail on the following inputs: NewFunctionSpec(_, None, Some(_)), NewFunctionSpec(_, Some(_), None)
[error] [warn]     newFunction match {
[error] [warn]
[error] [warn] [error] [warn] /home/ishizaki/Spark/PR/scala212/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala:709: match may not be exhaustive.
[error] It would fail on the following input: Schema((x: org.apache.spark.sql.types.DataType forSome x not in org.apache.spark.sql.types.StructType), _)
[error] [warn]   def attributesFor[T: TypeTag]: Seq[Attribute] = schemaFor[T] match {
[error] [warn]
```

## How was this patch tested?

Existing UTs with Scala-2.11.
Manually build with Scala-2.12

Closes #22014 from kiszk/SPARK-25036b.

Authored-by: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
Signed-off-by: hyukjinkwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproxCountDistinctForIntervals.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/ValueInterval.scala']","Match case statements in multiple Scala modules are not exhaustive, leading to potential build errors when building with Scala-2.12."
99ea324b6f22e979d2b4238eef0effa3709d03bd,1576056389,"[SPARK-27506][SQL] Allow deserialization of Avro data using compatible schemas

Follow up of https://github.com/apache/spark/pull/24405

### What changes were proposed in this pull request?
The current implementation of _from_avro_ and _AvroDataToCatalyst_ doesn't allow doing schema evolution since it requires the deserialization of an Avro record with the exact same schema with which it was serialized.

The proposed change is to add a new option `actualSchema` to allow passing the schema used to serialize the records. This allows using a different compatible schema for reading by passing both schemas to _GenericDatumReader_. If no writer's schema is provided, nothing changes from before.

### Why are the changes needed?
Consider the following example.

```
// schema ID: 1
val schema1 = """"""
{
    ""type"": ""record"",
    ""name"": ""MySchema"",
    ""fields"": [
        {""name"": ""col1"", ""type"": ""int""},
        {""name"": ""col2"", ""type"": ""string""}
     ]
}
""""""

// schema ID: 2
val schema2 = """"""
{
    ""type"": ""record"",
    ""name"": ""MySchema"",
    ""fields"": [
        {""name"": ""col1"", ""type"": ""int""},
        {""name"": ""col2"", ""type"": ""string""},
        {""name"": ""col3"", ""type"": ""string"", ""default"": """"}
     ]
}
""""""
```

The two schemas are compatible - i.e. you can use `schema2` to deserialize events serialized with `schema1`, in which case there will be the field `col3` with the default value.

Now imagine that you have two dataframes (read from batch or streaming), one with Avro events from schema1 and the other with events from schema2. **We want to combine them into one dataframe** for storing or further processing.

With the current `from_avro` function we can only decode each of them with the corresponding schema:

```
scalaval df1 = ... // Avro events created with schema1
df1: org.apache.spark.sql.DataFrame = [eventBytes: binary]
scalaval decodedDf1 = df1.select(from_avro('eventBytes, schema1) as ""decoded"")
decodedDf1: org.apache.spark.sql.DataFrame = [decoded: struct<col1: int, col2: string>]

scalaval df2= ... // Avro events created with schema2
df2: org.apache.spark.sql.DataFrame = [eventBytes: binary]
scalaval decodedDf2 = df2.select(from_avro('eventBytes, schema2) as ""decoded"")
decodedDf2: org.apache.spark.sql.DataFrame = [decoded: struct<col1: int, col2: string, col3: string>]
```

but then `decodedDf1` and `decodedDf2` have different Spark schemas and we can't union them. Instead, with the proposed change we can decode `df1` in the following way:

```
scalaimport scala.collection.JavaConverters._
scalaval decodedDf1 = df1.select(from_avro(data = 'eventBytes, jsonFormatSchema = schema2, options = Map(""actualSchema"" -> schema1).asJava) as ""decoded"")
decodedDf1: org.apache.spark.sql.DataFrame = [decoded: struct<col1: int, col2: string, col3: string>]
```

so that both dataframes have the same schemas and can be merged.

### Does this PR introduce any user-facing change?
This PR allows users to pass a new configuration but it doesn't affect current code.

### How was this patch tested?
A new unit test was added.

Closes #26780 from Fokko/SPARK-27506.

Lead-authored-by: Fokko Driesprong <fokko@apache.org>
Co-authored-by: Gianluca Amori <gianluca.amori@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala', 'external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala', 'external/avro/src/main/scala/org/apache/spark/sql/avro/functions.scala', 'external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala', 'python/pyspark/sql/avro/functions.py']","The current implementation of _from_avro_ and _AvroDataToCatalyst_ doesn't allow schema evolution, it mandates Avro data to have the exact same schema they were serialized with, which is limiting and doesn't accommodate the usage of an updated, but compatible schemas for deserialization, thus hindering data combining from different dataframes."
ef617755868077dbc57de4e7edea8f01335f5556,1500497608,"[SPARK-21243][Core] Limit no. of map outputs in a shuffle fetch

## What changes were proposed in this pull request?
For configurations with external shuffle enabled, we have observed that if a very large no. of blocks are being fetched from a remote host, it puts the NM under extra pressure and can crash it. This change introduces a configuration `spark.reducer.maxBlocksInFlightPerAddress` , to limit the no. of map outputs being fetched from a given remote address. The changes applied here are applicable for both the scenarios - when external shuffle is enabled as well as disabled.

## How was this patch tested?
Ran the job with the default configuration which does not change the existing behavior and ran it with few configurations of lower values -10,20,50,100. The job ran fine and there is no change in the output. (I will update the metrics related to NM in some time.)

Author: Dhruve Ashar <dhruveashar@gmail.com>

Closes #18487 from dhruve/impr/SPARK-21243.
","['core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala', 'core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala', 'core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala']",Fetching a large number of blocks from a remote host when external shuffle is enabled puts NodeManager under strain and can lead to crashes.
9c817a83fce4a2260f9da86b159d4c59632e974c,1572375652,"[SPARK-29637][CORE] Add description to Job SHS web API

### Why are the changes needed?
Starting from Spark 2.3, the SHS REST API endpoint `/applications/<app_id>/jobs/` is not including `description` in the JobData returned. This is not the case until Spark 2.2.

In this PR I've added the mentioned field.

### Does this PR introduce any user-facing change?
Yes.

Old API response:
```
[ {
  ""jobId"" : 0,
  ""name"" : ""foreach at <console>:26"",
  ""submissionTime"" : ""2019-10-28T12:41:54.301GMT"",
  ""completionTime"" : ""2019-10-28T12:41:54.731GMT"",
  ""stageIds"" : [ 0 ],
  ""jobGroup"" : ""test"",
  ""status"" : ""SUCCEEDED"",
  ""numTasks"" : 1,
  ""numActiveTasks"" : 0,
  ""numCompletedTasks"" : 1,
  ""numSkippedTasks"" : 0,
  ""numFailedTasks"" : 0,
  ""numKilledTasks"" : 0,
  ""numCompletedIndices"" : 1,
  ""numActiveStages"" : 0,
  ""numCompletedStages"" : 1,
  ""numSkippedStages"" : 0,
  ""numFailedStages"" : 0,
  ""killedTasksSummary"" : { }
} ]
```
New API response:
```
[ {
  ""jobId"" : 0,
  ""name"" : ""foreach at <console>:26"",
  ""description"" : ""job"",                            <= This is the addition here
  ""submissionTime"" : ""2019-10-28T13:37:24.107GMT"",
  ""completionTime"" : ""2019-10-28T13:37:24.613GMT"",
  ""stageIds"" : [ 0 ],
  ""jobGroup"" : ""test"",
  ""status"" : ""SUCCEEDED"",
  ""numTasks"" : 1,
  ""numActiveTasks"" : 0,
  ""numCompletedTasks"" : 1,
  ""numSkippedTasks"" : 0,
  ""numFailedTasks"" : 0,
  ""numKilledTasks"" : 0,
  ""numCompletedIndices"" : 1,
  ""numActiveStages"" : 0,
  ""numCompletedStages"" : 1,
  ""numSkippedStages"" : 0,
  ""numFailedStages"" : 0,
  ""killedTasksSummary"" : { }
} ]
```

### How was this patch tested?
Extended + existing unit tests.

Manually:
* Open spark-shell
```
scala> sc.setJobGroup(""test"", ""job"", false);
scala> val foo = sc.textFile(""/user/foo.txt"");
foo: org.apache.spark.rdd.RDD[String] = /user/foo.txt MapPartitionsRDD[1] at textFile at <console>:24
scala> foo.foreach(println);
```
* Access REST API `http://SHS-host:port/api/v1/applications/<app-id>/jobs/`

Closes #26295 from gaborgsomogyi/SPARK-29637.

Authored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['core/src/main/scala/org/apache/spark/status/AppStatusListener.scala', 'core/src/main/scala/org/apache/spark/status/LiveEntity.scala', 'core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala']",The REST API endpoint `/applications/<app_id>/jobs/` is not including the `description` field in the JobData returned from Spark 2.3 onwards.
d7b1fcf8f0a267322af0592b2cb31f1c8970fb16,1504042944,"[SPARK-21728][CORE] Allow SparkSubmit to use Logging.

This change initializes logging when SparkSubmit runs, using
a configuration that should avoid printing log messages as
much as possible with most configurations, and adds code to
restore the Spark logging system to as close as possible to
its initial state, so the Spark app being run can re-initialize
logging with its own configuration.

With that feature, some duplicate code in SparkSubmit can now
be replaced with the existing methods in the Utils class, which
could not be used before because they initialized logging. As part
of that I also did some minor refactoring, moving methods that
should really belong in DependencyUtils.

The change also shuffles some code in SparkHadoopUtil so that
SparkSubmit can create a Hadoop config like the rest of Spark
code, respecting the user's Spark configuration.

The behavior was verified running spark-shell, pyspark and
normal applications, then verifying the logging behavior,
with and without dependency downloads.

Author: Marcelo Vanzin <vanzin@cloudera.com>

Closes #19013 from vanzin/SPARK-21728.
","['core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala', 'core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala', 'core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala', 'core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala', 'core/src/main/scala/org/apache/spark/internal/Logging.scala', 'core/src/main/scala/org/apache/spark/util/Utils.scala', 'core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala']","SparkSubmit is unable to initialize logging, leading to duplicated code and inconsistencies in Hadoop config creation and logging behavior across Spark applications."
fb5647732fa2f49838f803f67ea11b20fc14282b,1677029656,"[SPARK-42406][SQL] Fix check for missing required fields of to_protobuf

### What changes were proposed in this pull request?

Protobuf serializer (used in `to_protobuf()`) should error if non-nullable fields (i.e. protobuf `required` fields) are present in the schema of the catalyst record being converted to a protobuf.

But `isNullable()` method used for this check returns opposite (see PR comment in the diff).  As a result, Serializer incorrectly requires the fields that are optional. This PR fixes this check (see PR comment in the diff).

This also requires corresponding fix for couple of unit tests. In order use a Protobuf message with a `required` field, Protobuf version 2 file `proto2_messages.proto` is added.
Two tests are updated to verify missing required fields results in an error.

### Why are the changes needed?

This is need to fix a bug where we were incorrectly enforcing a schema check on optional fields rather than on required fields.

### Does this PR introduce _any_ user-facing change?

It fixes a bug, and gives more flexibility for user queries.

### How was this patch tested?
 - Updated unit tests

Closes #40080 from rangadi/fix-required-field-check.

Authored-by: Raghu Angadi <raghu.angadi@databricks.com>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['connector/protobuf/src/main/scala/org/apache/spark/sql/protobuf/utils/ProtobufUtils.scala', 'connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufSerdeSuite.scala', 'connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufTestBase.scala']","Protobuf serializer erroneously enforces schema checks on optional fields rather than required fields during `to_protobuf()` conversions, causing improper behavior."
ea45fc51921e64302b9220b264156bb4f757fe01,1591054299,"[SPARK-28344][SQL][FOLLOW-UP] Check the ambiguous self-join only if there is a join in the plan

### What changes were proposed in this pull request?

This PR proposes to check `DetectAmbiguousSelfJoin` only if there is `Join` in the plan. Currently, the checking is too strict even to non-join queries.

For example, the codes below don't have join at all but it fails as the ambiguous self-join:

```scala
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.sum
val df = Seq(1, 1, 2, 2).toDF(""A"")
val w = Window.partitionBy(df(""A""))
df.select(df(""A"").alias(""X""), sum(df(""A"")).over(w)).explain(true)
```

It is because `ExtractWindowExpressions` can create a `AttributeReference` with the same metadata but a different expression ID, see:

https://github.com/apache/spark/blob/0fd98abd859049dc3b200492487041eeeaa8f737/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L2679
https://github.com/apache/spark/blob/71c73d58f6e88d2558ed2e696897767d93bac60f/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala#L63
https://github.com/apache/spark/blob/5945d46c11a86fd85f9e65f24c2e88f368eee01f/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala#L180

Before:

```
'Project [A#19 AS X#21, sum(A#19) windowspecdefinition(A#19, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS sum(A) OVER (PARTITION BY A unspecifiedframe$())#23L]
+- Relation[A#19] parquet
```

After:

```
Project [X#21, sum(A) OVER (PARTITION BY A unspecifiedframe$())#23L]
+- Project [X#21, A#19, sum(A) OVER (PARTITION BY A unspecifiedframe$())#23L, sum(A) OVER (PARTITION BY A unspecifiedframe$())#23L]
   +- Window [sum(A#19) windowspecdefinition(A#19, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS sum(A) OVER (PARTITION BY A unspecifiedframe$())#23L], [A#19]
      +- Project [A#19 AS X#21, A#19]
         +- Relation[A#19] parquet
```

`X#21` holds the same metadata of DataFrame ID and column position with `A#19` but it has a different expression ID which ends up with the checking fails.

### Why are the changes needed?

To loose the checking and make users not surprised.

### Does this PR introduce _any_ user-facing change?

It's the changes in unreleased branches only.

### How was this patch tested?

Manually tested and unittest was added.

Closes #28695 from HyukjinKwon/SPARK-28344-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/analysis/DetectAmbiguousSelfJoin.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSelfJoinSuite.scala']","The `DetectAmbiguousSelfJoin` check fails even for non-join queries, triggering an error for queries without join but creating `AttributeReference` with the same metadata and a different expression ID."
df8d3f1bf779ce1a9f3520939ab85814f09b48b7,1606924988,"[SPARK-33544][SQL][FOLLOW-UP] Rename NoSideEffect to NoThrow and clarify the documentation more

### What changes were proposed in this pull request?

This PR is a followup of https://github.com/apache/spark/pull/30504. It proposes:

- Rename `NoSideEffect` to `NoThrow`, and use `Expression.deterministic` together where it is used.
- Clarify, in the docs in the expressions, that it means they don't throw exceptions

### Why are the changes needed?

`NoSideEffect` virtually means that `Expression.eval` does not throw an exception, and the expressions are deterministic.
It's best to be explicit so `NoThrow` was proposed -  I looked if there's a similar name to represent this concept and borrowed the name of [nothrow](https://clang.llvm.org/docs/AttributeReference.html#nothrow).
For determinism, we already have a way to note it under `Expression.deterministic`.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Manually ran the existing unittests written.

Closes #30570 from HyukjinKwon/SPARK-33544.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InferFiltersFromGenerateSuite.scala']","The term 'NoSideEffect' in 'Expression.eval' is causing confusion as it signifies absence of exception throwing, rather than side effects free nature of an operation, leading to misunderstandings in its purpose and functionality."
12206058e8780e202c208b92774df3773eff36ae,1491439604,"[SPARK-20214][ML] Make sure converted csc matrix has sorted indices

## What changes were proposed in this pull request?

`_convert_to_vector` converts a scipy sparse matrix to csc matrix for initializing `SparseVector`. However, it doesn't guarantee the converted csc matrix has sorted indices and so a failure happens when you do something like that:

    from scipy.sparse import lil_matrix
    lil = lil_matrix((4, 1))
    lil[1, 0] = 1
    lil[3, 0] = 2
    _convert_to_vector(lil.todok())

    File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 78, in _convert_to_vector
      return SparseVector(l.shape[0], csc.indices, csc.data)
    File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 556, in __init__
      % (self.indices[i], self.indices[i + 1]))
    TypeError: Indices 3 and 1 are not strictly increasing

A simple test can confirm that `dok_matrix.tocsc()` won't guarantee sorted indices:

    >>> from scipy.sparse import lil_matrix
    >>> lil = lil_matrix((4, 1))
    >>> lil[1, 0] = 1
    >>> lil[3, 0] = 2
    >>> dok = lil.todok()
    >>> csc = dok.tocsc()
    >>> csc.has_sorted_indices
    0
    >>> csc.indices
    array([3, 1], dtype=int32)

I checked the source codes of scipy. The only way to guarantee it is `csc_matrix.tocsr()` and `csr_matrix.tocsc()`.

## How was this patch tested?

Existing tests.

Please review http://spark.apache.org/contributing.html before opening a pull request.

Author: Liang-Chi Hsieh <viirya@gmail.com>

Closes #17532 from viirya/make-sure-sorted-indices.
","['python/pyspark/ml/linalg/__init__.py', 'python/pyspark/mllib/linalg/__init__.py', 'python/pyspark/mllib/tests.py']",Converting a scipy sparse matrix to csc for initializing SparseVector does not ensure sorted indices in the resultant csc matrix causing TypeError when performing certain operations.
7e14c8cc33f0ed0a9c53a888e8a3b17dd2a5d493,1695098229,"[SPARK-44910][SQL] Encoders.bean does not support superclasses with generic type arguments

### What changes were proposed in this pull request?
This pull request adds Encoders.bean support for beans having a superclass declared with generic type arguments.
For example:

```
class JavaBeanWithGenericsA<T> {
    public T getPropertyA() {
        return null;
    }

    public void setPropertyA(T a) {

    }
}

class JavaBeanWithGenericBase extends JavaBeanWithGenericsA<String> {
}

Encoders.bean(JavaBeanWithGenericBase.class); // Exception
```

That feature had to be part of [PR 42327](https://github.com/apache/spark/commit/1f5d78b5952fcc6c7d36d3338a5594070e3a62dd) but was missing as I was focusing on nested beans only (hvanhovell )

### Why are the changes needed?
JavaTypeInference.encoderFor did not solve TypeVariable objects for superclasses so when managing a case like in the example above an exception was thrown.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing tests have been extended, new specific tests have been added

### Was this patch authored or co-authored using generative AI tooling?
No

Closes #42634 from gbloisi-openaire/SPARK-44910.

Lead-authored-by: Giambattista Bloisi <gbloisi@gmail.com>
Co-authored-by: gbloisi-openaire <141144100+gbloisi-openaire@users.noreply.github.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/api/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala', 'sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/JavaTypeInferenceBeans.java', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/JavaTypeInferenceSuite.scala']",Encoders.bean function throws an exception when trying to work on beans having superclasses declared with generic type arguments.
e187bb3940a465613b31e5a5d6b75b469d0423ec,1659094817,"[SPARK-37194][SQL] Avoid unnecessary sort in v1 write if it's not dynamic partition

### What changes were proposed in this pull request?

This is a rework for https://github.com/apache/spark/pull/34468, since we pull out v1write required ordering.

This prs add a new parameter numStaticPartitions to v1write and FileFormatWriter so we can skip unnecessary local sort for static partition write.

### Why are the changes needed?

The v1 write requires ordering for dynamic partition, bucket expression and sort column during writing. The reason is the `DynamicPartitionDataSingleWriter` and `DynamicPartitionDataConcurrentWriter` assume the partition and bucket columns are continuous. Then if partition column is static, it's unnecessary to do the local sort.

For  v1 write, `InsertIntoHadoopFsRelationCommand` is the only case which adds a local sort even if the partition column is static.

### Does this PR introduce _any_ user-facing change?

no, only improve performance

### How was this patch tested?

add test

Closes #37290 from ulysses-you/v1write.

Authored-by: ulysses-you <ulyssesyou18@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/V1Writes.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/V1WriteCommandSuite.scala']","The v1 write in Spark SQL is performing an unnecessary local sort operation for static partitions, adversely impacting performance."
a6576de9719204f6a87d2fc5e2e344bd1d0017a3,1640765226,"[SPARK-34755][SQL] Support the utils for transform number format

### What changes were proposed in this pull request?
Data Type Formatting Functions: `to_number` and `to_char` is very useful.
The implement has many different between `Postgresql` ,`Oracle` and `Phoenix`.
So, this PR follows the implement of `to_number` in `Oracle` that give a strict parameter verification.
So, this PR follows the implement of `to_number` in `Phoenix` that uses BigDecimal.

This PR support the patterns for numeric formatting as follows:

Pattern | Description
-- | --
9 | Value with the specified number of digits
0 | Value with leading zeros
. (period) | Decimal point
, (comma) | Group (thousand) separator
S | Sign anchored to number (uses locale)
$ | a value with a leading dollar sign
D | Decimal point (uses locale)
G | Group separator (uses locale)

There are some mainstream database support the syntax.
**PostgreSQL:**
https://www.postgresql.org/docs/12/functions-formatting.html

**Oracle:**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TO_NUMBER.html#GUID-D4807212-AFD7-48A7-9AED-BEC3E8809866

**Vertica**
https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Formatting/TO_NUMBER.htm?tocpath=SQL%20Reference%20Manual%7CSQL%20Functions%7CFormatting%20Functions%7C_____7

**Redshift**
https://docs.aws.amazon.com/redshift/latest/dg/r_TO_NUMBER.html

**DB2**
https://www.ibm.com/support/knowledgecenter/SSGU8G_14.1.0/com.ibm.sqls.doc/ids_sqs_1544.htm

**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/TH2cDXBn6tala29S536nqg

**Snowflake:**
https://docs.snowflake.net/manuals/sql-reference/functions/to_decimal.html

**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/to_number.htm#TO_NUMBER

**Phoenix**
http://phoenix.incubator.apache.org/language/functions.html#to_number

**Singlestore**
https://docs.singlestore.com/v7.3/reference/sql-reference/numeric-functions/to-number/

**Intersystems**
https://docs.intersystems.com/latest/csp/docbook/DocBook.UI.Page.cls?KEY=RSQL_TONUMBER

Note: Based on discussion offline with cloud-fan ten months ago, this PR only implement the utils for transform number format. Because the utils should be review better.

### Why are the changes needed?
`to_number` and `to_char` are very useful for formatted currency to number conversion.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Jenkins test

Closes #31847 from beliefer/SPARK-34755.

Lead-authored-by: Jiaan Geng <beliefer@163.com>
Co-authored-by: gengjiaan <gengjiaan@360.cn>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberUtils.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberUtilsSuite.scala']",Inconsistencies noticed while formatting numbers with Data Type Formatting Functions: `to_number` and `to_char`. A strict parameter verification is required for efficient number transformations.
7fe2759e9f81ec267e92e1c6f8a48f42042db791,1651111157,"[SPARK-39046][SQL] Return an empty context string if TreeNode.origin is wrongly set

### What changes were proposed in this pull request?

For the query context `TreeNode.origin.context`, this PR proposal to return an empty context string if
* the query text/ the start index/ the stop index is missing
* the start index is less than 0
* the stop index is larger than the length of query text
* the start index is larger than the stop index

### Why are the changes needed?

There are downstream projects that depend on Spark. There is no guarantee for the correctness of TreeNode.origin. Developers may create a plan/expression with a Origin containing wrong startIndex/stopIndex/sqlText.
Thus, to avoid errors in calling `String.substring` or showing misleading debug information, I suggest returning an empty context string if TreeNode.origin is wrongly set. The query context is just for better error messages and we should handle it cautiously.

### Does this PR introduce _any_ user-facing change?

No, the context framework is not released yet.

### How was this patch tested?

UT

Closes #36379 from gengliangwang/safeContext.

Authored-by: Gengliang Wang <gengliang@apache.org>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala']",Incorrect or missing values in the TreeNode.origin for query context can lead to errors in substrings or misleading debug information.
3c97665dad810bd9d0cc3ca8bd735914bb0d38d6,1600900433,"[SPARK-32981][BUILD] Remove hive-1.2/hadoop-2.7 from Apache Spark 3.1 distribution

### What changes were proposed in this pull request?

Apache Spark 3.0 switches its Hive execution version from 1.2 to 2.3, but it still provides the unofficial forked Hive 1.2 version from our distribution like the following. This PR aims to remove it from Apache Spark 3.1.0 officially while keeping `hive-1.2` profile.
```
spark-3.0.1-bin-hadoop2.7-hive1.2.tgz
spark-3.0.1-bin-hadoop2.7-hive1.2.tgz.asc
spark-3.0.1-bin-hadoop2.7-hive1.2.tgz.sha512
```

### Why are the changes needed?

The unofficial Hive 1.2.1 fork has many bugs and is not maintained for a long time. We had better not recommend this in the official Apache Spark distribution.

### Does this PR introduce _any_ user-facing change?

There is no user-facing change in the default distribution (Hadoop 3.2/Hive 2.3).

### How was this patch tested?

Manually because this is a change in release script .

Closes #29856 from dongjoon-hyun/SPARK-32981.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['dev/create-release/release-build.sh'],"The Apache Spark 3.1 distribution currently includes an outdated, unofficial fork of Hive 1.2, which has many bugs and is no longer maintained."
2ca5aae47a25dc6bc9e333fb592025ff14824501,1510117334,"[SPARK-22281][SPARKR] Handle R method breaking signature changes

## What changes were proposed in this pull request?

This is to fix the code for the latest R changes in R-devel, when running CRAN check
```
checking for code/documentation mismatches ... WARNING
Codoc mismatches from documentation object 'attach':
attach
Code: function(what, pos = 2L, name = deparse(substitute(what),
backtick = FALSE), warn.conflicts = TRUE)
Docs: function(what, pos = 2L, name = deparse(substitute(what)),
warn.conflicts = TRUE)
Mismatches in argument default values:
Name: 'name' Code: deparse(substitute(what), backtick = FALSE) Docs: deparse(substitute(what))

Codoc mismatches from documentation object 'glm':
glm
Code: function(formula, family = gaussian, data, weights, subset,
na.action, start = NULL, etastart, mustart, offset,
control = list(...), model = TRUE, method = ""glm.fit"",
x = FALSE, y = TRUE, singular.ok = TRUE, contrasts =
NULL, ...)
Docs: function(formula, family = gaussian, data, weights, subset,
na.action, start = NULL, etastart, mustart, offset,
control = list(...), model = TRUE, method = ""glm.fit"",
x = FALSE, y = TRUE, contrasts = NULL, ...)
Argument names in code not in docs:
singular.ok
Mismatches in argument names:
Position: 16 Code: singular.ok Docs: contrasts
Position: 17 Code: contrasts Docs: ...
```

With attach, it's pulling in the function definition from base::attach. We need to disable that but we would still need a function signature for roxygen2 to build with.

With glm it's pulling in the function definition (ie. ""usage"") from the stats::glm function. Since this is ""compiled in"" when we build the source package into the .Rd file, when it changes at runtime or in CRAN check it won't match the latest signature. The solution is not to pull in from stats::glm since there isn't much value in doing that (none of the param we actually use, the ones we do use we have explicitly documented them)

Also with attach we are changing to call dynamically.

## How was this patch tested?

Manually.
- [x] check documentation output - yes
- [x] check help `?attach` `?glm` - yes
- [x] check on other platforms, r-hub, on r-devel etc..

Author: Felix Cheung <felixcheung_m@hotmail.com>

Closes #19557 from felixcheung/rattachglmdocerror.
",['R/run-tests.sh'],Mismatch between documentation and code in 'attach' and 'glm' functions signature during CRAN check due to runtime changes causes errors.
11d3a744e20fe403dd76e18d57963b6090a7c581,1591895625,"[SPARK-31705][SQL] Push more possible predicates through Join via CNF conversion

### What changes were proposed in this pull request?

This PR add a new rule to support push predicate through join by rewriting join condition to CNF(conjunctive normal form). The following example is the steps of this rule:

1. Prepare Table:

```sql
CREATE TABLE x(a INT);
CREATE TABLE y(b INT);
...
SELECT * FROM x JOIN y ON ((a < 0 and a > b) or a > 10);
```

2. Convert the join condition to CNF:
```
(a < 0 or a > 10) and (a > b or a > 10)
```

3. Split conjunctive predicates

Predicates
---|
(a < 0 or a > 10)
(a > b or a > 10)

4. Push predicate

Table | Predicate
--- | ---
x | (a < 0 or a > 10)

### Why are the changes needed?
Improve query performance. PostgreSQL, [Impala](https://issues.apache.org/jira/browse/IMPALA-9183) and Hive support this feature.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Unit test and benchmark test.

SQL | Before this PR | After this PR
--- | --- | ---
TPCDS 5T Q13 | 84s | 21s
TPCDS 5T q85 | 66s | 34s
TPCH 1T q19 | 37s | 32s

Closes #28733 from gengliangwang/cnf.

Lead-authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Co-authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushCNFPredicateThroughJoin.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ConjunctiveNormalFormPredicateSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala']",The push predicate operation through join is inefficient which potentially slows down SQL query performance; current join conditions do not support CNF (Conjunctive Normal Form) rewriting.
150769bcedb6e4a97596e0f04d686482cd09e92a,1616628717,"[SPARK-34833][SQL] Apply right-padding correctly for correlated subqueries

### What changes were proposed in this pull request?

This PR intends to fix the bug that does not apply right-padding for char types inside correlated subquries.
For example,  a query below returns nothing in master, but a correct result is `c`.
```
scala> sql(s""CREATE TABLE t1(v VARCHAR(3), c CHAR(5)) USING parquet"")
scala> sql(s""CREATE TABLE t2(v VARCHAR(5), c CHAR(7)) USING parquet"")
scala> sql(""INSERT INTO t1 VALUES ('c', 'b')"")
scala> sql(""INSERT INTO t2 VALUES ('a', 'b')"")
scala> val df = sql(""""""
  |SELECT v FROM t1
  |WHERE 'a' IN (SELECT v FROM t2 WHERE t2.c = t1.c )"""""".stripMargin)

scala> df.show()
+---+
|  v|
+---+
+---+

```

This is because `ApplyCharTypePadding`  does not handle the case above to apply right-padding into `'abc'`. This PR modifies the code in `ApplyCharTypePadding` for handling it correctly.

```
// Before this PR:
scala> df.explain(true)
== Analyzed Logical Plan ==
v: string
Project [v#13]
+- Filter a IN (list#12 [c#14])
   :  +- Project [v#15]
   :     +- Filter (c#16 = outer(c#14))
   :        +- SubqueryAlias spark_catalog.default.t2
   :           +- Relation default.t2[v#15,c#16] parquet
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation default.t1[v#13,c#14] parquet

scala> df.show()
+---+
|  v|
+---+
+---+

// After this PR:
scala> df.explain(true)
== Analyzed Logical Plan ==
v: string
Project [v#43]
+- Filter a IN (list#42 [c#44])
   :  +- Project [v#45]
   :     +- Filter (c#46 = rpad(outer(c#44), 7,  ))
   :        +- SubqueryAlias spark_catalog.default.t2
   :           +- Relation default.t2[v#45,c#46] parquet
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation default.t1[v#43,c#44] parquet

scala> df.show()
+---+
|  v|
+---+
|  c|
+---+
```

This fix is lated to TPCDS q17; the query returns nothing because of this bug: https://github.com/apache/spark/pull/31886/files#r599333799

### Why are the changes needed?

Bugfix.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit tests added.

Closes #31940 from maropu/FixCharPadding.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala']","Queries with correlated subqueries are incorrectly executing char type padding, resulting in unexpected empty output where a correct result is expected."
dc84e529ba96ba8afc24216e5fc28d95ce8ce290,1681788178,"[SPARK-43122][CONNECT][PYTHON][ML][TESTS] Reenable TorchDistributorLocalUnitTestsOnConnect and TorchDistributorLocalUnitTestsIIOnConnect

### What changes were proposed in this pull request?
`TorchDistributorLocalUnitTestsOnConnect` and `TorchDistributorLocalUnitTestsIIOnConnect` were not stable and occasionally got stuck. However, I can not reproduce the issue locally.

The two UTs were disabled, and this PR is to reenable them. I found that the all the tests for PyTorch set up the regular sessions or connect sessions in `setUp` and close them in `tearDown`, however such session operations are very expensive and should be placed into `setUpClass` and `tearDownClass` instead. After this change, the related tests seems much stable. So I think the root cause is still related to the resources, since TorchDistributor works on barrier mode, when there is not enough resources in Github Action, the tests just keep waiting.

### Why are the changes needed?
for test coverage

### Does this PR introduce _any_ user-facing change?
No, test-only

### How was this patch tested?
CI

Closes #40793 from zhengruifeng/torch_reenable.

Lead-authored-by: Ruifeng Zheng <ruifengz@foxmail.com>
Co-authored-by: Ruifeng Zheng <ruifengz@apache.org>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/ml/tests/connect/test_parity_torch_distributor.py', 'python/pyspark/ml/torch/tests/test_distributor.py']",Unstable unit tests `TorchDistributorLocalUnitTestsOnConnect` and `TorchDistributorLocalUnitTestsIIOnConnect` occasionally get stuck leading to lack in test coverage. Session operations seem to be resource-expensive possibly causing the tests to wait indefinitely in case of resource shortage.
0e6e15ca6331d37a6c38c970556903c6df5d5dfb,1694083378,"[SPARK-45080][SS] Explicitly call out support for columnar in DSv2 streaming data sources

### What changes were proposed in this pull request?

This PR proposes to override `Scan.columnarSupportMode` for DSv2 streaming data sources. All of them don't support columnar. This applies [SPARK-44505](https://issues.apache.org/jira/browse/SPARK-44505) to the DSv2 streaming data sources.

Rationalization will be explained in the next section.

### Why are the changes needed?

The default value for `Scan.columnarSupportMode` is `PARTITION_DEFINED`, which requires `inputPartitions` to be called/evaluated. That could be referenced multiple times during planning.

In `MicrobatchScanExec`, we define `inputPartitions` as lazy val, so that there is no multiple evaluation of inputPartitions, which calls `MicroBatchStream.planInputPartitions`. But we missed that there is no guarantee that the instance will be initialized only once (although the actual execution will happen once) - for example, executedPlan clones the plan (internally we call constructor to make a deep copy of the node), explain (internally called to build a SQL execution start event), etc...

I see `MicroBatchStream.planInputPartitions` gets called 4 times per microbatch, which can be concerning if the overhead of planInputPartitions is non-trivial, specifically Kafka.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing UTs.

### Was this patch authored or co-authored using generative AI tooling?

No.

Closes #42823 from HeartSaVioR/SPARK-45080.

Authored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RatePerMicroBatchProvider.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamProvider.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketSourceProvider.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala']","DSv2 streaming data sources are not supporting columnar, causing `inputPartitions` to be referenced multiple times during planning, thus leading to potential performance issues for non-trivial overheads of planInputPartitions, specifically Kafka.
"
888b343587c98ae0252311d72e20abbca8262ab3,1600179090,"[SPARK-32827][SQL] Add spark.sql.maxMetadataStringLength config

### What changes were proposed in this pull request?

Add a new config `spark.sql.maxMetadataStringLength`. This config aims to limit metadata value length, e.g. file location.

### Why are the changes needed?

Some metadata have been abbreviated by `...` when I tried to add some test in `SQLQueryTestSuite`. We need to replace such value to `notIncludedMsg`. That caused we can't replace that like location value by `className` since the `className` has been abbreviated.

Here is a case:
```
CREATE table  explain_temp1 (key int, val int) USING PARQUET;

EXPLAIN EXTENDED SELECT sum(distinct val) FROM explain_temp1;

-- ignore parsed,analyzed,optimized
-- The output like
== Physical Plan ==
*HashAggregate(keys=[], functions=[sum(distinct cast(val#x as bigint)#xL)], output=[sum(DISTINCT val)#xL])
+- Exchange SinglePartition, true, [id=#x]
   +- *HashAggregate(keys=[], functions=[partial_sum(distinct cast(val#x as bigint)#xL)], output=[sum#xL])
      +- *HashAggregate(keys=[cast(val#x as bigint)#xL], functions=[], output=[cast(val#x as bigint)#xL])
         +- Exchange hashpartitioning(cast(val#x as bigint)#xL, 4), true, [id=#x]
            +- *HashAggregate(keys=[cast(val#x as bigint) AS cast(val#x as bigint)#xL], functions=[], output=[cast(val#x as bigint)#xL])
               +- *ColumnarToRow
                  +- FileScan parquet default.explain_temp1[val#x] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/runner/work/spark/spark/sql/core/spark-warehouse/org.apache.spark.sq...], PartitionFilters: ...
```

### Does this PR introduce _any_ user-facing change?

No, a new config.

### How was this patch tested?

new test.

Closes #29688 from ulysses-you/SPARK-32827.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScan.scala', 'sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala']","Metadata abbreviation in SQLQueryTestSuite can cause inability to replace some values such as locations with their respective classNames, limiting test capability."
9553ed7072624cd42d5d74c4ce578801d81ec6f9,1637160611,"[SPARK-36924][SQL] CAST between ANSI intervals and IntegralType

### What changes were proposed in this pull request?
Add cast `AnsiIntervalType` to `IntegralType`

requirement:
1. `YearMonthIntervalType` just have one unit
2. `DayTimeIntervalType` just have one unit

cast rule:
1. The value corresponding to the unit of `YearMonthIntervalType` is the value of the `IntegralType` after conversion.
2. The value corresponding to the unit of `DayTimeIntervalType` is the value of the `IntegralType` after conversion.

Add cast `IntegralType` to `AnsiIntervalType`
requirement:

1. `YearMonthIntervalType` just have one unit
2. `DayTimeIntervalType` just have one unit

cast rule:

1. The value of the IntegralTypeis the value of  `YearMonthIntervalType` that with the single unit after conversion.
2. The value of the IntegralTypeis the value of  `DayTimeIntervalType` that with the single unit after conversion.

### Why are the changes needed?
According to 2011 Standards
![截图](https://user-images.githubusercontent.com/41178002/140504037-b86793f0-2c97-49f7-bcbf-bb6864592aa8.PNG)

7) If TD is an interval and SD is exact numeric, then TD shall contain only a single <primary datetime field>.
8) If TD is exact numeric and SD is an interval, then SD shall contain only a single <primary datetime field>.

### Does this PR introduce _any_ user-facing change?
Yes, user can use cast function between YearMonthIntervalType to NumericType

### How was this patch tested?
add ut testcase

Closes #34494 from Peng-Lei/SPARK-36924.

Authored-by: PengLei <peng.8lei@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala']",Conversions (CAST) between ANSI intervals and IntegralType doesn't comply 2011 standards when they have more than one unit causing incorrect values.
653215377adfd1ff873a15175b2c44cbbf6df7a9,1562807523,"[SPARK-28015][SQL] Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats

## What changes were proposed in this pull request?

Fix `stringToDate()` for the formats `yyyy` and `yyyy-[m]m` that assumes there are no additional chars after the last components `yyyy` and `[m]m`. In the PR, I propose to check that entire input was consumed for the formats.

After the fix, the input `1999 08 01` will be invalid because it matches to the pattern `yyyy` but the strings contains additional chars ` 08 01`.

Since Spark 1.6.3 ~ 2.4.3, the behavior is the same.
```
spark-sql> SELECT CAST('1999 08 01' AS DATE);
1999-01-01
```

This PR makes it return NULL like Hive.
```
spark-sql> SELECT CAST('1999 08 01' AS DATE);
NULL
```

## How was this patch tested?

Added new checks to `DateTimeUtilsSuite` for the `1999 08 01` and `1999 08` inputs.

Closes #25097 from MaxGekk/spark-28015-invalid-date-format.

Authored-by: Maxim Gekk <maxim.gekk@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeUtilsSuite.scala']","The stringToDate() function inaccurately interprets dates, considering inputs like '1999 08 01' valid and returning incorrect dates instead of NULL for formats 'yyyy' and 'yyyy-[m]m'."
9ff1d96f01e2c89acfd248db917e068b93f519a6,1517435567,"[SPARK-23281][SQL] Query produces results in incorrect order when a composite order by clause refers to both original columns and aliases

## What changes were proposed in this pull request?
Here is the test snippet.
``` SQL
scala> Seq[(Integer, Integer)](
     |         (1, 1),
     |         (1, 3),
     |         (2, 3),
     |         (3, 3),
     |         (4, null),
     |         (5, null)
     |       ).toDF(""key"", ""value"").createOrReplaceTempView(""src"")

scala> sql(
     |         """"""
     |           |SELECT MAX(value) as value, key as col2
     |           |FROM src
     |           |GROUP BY key
     |           |ORDER BY value desc, key
     |         """""".stripMargin).show
+-----+----+
|value|col2|
+-----+----+
|    3|   3|
|    3|   2|
|    3|   1|
| null|   5|
| null|   4|
+-----+----+
```SQL
Here is the explain output :

```SQL
== Parsed Logical Plan ==
'Sort ['value DESC NULLS LAST, 'key ASC NULLS FIRST], true
+- 'Aggregate ['key], ['MAX('value) AS value#9, 'key AS col2#10]
   +- 'UnresolvedRelation `src`

== Analyzed Logical Plan ==
value: int, col2: int
Project [value#9, col2#10]
+- Sort [value#9 DESC NULLS LAST, col2#10 DESC NULLS LAST], true
   +- Aggregate [key#5], [max(value#6) AS value#9, key#5 AS col2#10]
      +- SubqueryAlias src
         +- Project [_1#2 AS key#5, _2#3 AS value#6]
            +- LocalRelation [_1#2, _2#3]
``` SQL
The sort direction is being wrongly changed from ASC to DSC while resolving ```Sort``` in
resolveAggregateFunctions.

The above testcase models TPCDS-Q71 and thus we have the same issue in Q71 as well.

## How was this patch tested?
A few tests are added in SQLQuerySuite.

Author: Dilip Biswal <dbiswal@us.ibm.com>

Closes #20453 from dilipbiswal/local_spark.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala']","When a composite ORDER BY clause refers to both original columns and aliases, the query produces results in incorrect order, with sort direction being wrongly changed from ASC to DSC. This specifically affects TPCDS-Q71 model."
49581b35a07758e17af185aa465abc055f912404,1682525547,"[SPARK-43198][CONNECT] Fix ""Could not initialise class ammonite..."" error when using filter

### What changes were proposed in this pull request?

This PR makes the ammonite REPL use the `CodeClassWrapper` mode for classfile generation (make ammonite generate classes instead of objects) and changes the UDF serialization from lazy to eager.

### Why are the changes needed?

The changes have the following impact:
- `CodeClassWrapper` change
  - Fixes the `io.grpc.StatusRuntimeException: UNKNOWN: ammonite/repl/ReplBridge$` error when trying to use the `filter` method (see [jira](https://issues.apache.org/jira/browse/SPARK-43198) for reproduction)
- Lazy to eager UDF serialization
  - With class-based generation, UDFs defined using `def` would hit CNFE because the `ScalarUserDefinedFuntion` class gets captured during serialisation and sent over to the server (the class is a client-only class).

### Does this PR introduce _any_ user-facing change?

Yes. There are two significant changes:

- Filter works as expected when using ""in-place"" lambda expressions such as in `spark.range(10).filter(n => n % 2 == 0).collectAsList()`
- UDFs defined using a lambda expression which is stored in a `val` fail due to deserialisation issues on the server.
  - Root cause is currently unknown but a ticket has been [filed](https://issues.apache.org/jira/browse/SPARK-43227) to address the issue.
  - Example: see [this](https://github.com/apache/spark/compare/master...vicennial:spark:SPARK-43198?expand=1#diff-8d8a214eff5d2c8d523b59f2a39758ddfa84912ef7d4e0276f54e979a58f88e0R120-R129) test.
  - Currently, it is a compromise to get `filter` working as expected since that bug is a higher-impact due to it impacting the ""general"" way of using the method.

### How was this patch tested?

New unit test.

Closes #40894 from vicennial/SPARK-43198.

Authored-by: vicennial <venkata.gudesa@databricks.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
","['connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/application/ConnectRepl.scala', 'connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala', 'connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/application/ReplE2ESuite.scala']",Using the filter method results in an 'io.grpc.StatusRuntimeException' error and UDFs defined with 'def' cause a Class Not Found Exception (CNFE) due to serialization issues.
4ecbdbb6a7bd3908da32c82832e886b4f9f9e596,1571161518,"[SPARK-29182][CORE] Cache preferred locations of checkpointed RDD

### What changes were proposed in this pull request?

This proposes to add a Spark config to control the caching behavior of ReliableCheckpointRDD.getPreferredLocations. If it is enabled, getPreferredLocations will only compute preferred locations once and cache it for later usage.

The drawback of caching the preferred locations is that when the cached locations are outdated, and lose data locality. It was documented in config document. To mitigate this, this patch also adds a config to set up expire time (default is 60 mins) for the cache. If time expires, the cache will be invalid and it needs to query updated location info.

This adds a test case. Looks like the most suitable test suite is CheckpointCompressionSuite. So this renames CheckpointCompressionSuite to CheckpointStorageSuite and put the test case into.

### Why are the changes needed?

One Spark job in our cluster fits many ALS models in parallel. The fitting goes well, but in next when we union all factors, the union operation is very slow.

By looking into the driver stack dump, looks like the driver spends a lot of time on computing preferred locations. As we checkpoint training data before fitting ALS, the time is spent on ReliableCheckpointRDD.getPreferredLocations. In this method, it will call DFS interface to query file status and block locations. As we have big number of partitions derived from the checkpointed RDD,  the union will spend a lot of time on querying the same information.

It reduces the time on huge union from few hours to dozens of minutes.

This issue is not limited to ALS so this change is not specified to ALS. Actually it is common usage to checkpoint data in Spark, to increase reliability and cut RDD linage. Spark operations on the checkpointed data, will be beneficial.

### Does this PR introduce any user-facing change?

Yes. This adds a Spark config users can use to control the cache behavior of preferred locations of checkpointed RDD.

### How was this patch tested?

Unit test added and manual test on development cluster.

Closes #25856 from viirya/cache-checkpoint-preferredloc.

Authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Liang-Chi Hsieh <liangchi@uber.com>
","['core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala', 'core/src/test/scala/org/apache/spark/CheckpointSuite.scala']","Computing preferred locations for ReliableCheckpointRDD is time-consuming during operations like union on checkpointed data, resulting in significant runtime slowdowns."
da65a955ed61a5f82181ea051959e91de884efcc,1577275406,"[SPARK-30266][SQL] Avoid  match error and int overflow in ApproximatePercentile and Percentile

### What changes were proposed in this pull request?
accuracyExpression can accept Long which may cause overflow error.
accuracyExpression can accept fractions which are implicitly floored.
accuracyExpression can accept null which is implicitly changed to 0.
percentageExpression can accept null but cause MatchError.
percentageExpression can accept ArrayType(_, nullable=true) in which the nulls are implicitly changed to zeros.

##### cases
```sql
select percentile_approx(10.0, 0.5, 2147483648); -- overflow and fail
select percentile_approx(10.0, 0.5, 4294967297); -- overflow but success
select percentile_approx(10.0, 0.5, null); -- null cast to 0
select percentile_approx(10.0, 0.5, 1.2); -- 1.2 cast to 1
select percentile_approx(10.0, null, 1); -- scala.MatchError
select percentile_approx(10.0, array(0.2, 0.4, null), 1); -- null cast to zero.
```

##### behavior before

```sql
+select percentile_approx(10.0, 0.5, 2147483648)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, CAST(0.5BD AS DOUBLE), CAST(2147483648L AS INT))' due to data type mismatch: The accuracy provided must be a positive integer literal (current value = -2147483648); line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 4294967297)
+10.0
+

+select percentile_approx(10.0, 0.5, null)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, CAST(0.5BD AS DOUBLE), CAST(NULL AS INT))' due to data type mismatch: The accuracy provided must be a positive integer literal (current value = 0); line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 1.2)
+10.0
+
+select percentile_approx(10.0, null, 1)
+scala.MatchError
+null
+
+
+select percentile_approx(10.0, array(0.2, 0.4, null), 1)
+[10.0,10.0,10.0]
```

##### behavior after

```sql

+select percentile_approx(10.0, 0.5, 2147483648)
+10.0
+
+select percentile_approx(10.0, 0.5, 4294967297)
+10.0
+
+select percentile_approx(10.0, 0.5, null)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, 0.5BD, NULL)' due to data type mismatch: argument 3 requires integral type, however, 'NULL' is of null type.; line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 1.2)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, 0.5BD, 1.2BD)' due to data type mismatch: argument 3 requires integral type, however, '1.2BD' is of decimal(2,1) type.; line 1 pos 7
+

+select percentile_approx(10.0, null, 1)
+java.lang.IllegalArgumentException
+The value of percentage must be be between 0.0 and 1.0, but got null
+
+select percentile_approx(10.0, array(0.2, 0.4, null), 1)
+java.lang.IllegalArgumentException
+Each value of the percentage array must be be between 0.0 and 1.0, but got [0.2,0.4,null]
```

### Why are the changes needed?

bug fix

### Does this PR introduce any user-facing change?

yes, fix some improper usages of percentile_approx as cases list above

### How was this patch tested?

add ut

Closes #26905 from yaooqinn/SPARK-30266.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentile.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Percentile.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentileSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/ApproximatePercentileQuerySuite.scala']","The ApproximatePercentile and Percentile functions are having issues handling large or fractional values for accuracyExpression, null inputs, and ArrayType with nullable elements in percentageExpression; causing both overflow errors and MatchErrors."
6a424b93e5bdb79b1f1310cf48bd034397779e14,1591721805,"[SPARK-31830][SQL] Consistent error handling for datetime formatting and parsing functions

### What changes were proposed in this pull request?
Currently, `date_format` and `from_unixtime`, `unix_timestamp`,`to_unix_timestamp`, `to_timestamp`, `to_date`  have different exception handling behavior for formatting datetime values.

In this PR, we apply the exception handling behavior of `date_format` to `from_unixtime`, `unix_timestamp`,`to_unix_timestamp`, `to_timestamp` and `to_date`.

In the phase of creating the datetime formatted or formating, exceptions will be raised.

e.g.

```java
spark-sql> select date_format(make_timestamp(1, 1 ,1,1,1,1), 'yyyyyyyyyyy-MM-aaa');
20/05/28 15:25:38 ERROR SparkSQLDriver: Failed in [select date_format(make_timestamp(1, 1 ,1,1,1,1), 'yyyyyyyyyyy-MM-aaa')]
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'yyyyyyyyyyy-MM-aaa' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
```

```java
spark-sql> select date_format(make_timestamp(1, 1 ,1,1,1,1), 'yyyyyyyyyyy-MM-AAA');
20/05/28 15:26:10 ERROR SparkSQLDriver: Failed in [select date_format(make_timestamp(1, 1 ,1,1,1,1), 'yyyyyyyyyyy-MM-AAA')]
java.lang.IllegalArgumentException: Illegal pattern character: A
```

```java
spark-sql> select date_format(make_timestamp(1,1,1,1,1,1), 'yyyyyyyyyyy-MM-dd');
20/05/28 15:23:23 ERROR SparkSQLDriver: Failed in [select date_format(make_timestamp(1,1,1,1,1,1), 'yyyyyyyyyyy-MM-dd')]
java.lang.ArrayIndexOutOfBoundsException: 11
	at java.time.format.DateTimeFormatterBuilder$NumberPrinterParser.format(DateTimeFormatterBuilder.java:2568)
```
In the phase of parsing, `DateTimeParseException | DateTimeException | ParseException` will be suppressed, but `SparkUpgradeException` will still be raised

e.g.

```java
spark-sql> set spark.sql.legacy.timeParserPolicy=exception;
spark.sql.legacy.timeParserPolicy	exception
spark-sql> select to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"");
20/05/28 15:31:15 ERROR SparkSQLDriver: Failed in [select to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"")]
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse '2020-01-27T20:06:11.847-0800' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.
```

```java
spark-sql> set spark.sql.legacy.timeParserPolicy=corrected;
spark.sql.legacy.timeParserPolicy	corrected
spark-sql> select to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"");
NULL
spark-sql> set spark.sql.legacy.timeParserPolicy=legacy;
spark.sql.legacy.timeParserPolicy	legacy
spark-sql> select to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"");
2020-01-28 12:06:11.847
```

### Why are the changes needed?
Consistency

### Does this PR introduce _any_ user-facing change?

Yes, invalid datetime patterns will fail `from_unixtime`, `unix_timestamp`,`to_unix_timestamp`, `to_timestamp` and `to_date` instead of resulting `NULL`

### How was this patch tested?

add more tests

Closes #28650 from yaooqinn/SPARK-31830.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/datetime.sql', 'sql/core/src/test/scala/org/apache/spark/sql/DateFunctionsSuite.scala']","Inconsistencies in error handling for datetime formatting and parsing functions such as `date_format`, `from_unixtime`, `unix_timestamp`,`to_unix_timestamp`, `to_timestamp`, `to_date` may lead to discrepancies and confusion."
d3596c04b0275b19d6edc0126a77f749b4e9ba70,1595607206,"[SPARK-32406][SQL] Make RESET syntax support single configuration reset

### What changes were proposed in this pull request?

This PR extends the RESET command to support reset SQL configuration one by one.

### Why are the changes needed?

Currently, the reset command only supports restore all of the runtime configurations to their defaults. In most cases, users do not want this,  but just want to restore one or a small group of settings.
The SET command can work as a workaround for this, but you have to keep the defaults in your mind or by temp variables, which turns out not very convenient to use.

Hive supports this:
https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-BeelineExample

reset <key> | Resets the value of a particular configuration variable (key) to the default value.Note: If you misspell the variable name, Beeline will not show an error.
-- | --

PostgreSQL supports this too

https://www.postgresql.org/docs/9.1/sql-reset.html

### Does this PR introduce _any_ user-facing change?

yes, reset can restore one configuration now
### How was this patch tested?

add new unit tests.

Closes #29202 from yaooqinn/SPARK-32406.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala', 'sql/core/src/test/scala/org/apache/spark/sql/internal/SQLConfSuite.scala']","Current RESET command supports only restoring all runtime configurations to their defaults, causing inconvenience when users want to reset individual or small groups of settings."
45df6db906b39646f5b5f6b4a88addf1adcbe107,1609450495,"[SPARK-33906][WEBUI] Fix the bug of UI Executor page stuck due to undefined peakMemoryMetrics

### What changes were proposed in this pull request?
Check if the executorSummary.peakMemoryMetrics is defined before accessing it. Without checking, the UI has risked being stuck at the Executors page.

### Why are the changes needed?
App live UI may stuck at Executors page without this fix.
Steps to reproduce (with master branch):
In mac OS standalone mode, open a spark-shell
$SPARK_HOME/bin/spark-shell --master spark://localhost:7077

val x = sc.makeRDD(1 to 100000, 5)
x.count()

Then open the app UI in the browser, and click the Executors page, will get stuck at this page:
![image](https://user-images.githubusercontent.com/26694233/103105677-ca1a7380-45f4-11eb-9245-c69f4a4e816b.png)

Also, the return JSON from API endpoint http://localhost:4040/api/v1/applications/app-20201224134418-0003/executors miss ""peakMemoryMetrics"" for executor objects. I attached the full json text in https://issues.apache.org/jira/browse/SPARK-33906.

I debugged it and observed that ExecutorMetricsPoller
.getExecutorUpdates returns an empty map, which causes peakExecutorMetrics to None in https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/LiveEntity.scala#L345. The possible reason for returning the empty map is that the stage completion time is shorter than the heartbeat interval, so the stage entry in stageTCMP has already been removed before the reportHeartbeat is called.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Manual test, rerun the steps of bug reproduce and see the bug is gone.

Closes #30920 from baohe-zhang/SPARK-33906.

Authored-by: Baohe Zhang <baohe.zhang@verizonmedia.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['core/src/main/resources/org/apache/spark/ui/static/executorspage.js'],"Accessing undefined 'executorSummary.peakMemoryMetrics' in Spark UI results in the Executors page not loading, thereby leaving the entire app live UI stuck."
6099edc66eb35db548230eeaba791c730eb38f84,1631885282,"[SPARK-36764][SS][TEST] Fix race-condition on ""ensure continuous stream is being used"" in KafkaContinuousTest

### What changes were proposed in this pull request?

The test “ensure continuous stream is being used“ in KafkaContinuousTest quickly checks the actual type of the execution, and stops the query. Stopping the streaming query in continuous mode is done by interrupting query execution thread and join with indefinite timeout.

In parallel, started streaming query is going to generate execution plan, including running optimizer. Some parts of SessionState can be built at that time, as they are defined as lazy. The problem is, some of them seem to “swallow” the InterruptedException and let the thread run continuously.

That said, the query can’t indicate whether there is a request on stopping query, so the query won’t stop.

This PR fixes such scenario via ensuring that streaming query has started before the test stops the query.

### Why are the changes needed?

Race-condition could end up with test hang till test framework marks it as timed-out.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #34004 from HeartSaVioR/SPARK-36764.

Authored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaContinuousTest.scala'],"In the ""ensure continuous stream is being used"" KafkaContinuousTest, a race condition occurs where a request to stop the query is not recognized due to some parts of the SessionState ""swallowing"" the InterruptedException. This leads to the test hanging indefinitely."
6fa80ed1dd43c2ecd092c10933330b501641c51b,1605053820,"[SPARK-33337][SQL] Support subexpression elimination in branches of conditional expressions

### What changes were proposed in this pull request?

Currently we skip subexpression elimination in branches of conditional expressions including `If`, `CaseWhen`, and `Coalesce`. Actually we can do subexpression elimination for such branches if the subexpression is common across all branches. This patch proposes to support subexpression elimination in branches of conditional expressions.

### Why are the changes needed?

We may miss subexpression elimination chances in branches of conditional expressions. This kind of subexpression is frequently seen. It may be written manually by users or come from query optimizer. For example, project collapsing could embed expressions between two `Project`s and produces conditional expression like:

```
CASE WHEN jsonToStruct(json).a = '1' THEN 1.0 WHEN jsonToStruct(json).a = '2' THEN 2.0 ... ELSE 1.2 END
```

If `jsonToStruct(json)` is time-expensive expression, we don't eliminate the duplication and waste time on running it repeatedly now.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Unit test.

Closes #30245 from viirya/SPARK-33337.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Liang-Chi Hsieh <viirya@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala']","Subexpression elimination is currently skipped in branches of conditional expressions, leading to possible performance issues due to unnecessary replication of time-expensive expressions."
eb8c420edb48315ae4d2ea76c3af51c87f33d810,1571206939,"[SPARK-29349][SQL] Support FETCH_PRIOR in Thriftserver fetch request

### What changes were proposed in this pull request?

Support FETCH_PRIOR fetching in Thriftserver, and report correct fetch start offset it TFetchResultsResp.results.startRowOffset

The semantics of FETCH_PRIOR are as follow: Assuming the previous fetch returned a block of rows from offsets [10, 20)
* calling FETCH_PRIOR(maxRows=5) will scroll back and return rows [5, 10)
* calling FETCH_PRIOR(maxRows=10) again, will scroll back, but can't go earlier than 0. It will nevertheless return 10 rows, returning rows [0, 10) (overlapping with the previous fetch)
* calling FETCH_PRIOR(maxRows=4) again will again return rows starting from offset 0 - [0, 4)
* calling FETCH_NEXT(maxRows=6) after that will move the cursor forward and return rows [4, 10)

##### Client/server backwards/forwards compatibility:

Old driver with new server:
* Drivers that don't support FETCH_PRIOR will not attempt to use it
* Field TFetchResultsResp.results.startRowOffset was not set, old drivers don't depend on it.

New driver with old server
* Using an older thriftserver with FETCH_PRIOR will make the thriftserver return unsupported operation error. The driver can then recognize that it's an old server.
* Older thriftserver will return TFetchResultsResp.results.startRowOffset=0. If the client driver receives 0, it can know that it can not rely on it as correct offset. If the client driver intentionally wants to fetch from 0, it can use FETCH_FIRST.

### Why are the changes needed?

It's intended to be used to recover after connection errors. If a client lost connection during fetching (e.g. of rows [10, 20)), and wants to reconnect and continue, it could not know whether the request  got lost before reaching the server, or on the response back. When it issued another FETCH_NEXT(10) request after reconnecting, because TFetchResultsResp.results.startRowOffset was not set, it could not know if the server will return rows [10,20) (because the previous request didn't reach it) or rows [20, 30) (because it returned data from the previous request but the connection got broken on the way back). Now, with TFetchResultsResp.results.startRowOffset the client can know after reconnecting which rows it is getting, and use FETCH_PRIOR to scroll back if a fetch block was lost in transmission.

Driver should always use FETCH_PRIOR after a broken connection.
* If the Thriftserver returns unsuported operation error, the driver knows that it's an old server that doesn't support it. The driver then must error the query, as it will also not support returning the correct startRowOffset, so the driver cannot reliably guarantee if it hadn't lost any rows on the fetch cursor.
* If the driver gets a response to FETCH_PRIOR, it should also have a correctly set startRowOffset, which the driver can use to position itself back where it left off before the connection broke.
* If FETCH_NEXT was used after a broken connection on the first fetch, and returned with an startRowOffset=0, then the client driver can't know if it's 0 because it's the older server version, or if it's genuinely 0. Better to call FETCH_PRIOR, as scrolling back may anyway be possibly required after a broken connection.

This way it is implemented in a backwards/forwards compatible way, and doesn't require bumping the protocol version. FETCH_ABSOLUTE might have been better, but that would require a bigger protocol change, as there is currently no field to specify the requested absolute offset.

### Does this PR introduce any user-facing change?

ODBC/JDBC drivers connecting to Thriftserver may now implement using the FETCH_PRIOR fetch order to scroll back in query results, and check TFetchResultsResp.results.startRowOffset if their cursor position is consistent after connection errors.

### How was this patch tested?

Added tests to HiveThriftServer2Suites

Closes #26014 from juliuszsompolski/SPARK-29349.

Authored-by: Juliusz Sompolski <julek@databricks.com>
Signed-off-by: Yuming Wang <wgyumg@gmail.com>
","['sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala', 'sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala', 'sql/hive-thriftserver/v1.2.1/if/TCLIService.thrift', 'sql/hive-thriftserver/v1.2.1/src/main/java/org/apache/hive/service/cli/operation/Operation.java', 'sql/hive-thriftserver/v2.3.5/if/TCLIService.thrift', 'sql/hive-thriftserver/v2.3.5/src/main/java/org/apache/hive/service/cli/operation/Operation.java']","Thriftserver fetch request doesn't support FETCH_PRIOR, making recovery after connection errors difficult as the client can't reliably determine the start of data retrieval after a connection is re-established."
e736c62764137b2c3af90d2dc8a77e391891200a,1584379916,"[SPARK-31116][SQL] Fix nested schema case-sensitivity in ParquetRowConverter

### What changes were proposed in this pull request?

This PR (SPARK-31116) add caseSensitive parameter to ParquetRowConverter so that it handle materialize parquet properly with respect to case sensitivity

### Why are the changes needed?

From spark 3.0.0, below statement throws IllegalArgumentException in caseInsensitive mode because of explicit field index searching in ParquetRowConverter. As we already constructed parquet requested schema and catalyst requested schema during schema clipping in ParquetReadSupport, just follow these behavior.

```scala
val path = ""/some/temp/path""

spark
  .range(1L)
  .selectExpr(""NAMED_STRUCT('lowercase', id, 'camelCase', id + 1) AS StructColumn"")
  .write.parquet(path)

val caseInsensitiveSchema = new StructType()
  .add(
    ""StructColumn"",
    new StructType()
      .add(""LowerCase"", LongType)
      .add(""camelcase"", LongType))

spark.read.schema(caseInsensitiveSchema).parquet(path).show()
```

### Does this PR introduce any user-facing change?

No. The changes are only in unreleased branches (`master` and `branch-3.0`).

### How was this patch tested?

Passed new test cases that check parquet column selection with respect to schemas and case sensitivities

Closes #27888 from kimtkyeom/parquet_row_converter_case_sensitivity.

Authored-by: Tae-kyeom, Kim <kimtkyeom@devsisters.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala', 'sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala']","In case-insensitive mode, an IllegalArgumentException is thrown during explicit field index searching in ParquetRowConverter when attempting to handle parquet materialization in a nested schema."
51d350942865302e33480c83324704a54a24a494,1569186726,"[SPARK-28599][SQL] Fix `Execution Time` and `Duration` column sorting for ThriftServerSessionPage

### What changes were proposed in this pull request?

This PR add support sorting `Execution Time` and `Duration` columns for `ThriftServerSessionPage`.

### Why are the changes needed?

Previously, it's not sorted correctly.

### Does this PR introduce any user-facing change?

Yes.

### How was this patch tested?

Manually do the following and test sorting on those columns in the Spark Thrift Server Session Page.
```
$ sbin/start-thriftserver.sh
$ bin/beeline -u jdbc:hive2://localhost:10000
0: jdbc:hive2://localhost:10000> create table t(a int);
+---------+--+
| Result  |
+---------+--+
+---------+--+
No rows selected (0.521 seconds)
0: jdbc:hive2://localhost:10000> select * from t;
+----+--+
| a  |
+----+--+
+----+--+
No rows selected (0.772 seconds)
0: jdbc:hive2://localhost:10000> show databases;
+---------------+--+
| databaseName  |
+---------------+--+
| default       |
+---------------+--+
1 row selected (0.249 seconds)
```

**Sorted by `Execution Time` column**:
![image](https://user-images.githubusercontent.com/5399861/65387476-53038900-dd7a-11e9-885c-fca80287f550.png)

**Sorted by `Duration` column**:
![image](https://user-images.githubusercontent.com/5399861/65387481-6e6e9400-dd7a-11e9-9318-f917247efaa8.png)

Closes #25892 from wangyum/SPARK-28599.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/ui/ThriftServerSessionPage.scala'],Sorting by 'Execution Time' and 'Duration' columns on the ThriftServerSessionPage isn't working as expected.
2bc3272880515649d3e10eba135831a2ed0e3465,1496276677,"[SPARK-20894][SS] Resolve the checkpoint location in driver and use the resolved path in state store

## What changes were proposed in this pull request?

When the user runs a Structured Streaming query in a cluster, if the driver uses the local file system, StateStore running in executors will throw a file-not-found exception. However, the current error is not obvious.

This PR makes StreamExecution resolve the path in driver and uses the full path including the scheme part (such as `hdfs:/`, `file:/`) in StateStore.

Then if the above error happens, StateStore will throw an error with this full path which starts with `file:/`, and it makes this error obvious: the checkpoint location is on the local file system.

One potential minor issue is that the user cannot use different default file system settings in driver and executors (e.g., use a public HDFS address in driver and a private HDFS address in executors) after this change. However, since the batch query also has this issue (See https://github.com/apache/spark/blob/4bb6a53ebd06de3de97139a2dbc7c85fc3aa3e66/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L402), it doesn't make things worse.

## How was this patch tested?

The new added test.

Author: Shixiong Zhu <shixiong@databricks.com>

Closes #18149 from zsxwing/SPARK-20894.
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/test/DataStreamReaderWriterSuite.scala']","StateStore in Structured Streaming query throws file-not-found exception for checkpoint location resolved in driver, making it improbable to use different default file system settings in driver and executors."
70ef196d59c355e9fcba2abd7d0feda23d7f2c1e,1612790759,"[SPARK-34157][BUILD][FOLLOW-UP] Fix Scala 2.13 compilation error via using Array.deep

### What changes were proposed in this pull request?

This PR is a followup of https://github.com/apache/spark/pull/31245:

```
[error] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTablesSuite.scala:112:53: value deep is not a member of Array[String]
[error]         assert(sql(""show tables"").schema.fieldNames.deep ==
[error]                                                     ^
[error] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTablesSuite.scala:115:72: value deep is not a member of Array[String]
[error]         assert(sql(""show table extended like 'tbl'"").schema.fieldNames.deep ==
[error]                                                                        ^
[error] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTablesSuite.scala:121:55: value deep is not a member of Array[String]
[error]           assert(sql(""show tables"").schema.fieldNames.deep ==
[error]                                                       ^
[error] /home/runner/work/spark/spark/sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTablesSuite.scala:124:74: value deep is not a member of Array[String]
[error]           assert(sql(""show table extended like 'tbl'"").schema.fieldNames.deep ==
[error]                                                                          ^
```

It broke Scala 2.13 build. This PR works around by using ScalaTests' `===` that can compare `Array`s safely.

### Why are the changes needed?

To fix the build.

### Does this PR introduce _any_ user-facing change?

No, dev-only.

### How was this patch tested?

CI in this PR should test it out.

Closes #31526 from HyukjinKwon/SPARK-34157.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTablesSuite.scala'],Scala 2.13 build fails due to 'value deep is not a member of Array[String]' error showing up in multiple instances in ShowTablesSuite.scala
2b6dfa5f7bdd2f2ae7b4d53bb811ccb8563377c5,1604246277,"[SPARK-20044][UI] Support Spark UI behind front-end reverse proxy using a path prefix Revert proxy url

### What changes were proposed in this pull request?

Allow to run the Spark web UI behind a reverse proxy with URLs prefixed by a context root, like www.mydomain.com/spark. In particular, this allows to access multiple Spark clusters through the same virtual host, only distinguishing them by context root, like www.mydomain.com/cluster1, www.mydomain.com/cluster2, and it allows to run the Spark UI in a common cookie domain (for SSO) with other services.

### Why are the changes needed?

This PR is to take over https://github.com/apache/spark/pull/17455.
After changes, Spark allows showing customized prefix URL in all the `href` links of the HTML pages.

### Does this PR introduce _any_ user-facing change?

Yes, all the links of UI pages will be contains the value of `spark.ui.reverseProxyUrl` if it is configurated.
### How was this patch tested?

New HTML Unit tests in MasterSuite
Manual UI testing for master, worker and app UI with an nginx proxy
Spark config:
```
spark.ui.port 8080
spark.ui.reverseProxy=true
spark.ui.reverseProxyUrl=/path/to/spark/
```
nginx config:
```
server {
    listen 9000;
    set $SPARK_MASTER http://127.0.0.1:8080;
    # split spark UI path into prefix and local path within master UI
    location ~ ^(/path/to/spark/) {
        # strip prefix when forwarding request
        rewrite /path/to/spark(/.*) $1  break;
        #rewrite /path/to/spark/ ""/"" ;
        # forward to spark master UI
        proxy_pass $SPARK_MASTER;
        proxy_intercept_errors on;
        error_page 301 302 307 = handle_redirects;
    }
    location handle_redirects {
        set $saved_redirect_location '$upstream_http_location';
        proxy_pass $saved_redirect_location;
    }
}
```

Closes #29820 from gengliangwang/revertProxyURL.

Lead-authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Co-authored-by: Oliver Köth <okoeth@de.ibm.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['core/src/main/scala/org/apache/spark/SparkContext.scala', 'core/src/main/scala/org/apache/spark/deploy/master/Master.scala', 'core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala', 'core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala', 'core/src/main/scala/org/apache/spark/ui/UIUtils.scala', 'core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala']","Spark UI doesn't function correctly when run behind a reverse proxy with URLs prefixed by a context root, affecting ability to access multiple Spark clusters through the same virtual host and preventing Spark UI from running in a common cookie domain."
3433f2a77d3dd665f42aa3d558152cf4c912c54c,1670544883,"[SPARK-41452][SQL] `to_char` should return null when format is null

### What changes were proposed in this pull request?

When a user specifies a null format in `to_char`, return null instead of throwing a `NullPointerException`.

### Why are the changes needed?

`to_char` currently throws a `NullPointerException` when the format is null:
```
spark-sql> select to_char(454, null);
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
org.apache.spark.SparkException: [INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
...
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormat$lzycompute(numberFormatExpressions.scala:227)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormat(numberFormatExpressions.scala:227)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormatter$lzycompute(numberFormatExpressions.scala:228)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.numberFormatter(numberFormatExpressions.scala:228)
	at org.apache.spark.sql.catalyst.expressions.ToCharacter.checkInputDataTypes(numberFormatExpressions.scala:236)
```
Compare to `to_binary`:
```
spark-sql> SELECT to_binary('abc', null);
NULL
Time taken: 3.097 seconds, Fetched 1 row(s)
spark-sql>
```
Also compare to `to_char` in PostgreSQL 14.6:
```
select to_char(454, null) is null as to_char_is_null;

 to_char_is_null
-----------------
 t
(1 row)
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New unit test.

Closes #38986 from bersprockets/to_char_issue.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala']",Using `to_char` function with a null format in Spark SQL throws a NullPointerException instead of returning null.
949d71283932ba4ce50aa6b329665e0f8be7ecf1,1554247661,"[SPARK-27346][SQL] Loosen the newline assert condition on 'examples' field in ExpressionInfo

## What changes were proposed in this pull request?

I haven't tested by myself on Windows and I am not 100% sure if this is going to cause an actual problem.

However, this one line:

https://github.com/apache/spark/blob/827383a97c11a61661440ff86ce0c3382a2a23b2/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionInfo.java#L82

made me to investigate a lot today.

Given my speculation, if Spark is built in Linux and it's executed on Windows, it looks possible for multiline strings, like,

https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala#L146-L150

to throw an exception because the newline in the binary is `\n` but `System.lineSeparator` returns `\r\n`.

I think this is not yet found because this particular codes are not released yet (see SPARK-26426).

Looks just better to loosen the condition and forget about this stuff.

This should be backported into branch-2.4 as well.

## How was this patch tested?

N/A

Closes #24274 from HyukjinKwon/SPARK-27346.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionInfo.java'],Possible exception thrown when multiline strings are built in Linux and executed on Windows due to a mismatch between the newline in the binary (`\n`) and `System.lineSeparator` return (`\r\n`).
2931993e059f5d3741fc09438b7da88ccd8d4446,1681330315,"[SPARK-42437][PYTHON][CONNECT] PySpark catalog.cacheTable will allow to specify storage level

Currently PySpark version of `catalog.cacheTable` function does not support to specify storage level. This is to add that.

After changes:

## Spark Connect

```
bin/pyspark --remote ""local[*]""
Python 3.9.5 (default, Nov 23 2021, 15:27:38)
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/02/17 20:41:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to

      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.0.dev0
      /_/

Using Python version 3.9.5 (default, Nov 23 2021 15:27:38)
Client connected to the Spark Connect server at localhost
SparkSession available as 'spark'.
>>> spark.range(1).write.saveAsTable(""tab1"", format=""csv"", mode=""overwrite"")
>>> spark.catalog.listTables()
[Table(name='tab1', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]
>>> spark.catalog.cacheTable(""tab1"")
>>> spark.catalog.isCached(""tab1"")
True
>>> spark.catalog.clearCache()
>>> spark.catalog.isCached(""tab1"")
False
>>> from pyspark.storagelevel import StorageLevel
>>> spark.catalog.cacheTable(""tab1"", StorageLevel.OFF_HEAP)
>>> spark.catalog.isCached(""tab1"")
True
```

## PySpark
```
/home/spark# bin/pyspark
Python 3.9.5 (default, Nov 23 2021, 15:27:38)
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/02/17 20:43:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.0-SNAPSHOT
      /_/

Using Python version 3.9.5 (default, Nov 23 2021 15:27:38)
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1676666626670).
SparkSession available as 'spark'.

>>> spark.range(1).write.saveAsTable(""tab2"", format=""csv"", mode=""overwrite"")
>>> spark.catalog.listTables()
[Table(name='tab2', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]
>>> spark.catalog.cacheTable(""tab2"")
>>>
>>> spark.catalog.isCached(""tab2"")
True
>>> spark.catalog.clearCache()
>>> spark.catalog.isCached(""tab2"")
False
>>> from pyspark.storagelevel import StorageLevel
>>> spark.catalog.cacheTable(""tab2"", StorageLevel.OFF_HEAP)
>>> spark.catalog.isCached(""tab2"")
True

```

### What changes were proposed in this pull request?
Add extra parameter to catalog.cacheTable

### Why are the changes needed?
To allow users specify which storage level to use in cache in PySpark/Connect code

### Does this PR introduce _any_ user-facing change?
Yes

### How was this patch tested?
Updated existing test cases

Closes #40015 from khalidmammadov/add_storage_level_cache_table.

Authored-by: Khalid Mammadov <khalidmammadov9@gmail.com>
Signed-off-by: Takuya UESHIN <ueshin@databricks.com>
","['python/pyspark/sql/catalog.py', 'python/pyspark/sql/connect/catalog.py', 'python/pyspark/sql/connect/plan.py', 'python/pyspark/sql/tests/test_catalog.py', 'python/pyspark/sql/tests/test_dataframe.py']","The `catalog.cacheTable` function in PySpark does not support specifying a storage level, limiting user control over cache settings in PySpark/Connect code."
c01152dd22093e9f5d2aa533598e4d4209d30922,1547408352,"[SPARK-23182][CORE] Allow enabling TCP keep alive on the RPC connections

## What changes were proposed in this pull request?

Make it possible for the master to enable TCP keep alive on the RPC connections with clients.

## How was this patch tested?

Manually tested.

Added the following:
```
spark.rpc.io.enableTcpKeepAlive  true
```
to spark-defaults.conf.

Observed the following on the Spark master:
```
$ netstat -town | grep 7077
tcp6       0      0 10.240.3.134:7077       10.240.1.25:42851       ESTABLISHED keepalive (6736.50/0/0)
tcp6       0      0 10.240.3.134:44911      10.240.3.134:7077       ESTABLISHED keepalive (4098.68/0/0)
tcp6       0      0 10.240.3.134:7077       10.240.3.134:44911      ESTABLISHED keepalive (4098.68/0/0)
```

Which proves that the keep alive setting is taking effect.

It's currently possible to enable TCP keep alive on the worker / executor, but is not possible to configure on other RPC connections. It's unclear to me why this could be the case. Keep alive is more important for the master to protect it against suddenly departing workers / executors, thus I think it's very important to have it. Particularly this makes the master resilient in case of using preemptible worker VMs in GCE. GCE has the concept of shutdown scripts, which it doesn't guarantee to execute. So workers often don't get shutdown gracefully and the TCP connections on the master linger as there's nothing to close them. Thus the need of enabling keep alive.

This enables keep-alive on connections besides the master's connections, but that shouldn't cause harm.

Closes #20512 from peshopetrov/master.

Authored-by: Petar Petrov <petar.petrov@leanplum.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
","['common/network-common/src/main/java/org/apache/spark/network/server/TransportServer.java', 'common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java']","TCP keep alive is not enabled on RPC connections with clients, causing lingering TCP connections if workers don't shutdown gracefully."
0b2eefb674151a0af64806728b38d9410da552ec,1515695855,"[SPARK-22994][K8S] Use a single image for all Spark containers.

This change allows a user to submit a Spark application on kubernetes
having to provide a single image, instead of one image for each type
of container. The image's entry point now takes an extra argument that
identifies the process that is being started.

The configuration still allows the user to provide different images
for each container type if they so desire.

On top of that, the entry point was simplified a bit to share more
code; mainly, the same env variable is used to propagate the user-defined
classpath to the different containers.

Aside from being modified to match the new behavior, the
'build-push-docker-images.sh' script was renamed to 'docker-image-tool.sh'
to more closely match its purpose; the old name was a little awkward
and now also not entirely correct, since there is a single image. It
was also moved to 'bin' since it's not necessarily an admin tool.

Docs have been updated to match the new behavior.

Tested locally with minikube.

Author: Marcelo Vanzin <vanzin@cloudera.com>

Closes #20192 from vanzin/SPARK-22994.
","['bin/docker-image-tool.sh', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/InitContainerBootstrap.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/steps/BasicDriverConfigurationStep.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodFactory.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/DriverConfigOrchestratorSuite.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/steps/BasicDriverConfigurationStepSuite.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/steps/initcontainer/InitContainerConfigOrchestratorSuite.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodFactorySuite.scala', 'resource-managers/kubernetes/docker/src/main/dockerfiles/spark-base/entrypoint.sh', 'resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh']","Submitting a Spark application on Kubernetes requires providing multiple images for different types of containers, leading to complexity and potential inconsistencies in configurations."
0ba6ba9a3829cf63f8917367ea3c066e422ad04f,1677187410,"[SPARK-25050][SQL] Avro: writing complex unions

### What changes were proposed in this pull request?

Spark was able to read complex unions already but not write them.
Now it is possible to also write them.  If you have a schema with a complex union the following code is now working:

```scala
spark
  .read.format(""avro"").option(""avroSchema"", avroSchema).load(path)
  .write.format(""avro"").option(""avroSchema"", avroSchema).save(""/tmp/b"")
```
While before this patch it would throw `Unsupported Avro UNION type` when writing.

Add the capability to write complex unions, next to reading them.
Complex unions map to struct types where field names are member0, member1, etc.
This is consistent with the behavior in SchemaConverters for reading them
and when converting between Avro and Parquet.

### Why are the changes needed?
Fixes SPARK-25050, lines up read and write compatibility.

### Does this PR introduce _any_ user-facing change?
The behaviour improved of course, this is as far as I could see not impacting any customer facing API's or documentation.

### How was this patch tested?
- Added extra unit tests.
- Updated existing unit tests for improved behaviour.
- Validated manually with an internal corpus of avro files if they now could be read and written without problems.  Which was not before this patch.

Closes #36506 from steven-aerts/spark-25050.

Authored-by: Steven Aerts <steven.aerts@gmail.com>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['connector/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala', 'connector/avro/src/main/scala/org/apache/spark/sql/avro/AvroSerializer.scala', 'connector/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala', 'connector/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala', 'connector/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala', 'connector/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala']","Spark is unable to write complex unions in avro format, throwing ""Unsupported Avro UNION type"" error and resulting in compatibility issues between reading and writing operations."
48e333af5473f849274c4247313ea7b51de70faf,1628787754,"[SPARK-36445][SQL][FOLLOWUP] ANSI type coercion: revisit promoting string literals in datetime expressions

### What changes were proposed in this pull request?

1. Promote more string literal in subtractions. In the ANSI type coercion rule, we already promoted
```
string - timestamp => cast(string as timestamp) - timestamp
```
This PR is to promote the following string literals:
```
string - date => cast(string as date) - date
date - string => date - cast(date as string)
timestamp - string => timestamp
```
It is very straightforward to cast the string literal as the data type of the other side in the subtraction.

2. Merge the string promotion logic from the rule `StringLiteralCoercion`:
```
date_sub(date, string) => date_sub(date, cast(string as int))
date_add(date, string) => date_add(date, cast(string as int))
```

### Why are the changes needed?

1. Promote the string literal in the subtraction as the data type of the other side. This is straightforward and consistent with PostgreSQL
2. Certerize all the string literal promotion in the ANSI type coercion rule

### Does this PR introduce _any_ user-facing change?

No, the new ANSI type coercion rules are not released yet.

### How was this patch tested?

Existing UT

Closes #33724 from gengliangwang/datetimeTypeCoercion.

Authored-by: Gengliang Wang <gengliang@apache.org>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AnsiTypeCoercion.scala', 'sql/core/src/test/resources/sql-tests/inputs/date.sql', 'sql/core/src/test/resources/sql-tests/inputs/timestamp.sql']",ANSI type coercion rule is not promoting certain string literals in datetime expressions and inconsistencies exist in promotion logic from the rule StringLiteralCoercion.
882122d6b727463a5d971080fe68831b9f2ecd64,1624058458,"[SPARK-35565][SS] Add config for ignoring metadata directory of FileStreamSink

### What changes were proposed in this pull request?

This patch proposes to add an internal config for ignoring metadata of `FileStreamSink` when reading the output path.

### Why are the changes needed?

`FileStreamSink` produces a metadata directory which logs output files per micro-batch. When we read from the output path, Spark will look at the metadata and ignore other files not in the log.

Normally it works well. But for some use-cases, we may need to ignore the metadata when reading the output path. For example, when we change the streaming query and must to run it with new checkpoint directory, we cannot use previous metadata. If we create a new metadata too, when we read the output path later in Spark, Spark only reads the files listed in the new metadata. The files written before we use new checkpoint and metadata are ignored by Spark.

Although seems we can output to different output directory every time, but it is bad idea as we will produce many directories unnecessarily.

We need a config for ignoring the metadata of `FileStreamSink` when reading the output path.

### Does this PR introduce _any_ user-facing change?

Added a config for ignoring metadata of FileStreamSink when reading the output.

### How was this patch tested?

Unit tests.

Closes #32702 from viirya/ignore-metadata.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSinkSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala']","When changing a streaming query and running it with a new checkpoint directory, Spark ignores files written before the change due to its reliance on previous metadata, possibly causing data loss."
c8c082ce380b2357623511c6625503fb3f1d65bf,1599535333,"[SPARK-32812][PYTHON][TESTS] Avoid initiating a process during the main process for run-tests.py

### What changes were proposed in this pull request?

In certain environments, seems it fails to run `run-tests.py` script as below:

```
Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
...

raise RuntimeError('''
RuntimeError:
 An attempt has been made to start a new process before the
 current process has finished its bootstrapping phase.

This probably means that you are not using fork to start your
 child processes and you have forgotten to use the proper idiom
 in the main module:

if __name__ == '__main__':
 freeze_support()
 ...

The ""freeze_support()"" line can be omitted if the program
 is not going to be frozen to produce an executable.
Traceback (most recent call last):
...
 raise EOFError
EOFError

```

The reason is that `Manager.dict()` launches another process when the main process is initiated.

It works in most environments for an unknown reason but it should be good to avoid such pattern as guided from Python itself.

### Why are the changes needed?

To prevent the test failure for Python.

### Does this PR introduce _any_ user-facing change?

No, it fixes a test script.

### How was this patch tested?

Manually ran the script after fixing.

```
Running PySpark tests. Output is in /.../python/unit-tests.log
Will test against the following Python executables: ['/.../python3', 'python3.8']
Will test the following Python tests: ['pyspark.sql.dataframe']
/.../python3 python_implementation is CPython
/.../python3 version is: Python 3.8.5
python3.8 python_implementation is CPython
python3.8 version is: Python 3.8.5
Starting test(/.../python3): pyspark.sql.dataframe
Starting test(python3.8): pyspark.sql.dataframe
Finished test(/.../python3): pyspark.sql.dataframe (33s)
Finished test(python3.8): pyspark.sql.dataframe (34s)
Tests passed in 34 seconds
```

Closes #29666 from itholic/SPARK-32812.

Authored-by: itholic <haejoon309@naver.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['python/run-tests.py'],The `run-tests.py` script fails to execute in certain environments due to a RuntimeError related to an attempted process launch during the main process initiation.
a1b90bfc0faa2b4b2b7388443a734a217792d585,1569434348,"[SPARK-23197][STREAMING][TESTS] Fix ReceiverSuite.""receiver_life_cycle"" to not rely on timing

### What changes were proposed in this pull request?

This patch changes ReceiverSuite.""receiver_life_cycle"" to record actual calls with timestamp in FakeReceiver/FakeReceiverSupervisor, which doesn't rely on timing of stopping and starting receiver in restarting receiver. It enables us to give enough huge timeout on verification of restart as we can verify both stopping and starting together.

### Why are the changes needed?

The test is flaky without this patch. We increased timeout to fix flakyness of this test (https://github.com/apache/spark/commit/15adcc8273e73352e5e1c3fc9915c0b004ec4836) but even with longer timeout it has been still failing intermittently.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

I've reproduced test failure artificially via below diff:

```
diff --git a/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala b/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
index faf6db82d5..d8977543c0 100644
--- a/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
+++ b/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala
 -191,9 +191,11  private[streaming] abstract class ReceiverSupervisor(
       // thread pool.
       logWarning(""Restarting receiver with delay "" + delay + "" ms: "" + message,
         error.getOrElse(null))
+      Thread.sleep(1000)
       stopReceiver(""Restarting receiver with delay "" + delay + ""ms: "" + message, error)
       logDebug(""Sleeping for "" + delay)
       Thread.sleep(delay)
+      Thread.sleep(1000)
       logInfo(""Starting receiver again"")
       startReceiver()
       logInfo(""Receiver started again"")
```

and confirmed this patch doesn't fail with the change.

Closes #25862 from HeartSaVioR/SPARK-23197-v2.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['streaming/src/test/scala/org/apache/spark/streaming/ReceiverSuite.scala'],"ReceiverSuite's ""receiver_life_cycle"" test is flaky and fails intermittently, even with increased timeout, due to dependence on timing of receiver restarting."
0be4b9799dadee7d59758172768da5be8162fcca,1686774430,"[SPARK-44057][SQL][TESTS] Mark all `local-cluster`-based tests as `ExtendedSQLTest`

### What changes were proposed in this pull request?

This PR aims to mark all `local-cluster`-based tests as `ExtendedSQLTest`.

```
$ git grep local-cluster sql/core/
sql/core/src/test/scala/org/apache/spark/sql/SparkSessionBuilderSuite.scala:    val session = SparkSession.builder().master(""local-cluster[3, 1, 1024]"").getOrCreate()
sql/core/src/test/scala/org/apache/spark/sql/SparkSessionBuilderSuite.scala:    val session = SparkSession.builder().master(""local-cluster[3, 1, 1024]"").getOrCreate()
sql/core/src/test/scala/org/apache/spark/sql/execution/BroadcastExchangeSuite.scala:// Additional tests run in 'local-cluster' mode.
sql/core/src/test/scala/org/apache/spark/sql/execution/BroadcastExchangeSuite.scala:      .setMaster(""local-cluster[2,1,1024]"")
sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSparkSubmitSuite.scala:      ""--master"", ""local-cluster[1,1,1024]"",
sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetCommitterSuite.scala:   * Create a new [[SparkSession]] running in local-cluster mode with unsafe and codegen enabled.
sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetCommitterSuite.scala:      .master(""local-cluster[2,1,1024]"")
sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala: * Tests in this suite we need to run Spark in local-cluster mode. In particular, the use of
sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala:   * Create a new [[SparkSession]] running in local-cluster mode with unsafe and codegen enabled.
sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala:      .master(""local-cluster[2,1,512]"")
sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDDSuite.scala:          .config(sparkConf.setMaster(""local-cluster[2, 1, 1024]""))
sql/core/src/test/scala/org/apache/spark/sql/internal/ExecutorSideSQLConfSuite.scala:  // Create a new [[SparkSession]] running in local-cluster mode.
sql/core/src/test/scala/org/apache/spark/sql/internal/ExecutorSideSQLConfSuite.scala:      .master(""local-cluster[2,1,1024]"")
```

### Why are the changes needed?

1. Like the following, we still see instability at `sql - others` pipeline. `ExecutorSideSQLConfSuite`-like suite are launching `local-cluster`.
- https://github.com/apache/spark/actions/runs/5264282377/jobs/9518640298

```
2023-06-14T12:32:17.9695374Z [0m[[0m[0minfo[0m] [0m[0m[32mExecutorSideSQLConfSuite:[0m[0m
2023-06-14T12:32:33.7423897Z [0m[[0m[0minfo[0m] [0m[0m[32m- ReadOnlySQLConf is correctly created at the executor side (15 seconds, 433 milliseconds)[0m[0m
...
2023-06-14T12:33:09.1262815Z
2023-06-14T12:33:09.4014734Z ##[error]The runner has received a shutdown signal. This can happen when the runner service is stopped, or a manually started runner is canceled.
2023-06-14T12:33:09.4599817Z Session terminated, killing shell...
2023-06-14T12:33:09.4999238Z ##[error]Process completed with exit code 143.
2023-06-14T12:33:09.7944983Z Cleaning up orphan processes
2023-06-14T12:33:09.9209125Z Terminate orphan process: pid (4095) (java)
2023-06-14T12:33:09.9911496Z Terminate orphan process: pid (628352) (java)
```

2. In addition, this PR balances two SQL pipelines.
- https://github.com/apache/spark/actions/runs/5263147272/jobs/9513024519 (`sql - slow tests` took 1h 26m 55s)
- https://github.com/apache/spark/actions/runs/5263147272/jobs/9513024582 (`sql - other tests` took 2h 22m 20s)

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs

Closes #41601 from dongjoon-hyun/SPARK-44057.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/core/src/test/scala/org/apache/spark/sql/SparkSessionBuilderSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/BroadcastExchangeSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetCommitterSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDDSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/internal/ExecutorSideSQLConfSuite.scala']","Stability issues with `sql - others` pipeline due to `ExecutorSideSQLConfSuite`-like suites launching `local-cluster`, potentially causing an imbalance in SQL pipeline times."
2ec9b866285fc059cae6816033babca64b4da7ec,1592330397,"[SPARK-31929][WEBUI] Close leveldbiterator when leveldb.close

### What changes were proposed in this pull request?

Close LevelDBIterator when LevelDB.close() is called.

### Why are the changes needed?

This pull request would prevent JNI resources leaking from Level DB instance and its' iterators. In before implementation JNI resources from LevelDBIterator are cleaned by finalize() function. This behavior is also mentioned in comments of [""LevelDBIterator.java""](https://github.com/apache/spark/blob/master/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java) by squito . But if DB instance is already closed, then iterator's close method would be ignored. LevelDB's iterator would keep level db files opened (for the case table cache is filled up), till iterator.close() is called. Then these JNI resources (file handle) would be leaked.
This JNI resource leaking issue would cause the problem described in [SPARK-31929](https://issues.apache.org/jira/browse/SPARK-31929) on Windows: in spark history server, leaked file handle for level db files would trigger ""IOException"" when HistoryServerDiskManager try to remove them for releasing disk space.
![IOException](https://user-images.githubusercontent.com/10524738/84134659-7c388680-aa7b-11ea-807f-04dcfa7886a0.JPG)

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Add unit test and manually tested it.

Closes #28769 from zhli1142015/close-leveldbiterator-when-leveldb.close.

Authored-by: Zhen Li <zhli@microsoft.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDB.java', 'common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java', 'common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java', 'core/src/main/scala/org/apache/spark/status/AppStatusStore.scala']",LevelDB instance and iterators are leaking JNI resources which results in IOExceptions during attempts to release disk space on Windows operated Spark History Server.
dfd8a8dc676c388c0c1bb7e4cb8d55eab10504ab,1623105832,"[SPARK-35341][PYTHON] Introduce BooleanExtensionOps

### What changes were proposed in this pull request?

- Introduce BooleanExtensionOps in order to make boolean operators `and` and `or` data-type-based.
- Improve error messages for operators `and` and `or`.

### Why are the changes needed?

Boolean operators __and__, __or__, __rand__, and __ror__ should be data-type-based

BooleanExtensionDtypes processes these boolean operators differently from bool, so BooleanExtensionOps is introduced.

These boolean operators themselves are also bitwise operators, which should be able to apply to other data types classes later. However, this is not the goal of this PR.

### Does this PR introduce _any_ user-facing change?

Yes. Error messages for operators `and` and `or` are improved.
Before:
```
>>> psser = ps.Series([1, ""x"", ""y""], dtype=""category"")
>>> psser | True
Traceback (most recent call last):
...
pyspark.sql.utils.AnalysisException: cannot resolve '(`0` OR true)' due to data type mismatch: differing types in '(`0` OR true)' (tinyint and boolean).;
'Project [unresolvedalias(CASE WHEN (isnull(0#9) OR isnull((0#9 OR true))) THEN false ELSE (0#9 OR true) END, Some(org.apache.spark.sql.Column$$Lambda$1442/17254916406fb8afba))]
+- Project [__index_level_0__#8L, 0#9, monotonically_increasing_id() AS __natural_order__#12L]
   +- LogicalRDD [__index_level_0__#8L, 0#9], false

```

After:
```
>>> psser = ps.Series([1, ""x"", ""y""], dtype=""category"")
>>> psser | True
Traceback (most recent call last):
...
TypeError: Bitwise or can not be applied to categoricals.
```

### How was this patch tested?

Unit tests.

Closes #32698 from xinrong-databricks/datatypeops_extension.

Authored-by: Xinrong Meng <xinrong.meng@databricks.com>
Signed-off-by: Takuya UESHIN <ueshin@databricks.com>
","['python/pyspark/pandas/base.py', 'python/pyspark/pandas/data_type_ops/base.py', 'python/pyspark/pandas/data_type_ops/boolean_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_date_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_num_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_string_ops.py', 'python/pyspark/pandas/tests/data_type_ops/testing_utils.py']","Boolean operators `and` and `or` are not data-type-based, causing incompatible type error messages when applied to non-boolean data types."
ba2d785b99461871f588de6a8260f3201204f313,1684784626,"[SPARK-43290][SQL] Adds AES IV and AAD support to ExpressionImplUtils

### What changes were proposed in this pull request?
This change adds support for optional IV and AAD fields to ExpressionImplUtils, which is the underlying library to support `aes_encrypt` and `aes_decrypt`. This allows callers to specify their own initialization vector values for some specific use cases, and to take advantage of AES-GCM's authenticated additional data optional input.

This change does **not** add the support to the user-facing `aes_encrypt` and `aes_decrypt` yet. That will be added in a follow-up, rather than in a single complex change.

### Why are the changes needed?

There are some use cases where callers to ExpressionImplUtils via aes_encrypt may want to provide initialization vectors (IVs) or additional authenticated data (AAD). The most common cases will be:
1. Ensuring that ciphertext matches values that have been encrypted by external tools. In those cases, the caller will need to provide an identical IV value.
2. For AES-CBC mode, there are some cases where callers want to generate deterministic encrypted output.
3. For AES-GCM mode, providing AAD fields allows callers to bind additional data to an encrypted ciphertext so that it can only be decrypted by a caller providing the same value. This is often used to enforce some context.

### Does this PR introduce _any_ user-facing change?

Not yet. This change adds support to the underlying implementation, but does not yet update the SQL support to include the new parameters.

### How was this patch tested?

All existing unit tests still pass and new tests in `ExpressionImplUtilsSuite` exercise the new code paths:
```
build/sbt ""sql/test:testOnly org.apache.spark.sql.DataFrameFunctionsSuite""
build/sbt ""catalyst/test:testOnly org.apache.spark.sql.catalyst.expressions.ExpressionImplUtilsSuite""
```

Closes #40970 from sweisdb/SPARK-43290.

Lead-authored-by: Steve Weis <steve.weis@databricks.com>
Co-authored-by: sweisdb <60895808+sweisdb@users.noreply.github.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtilsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala']","ExpressionImplUtils lacks support for specifying initialization vectors (IVs) or additional authenticated data (AAD) when using aes_encrypt and aes_decrypt, limiting its functionality in certain use cases."
3c80583879c59fc6ac051d5804c12b835e078059,1668631412,"[SPARK-40940] Remove Multi-stateful operator checkers for streaming queries

### What changes were proposed in this pull request?

As a followup to [SPARK-40925], [github PR](https://github.com/apache/spark/pull/38405), Remove corresponding checks in UnsupportedOperationChecker so that customers don't have to explicitly add new conf withSQLConf(""spark.sql.streaming.unsupportedOperationCheck"" -> ""false"") to use the new multi-stateful operators. In other words we are enabling multi-stateful operators by default.

As a side effect, the API of `checkStreamingQueryGlobalWatermarkLimit(LogicalPlan, OutputMode)` is also changed to `checkStreamingQueryGlobalWatermarkLimit(LogicalPlan)`

New tests are added to `MultiStatefulOperatorsSuite.scala`, but I could also add equivalent ones to `UnsupportedOperationsSuite.scala` if requested.

### Why are the changes needed?

To enable new multiple-stateful operators by default. Right now users need to set SQL conf `unsupportedOperationCheck` to false explicitly, which also disables many other useful checks.

### Does this PR introduce _any_ user-facing change?

No. All current running queries won't be impacted. But new queries could use chained stateful operators.

### How was this patch tested?

Unit Tests.

Closes #38503 from WweiL/SPARK-40940-multi-state-checkers.

Authored-by: Wei Liu <wei.liu@databricks.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/MultiStatefulOperatorsSuite.scala']","Multiple-stateful operators in streaming queries are not enabled by default, requiring users to manually change SQL configuration ""unsupportedOperationCheck"" to false to use them."
6d6ef769c8fd843f17732f60e5410e8cee75d9ed,1636846879,"[SPARK-37291][PYTHON][SQL] PySpark init SparkSession should copy conf to sharedState

### What changes were proposed in this pull request?
When use write pyspark script like
```
conf = SparkConf().setAppName(""test"")
sc = SparkContext(conf = conf)
session = SparkSession().build().enableHiveSupport().getOrCreate()
```

It will build a session without hive support since we use a existed SparkContext and we create SparkSession use
```
SparkSession(sc)
```
This cause we loss configuration added by `config()` such as catalog implement.

In scala class `SparkSession`, we create `SparkSession` with `SparkContext` and option configurations and will pass option configurations to `SharedState` then use `SharedState`'s conf create SessionState, but in pyspark, we won't pass options configuration to `SharedState`, but pass to `SessionState`, but this time `SessionState` has been initialized.  So it won't support hive.

In this pr, I pass option configurations to `SharedState` when first init `SparkSession`, then when  init `SessionState`, this options will be passed to `SessionState` too.

### Why are the changes needed?
Avoid loss configuration when build SparkSession in pyspark

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Manuel tested & added UT

Closes #34559 from AngersZhuuuu/SPARK-37291.

Authored-by: Angerszhuuuu <angers.zhu@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['python/pyspark/sql/session.py', 'python/pyspark/sql/tests/test_session.py']","In PySpark, building SparkSession with an existing SparkContext lead to loss of configurations added by `config()`, especially when initializing SparkSession with hive support."
ba8abdda3703ce9d60e26678290739d080020418,1676625077,"[SPARK-42474][CORE][K8S] Add extraJVMOptions JVM GC option K8s test cases

### What changes were proposed in this pull request?

This PR aims to add JVM GC option test coverage to K8s Integration Suite.
To reuse the existing code, `isG1GC` variable is moved from `MemoryManager` to `Utils`.

### Why are the changes needed?

To provide more test coverage for JVM Options.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs

```
[info] KubernetesSuite:
[info] - SPARK-42190: Run SparkPi with local[*] (4 seconds, 990 milliseconds)
[info] - Run SparkPi with no resources (7 seconds, 101 milliseconds)
[info] - Run SparkPi with no resources & statefulset allocation (7 seconds, 27 milliseconds)
[info] - Run SparkPi with a very long application name. (7 seconds, 100 milliseconds)
[info] - Use SparkLauncher.NO_RESOURCE (7 seconds, 947 milliseconds)
[info] - Run SparkPi with a master URL without a scheme. (6 seconds, 932 milliseconds)
[info] - Run SparkPi with an argument. (9 seconds, 47 milliseconds)
[info] - Run SparkPi with custom labels, annotations, and environment variables. (6 seconds, 969 milliseconds)
[info] - All pods have the same service account by default (6 seconds, 916 milliseconds)
[info] - Run extraJVMOptions check on driver (3 seconds, 964 milliseconds)
[info] - Run extraJVMOptions JVM GC option check - G1GC (3 seconds, 948 milliseconds)
[info] - Run extraJVMOptions JVM GC option check - Other GC (4 seconds, 51 milliseconds)
...
```

Closes #40062 from dongjoon-hyun/SPARK-42474.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/memory/MemoryManager.scala', 'core/src/main/scala/org/apache/spark/util/Utils.scala', 'examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala', 'resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala']","'K8s Integration Suite lacks test cases for JVM GC options, leading to uncovered areas in JVM Options testing.'"
3cb3ccce120fa9f0273133912624b877b42d95fd,1498555486,"[SPARK-21196] Split codegen info of query plan into sequence

codegen info of query plan can be very long.
In debugging console / web page, it would be more readable if the subtrees and corresponding codegen are split into sequence.

Example:

```java
codegenStringSeq(sql(""select 1"").queryExecution.executedPlan)
```
The example will return Seq[(String, String)] of length 1, containing the subtree as string and the corresponding generated code.

The subtree as string:

> (*Project [1 AS 1#0]
> +- Scan OneRowRelation[]

The generated code:
```java
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private scala.collection.Iterator[] inputs;
/* 008 */   private scala.collection.Iterator inputadapter_input;
/* 009 */   private UnsafeRow project_result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder project_holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter project_rowWriter;
/* 012 */
/* 013 */   public GeneratedIterator(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     inputadapter_input = inputs[0];
/* 021 */     project_result = new UnsafeRow(1);
/* 022 */     project_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(project_result, 0);
/* 023 */     project_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(project_holder, 1);
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   protected void processNext() throws java.io.IOException {
/* 028 */     while (inputadapter_input.hasNext() && !stopEarly()) {
/* 029 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();
/* 030 */       project_rowWriter.write(0, 1);
/* 031 */       append(project_result);
/* 032 */       if (shouldStop()) return;
/* 033 */     }
/* 034 */   }
/* 035 */
/* 036 */ }
```
## What changes were proposed in this pull request?
add method codegenToSeq: split codegen info of query plan into sequence

## How was this patch tested?
unit test

cloud-fan gatorsmile
Please review http://spark.apache.org/contributing.html before opening a pull request.

Author: Wang Gengliang <ltnwgl@gmail.com>

Closes #18409 from gengliangwang/codegen.
","['sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/debug/DebuggingSuite.scala']",Codegen info for query plan isn't easily readable in debugging console/web page due to its excessive length. Tree structures and their corresponding codegen are currently presented as one continuous string.
f9c105e1b693760d8c904066fdb65630aa4aeb91,1679506305,"[SPARK-42832][SQL] Remove repartition if it is the child of LocalLimit

### What changes were proposed in this pull request?

This PR enhances `CollapseRepartition` to remove repartition if it is the child of `LocalLimit`. Because its output is determined by the number of partitions and the expressions of the Repartition. Therefore, it is feasible to remove Repartition except for repartition by nondeterministic expressions, because users may expect to randomly take data.
For example:
```sql
SELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5;
```

Before this PR:
```
== Optimized Logical Plan ==
GlobalLimit 5
+- LocalLimit 5
   +- RebalancePartitions
      +- Filter (isnotnull(id#0L) AND (id#0L > 1))
         +- Relation spark_catalog.default.t[id#0L] parquet
```

After this PR:
```
== Optimized Logical Plan ==
GlobalLimit 5
+- LocalLimit 5
   +- Filter (isnotnull(id#0L) AND (id#0L > 1))
      +- Relation spark_catalog.default.t[id#0L] parquet
```

Note that we don't remove repartition if it looks like the user might want to take data randomly. For example:
```sql
SELECT /*+ REPARTITION(3) */ * FROM t WHERE id > 1 LIMIT 5;
SELECT * FROM t WHERE id > 1 DISTRIBUTE BY random() LIMIT 5;
```

### Why are the changes needed?

Reduce shuffle to improve query performance. The use case is that we add a repartition to improve the parallelism on a JDBC table:
<img src=""https://user-images.githubusercontent.com/5399861/225855582-c3c81c7d-4617-4104-b669-76749a7468a0.png"" width=""400"" height=""700"">

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #40462 from wangyum/SPARK-42832.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseRepartitionSuite.scala']","Repartition operation under LocalLimit resulting in unnecessary shuffles, affecting the overall query performance."
cfe012a4311a8fb1fc3a82390c3e68f6afcb1da6,1598738493,"[SPARK-32629][SQL] Track metrics of BitSet/OpenHashSet in full outer SHJ

### What changes were proposed in this pull request?

This is followup from https://github.com/apache/spark/pull/29342, where to do two things:
* Per https://github.com/apache/spark/pull/29342#discussion_r470153323, change from java `HashSet` to spark in-house `OpenHashSet` to track matched rows for non-unique join keys. I checked `OpenHashSet` implementation which is built from a key index (`OpenHashSet._bitset` as `BitSet`) and key array (`OpenHashSet._data` as `Array`). Java `HashSet` is built from `HashMap`, which stores value in `Node` linked list and by theory should have taken more memory than `OpenHashSet`. Reran the same benchmark query used in https://github.com/apache/spark/pull/29342, and verified the query has similar performance here between `HashSet` and `OpenHashSet`.
* Track metrics of the extra data structure `BitSet`/`OpenHashSet` for full outer SHJ. This depends on above thing, because there seems no easy way to get java `HashSet` memory size.

### Why are the changes needed?

To better surface the memory usage for full outer SHJ more accurately.
This can help users/developers to debug/improve full outer SHJ.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added unite test in `SQLMetricsSuite.scala` .

Closes #29566 from c21/add-metrics.

Authored-by: Cheng Su <chengsu@fb.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsTestUtils.scala']",The memory usage for full outer Sort Merge Join (SHJ) is not accurately surfaced due to lack of tracking metrics for extra data structures like BitSet/OpenHashSet.
6bc5c6a4e7c36361db437313cd950509a1ab6db2,1564356925,"[SPARK-28520][SQL] WholeStageCodegen does not work property for LocalTableScanExec

Code is not generated for LocalTableScanExec although proper situations.

If a LocalTableScanExec plan has the direct parent plan which supports WholeStageCodegen,
the LocalTableScanExec plan also should be within a WholeStageCodegen domain.
But code is not generated for LocalTableScanExec and InputAdapter is inserted for now.

```
val df1 = spark.createDataset(1 to 10).toDF
val df2 = spark.createDataset(1 to 10).toDF
val df3 = df1.join(df2, df1(""value"") === df2(""value""))
df3.explain(true)

...

== Physical Plan ==
*(1) BroadcastHashJoin [value#1], [value#6], Inner, BuildRight
:- LocalTableScan [value#1]                                             // LocalTableScanExec is not within a WholeStageCodegen domain
+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
   +- LocalTableScan [value#6]
```

```
scala> df3.queryExecution.executedPlan.children.head.children.head.getClass
res4: Class[_ <: org.apache.spark.sql.execution.SparkPlan] = class org.apache.spark.sql.execution.InputAdapter
```

For the current implementation of LocalTableScanExec, codegen is enabled in case `parent` is not null
but `parent` is set in `consume`, which is called after `insertInputAdapter` so it doesn't work as intended.

After applying this cnahge, we can get following plan, which means LocalTableScanExec is within a WholeStageCodegen domain.

```
== Physical Plan ==
*(1) BroadcastHashJoin [value#63], [value#68], Inner, BuildRight
:- *(1) LocalTableScan [value#63]
+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
   +- LocalTableScan [value#68]

## How was this patch tested?

New test cases are added into WholeStageCodegenSuite.

Closes #25260 from sarutak/localtablescan-improvement.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala']","LocalTableScanExec plan is not correctly included within WholeStageCodegen domain, even in appropriate scenarios - an issue observed when LocalTableScanExec has a direct parent plan that supports WholeStageCodegen."
12249fcdc7534c8be67b9331b1a4dfdeb7724d63,1576840903,"[SPARK-30301][SQL] Fix wrong results when datetimes as fields of complex types

### What changes were proposed in this pull request?

When date and timestamp values are fields of arrays, maps, etc, we convert them to hive string using `toString`. This makes the result wrong before the default transition ’1582-10-15‘.

https://bugs.openjdk.java.net/browse/JDK-8061577?focusedCommentId=13566712&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-13566712

cases to reproduce:

```sql
+-- !query 47
+select array(cast('1582-10-13' as date), date '1582-10-14', date '1582-10-15', null)
+-- !query 47 schema
+struct<array(CAST(1582-10-13 AS DATE), DATE '1582-10-14', DATE '1582-10-15', CAST(NULL AS DATE)):array<date>>
+-- !query 47 output
+[1582-10-03,1582-10-04,1582-10-15,null]
+
+
+-- !query 48
+select cast('1582-10-13' as date), date '1582-10-14', date '1582-10-15'
+-- !query 48 schema
+struct<CAST(1582-10-13 AS DATE):date,DATE '1582-10-14':date,DATE '1582-10-15':date>
+-- !query 48 output
+1582-10-13     1582-10-14      1582-10-15
```

other refencences
https://github.com/h2database/h2database/issues/831
### Why are the changes needed?

bug fix
### Does this PR introduce any user-facing change?

yes, complex types containing datetimes in `spark-sql `script and thrift server can result same as self-contained spark app or `spark-shell` script
### How was this patch tested?

add uts

Closes #26942 from yaooqinn/SPARK-30301.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/jsonExpressions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/HiveResult.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/HiveResultSuite.scala']","Issue: Incorrect results when using date and timestamp values as fields of complex types due to the conversion to hive string using `toString`, especially for dates before the default transition '1582-10-15'."
1faf26b81472407c2e0437808c35fd0b55d830b5,1670714501,"[SPARK-41460][CORE] Introduce `IsolatedThreadSafeRpcEndpoint` to extend `IsolatedRpcEndpoint`

### What changes were proposed in this pull request?

This PR introduces a new layer `IsolatedThreadSafeRpcEndpoint` to extend `IsolatedRpcEndpoint` and changes all the endpoints which currently extend `IsolatedRpcEndpoint` to extend `IsolatedThreadSafeRpcEndpoint` instead. Besides, `IsolatedThreadSafeRpcEndpoint` has overridden `def threadCount(): Int = 1` with `final` to avoid mistakenly having thread count > 1 for the thread-safe endpoint.

### Why are the changes needed?

Busy endpoints like `DriverEndpoint`, `BlockManagerMasterEndpoint` were known to be thread-safe when they extended `ThreadSafeRpcEndpoint`. After we introduce `IsolatedRpcEndpoint` in [SPARK-29398](https://issues.apache.org/jira/browse/SPARK-29398),  all these busy endpoints have been changed to extend `IsolatedRpcEndpoint`. These busy endpoints are still thread-safe since `IsolatedRpcEndpoint` by default only uses 1 thread. But that's not explicit to developers now compared to `ThreadSafeRpcEndpoint` (I actually find some people became to be confused about whether the endpoint is thread-safe or not). From time to time, `IsolatedRpcEndpoint#threadCount` might be mistakenly overridden with threadCount > 1 leading to thread-safety being broken. So this PR refactor here a bit to avoid the potential issue.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

N/A

Closes #38995 from Ngone51/isolated.

Authored-by: Yi Wu <yi.wu@databricks.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala', 'core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala', 'core/src/main/scala/org/apache/spark/internal/plugin/PluginEndpoint.scala', 'core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala', 'core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala', 'core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala', 'core/src/main/scala/org/apache/spark/storage/BlockManagerStorageEndpoint.scala', 'core/src/test/scala/org/apache/spark/rpc/RpcEnvSuite.scala']","The thread-safety of busy endpoints like `DriverEndpoint`, `BlockManagerMasterEndpoint` is not explicitly apparent after they've been extended to `IsolatedRpcEndpoint`. The potential for mistakenly overriding `IsolatedRpcEndpoint#threadCount` with a value greater than 1 exists, leading to potential thread-safety issues."
4ce735eed103f3bd055c087126acd1366c2537ec,1500277056,"[SPARK-21394][SPARK-21432][PYTHON] Reviving callable object/partial function support in UDF in PySpark

## What changes were proposed in this pull request?

This PR proposes to avoid `__name__` in the tuple naming the attributes assigned directly from the wrapped function to the wrapper function, and use `self._name` (`func.__name__` or `obj.__class__.name__`).

After SPARK-19161, we happened to break callable objects as UDFs in Python as below:

```python
from pyspark.sql import functions

class F(object):
    def __call__(self, x):
        return x

foo = F()
udf = functions.udf(foo)
```

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/sql/functions.py"", line 2142, in udf
    return _udf(f=f, returnType=returnType)
  File "".../spark/python/pyspark/sql/functions.py"", line 2133, in _udf
    return udf_obj._wrapped()
  File "".../spark/python/pyspark/sql/functions.py"", line 2090, in _wrapped
    functools.wraps(self.func)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/functools.py"", line 33, in update_wrapper
    setattr(wrapper, attr, getattr(wrapped, attr))
AttributeError: F instance has no attribute '__name__'
```

This worked in Spark 2.1:

```python
from pyspark.sql import functions

class F(object):
    def __call__(self, x):
        return x

foo = F()
udf = functions.udf(foo)
spark.range(1).select(udf(""id"")).show()
```

```
+-----+
|F(id)|
+-----+
|    0|
+-----+
```

**After**

```python
from pyspark.sql import functions

class F(object):
    def __call__(self, x):
        return x

foo = F()
udf = functions.udf(foo)
spark.range(1).select(udf(""id"")).show()
```

```
+-----+
|F(id)|
+-----+
|    0|
+-----+
```

_In addition, we also happened to break partial functions as below_:

```python
from pyspark.sql import functions
from functools import partial

partial_func = partial(lambda x: x, x=1)
udf = functions.udf(partial_func)
```

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/sql/functions.py"", line 2154, in udf
    return _udf(f=f, returnType=returnType)
  File "".../spark/python/pyspark/sql/functions.py"", line 2145, in _udf
    return udf_obj._wrapped()
  File "".../spark/python/pyspark/sql/functions.py"", line 2099, in _wrapped
    functools.wraps(self.func, assigned=assignments)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/functools.py"", line 33, in update_wrapper
    setattr(wrapper, attr, getattr(wrapped, attr))
AttributeError: 'functools.partial' object has no attribute '__module__'
```

This worked in Spark 2.1:

```python
from pyspark.sql import functions
from functools import partial

partial_func = partial(lambda x: x, x=1)
udf = functions.udf(partial_func)
spark.range(1).select(udf()).show()
```

```
+---------+
|partial()|
+---------+
|        1|
+---------+
```

**After**

```python
from pyspark.sql import functions
from functools import partial

partial_func = partial(lambda x: x, x=1)
udf = functions.udf(partial_func)
spark.range(1).select(udf()).show()
```

```
+---------+
|partial()|
+---------+
|        1|
+---------+
```

## How was this patch tested?

Unit tests in `python/pyspark/sql/tests.py` and manual tests.

Author: hyukjinkwon <gurwls223@gmail.com>

Closes #18615 from HyukjinKwon/callable-object.
","['python/pyspark/sql/functions.py', 'python/pyspark/sql/tests.py']","The usage of callable objects and partial functions as UDFs in PySpark is causing AttributeError due to the absence of '__name__' attribute. The issue is present after SPARK-19161 was merged, whereas the functionality worked in Spark 2.1.
"
090fd18f36242857a8d7b81ef78428775c1d1e42,1694713447,"[SPARK-45118][PYTHON] Refactor converters for complex types to short cut when the element types don't need converters

### What changes were proposed in this pull request?

Refactors converters for complex types to short cut when the element types don't need converters.

The following refactors are done in this PR:

- Provide a shortcut when the element types in complex types don't need converters
- Check `None`s before calling the converter
- Remove extra type checks just for assertions

### Why are the changes needed?

When the element types in complex types don't need converters, we can provide a shortcut to avoid extra function calls.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added related tests and existing tests.

### Was this patch authored or co-authored using generative AI tooling?

No.

Closes #42874 from ueshin/issues/SPARK-45118/converters.

Authored-by: Takuya UESHIN <ueshin@databricks.com>
Signed-off-by: Takuya UESHIN <ueshin@databricks.com>
","['dev/sparktestsupport/modules.py', 'python/pyspark/sql/pandas/types.py', 'python/pyspark/sql/tests/pandas/test_converter.py']",Complex types converters lack efficiency: unnecessary function calls if element types don't need converters and inefficiency in handling None values.
123365e05cb6dabc3b6d3ecc5b3728221b0c33c2,1612927192,"[SPARK-34240][SQL] Unify output of `SHOW TBLPROPERTIES` clause's output attribute's schema and ExprID

### What changes were proposed in this pull request?
Passing around the output attributes should have more benefits like keeping the exprID unchanged to avoid bugs when we apply more operators above the command output DataFrame.

This PR did 2 things ：

1. After this pr, a `SHOW TBLPROPERTIES` clause's output shows `key` and `value` columns whether you specify the table property `key`. Before this pr, a `SHOW TBLPROPERTIES` clause's output only show a `value` column when you specify the table property `key`..
2. Keep `SHOW TBLPROPERTIES` command's output attribute exprId unchanged.

### Why are the changes needed?
 1. Keep `SHOW TBLPROPERTIES`'s output schema consistence
 2. Keep `SHOW TBLPROPERTIES` command's output attribute exprId unchanged.

### Does this PR introduce _any_ user-facing change?
After this pr, a `SHOW TBLPROPERTIES` clause's output shows `key` and `value` columns whether you specify the table property `key`. Before this pr, a `SHOW TBLPROPERTIES` clause's output only show a `value` column when you specify the table property `key`.

Before this PR:
```
sql > SHOW TBLPROPERTIES tabe_name('key')
value
value_of_key
```

After this PR
```
sql > SHOW TBLPROPERTIES tabe_name('key')
key value
key value_of_key
```

### How was this patch tested?
Added UT

Closes #31378 from AngersZhuuuu/SPARK-34240.

Lead-authored-by: Angerszhuuuu <angers.zhu@gmail.com>
Co-authored-by: AngersZhuuuu <angers.zhu@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveCommandSuite.scala']","The `SHOW TBLPROPERTIES` clause inconsistently presents output and only shows a 'value' column when a table property key is specified. Also, the clause's output attribute's ExprID changes unexpectedly."
969eda4a02faa7ca6cf3aff5cd10e6d51026b845,1517543002,"[SPARK-23020][CORE] Fix another race in the in-process launcher test.

First the bad news: there's an unfixable race in the launcher code.
(By unfixable I mean it would take a lot more effort than this change
to fix it.) The good news is that it should only affect super short
lived applications, such as the one run by the flaky test, so it's
possible to work around it in our test.

The fix also uncovered an issue with the recently added ""closeAndWait()""
method; closing the connection would still possibly cause data loss,
so this change waits a while for the connection to finish itself, and
closes the socket if that times out. The existing connection timeout
is reused so that if desired it's possible to control how long to wait.

As part of that I also restored the old behavior that disconnect() would
force a disconnection from the child app; the ""wait for data to arrive""
approach is only taken when disposing of the handle.

I tested this by inserting a bunch of sleeps in the test and the socket
handling code in the launcher library; with those I was able to reproduce
the error from the jenkins jobs. With the changes, even with all the
sleeps still in place, all tests pass.

Author: Marcelo Vanzin <vanzin@cloudera.com>

Closes #20462 from vanzin/SPARK-23020.
","['core/src/test/java/org/apache/spark/launcher/SparkLauncherSuite.java', 'launcher/src/main/java/org/apache/spark/launcher/AbstractAppHandle.java', 'launcher/src/main/java/org/apache/spark/launcher/ChildProcAppHandle.java', 'launcher/src/main/java/org/apache/spark/launcher/InProcessAppHandle.java', 'launcher/src/main/java/org/apache/spark/launcher/LauncherServer.java', 'launcher/src/test/java/org/apache/spark/launcher/LauncherServerSuite.java']","Race condition in the in-process launcher leads to issues in short lived applications and results in data loss when closing connections, as evidenced in a flaky test."
ef7f6903b4fa28c554a1f0b58b9da194979b61ee,1608097544,"[SPARK-33786][SQL] The storage level for a cache should be respected when a table name is altered

### What changes were proposed in this pull request?

This PR proposes to retain the cache's storage level when a table name is altered by `ALTER TABLE ... RENAME TO ...`.

### Why are the changes needed?

Currently, when a table name is altered, the table's cache is refreshed (if exists), but the storage level is not retained. For example:
```scala
        def getStorageLevel(tableName: String): StorageLevel = {
          val table = spark.table(tableName)
          val cachedData = spark.sharedState.cacheManager.lookupCachedData(table).get
          cachedData.cachedRepresentation.cacheBuilder.storageLevel
        }

        Seq(1 -> ""a"").toDF(""i"", ""j"").write.parquet(path.getCanonicalPath)
        sql(s""CREATE TABLE old USING parquet LOCATION '${path.toURI}'"")
        sql(""CACHE TABLE old OPTIONS('storageLevel' 'MEMORY_ONLY')"")
        val oldStorageLevel = getStorageLevel(""old"")

        sql(""ALTER TABLE old RENAME TO new"")
        val newStorageLevel = getStorageLevel(""new"")
```
`oldStorageLevel` will be `StorageLevel(memory, deserialized, 1 replicas)` whereas `newStorageLevel` will be `StorageLevel(disk, memory, deserialized, 1 replicas)`, which is the default storage level.

### Does this PR introduce _any_ user-facing change?

Yes, now the storage level for the cache will be retained.

### How was this patch tested?

Added a unit test.

Closes #30774 from imback82/alter_table_rename_cache_fix.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala', 'sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala']","Upon renaming a table through ALTER TABLE, the original storage level of the table's cache is not preserved, defaulting to disk storage instead of the initial configuration."
e9145d41f3eae53dcee7d298ee1ae9d065645019,1592287205,"[SPARK-31986][SQL] Fix Julian-Gregorian micros rebasing of overlapping local timestamps

### What changes were proposed in this pull request?
It fixes microseconds rebasing from the hybrid calendar (Julian + Gregorian) to Proleptic Gregorian calendar in the function `RebaseDateTime`.`rebaseJulianToGregorianMicros(zoneId: ZoneId, micros: Long): Long` in the case of local timestamp overlapping.

In the case of overlapping, we look ahead of 1 day to determinate which instant we should take - earlier or later zoned timestamp. If our current standard zone and DST offsets are equal to zone offset of the next date, we choose the later timestamp otherwise the earlier one. For example, the local timestamp **1945-11-18 01:30:00.0** can be mapped to two instants (microseconds since 1970-01-01 00:00:00Z): -761211000000000 or -761207400000000. If the first one is passed to `rebaseJulianToGregorianMicros()`, we take the earlier instant in Proleptic Gregorian calendar while rebasing **1945-11-18T01:30+09:00[Asia/Hong_Kong]** otherwise the later one **1945-11-18T01:30+08:00[Asia/Hong_Kong]**.

Note: The fix assumes that only one transition of standard or DST offsets can occur during a day.

### Why are the changes needed?
Current implementation of `rebaseJulianToGregorianMicros()` handles timestamps overlapping only during daylight saving time but overlapping can happen also during transition from one standard time zone to another one. For example in the case of `Asia/Hong_Kong`, the time zone switched from `Japan Standard Time` (UTC+9) to `Hong Kong Time` (UTC+8) on _Sunday, 18 November, 1945 01:59:59 AM_. The changes allow to handle the special case as well.

### Does this PR introduce _any_ user-facing change?
There is no behaviour change for timestamps of CE after 0001-01-01. The PR might affects timestamps of BCE for which the modified `rebaseJulianToGregorianMicros()` is called directly.

### How was this patch tested?

1. By existing tests in `DateTimeUtilsSuite`, `RebaseDateTimeSuite`, `DateFunctionsSuite`, `DateExpressionsSuite` and `TimestampFormatterSuite`.

2. Added new checks to `RebaseDateTimeSuite`.`SPARK-31959: JST -> HKT at Asia/Hong_Kong in 1945`:
```scala
      assert(rebaseJulianToGregorianMicros(hkZid, rebasedEarlierMicros) === earlierMicros)
      assert(rebaseJulianToGregorianMicros(hkZid, rebasedLaterMicros) === laterMicros)
```

3. Regenerated `julian-gregorian-rebase-micros.json` with the step of 30 minutes, and got the same JSON file. The JSON file isn't affected because previously it was generated with the step of 1 week. And the spike in diffs/switch points during 1 hour of timestamp overlapping wasn't detected.

Closes #28816 from MaxGekk/fix-overlap-julian-2-grep.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/RebaseDateTimeSuite.scala']","'Rebasing of microseconds from hybrid Julian + Gregorian to Proleptic Gregorian calendar in `RebaseDateTime.rebaseJulianToGregorianMicros()` function incorrectly handles local timestamp overlapping, especially during timezone transitions.'"
3235edbc31c9a4c487c5fe18f646191b91df83b8,1637661165,"[SPARK-36231][PYTHON] Support arithmetic operations of decimal(nan) series

### What changes were proposed in this pull request?
This patch has changes as below to follow the pandas behavior:
- **Add nan value process in _non_fractional_astype**: Follow the pandas [to_string](https://github.com/pandas-dev/pandas/blob/0a9f9eed3e3eb7d5fa23cbc588e78b9bef915a89/pandas/core/series.py#L1486) covert method, it should be `""NaN""` rather than `str(np.nan)`(`""nan""`)， which is covered by `self.assert_eq(pser.astype(str), psser.astype(str))`.
- **Add null value process in rpow**, which is covered by `def test_rpow(self)`
- **Add index_ops.hasnans in `astype`**, which is covered by `test_astype`.

This patch also move `numeric_w_nan_pdf` into `numeric_pdf`, that means all float_nan/decimal_nan separated test case have been cleaned up and merged into numeric test.

### Why are the changes needed?
Follow the pandas behavior

### Does this PR introduce _any_ user-facing change?
Yes, correct the null value result to follow the pandas behavior

### How was this patch tested?
1. ut to cover all changes
2. Passed all python test case with pandas v1.1.x
3. Passed all python test case with pandas v1.2.x

Closes #34687 from Yikun/SPARK-36337-skip.

Authored-by: Yikun Jiang <yikunkero@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/data_type_ops/num_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_num_ops.py', 'python/pyspark/pandas/tests/data_type_ops/testing_utils.py']","Arithmetic operations on decimal NaN series in Spark are not in line with pandas behavior, leading to incorrect null value results."
6bbdf34baed7b2bab1fbfbce7782b3093a72812f,1542814686,"[SPARK-8288][SQL] ScalaReflection can use companion object constructor

## What changes were proposed in this pull request?

This change fixes a particular scenario where default spark SQL can't encode (thrift) types that are generated by twitter scrooge. These types are a trait that extends `scala.ProductX` with a constructor defined only in a companion object, rather than a actual case class. The actual case class used is child class, but that type is almost never referred to in code. The type has no corresponding constructor symbol and causes an exception. For all other purposes, these classes act just like case classes, so it is unfortunate that spark SQL can't serialize them nicely as it can actual case classes. For an full example of a scrooge codegen class, see https://gist.github.com/anonymous/ba13d4b612396ca72725eaa989900314.

This change catches the case where the type has no constructor but does have an `apply` method on the type's companion object. This allows for thrift types to be serialized/deserialized with implicit encoders the same way as normal case classes. This fix had to be done in three places where the constructor is assumed to be an actual constructor:

1) In serializing, determining the schema for the dataframe relies on inspecting its constructor (`ScalaReflection.constructParams`). Here we fall back to using the companion constructor arguments.
2) In deserializing or evaluating, in the java codegen ( `NewInstance.doGenCode`), the type couldn't be constructed with the new keyword. If there is no constructor, we change the constructor call to try the companion constructor.
3)  In deserializing or evaluating, without codegen, the constructor is directly invoked (`NewInstance.constructor`). This was fixed with scala reflection to get the actual companion apply method.

The return type of `findConstructor` was changed because the companion apply method constructor can't be represented as a `java.lang.reflect.Constructor`.

There might be situations in which this approach would also fail in a new way, but it does at a minimum work for the specific scrooge example and will not impact cases that were already succeeding prior to this change

Note: this fix does not enable using scrooge thrift enums, additional work for this is necessary. With this patch, it seems like you could patch `com.twitter.scrooge.ThriftEnum` to extend `_root_.scala.Product1[Int]` with `def _1 = value` to get spark's implicit encoders to handle enums, but I've yet to use this method myself.

Note: I previously opened a PR for this issue, but only was able to fix case 1) there: https://github.com/apache/spark/pull/18766

## How was this patch tested?

I've fixed all 3 cases and added two tests that use a case class that is similar to scrooge generated one. The test in ScalaReflectionSuite checks 1), and the additional asserting in ObjectExpressionsSuite checks 2) and 3).

Closes #23062 from drewrobb/SPARK-8288.

Authored-by: Drew Robb <drewrobb@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ObjectExpressionsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala']","Default Spark SQL unable to encode (thrift) types generated by Twitter Scrooge due to lack of corresponding constructor symbol, preventing these types from being serialized/deserialized just like normal case classes."
2d0a0a00cb5dde6bcb8e561278357b6bb8b76dcc,1692808924,"[SPARK-44816][CONNECT] Improve error message when UDF class is not found

### What changes were proposed in this pull request?

Improve the error messaging on the connect client when using
a UDF whose corresponding class has not been sync'ed with the
spark connect service.

Prior to this change, the client receives a cryptic error:

```
Exception in thread ""main"" org.apache.spark.SparkException: Main$
```

With this change, the message is improved to be:

```
Exception in thread ""main"" org.apache.spark.SparkException: Failed to load class: Main$. Make sure the artifact where the class is defined is installed by calling session.addArtifact.
```

### Why are the changes needed?

This change makes it clear to the user on what the error is.

### Does this PR introduce _any_ user-facing change?

Yes. The error message is improved. See details above.

### How was this patch tested?

Manually by running a connect server and client.

Closes #42500 from nija-at/improve-error.

Authored-by: Niranjan Jayakar <nija@databricks.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
",['connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala'],"When using a UDF whose class is not sync'ed with the Spark connect service, a non-informative error message 'org.apache.spark.SparkException: Main$' is displayed to the user."
3165ca742a7508dca35a1e40b303c337939df86f,1605022864,"[SPARK-33376][SQL] Remove the option of ""sharesHadoopClasses"" in Hive IsolatedClientLoader

### What changes were proposed in this pull request?

This removes the `sharesHadoopClasses` flag from `IsolatedClientLoader` in Hive module.

### Why are the changes needed?

Currently, when initializing `IsolatedClientLoader`, users can set the `sharesHadoopClasses` flag to decide whether the `HiveClient` created should share Hadoop classes with Spark itself or not. In the latter case, the client will only load Hadoop classes from the Hive dependencies.

There are two reasons to remove this:
1. this feature is currently used in two cases: 1) unit tests, 2) when the Hadoop version defined in Maven can not be found when `spark.sql.hive.metastore.jars` is equal to ""maven"", which could be very rare.
2. when `sharesHadoopClasses` is false, Spark doesn't really only use Hadoop classes from Hive jars: we also download `hadoop-client` jar and put all the sub-module jars (e.g., `hadoop-common`, `hadoop-hdfs`) together with the Hive jars, and the Hadoop version used by `hadoop-client` is the same version used by Spark itself. As result, we're mixing two versions of Hadoop jars in the classpath, which could potentially cause issues, especially considering that the default Hadoop version is already 3.2.0 while most Hive versions supported by the `IsolatedClientLoader` is still using Hadoop 2.x or even lower.

### Does this PR introduce _any_ user-facing change?

This affects Spark users in one scenario: when `spark.sql.hive.metastore.jars` is set to `maven` AND the Hadoop version specified in pom file cannot be downloaded, currently the behavior is to switch to _not_ share Hadoop classes, but with the PR it will share Hadoop classes with Spark.

### How was this patch tested?

Existing UTs.

Closes #30284 from sunchao/SPARK-33376.

Authored-by: Chao Sun <sunchao@apple.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HadoopVersionInfoSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HiveClientBuilder.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HiveVersionSuite.scala']",Mixing two versions of Hadoop jars in the classpath potentially causes issues especially when 'spark.sql.hive.metastore.jars' is set to 'maven' and Hadoop version defined in Maven can't be found.
0cc331dc7e51e53000063052b0c8ace417eb281b,1659696177,"[SPARK-38034][SQL] Optimize TransposeWindow rule

### What changes were proposed in this pull request?

Optimize the TransposeWindow rule to extend applicable cases and optimize time complexity.
TransposeWindow rule will try to eliminate unnecessary shuffle:

but the function compatiblePartitions will only take the first n elements of the window2 partition sequence, for some cases, this will not take effect, like the case below: 

val df = spark.range(10).selectExpr(""id AS a"", ""id AS b"", ""id AS c"", ""id AS d"")
df.selectExpr(
    ""sum(`d`) OVER(PARTITION BY `b`,`a`) as e"",
    ""sum(`c`) OVER(PARTITION BY `a`) as f""
  ).explain

Current plan

== Physical Plan ==
*(5) Project [e#10L, f#11L]
+- Window [sum(c#4L) windowspecdefinition(a#2L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS f#11L], [a#2L]
   +- *(4) Sort [a#2L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(a#2L, 200), true, [id=#41]
         +- *(3) Project [a#2L, c#4L, e#10L]
            +- Window [sum(d#5L) windowspecdefinition(b#3L, a#2L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS e#10L], [b#3L, a#2L]
               +- *(2) Sort [b#3L ASC NULLS FIRST, a#2L ASC NULLS FIRST], false, 0
                  +- Exchange hashpartitioning(b#3L, a#2L, 200), true, [id=#33]
                     +- *(1) Project [id#0L AS d#5L, id#0L AS b#3L, id#0L AS a#2L, id#0L AS c#4L]
                        +- *(1) Range (0, 10, step=1, splits=10)

Expected plan:

== Physical Plan ==
*(4) Project [e#924L, f#925L]
+- Window [sum(d#43L) windowspecdefinition(b#41L, a#40L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS e#924L], [b#41L, a#40L]
   +- *(3) Sort [b#41L ASC NULLS FIRST, a#40L ASC NULLS FIRST], false, 0
      +- *(3) Project [d#43L, b#41L, a#40L, f#925L]
         +- Window [sum(c#42L) windowspecdefinition(a#40L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS f#925L], [a#40L]
            +- *(2) Sort [a#40L ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(a#40L, 200), true, [id=#282]
                  +- *(1) Project [id#38L AS d#43L, id#38L AS b#41L, id#38L AS a#40L, id#38L AS c#42L]
                     +- *(1) Range (0, 10, step=1, splits=10)

Also the permutations method has a O(n!) time complexity, which is very expensive when there are many partition columns, we could try to optimize it.

### Why are the changes needed?

We could apply the rule for more cases, which could improve the execution performance by eliminate unnecessary shuffle, and by reducing the time complexity from O(n!) to O(n2), the performance for the rule itself could improve

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

UT

Closes #35334 from constzhou/SPARK-38034_optimize_transpose_window_rule.

Authored-by: xzhou <15210830305@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala']","The TransposeWindow rule in SQL does not eliminate unnecessary shuffles in some cases. It also has a time complexity of O(n!), leading to inefficiencies with multiple partition columns."
71cfba04aeec5ae9b85a507b13996e80f8750edc,1518013690,"[SPARK-23319][TESTS] Explicitly specify Pandas and PyArrow versions in PySpark tests (to skip or test)

## What changes were proposed in this pull request?

This PR proposes to explicitly specify Pandas and PyArrow versions in PySpark tests to skip or test.

We declared the extra dependencies:

https://github.com/apache/spark/blob/b8bfce51abf28c66ba1fc67b0f25fe1617c81025/python/setup.py#L204

In case of PyArrow:

Currently we only check if pyarrow is installed or not without checking the version. It already fails to run tests. For example, if PyArrow 0.7.0 is installed:

```
======================================================================
ERROR: test_vectorized_udf_wrong_return_type (pyspark.sql.tests.ScalarPandasUDF)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/.../spark/python/pyspark/sql/tests.py"", line 4019, in test_vectorized_udf_wrong_return_type
    f = pandas_udf(lambda x: x * 1.0, MapType(LongType(), LongType()))
  File ""/.../spark/python/pyspark/sql/functions.py"", line 2309, in pandas_udf
    return _create_udf(f=f, returnType=return_type, evalType=eval_type)
  File ""/.../spark/python/pyspark/sql/udf.py"", line 47, in _create_udf
    require_minimum_pyarrow_version()
  File ""/.../spark/python/pyspark/sql/utils.py"", line 132, in require_minimum_pyarrow_version
    ""however, your version was %s."" % pyarrow.__version__)
ImportError: pyarrow >= 0.8.0 must be installed on calling Python process; however, your version was 0.7.0.

----------------------------------------------------------------------
Ran 33 tests in 8.098s

FAILED (errors=33)
```

In case of Pandas:

There are few tests for old Pandas which were tested only when Pandas version was lower, and I rewrote them to be tested when both Pandas version is lower and missing.

## How was this patch tested?

Manually tested by modifying the condition:

```
test_createDataFrame_column_name_encoding (pyspark.sql.tests.ArrowTests) ... skipped 'Pandas >= 1.19.2 must be installed; however, your version was 0.19.2.'
test_createDataFrame_does_not_modify_input (pyspark.sql.tests.ArrowTests) ... skipped 'Pandas >= 1.19.2 must be installed; however, your version was 0.19.2.'
test_createDataFrame_respect_session_timezone (pyspark.sql.tests.ArrowTests) ... skipped 'Pandas >= 1.19.2 must be installed; however, your version was 0.19.2.'
```

```
test_createDataFrame_column_name_encoding (pyspark.sql.tests.ArrowTests) ... skipped 'Pandas >= 0.19.2 must be installed; however, it was not found.'
test_createDataFrame_does_not_modify_input (pyspark.sql.tests.ArrowTests) ... skipped 'Pandas >= 0.19.2 must be installed; however, it was not found.'
test_createDataFrame_respect_session_timezone (pyspark.sql.tests.ArrowTests) ... skipped 'Pandas >= 0.19.2 must be installed; however, it was not found.'
```

```
test_createDataFrame_column_name_encoding (pyspark.sql.tests.ArrowTests) ... skipped 'PyArrow >= 1.8.0 must be installed; however, your version was 0.8.0.'
test_createDataFrame_does_not_modify_input (pyspark.sql.tests.ArrowTests) ... skipped 'PyArrow >= 1.8.0 must be installed; however, your version was 0.8.0.'
test_createDataFrame_respect_session_timezone (pyspark.sql.tests.ArrowTests) ... skipped 'PyArrow >= 1.8.0 must be installed; however, your version was 0.8.0.'
```

```
test_createDataFrame_column_name_encoding (pyspark.sql.tests.ArrowTests) ... skipped 'PyArrow >= 0.8.0 must be installed; however, it was not found.'
test_createDataFrame_does_not_modify_input (pyspark.sql.tests.ArrowTests) ... skipped 'PyArrow >= 0.8.0 must be installed; however, it was not found.'
test_createDataFrame_respect_session_timezone (pyspark.sql.tests.ArrowTests) ... skipped 'PyArrow >= 0.8.0 must be installed; however, it was not found.'
```

Author: hyukjinkwon <gurwls223@gmail.com>

Closes #20487 from HyukjinKwon/pyarrow-pandas-skip.
","['python/pyspark/sql/dataframe.py', 'python/pyspark/sql/session.py', 'python/pyspark/sql/tests.py', 'python/pyspark/sql/utils.py', 'python/setup.py']","PySpark tests are failing due to non-checking of PyArrow and Pandas versions leading to incompatibilities, especially for old or absent versions.
"
f9a50ba2d1bfa3f55199df031e71154611ba51f6,1491858409,"[SPARK-20285][TESTS] Increase the pyspark streaming test timeout to 30 seconds

## What changes were proposed in this pull request?

Saw the following failure locally:

```
Traceback (most recent call last):
  File ""/home/jenkins/workspace/python/pyspark/streaming/tests.py"", line 351, in test_cogroup
    self._test_func(input, func, expected, sort=True, input2=input2)
  File ""/home/jenkins/workspace/python/pyspark/streaming/tests.py"", line 162, in _test_func
    self.assertEqual(expected, result)
AssertionError: Lists differ: [[(1, ([1], [2])), (2, ([1], [... != []

First list contains 3 additional elements.
First extra element 0:
[(1, ([1], [2])), (2, ([1], [])), (3, ([1], []))]

+ []
- [[(1, ([1], [2])), (2, ([1], [])), (3, ([1], []))],
-  [(1, ([1, 1, 1], [])), (2, ([1], [])), (4, ([], [1]))],
-  [('', ([1, 1], [1, 2])), ('a', ([1, 1], [1, 1])), ('b', ([1], [1]))]]
```

It also happened on Jenkins: http://spark-tests.appspot.com/builds/spark-branch-2.1-test-sbt-hadoop-2.7/120

It's because when the machine is overloaded, the timeout is not enough. This PR just increases the timeout to 30 seconds.

## How was this patch tested?

Jenkins

Author: Shixiong Zhu <shixiong@databricks.com>

Closes #17597 from zsxwing/SPARK-20285.
",['python/pyspark/streaming/tests.py'],"When machine is overloaded, the pyspark streaming test fails due to insufficient timeout."
801ca252f43b20cdd629c01d734ca9049e6eccf4,1663116008,"[SPARK-39915][SQL] Ensure the output partitioning is user-specified in AQE

### What changes were proposed in this pull request?

- Support get user-specified root repartition through  `DeserializeToObjectExec`
- Skip optimize empty for the root repartition which is user-specified
- Add a new rule `AdjustShuffleExchangePosition` to adjust the shuffle we add back, so that we can restore shuffle safely.

### Why are the changes needed?

AQE can not completely respect the user-specified repartition. The main reasons are:

1. the AQE optimzier will convert empty to local relation which does not reserve the partitioning info
2. the machine of AQE `requiredDistribution` only restore the repartition which does not support through `DeserializeToObjectExec`

After the fix:
The partition number of `spark.range(0).repartition(5).rdd.getNumPartitions` should be 5.

### Does this PR introduce _any_ user-facing change?

yes, ensure the user-specified distribution.

### How was this patch tested?

add tests

Closes #37612 from ulysses-you/output-partition.

Lead-authored-by: ulysses-you <ulyssesyou18@gmail.com>
Co-authored-by: Wenchen Fan <cloud0fan@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala']","Adaptive Query Execution (AQE) does not correctly respect user-specified repartitioning, leading to incorrect output partitioning, especially when handling empty to local relation conversions and unsupported operations through `DeserializeToObjectExec`."
b4f7758944505c7c957e2e0c2c70da5ea746099b,1626248326,"[SPARK-36037][SQL] Support ANSI SQL LOCALTIMESTAMP datetime value function

### What changes were proposed in this pull request?
`LOCALTIMESTAMP()` is a datetime value function from ANSI SQL.
The syntax show below:
```
<datetime value function> ::=
    <current date value function>
  | <current time value function>
  | <current timestamp value function>
  | <current local time value function>
  | <current local timestamp value function>
<current date value function> ::=
CURRENT_DATE
<current time value function> ::=
CURRENT_TIME [ <left paren> <time precision> <right paren> ]
<current local time value function> ::=
LOCALTIME [ <left paren> <time precision> <right paren> ]
<current timestamp value function> ::=
CURRENT_TIMESTAMP [ <left paren> <timestamp precision> <right paren> ]
<current local timestamp value function> ::=
LOCALTIMESTAMP [ <left paren> <timestamp precision> <right paren> ]
```

`LOCALTIMESTAMP()` returns the current timestamp at the start of query evaluation as TIMESTAMP WITH OUT TIME ZONE. This is similar to `CURRENT_TIMESTAMP()`.
Note we need to update the optimization rule `ComputeCurrentTime` so that Spark returns the same result in a single query if the function is called multiple times.

### Why are the changes needed?
`CURRENT_TIMESTAMP()` returns the current timestamp at the start of query evaluation.
`LOCALTIMESTAMP()` returns the current timestamp without time zone at the start of query evaluation.
The `LOCALTIMESTAMP` function is an ANSI SQL.
The `LOCALTIMESTAMP` function is very useful.

### Does this PR introduce _any_ user-facing change?
'Yes'. Support new function `LOCALTIMESTAMP()`.

### How was this patch tested?
New tests.

Closes #33258 from beliefer/SPARK-36037.

Lead-authored-by: gengjiaan <gengjiaan@360.cn>
Co-authored-by: Jiaan Geng <beliefer@163.com>
Co-authored-by: Wenchen Fan <cloud0fan@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/functions.scala', 'sql/core/src/test/resources/sql-tests/inputs/datetime.sql', 'sql/core/src/test/scala/org/apache/spark/sql/expressions/ExpressionInfoSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingAggregationSuite.scala']","Spark SQL currently lacks support for the ANSI SQL LOCALTIMESTAMP datetime value function, which returns the current timestamp without a time zone at query evaluation start."
4a803ca22a9a98f9bbbbd1a5a33b9ae394fb7c49,1655168044,"[SPARK-38796][SQL] Update to_number and try_to_number functions to allow PR with positive numbers

### What changes were proposed in this pull request?

Update `to_number` and `try_to_number` functions to allow the `PR` format token with input strings comprising positive numbers.

Before this bug fix, function calls like `to_number(' 123 ', '999PR')` would fail. Now they succeed, which is helpful since `PR` should allow both positive and negative numbers.

This satisfies the following specification:

```
to_number(expr, fmt)
fmt
  { ' [ MI | S ] [ L | $ ]
      [ 0 | 9 | G | , ] [...]
      [ . | D ]
      [ 0 | 9 ] [...]
      [ L | $ ] [ PR | MI | S ] ' }
```

### Why are the changes needed?

After reviewing the specification, this behavior makes the most sense.

### Does this PR introduce _any_ user-facing change?

Yes, a slight change in the behavior of the format string.

### How was this patch tested?

Existing and updated unit test coverage.

Closes #36861 from dtenedor/to-number-fix-pr.

Authored-by: Daniel Tenedorio <daniel.tenedorio@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala']",The `to_number` and `try_to_number` functions fail to handle strings of positive numbers when using the `PR` format token.
e64512558f4b9455a7758c20de129a03203fddb1,1578084289,"[SPARK-30267][SQL] Avro arrays can be of any List

The Deserializer assumed that avro arrays are always of type `GenericData$Array` which is not the case.
Assuming they are from java.util.List is safer and fixes a ClassCastException in some avro code.

### What changes were proposed in this pull request?
Java.util.List has all the necessary methods and is the base class of GenericData$Array.

### Why are the changes needed?
To prevent the following exception in more complex avro objects:

```
java.lang.ClassCastException: java.util.ArrayList cannot be cast to org.apache.avro.generic.GenericData$Array
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$newWriter$19(AvroDeserializer.scala:170)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$newWriter$19$adapted(AvroDeserializer.scala:169)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1(AvroDeserializer.scala:314)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1$adapted(AvroDeserializer.scala:310)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$2(AvroDeserializer.scala:332)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$2$adapted(AvroDeserializer.scala:329)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$converter$3(AvroDeserializer.scala:56)
	at org.apache.spark.sql.avro.AvroDeserializer.deserialize(AvroDeserializer.scala:70)
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
The current tests already test this behavior.  In essesence this patch just changes a type case to a more basic type.  So I expect no functional impact.

Closes #26907 from steven-aerts/spark-30267.

Authored-by: Steven Aerts <steven.aerts@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>
","['external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala', 'external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala']","The Deserializer mistakenly assumes Avro arrays are always of type `GenericData$Array`, leading to a ClassCastException in complex Avro objects."
ba974ea8e4cc8075056682c2badab5ca64b90047,1609334071,"[SPARK-30789][SQL] Support (IGNORE | RESPECT) NULLS for LEAD/LAG/NTH_VALUE/FIRST_VALUE/LAST_VALUE

### What changes were proposed in this pull request?
All of `LEAD`/`LAG`/`NTH_VALUE`/`FIRST_VALUE`/`LAST_VALUE` should support IGNORE NULLS | RESPECT NULLS. For example:
```
LEAD (value_expr [, offset ])
[ IGNORE NULLS | RESPECT NULLS ]
OVER ( [ PARTITION BY window_partition ] ORDER BY window_ordering )
```

```
LAG (value_expr [, offset ])
[ IGNORE NULLS | RESPECT NULLS ]
OVER ( [ PARTITION BY window_partition ] ORDER BY window_ordering )
```

```
NTH_VALUE (expr, offset)
[ IGNORE NULLS | RESPECT NULLS ]
OVER
( [ PARTITION BY window_partition ]
[ ORDER BY window_ordering
 frame_clause ] )
```

The mainstream database or engine supports this syntax contains:
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/NTH_VALUE.html#GUID-F8A0E88C-67E5-4AA6-9515-95D03A7F9EA0

**Redshift**
https://docs.aws.amazon.com/redshift/latest/dg/r_WF_NTH.html

**Presto**
https://prestodb.io/docs/current/functions/window.html

**DB2**
https://www.ibm.com/support/knowledgecenter/SSGU8G_14.1.0/com.ibm.sqls.doc/ids_sqs_1513.htm

**Teradata**
https://docs.teradata.com/r/756LNiPSFdY~4JcCCcR5Cw/GjCT6l7trjkIEjt~7Dhx4w

**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/lead.html
https://docs.snowflake.com/en/sql-reference/functions/lag.html
https://docs.snowflake.com/en/sql-reference/functions/nth_value.html
https://docs.snowflake.com/en/sql-reference/functions/first_value.html
https://docs.snowflake.com/en/sql-reference/functions/last_value.html

**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/lead.htm
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/lag.htm
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/nth_value.htm
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/first_value.htm
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/last_value.htm

### Why are the changes needed?
Support `(IGNORE | RESPECT) NULLS` for `LEAD`/`LAG`/`NTH_VALUE`/`FIRST_VALUE`/`LAST_VALUE `is very useful.

### Does this PR introduce _any_ user-facing change?
Yes.

### How was this patch tested?
Jenkins test

Closes #30943 from beliefer/SPARK-30789.

Lead-authored-by: gengjiaan <gengjiaan@360.cn>
Co-authored-by: beliefer <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/QueryCompilationErrors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/window.sql']","The SQL functions LEAD, LAG, NTH_VALUE, FIRST_VALUE, and LAST_VALUE currently do not support 'IGNORE NULLS' or 'RESPECT NULLS' options, limiting usage flexibility."
ee83d09b536836f5f26abfcda770a098342b47f4,1570591322,"[SPARK-29401][CORE][ML][SQL][GRAPHX][TESTS] Replace calls to .parallelize Arrays of tuples, ambiguous in Scala 2.13, with Seqs of tuples

### What changes were proposed in this pull request?

Invocations like `sc.parallelize(Array((1,2)))` cause a compile error in 2.13, like:
```
[ERROR] [Error] /Users/seanowen/Documents/spark_2.13/core/src/test/scala/org/apache/spark/ShuffleSuite.scala:47: overloaded method value apply with alternatives:
  (x: Unit,xs: Unit*)Array[Unit] <and>
  (x: Double,xs: Double*)Array[Double] <and>
  (x: Float,xs: Float*)Array[Float] <and>
  (x: Long,xs: Long*)Array[Long] <and>
  (x: Int,xs: Int*)Array[Int] <and>
  (x: Char,xs: Char*)Array[Char] <and>
  (x: Short,xs: Short*)Array[Short] <and>
  (x: Byte,xs: Byte*)Array[Byte] <and>
  (x: Boolean,xs: Boolean*)Array[Boolean]
 cannot be applied to ((Int, Int), (Int, Int), (Int, Int), (Int, Int))
```
Using a `Seq` instead appears to resolve it, and is effectively equivalent.

### Why are the changes needed?

To better cross-build for 2.13.

### Does this PR introduce any user-facing change?

None.

### How was this patch tested?

Existing tests.

Closes #26062 from srowen/SPARK-29401.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['core/src/test/scala/org/apache/spark/DistributedSuite.scala', 'core/src/test/scala/org/apache/spark/FileSuite.scala', 'core/src/test/scala/org/apache/spark/PartitioningSuite.scala', 'core/src/test/scala/org/apache/spark/ShuffleSuite.scala', 'core/src/test/scala/org/apache/spark/rdd/PairRDDFunctionsSuite.scala', 'core/src/test/scala/org/apache/spark/rdd/SortingSuite.scala', 'graphx/src/test/scala/org/apache/spark/graphx/GraphSuite.scala', 'graphx/src/test/scala/org/apache/spark/graphx/lib/ConnectedComponentsSuite.scala', 'mllib/src/test/scala/org/apache/spark/mllib/rdd/MLPairRDDFunctionsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala']","Invocations like `sc.parallelize(Array((1,2)))` cause a compile error in Scala 2.13 in Apache Spark, impeding cross-building for Scala 2.13."
2c5cc36e3f59011009c3c6083e0d0c1c81857cbd,1606308113,"[SPARK-33509][SQL] List partition by names from a V2 table which supports partition management

### What changes were proposed in this pull request?
1. Add new method `listPartitionByNames` to the `SupportsPartitionManagement` interface. It allows to list partitions by partition names and their values.
2. Implement new method in `InMemoryPartitionTable` which is used in DSv2 tests.

### Why are the changes needed?
Currently, the `SupportsPartitionManagement` interface exposes only `listPartitionIdentifiers` which allows to list partitions by partition values. And it requires to specify all values for partition schema fields in the prefix. This restriction does not allow to list partitions by some of partition names (not all of them).

For example, the table `tableA` is partitioned by two column `year` and `month`
```
CREATE TABLE tableA (price int, year int, month int)
USING _
partitioned by (year, month)
```
and has the following partitions:
```
PARTITION(year = 2015, month = 1)
PARTITION(year = 2015, month = 2)
PARTITION(year = 2016, month = 2)
PARTITION(year = 2016, month = 3)
```
If we want to list all partitions with `month = 2`, we have to specify `year` for **listPartitionIdentifiers()** which not always possible as we don't know all `year` values in advance. New method **listPartitionByNames()** allows to specify partition values only for `month`, and get two partitions:
```
PARTITION(year = 2015, month = 2)
PARTITION(year = 2016, month = 2)
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
By running the affected test suite `SupportsPartitionManagementSuite`.

Closes #30452 from MaxGekk/column-names-listPartitionIdentifiers.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsPartitionManagement.java', 'sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryPartitionTable.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/SupportsPartitionManagementSuite.scala']","The SupportsPartitionManagement interface does not provide an option to list partitions by certain partition names, requiring all values for partition schema fields to be specified. This is restrictive when trying to list partitions with specific field values."
6bb68b5f75ac883423a68583750258656f381c33,1675631080,"[SPARK-41295][SPARK-41296][SQL] Rename the error classes

### What changes were proposed in this pull request?
In the PR, I propose to assign the proper names to the legacy error classes:

- `_LEGACY_ERROR_TEMP_1105` ->  `INVALID_EXTRACT_FIELD_TYPE`
- `_LEGACY_ERROR_TEMP_1106` ->  `INVALID_EXTRACT_BASE_FIELD_TYPE`
- `_LEGACY_ERROR_TEMP_1209` ->  `AMBIGUOUS_REFERENCE_TO_FIELDS`

Changed error messages for:
- `_LEGACY_ERROR_TEMP_1106` from `Can't extract value from <child>: need struct type but got <other>.` to `""Can't extract a value from <base>. Need a complex type [struct, array, map] but got <other>.""`
- `_LEGACY_ERROR_TEMP_1209` from `""Ambiguous reference to fields <fields>.""` to `""Ambiguous reference to the field <field>. It appears <count> times in the schema.""`
- `_LEGACY_ERROR_TEMP_1106` from `""Field name should be String Literal, but it's <extraction>.""` to `""Field name should be a non-null string literal, but it's <extraction>.""`

and modify test suite to use checkError() which checks the error class name, context and etc.

**Also this PR contains an additional change (AMBIGUOUS_REFERENCE_TO_FIELDS) which is not tracked in JIRA.**

### Why are the changes needed?
Proper name improves user experience w/ Spark SQL. fix a bug, you can clarify why it is a bug.

### Does this PR introduce _any_ user-facing change?
Yes, the PR changes an user-facing error message.

### How was this patch tested?
Added additional test cases in `org.apache.spark.sql.errors.QueryCompilationErrorsSuite`

Closes #39501 from NarekDW/SPARK-41295.

Authored-by: narek_karapetian <narek.karapetian93@yandex.ru>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveResolutionSuite.scala']","Legacy error classes have improper arbitrary names, affecting readability and understanding. Some error messages are also vague and do not accurately reflect the error condition."
397b843890db974a0534394b1907d33d62c2b888,1614436479,"[SPARK-34415][ML] Randomization in hyperparameter optimization

### What changes were proposed in this pull request?

Code in the PR generates random parameters for hyperparameter tuning. A discussion with Sean Owen can be found on the dev mailing list here:

http://apache-spark-developers-list.1001551.n3.nabble.com/Hyperparameter-Optimization-via-Randomization-td30629.html

All code is entirely my own work and I license the work to the project under the project’s open source license.

### Why are the changes needed?

Randomization can be a more effective techinique than a grid search since min/max points can fall between the grid and never be found. Randomisation is not so restricted although the probability of finding minima/maxima is dependent on the number of attempts.

Alice Zheng has an accessible description on how this technique works at https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/ch04.html

Although there are Python libraries with more sophisticated techniques, not every Spark developer is using Python.

### Does this PR introduce _any_ user-facing change?

A new class (`ParamRandomBuilder.scala`) and its tests have been created but there is no change to existing code. This class offers an alternative to `ParamGridBuilder` and can be dropped into the code wherever `ParamGridBuilder` appears. Indeed, it extends `ParamGridBuilder` and is completely compatible with  its interface. It merely adds one method that provides a range over which a hyperparameter will be randomly defined.

### How was this patch tested?

Tests `ParamRandomBuilderSuite.scala` and `RandomRangesSuite.scala` were added.

`ParamRandomBuilderSuite` is the analogue of the already existing `ParamGridBuilderSuite` which tests the user-facing interface.

`RandomRangesSuite` uses ScalaCheck to test the random ranges over which hyperparameters are distributed.

Closes #31535 from PhillHenry/ParamRandomBuilder.

Authored-by: Phillip Henry <PhillHenry@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaRandomHyperparametersExample.java', 'examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaRandomHyperparametersExample.scala', 'mllib/src/main/scala/org/apache/spark/ml/tuning/ParamRandomBuilder.scala', 'mllib/src/test/scala/org/apache/spark/ml/tuning/ParamRandomBuilderSuite.scala', 'mllib/src/test/scala/org/apache/spark/ml/tuning/RandomRangesSuite.scala', 'python/pyspark/ml/tests/test_tuning.py', 'python/pyspark/ml/tuning.py']","There is a lack of effective randomization algorithm for hyperparameter tuning, limiting optimization as current grid search method may miss min/max points that fall between the grid."
cf436233072b75e083a4455dc53b22edba0b3957,1634393406,"[SPARK-36900][SPARK-36464][CORE][TEST] Refactor `: size returns correct positive number even with over 2GB data` to pass with Java 8, 11 and 17

### What changes were proposed in this pull request?
Refactor `SPARK-36464: size returns correct positive number even with over 2GB data` in `ChunkedByteBufferOutputStreamSuite` to reduce the total use of memory for this test case, then this case can pass with Java 8, Java 11 and Java 17 use `-Xmx4g`.

### Why are the changes needed?
`SPARK-36464: size returns correct positive number even with over 2GB data` pass with Java 8 but OOM with Java 11 and Java 17.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?

- Pass the Jenkins or GitHub Action
- Manual test
```
mvn clean install -pl core -am -Dtest=none -DwildcardSuites=org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite
```
with Java 8, Java 11 and Java 17, all tests passed.

Closes #34284 from LuciferYang/SPARK-36900.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
",['core/src/test/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStreamSuite.scala'],"Test case for SPARK-36464 fails with OutOfMemoryError on Java 11 and Java 17 when data size exceeds 2GB, despite passing on Java 8."
64d1531bbd831d8fc3b595d1562bad502cf12b86,1682918147,"[SPARK-42769][TEST][FOLLOWUP] Add missing `assert` in integration test

### What changes were proposed in this pull request?

Add missing `assert` in integration test.

### Why are the changes needed?

The test does not make sense w/o `assert`, it won't fail even executors don't have the env var `SPARK_DRIVER_POD_IP`.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

```
build/sbt -Pkubernetes -Pvolcano -Pkubernetes-integration-tests \
  -Dtest.exclude.tags=local,r \
  ""kubernetes-integration-tests/testOnly *Suite -- -z SPARK-42769""
```

```
...
[info] KubernetesSuite:
[info] - SPARK-42769: All executor pods have SPARK_DRIVER_POD_IP env variable (19 seconds, 554 milliseconds)
[info] VolcanoSuite:
[info] - SPARK-42769: All executor pods have SPARK_DRIVER_POD_IP env variable (27 seconds, 697 milliseconds)
[info] YuniKornSuite:
[info] Run completed in 1 minute, 4 seconds.
[info] Total number of tests run: 2
[info] Suites: completed 3, aborted 0
[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[success] Total time: 102 s (01:42), completed Apr 28, 2023 11:48:26 PM
```

Closes #40992 from pan3793/SPARK-42769-followup.

Authored-by: Cheng Pan <chengpan@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala'],Integration test fails to verify if executors have the environment variable `SPARK_DRIVER_POD_IP` due to a missing `assert` statement.
00d43b1f829fb5f79f0355afbbacc804162648e5,1610688445,"[SPARK-32864][SQL] Support ORC forced positional evolution

### What changes were proposed in this pull request?
Add support for `orc.force.positional.evolution` config that forces ORC top level column matching by position rather than by name.

This does work in Hive:
```
> set orc.force.positional.evolution;
+--------------------------------------+
|                 set                  |
+--------------------------------------+
| orc.force.positional.evolution=true  |
+--------------------------------------+
> create table t (c1 string, c2 string) stored as orc;
> insert into t values ('foo', 'bar');
> alter table t change c1 c3 string;
```
The orc file in this case contains the original `c1` and `c2` columns that doesn't match the metadata in HMS. But due to the positional evolution setting, Hive is capable to return all the data:
```
> select * from t;
+--------+--------+
| t.c3   | t.c2   |
+--------+--------+
| foo    | bar    |
+--------+--------+
```
Without this PR Spark returns `null`s for the renamed `c3` column.

After this PR Spark returns the data in `c3` column.

### Why are the changes needed?
Hive/ORC does support it.

### Does this PR introduce _any_ user-facing change?
Yes, we will support `orc.force.positional.evolution`.

### How was this patch tested?
New UT.

Closes #29737 from peter-toth/SPARK-32864-support-orc-forced-positional-evolution.

Lead-authored-by: Peter Toth <peter.toth@gmail.com>
Co-authored-by: Peter Toth <ptoth@cloudera.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcQuerySuite.scala']","After renaming a top-level column in ORC, Spark returns null values indicating that the ORC top-level column matching by name doesn't work effectively, thus it is unable to return the data properly."
9d19c270b8a40d9ab5c2009ff4b70d4f2758f635,1637270576,"[SPARK-37356][CORE] Add fine grained locking to the BlockInfoManager

### What changes were proposed in this pull request?
This PR moves the BlockInfoManager from a single mutex per instance to much more fine grained locking at the block level. Concretely this PR makes the following changes:

- The use of `this.synchronized` for guarding against concurrent creation of a block has been replaced with a striped lock. We have effectively replaced a single coarse-grained lock with a lock per block.
- The use of `this.synchronized` for `wait()` and `notifyAll()` has been replaced with per-block Conditions.
- Extract common logic from `lockForWriting` and `lockForReading` into an acquireLock helper method. This deduplication is important given the size of the changes that this PR needed to make to the shared code.
- Optimization: call `currentTaskAttemptId` only once per method by storing its result into a local variable.

The PR is the first in a series of PRs. The next one will add group based locking and removal so we can remove broadcasts and cached RDDs in a constant time operation.

### Why are the changes needed?
The main motivation for this change is to increase and stabilize throughput of clusters that run a significant number of concurrent queries. In the current situation these queries are fighting for the BlockInfoManager lock when they create broadcasts. The worse problem arrises after we do a full GC. This triggers clean-up of unused broadcasts. The number of broadcasts in the system can be significant (>> 10K) when we run under load. We need to acquire a lock per broadcast block. We end up with all the BM worker threads, the DAG Scheduler thread, and the query threads fighting for a single lock. This causes the throughput to drop to close to 0 during these clean-up periods.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Functionally this is covered by existing tests. The performance has been checked by running 32 concurrent streams and hammering a cluster with queries. That shows the throughput drops are mostly gone now. I am not sure how well we can capture this in a non invasive benchmark.

Closes #34632 from hvanhovell/SPARK-37356.

Authored-by: herman <herman@databricks.com>
Signed-off-by: herman <herman@databricks.com>
","['core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala', 'core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala']","Clusters running many concurrent queries are experiencing low and unstable throughput due to all threads fighting for the single BlockInfoManager lock, especially noticeable during unused broadcasts clean-up periods."
0c2ba427bc7323729e6ffb34f1f06a97f0bf0c1d,1516154250,"[SPARK-23095][SQL] Decorrelation of scalar subquery fails with java.util.NoSuchElementException

## What changes were proposed in this pull request?
The following SQL involving scalar correlated query returns a map exception.
``` SQL
SELECT t1a
FROM   t1
WHERE  t1a = (SELECT   count(*)
              FROM     t2
              WHERE    t2c = t1c
              HAVING   count(*) >= 1)
```
``` SQL
key not found: ExprId(278,786682bb-41f9-4bd5-a397-928272cc8e4e)
java.util.NoSuchElementException: key not found: ExprId(278,786682bb-41f9-4bd5-a397-928272cc8e4e)
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at scala.collection.AbstractMap.default(Map.scala:59)
        at scala.collection.MapLike$class.apply(MapLike.scala:141)
        at scala.collection.AbstractMap.apply(Map.scala:59)
        at org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery$.org$apache$spark$sql$catalyst$optimizer$RewriteCorrelatedScalarSubquery$$evalSubqueryOnZeroTups(subquery.scala:378)
        at org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery$$anonfun$org$apache$spark$sql$catalyst$optimizer$RewriteCorrelatedScalarSubquery$$constructLeftJoins$1.apply(subquery.scala:430)
        at org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery$$anonfun$org$apache$spark$sql$catalyst$optimizer$RewriteCorrelatedScalarSubquery$$constructLeftJoins$1.apply(subquery.scala:426)
```

In this case, after evaluating the HAVING clause ""count(*) > 1"" statically
against the binding of aggregtation result on empty input, we determine
that this query will not have a the count bug. We should simply return
the evalSubqueryOnZeroTups with empty value.
(Please fill in changes proposed in this fix)

## How was this patch tested?
A new test was added in the Subquery bucket.

Author: Dilip Biswal <dbiswal@us.ibm.com>

Closes #20283 from dilipbiswal/scalar-count-defect.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala', 'sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-predicate.sql']","Scalar correlated queries using HAVING clause results in ""key not found: ExprId"" error, indicative of a failure in the process of decorrelation of scalar subquery."
544865db77d942fbbeabde96e644c98a892d5045,1638537445,"[SPARK-37455][SQL] Replace hash with sort aggregate if child is already sorted

### What changes were proposed in this pull request?

In the query plan, if the child of hash aggregate is already sorted on group-by columns, we can replace hash aggregate with sort aggregate for better performance, as sort aggregate does not have hashing overhead of hash aggregate. Add a physical plan rule `ReplaceHashWithSortAgg` here, and can be disabled by config `spark.sql.execution.replaceHashWithSortAgg`.

In addition, to help review as this PR triggers several TPCDS plan files change. The files below are having the real code change:

* `SQLConf.scala`
* `QueryExecution.scala`
* `ReplaceHashWithSortAgg.scala`
* `AdaptiveSparkPlanExec.scala`
* `HashAggregateExec.scala`
* `ReplaceHashWithSortAggSuite.scala`
* `SQLMetricsSuite.scala`

### Why are the changes needed?

To get better query performance by leveraging sort ordering in query plan.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added unit test in `ReplaceHashWithSortAggSuite.scala`.

Closes #34702 from c21/agg-rule.

Authored-by: Cheng Su <chengsu@fb.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAgg.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAggSuite.scala']","Despite having already sorted child nodes in the query plan, Hash aggregates are inefficiently applied instead of Sort aggregates, resulting in potential hashing overhead penalty."
280ff523f4079dd9541efc95e6efcb69f9374d22,1505890881,"[SPARK-21977] SinglePartition optimizations break certain Streaming Stateful Aggregation requirements

## What changes were proposed in this pull request?

This is a bit hard to explain as there are several issues here, I'll try my best. Here are the requirements:
  1. A StructuredStreaming Source that can generate empty RDDs with 0 partitions
  2. A StructuredStreaming query that uses the above source, performs a stateful aggregation
     (mapGroupsWithState, groupBy.count, ...), and coalesce's by 1

The crux of the problem is that when a dataset has a `coalesce(1)` call, it receives a `SinglePartition` partitioning scheme. This scheme satisfies most required distributions used for aggregations such as HashAggregateExec. This causes a world of problems:
  Symptom 1. If the input RDD has 0 partitions, the whole lineage will receive 0 partitions, nothing will be executed, the state store will not create any delta files. When this happens, the next trigger fails, because the StateStore fails to load the delta file for the previous trigger
  Symptom 2. Let's say that there was data. Then in this case, if you stop your stream, and change `coalesce(1)` with `coalesce(2)`, then restart your stream, your stream will fail, because `spark.sql.shuffle.partitions - 1` number of StateStores will fail to find its delta files.

To fix the issues above, we must check that the partitioning of the child of a `StatefulOperator` satisfies:
If the grouping expressions are empty:
  a) AllTuple distribution
  b) Single physical partition
If the grouping expressions are non empty:
  a) Clustered distribution
  b) spark.sql.shuffle.partition # of partitions
whether or not `coalesce(1)` exists in the plan, and whether or not the input RDD for the trigger has any data.

Once you fix the above problem by adding an Exchange to the plan, you come across the following bug:
If you call `coalesce(1).groupBy().count()` on a Streaming DataFrame, and if you have a trigger with no data, `StateStoreRestoreExec` doesn't return the prior state. However, for this specific aggregation, `HashAggregateExec` after the restore returns a (0, 0) row, since we're performing a count, and there is no data. Then this data gets stored in `StateStoreSaveExec` causing the previous counts to be overwritten and lost.

## How was this patch tested?

Regression tests

Author: Burak Yavuz <brkyvz@gmail.com>

Closes #19196 from brkyvz/sa-0.
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/EnsureStatefulOpPartitioningSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingAggregationSuite.scala']",Stateful Aggregation with StructuredStreaming using coalesce(1) leads to failure in loading delta files from StateStore and inappropriate overwriting of count in case of no data.
46c7ff8ddf49156b0d79b5c8c6a0103d3fd0487c,1696572591,"[SPARK-45357][CONNECT][TESTS] Normalize `dataframeId` when comparing `CollectMetrics` in `SparkConnectProtoSuite`

### What changes were proposed in this pull request?
This PR add a new function `normalizeDataframeId` to sets the `dataframeId` to the constant 0 of `CollectMetrics`  before comparing `LogicalPlan` in the test case of `SparkConnectProtoSuite`.

### Why are the changes needed?
The test scenario in `SparkConnectProtoSuite` does not need to compare the `dataframeId` in `CollectMetrics`

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
- Manually check

run

```
build/mvn clean install -pl connector/connect/server -am -DskipTests
build/mvn test -pl connector/connect/server
```

**Before**

```
- Test observe *** FAILED ***
  == FAIL: Plans do not match ===
  !CollectMetrics my_metric, [min(id#0) AS min_val#0, max(id#0) AS max_val#0, sum(id#0) AS sum(id)#0L], 0   CollectMetrics my_metric, [min(id#0) AS min_val#0, max(id#0) AS max_val#0, sum(id#0) AS sum(id)#0L], 53
   +- LocalRelation <empty>, [id#0, name#0]                                                                 +- LocalRelation <empty>, [id#0, name#0] (PlanTest.scala:179)
```

**After**

```
Run completed in 41 seconds, 631 milliseconds.
Total number of tests run: 882
Suites: completed 24, aborted 0
Tests: succeeded 882, failed 0, canceled 0, ignored 0, pending 0
All tests passed.
```

### Was this patch authored or co-authored using generative AI tooling?
No

Closes #43155 from LuciferYang/SPARK-45357.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
",['connector/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectProtoSuite.scala'],Inconsistent `dataframeId` in `CollectMetrics` is causing `SparkConnectProtoSuite` test scenario to fail while comparing `LogicalPlan`.
83751035685c84c681e88ac6e55fbcc9d6d37ef5,1556727978,"[SPARK-27557][DOC] Add copy button to Python API docs for easier copying of code-blocks

## What changes were proposed in this pull request?

Add a non-intrusive button for python API documentation, which will remove "">>>"" prompts and outputs of code - for easier copying of code.

For example: The below code-snippet in the document is difficult to copy due to "">>>"" prompts
```
>>> l = [('Alice', 1)]
>>> spark.createDataFrame(l).collect()
[Row(_1='Alice', _2=1)]

```
Becomes this - After the copybutton in the corner of of code-block is pressed - which is easier to copy
```
l = [('Alice', 1)]
spark.createDataFrame(l).collect()
```

![image](https://user-images.githubusercontent.com/9406431/56715817-560c3600-6756-11e9-8bae-58a3d2d57df3.png)

## File changes
Made changes to python/docs/conf.py and copybutton.js - thus only modifying sphinx frontend and no changes were made to the documentation itself- Build process for documentation remains the same.

copybutton.js -> This JS snippet was taken from the official python.org documentation site.

## How was this patch tested?
NA

Closes #24456 from sangramga/copybutton.

Authored-by: sangramga <sangramga@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
","['python/docs/_static/copybutton.js', 'python/docs/conf.py']","Python API documentation has "">>>"" prompts and outputs in code snippets that makes it difficult for users to copy the code directly."
7b9d7551a6de840d6ec8bc031ed8c314bf33b252,1595392054,"[SPARK-32350][CORE] Add batch-write on LevelDB to improve performance of HybridStore

### What changes were proposed in this pull request?
The idea is to improve the performance of HybridStore by adding batch write support to LevelDB. #28412  introduces HybridStore. HybridStore will write data to InMemoryStore at first and use a background thread to dump data to LevelDB once the writing to InMemoryStore is completed. In the comments section of #28412 , mridulm mentioned using batch writing can improve the performance of this dumping process and he wrote the code of writeAll().

### Why are the changes needed?
I did the comparison of the HybridStore switching time between one-by-one write and batch write on an HDD disk. When the disk is free, the batch-write has around 25% improvement, and when the disk is 100% busy, the batch-write has 7x - 10x improvement.

when the disk is at 0% utilization:
| log size, jobs and tasks per job   | original switching time, with write() | switching time with writeAll() |
| ---------------------------------- | ------------------------------------- | ------------------------------ |
| 133m, 400 jobs, 100 tasks per job  | 16s                                   | 13s                            |
| 265m, 400 jobs, 200 tasks per job  | 30s                                   | 23s                            |
| 1.3g, 1000 jobs, 400 tasks per job | 136s                                  | 108s                           |

when the disk is at 100% utilization:
| log size, jobs and tasks per job  | original switching time, with write() | switching time with writeAll() |
| --------------------------------- | ------------------------------------- | ------------------------------ |
| 133m, 400 jobs, 100 tasks per job | 116s                                  | 17s                            |
| 265m, 400 jobs, 200 tasks per job | 251s                                  | 26s                            |

I also ran some write related benchmarking tests on LevelDBBenchmark.java and measured the total time of writing 1024 objects. The tests were conducted when the disk is at 0% utilization.

| Benchmark test           | with write(), ms | with writeAll(), ms |
| ------------------------ | ---------------- | ------------------- |
| randomUpdatesIndexed     | 213.06           | 157.356             |
| randomUpdatesNoIndex     | 57.869           | 35.439              |
| randomWritesIndexed      | 298.854          | 229.274             |
| randomWritesNoIndex      | 66.764           | 38.361              |
| sequentialUpdatesIndexed | 87.019           | 56.219              |
| sequentialUpdatesNoIndex | 61.851           | 41.942              |
| sequentialWritesIndexed  | 94.044           | 56.534              |
| sequentialWritesNoIndex  | 118.345          | 66.483              |

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Manually tested.

Closes #29149 from baohe-zhang/SPARK-32350.

Authored-by: Baohe Zhang <baohe.zhang@verizonmedia.com>
Signed-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
","['common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDB.java', 'core/src/main/scala/org/apache/spark/deploy/history/HybridStore.scala']","Performance issues arise in HybridStore due to the lack of batch write support in LevelDB, which results in slower store switching times especially when the disk is utilized."
193011632ba41dc4035460c429374981a8ebe0b7,1643258187,"[SPARK-38040][BUILD] Enable binary compatibility check for APIs in Catalyst, KVStore and Avro modules

### What changes were proposed in this pull request?

We don't currently run binary compatibility check in below modules:

```
[info] spark-parent: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-network-common: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-tags: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-unsafe: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-network-shuffle: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-kvstore: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-tools: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-token-provider-kafka-0-10: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-streaming-kafka-0-10-assembly: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-catalyst: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-repl: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-avro: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-sql-kafka-0-10: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-hive: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-assembly: mimaPreviousArtifacts not set, not analyzing binary compatibility
[info] spark-examples: mimaPreviousArtifacts not set, not analyzing binary compatibility
```

However, there are some APIs under these modules. For example, https://github.com/apache/spark/blob/master/external/avro/src/main/scala/org/apache/spark/sql/avro/functions.scala for Avro,  https://github.com/apache/spark/tree/master/common/kvstore/src/main/java/org/apache/spark/util/kvstore for KVStore (to be API), and https://github.com/apache/spark/tree/master/sql/catalyst/src/main/java/org/apache/spark/sql/connector for Catalyst

### Why are the changes needed?

To detect binary compatibility.

### Does this PR introduce _any_ user-facing change?

No, dev-only.

### How was this patch tested?

Manually tested via running `dev/mima`.

Closes #35339 from HyukjinKwon/SPARK-38040.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['project/MimaExcludes.scala', 'project/SparkBuild.scala']","Binary compatibility check is not running for certain modules including Catalyst, KVStore, and Avro, which can lead to undetected compatibility issues."
5630f700768432396a948376f5b46b00d4186e1b,1651458516,"[SPARK-38085][SQL][FOLLOWUP] Do not fail too early for DeleteFromTable

### What changes were proposed in this pull request?

`DeleteFromTable` has been in Spark for a long time and there are existing Spark extensions to compile `DeleteFromTable` to physical plans. However, the new analyzer rule `RewriteDeleteFromTable` fails very early if the v2 table does not support delete. This breaks certain Spark extensions which can still execute `DeleteFromTable` for certain v2 tables.

This PR simply removes the error throwing in `RewriteDeleteFromTable`. It's a safe change because:
1. the new delete-related rules only match v2 table with `SupportsRowLevelOperations`, so won't be affected by this change
2. the planner rule will fail eventually if the v2 table doesn't support deletion. Spark eagerly executes commands so Spark users can still see this error immediately.

### Why are the changes needed?

To not break existing Spark extesions.

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

existing tests

Closes #36402 from cloud-fan/follow.

Lead-authored-by: Wenchen Fan <wenchen@databricks.com>
Co-authored-by: Wenchen Fan <cloud0fan@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala'],"'RewriteDeleteFromTable' analyzer rule prematurely failing for V2 tables that do not support delete operations, leading to disruption for some functioning Spark extensions."
2a1267aeb75bf838c74d1cf274aa258be060c17b,1636546893,"[SPARK-37261][SQL] Allow adding partitions with ANSI intervals in DSv2

### What changes were proposed in this pull request?
In the PR, I propose to skip checking of ANSI interval types while creating or writing to a table using V2 catalogs. As the consequence of that, users can creating tables in V2 catalogs partitioned by ANSI interval columns (the legacy intervals of `CalendarIntervalType` are still prohibited). Also this PR adds new test which checks:
1. Adding new partition with ANSI intervals via `ALTER TABLE .. ADD PARTITION`
2. INSERT INTO a table partitioned by ANSI intervals

for V1/V2 In-Memory catalogs (skips V1 Hive external catalog).

### Why are the changes needed?
To allow users saving of ANSI intervals as partition values using DSv2.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
By running new test for V1/V2 In-Memory and V1 Hive external catalogs:
```
$ build/sbt ""test:testOnly org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite""
$ build/sbt ""test:testOnly org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite""
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite""
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *DataSourceV2SQLSuite""
```

Closes #34537 from MaxGekk/alter-table-ansi-interval.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterTableAddPartitionSuiteBase.scala']","Tables using V2 catalogs cannot be partitioned by ANSI interval columns, and attempting to add new partitions with ANSI intervals results in errors."
fa3f096e02d408fbeab5f69af451ef8bc8f5b3db,1652450464,"[SPARK-39157][SQL] H2Dialect should override getJDBCType so as make the data type is correct

### What changes were proposed in this pull request?
Currently, `H2Dialect` not implement `getJDBCType` of `JdbcDialect`, so the DS V2 push-down will throw exception show below:
```
Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13) (jiaan-gengdembp executor driver):
 org.h2.jdbc.JdbcSQLNonTransientException: Unknown data type: ""STRING""; SQL statement:
SELECT ""DEPT"",""NAME"",""SALARY"",""BONUS"",""IS_MANAGER"" FROM ""test"".""employee""  WHERE (""BONUS"" IS NOT NULL) AND (""DEPT"" IS NOT NULL) AND (CAST(""BONUS"" AS string) LIKE '%30%') AND (CAST(""DEPT"" AS byte) > 1) AND (CAST(""DEPT"" AS short) > 1) AND (CAST(""BONUS"" AS decimal(20,2)) > 1200.00)    [50004-210]
```
H2Dialect should implement `getJDBCType` of `JdbcDialect`.

### Why are the changes needed?
 make the H2 data type is correct.

### Does this PR introduce _any_ user-facing change?
'Yes'.
Fix a bug for `H2Dialect`.

### How was this patch tested?
New tests.

Closes #36516 from beliefer/SPARK-39157.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala', 'sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala']","Data Source V2 push-down throws an exception due to H2Dialect not implementing getJDBCType of JdbcDialect, rendering H2 data type incorrect."
0c6bd3bd0b95d17bc1eebb503269eda43df90394,1579025147,"[SPARK-27142][SQL] Provide REST API for SQL information

### What changes were proposed in this pull request?

Currently for Monitoring Spark application SQL information is not available from REST but only via UI. REST provides only applications,jobs,stages,environment. This Jira is targeted to provide a REST API so that SQL level information can be found
A single SQL query can result into multiple jobs. So for end user who is using STS or spark-sql, the intended highest level of probe is the SQL which he has executed. This information can be seen from SQL tab. Attaching a sample.
![image](https://user-images.githubusercontent.com/22072336/54298729-5524a800-45df-11e9-8e4d-b99a8b882031.png)
But same information he cannot access using the REST API exposed by spark and he always have to rely on jobs API which may be difficult. So i intend to expose the information seen in SQL tab in UI via REST API

Mainly:

Id :  Long - execution id of the sql
status : String - possible values COMPLETED/RUNNING/FAILED
description : String - executed SQL string
planDescription : String - Plan representation
metrics : Seq[Metrics] - `Metrics` contain `metricName: String, metricValue: String`
submissionTime : String - formatted `Date` time of SQL submission
duration : Long - total run time in milliseconds
runningJobIds : Seq[Int] - sequence of running job ids
failedJobIds : Seq[Int] - sequence of failed job ids
successJobIds : Seq[Int] - sequence of success job ids

* To fetch sql executions: /sql?details=boolean&offset=integer&length=integer
* To fetch single execution:  /sql/{executionID}?details=boolean

| parameter | type | remarks |
| ------------- |:-------------:| -----|
| details | boolean | Optional. Set true to get plan description and metrics information, defaults to false |
| offset | integer | Optional. offset to fetch the executions, defaults to 0 |
| length | integer | Optional. total number of executions to be fetched, defaults to 20 |

### Why are the changes needed?
To support users query SQL information via REST API

### Does this PR introduce any user-facing change?
Yes. It provides a new monitoring URL for SQL

### How was this patch tested?
Tested manually

![image](https://user-images.githubusercontent.com/22072336/54282168-6d85ca00-45c1-11e9-8935-7586ccf0efff.png)

![image](https://user-images.githubusercontent.com/22072336/54282191-7b3b4f80-45c1-11e9-941c-f0ec37026192.png)

Closes #24076 from ajithme/restapi.

Lead-authored-by: Ajith <ajith2489@gmail.com>
Co-authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala', 'sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/ApiSqlRootResource.scala', 'sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/SqlResource.scala', 'sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/api.scala']","Monitoring Spark application SQL information is not available from REST, only from the UI. Users cannot access similar data as seen in UI's SQL tab through REST API."
7533cccc5dfe5788789fc95ae2b0cff5d7cc3e24,1560497614,"[SPARK-28053][INFRA] Handle a corner case where there is no `Link` header

## What changes were proposed in this pull request?

Currently, `github_jira_sync.py` assumes that there is `Link` always. However, it will fail when the number of the open PR is less than 100 (the default paging number). It will not happen in Apache Spark, but we had better fix that because it happens during review process for `github_jira_sync.py` script.
```
Traceback (most recent call last):
  File ""dev/github_jira_sync.py"", line 139, in <module>
    jira_prs = get_jira_prs()
  File ""dev/github_jira_sync.py"", line 83, in get_jira_prs
    link_header = filter(lambda k: k.startswith(""Link""), page.info().headers)[0]
IndexError: list index out of range
```

## How was this patch tested?

Manually check with another repo which has small number of open PRs (< 100).
```
$ export JIRA_PASSWORD=...
$ export GITHUB_API_BASE='https://api.github.com/repos/your-id/spark'
$ dev/github_jira_sync.py
```

Closes #24874 from dongjoon-hyun/SPARK-28053.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['dev/github_jira_sync.py'],"`github_jira_sync.py` assumes a `Link` header always exists, causing a failure when the number of open PRs is less than 100 (default paging number)."
03e30063127fd71bef8a14553381e805fe5b6679,1565057478,"[SPARK-28213][SQL][FOLLOWUP] code cleanup and bug fix for columnar execution framework

## What changes were proposed in this pull request?

I did a post-hoc review of https://github.com/apache/spark/pull/25008 , and would like to propose some cleanups/fixes/improvements:

1. Do not track the scanTime metrics in `ColumnarToRowExec`. This metrics is specific to file scan, and doesn't make sense for a general batch-to-row operator.
2. Because of 2, we need to track scanTime when building RDDs in the file scan node.
3. use `RDD#mapPartitionsInternal` instead of `flatMap` in several places, as `mapPartitionsInternal` is created for Spark SQL and we use it in almost all the SQL operators.
4. Add `limitNotReachedCond` in `ColumnarToRowExec`. This was in the `ColumnarBatchScan` before and is critical for performance.
5. Clear the relationship between codegen stage and columnar stage. The whole-stage-codegen framework is completely row-based, so these 2 kinds of stages can NEVER overlap. When they are adjacent, it's either a `RowToColumnarExec` above `WholeStageExec`, or a `ColumnarToRowExec` above the `InputAdapter`.
6. Reuse the `ColumnarBatch` in `RowToColumnarExec`. We don't need to create a new one every time, just need to reset it.
7. Do not skip testing full scan node in `LogicalPlanTagInSparkPlanSuite`
8. Add back the removed tests in `WholeStageCodegenSuite`.

## How was this patch tested?

existing tests

Closes #25264 from cloud-fan/minor.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExecBase.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExecSuite.scala']","Columnar execution framework has incongruities in scanTime metrics tracking, improper RDD building method, missing critical performance component, unclear stage relationships, inefficiencies in ColumnarBatch usage, and skipped test cases."
0de98fd39364d2704607b031ddec623d48459202,1659630627,"[SPARK-39872][SQL] Change to use `BytePackerForLong#unpack8Values` with Array input api in `VectorizedDeltaBinaryPackedReader`

### What changes were proposed in this pull request?
This pr use `BytePackerForLong#unpack8Values` with `Array` input api  instead of `BytePackerForLong#unpack8Values` with `ByteBuffer` input api in `VectorizedDeltaBinaryPackedReader` to improve the performance of scanning `INT` and `BIGINT` type data from Parquet DataPage V2.

### Why are the changes needed?
`BytePackerForLong#unpack8Values` with `ByteBuffer` input api has hotspot in using `HeapByteBuffer#get(int)` due to `HeapByteBuffer#get(int)` needs boundary check every time it is called,  this affects the performance of scanning `INT` and `BIGINT` type data from Parquet DataPage V2.

The results of scenario `SQL Single INT/BIGINT Column Scan` as follows:

**Before**

```
OpenJDK 64-Bit Server VM 1.8.0_322-b06 on Linux 5.13.0-1021-azure
Intel(R) Xeon(R) Platinum 8171M CPU  2.60GHz
SQL Single INT Column Scan:               Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
SQL Parquet Vectorized: DataPageV1                  192            203          12         82.0          12.2      93.5X
SQL Parquet Vectorized: DataPageV2                  363            373          10         43.4          23.1      49.5X

OpenJDK 64-Bit Server VM 1.8.0_322-b06 on Linux 5.13.0-1021-azure
Intel(R) Xeon(R) Platinum 8171M CPU  2.60GHz
SQL Single BIGINT Column Scan:            Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
SQL Parquet Vectorized: DataPageV1                  252            262          11         62.3          16.1      92.8X
SQL Parquet Vectorized: DataPageV2                  530            537          10         29.7          33.7      44.2X
```

**After**

```
OpenJDK 64-Bit Server VM 1.8.0_342-b07 on Linux 5.15.0-1014-azure
Intel(R) Xeon(R) Platinum 8272CL CPU  2.60GHz
SQL Single INT Column Scan:               Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
SQL Parquet Vectorized: DataPageV1                  173            201          70         91.1          11.0      91.2X
SQL Parquet Vectorized: DataPageV2                  266            272           9         59.2          16.9      59.2X

OpenJDK 64-Bit Server VM 1.8.0_342-b07 on Linux 5.15.0-1014-azure
Intel(R) Xeon(R) Platinum 8272CL CPU  2.60GHz
SQL Single BIGINT Column Scan:            Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
SQL Parquet Vectorized: DataPageV1                  235            243           9         66.9          15.0      88.3X
SQL Parquet Vectorized: DataPageV2                  398            404           6         39.5          25.3      52.1X
```

After this pr, for the scanning scenario of type `INT` and `BIGINT` data, the performance gap between Parquet DataPage V2 and Parquet DataPage V1 is reduced.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

- Pass GitHub Actions

Closes #37293 from LuciferYang/ParquetV2-int-and-long.

Lead-authored-by: yangjie01 <yangjie01@baidu.com>
Co-authored-by: YangJie <yangjie01@baidu.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaBinaryPackedReader.java'],Performance issue with scanning INT and BIGINT type data from Parquet DataPage V2 due to frequent boundary checks in `HeapByteBuffer#get(int)` method when using `BytePackerForLong#unpack8Values` with `ByteBuffer` input API.
fb036c4413c2cd4d90880d080f418ec468d6c0fc,1492168607,"[SPARK-20318][SQL] Use Catalyst type for min/max in ColumnStat for ease of estimation

## What changes were proposed in this pull request?

Currently when estimating predicates like col > literal or col = literal, we will update min or max in column stats based on literal value. However, literal value is of Catalyst type (internal type), while min/max is of external type. Then for the next predicate, we again need to do type conversion to compare and update column stats. This is awkward and causes many unnecessary conversions in estimation.

To solve this, we use Catalyst type for min/max in `ColumnStat`. Note that the persistent format in metastore is still of external type, so there's no inconsistency for statistics in metastore.

This pr also fixes a bug for boolean type in `IN` condition.

## How was this patch tested?

The changes for ColumnStat are covered by existing tests.
For bug fix, a new test for boolean type in IN condition is added

Author: wangzhenhua <wangzhenhua@huawei.com>

Closes #17630 from wzhfy/refactorColumnStat.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Statistics.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/EstimationUtils.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/FilterEstimation.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/Range.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/statsEstimation/FilterEstimationSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/statsEstimation/JoinEstimationSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/statsEstimation/ProjectEstimationSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala', 'sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala']","Repeated type conversions occur during estimation processes due to the use of different types for literal values and min/max column stats, causing inefficiencies. Also observed a bug in 'IN' condition handling for boolean type."
c20916a5dc4a7e771463838e797cb944569f6259,1535119099,"[SPARK-25073][YARN] AM and Executor Memory validation message is not proper while submitting spark yarn application

**## What changes were proposed in this pull request?**
When the yarn.nodemanager.resource.memory-mb or yarn.scheduler.maximum-allocation-mb
 memory assignment is insufficient, Spark always reports an error request to adjust
yarn.scheduler.maximum-allocation-mb even though in message it shows the memory value
of yarn.nodemanager.resource.memory-mb parameter,As the error Message is bit misleading to the user  we can modify the same, We can keep the error message same as executor memory validation message.

Defintion of **yarn.nodemanager.resource.memory-mb:**
Amount of physical memory, in MB, that can be allocated for containers. It means the amount of memory YARN can utilize on this node and therefore this property should be lower then the total memory of that machine.
**yarn.scheduler.maximum-allocation-mb:**
It defines the maximum memory allocation available for a container in MB
it means RM can only allocate memory to containers in increments of ""yarn.scheduler.minimum-allocation-mb"" and not exceed ""yarn.scheduler.maximum-allocation-mb"" and It should not be more than total allocated memory of the Node.

**## How was this patch tested?**
Manually tested in hdfs-Yarn clustaer

Closes #22199 from sujith71955/maste_am_log.

Authored-by: s71955 <sujithchacko.2010@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
",['resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala'],"Error messages displayed when ""yarn.nodemanager.resource.memory-mb"" or ""yarn.scheduler.maximum-allocation-mb"" have insufficient memory allocation are misleading, pointing to incorrect parameters for user adjustment."
5b1353f690bf416fdb3a34c94741425b95f97308,1619353632,"[SPARK-35168][SQL] mapred.reduce.tasks should be shuffle.partitions not adaptive.coalescePartitions.initialPartitionNum

### What changes were proposed in this pull request?

```sql
spark-sql> set spark.sql.adaptive.coalescePartitions.initialPartitionNum=1;
spark.sql.adaptive.coalescePartitions.initialPartitionNum	1
Time taken: 2.18 seconds, Fetched 1 row(s)
spark-sql> set mapred.reduce.tasks;
21/04/21 14:27:11 WARN SetCommand: Property mapred.reduce.tasks is deprecated, showing spark.sql.shuffle.partitions instead.
spark.sql.shuffle.partitions	1
Time taken: 0.03 seconds, Fetched 1 row(s)
spark-sql> set spark.sql.shuffle.partitions;
spark.sql.shuffle.partitions	200
Time taken: 0.024 seconds, Fetched 1 row(s)
spark-sql> set mapred.reduce.tasks=2;
21/04/21 14:31:52 WARN SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
spark.sql.shuffle.partitions	2
Time taken: 0.017 seconds, Fetched 1 row(s)
spark-sql> set mapred.reduce.tasks;
21/04/21 14:31:55 WARN SetCommand: Property mapred.reduce.tasks is deprecated, showing spark.sql.shuffle.partitions instead.
spark.sql.shuffle.partitions	1
Time taken: 0.017 seconds, Fetched 1 row(s)
spark-sql>
```

`mapred.reduce.tasks` is mapping to `spark.sql.shuffle.partitions` at write-side, but `spark.sql.adaptive.coalescePartitions.initialPartitionNum` might take precede of `spark.sql.shuffle.partitions`

### Why are the changes needed?

roundtrip for `mapred.reduce.tasks`

### Does this PR introduce _any_ user-facing change?

yes, `mapred.reduce.tasks` will always report `spark.sql.shuffle.partitions` whether `spark.sql.adaptive.coalescePartitions.initialPartitionNum` exists or not.

### How was this patch tested?

a new test

Closes #32265 from yaooqinn/SPARK-35168.

Authored-by: Kent Yao <yao@apache.org>
Signed-off-by: Kent Yao <yao@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala', 'sql/core/src/test/scala/org/apache/spark/sql/internal/SQLConfSuite.scala']","Setting 'mapred.reduce.tasks' incorrectly maps to 'spark.sql.adaptive.coalescePartitions.initialPartitionNum' instead of 'spark.sql.shuffle.partitions', improper parameter precedence causes wrong values."
01bdf848c86eb5ff8aa6bbdbb7720babfb71b3d8,1634822674,"[SPARK-37047][SQL] Add lpad and rpad functions for binary strings

### What changes were proposed in this pull request?

This PR overloads the `lpad` and `rpad` functions to work correctly with BINARY string inputs.

### Why are the changes needed?

The current behavior of the `lpad` and `rpad` functions is problematic. BINARY string inputs get converted automatically to UTF8 strings and then padded. The result can be an invalid UTF8 string.

### Does this PR introduce _any_ user-facing change?

Yes. We are adding overloads for `lpad` and `rpad` for BINARY strings. This PR should be viewed as a breaking change in the sense that the result of `lpad` and `rpad` for BINARY string inputs is now BINARY string, as opposed to the previous behavior which was returning a UTF8 string.

### How was this patch tested?

Unit tests.

Closes #34154 from mkaravel/binary-lpad-rpad.

Authored-by: Menelaos Karavelas <menelaos.karavelas@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['common/unsafe/src/main/java/org/apache/spark/unsafe/types/ByteArray.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/functions.scala', 'sql/core/src/test/resources/sql-tests/inputs/string-functions.sql']","Current implementation of `lpad` and `rpad` functions convert BINARY strings to UTF8, leading to potential invalid UTF8 strings when padded."
c134c7597d19df783c085bb79b6deb01e21c769a,1663206445,"[SPARK-40339][SPARK-40342][SPARK-40345][SPARK-40348][PS] Implement quantile in Rolling/RollingGroupby/Expanding/ExpandingGroupby

### What changes were proposed in this pull request?
Implement quantile in Rolling/RollingGroupby/Expanding/ExpandingGroupby

### Why are the changes needed?

Improve PS api coverage

```python
>>> s = ps.Series([4, 3, 5, 2, 6])
>>> s.rolling(2).quantile(0.5)
0    NaN
1    3.0
2    3.0
3    2.0
4    2.0
dtype: float64

>>> s = ps.Series([2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5])
>>> s.groupby(s).rolling(3).quantile(0.5).sort_index()
2  0     NaN
   1     NaN
3  2     NaN
   3     NaN
   4     3.0
4  5     NaN
   6     NaN
   7     4.0
   8     4.0
5  9     NaN
   10    NaN
dtype: float64

>>> s = ps.Series([1, 2, 3, 4])
>>> s.expanding(2).quantile(0.5)
0    NaN
1    1.0
2    2.0
3    2.0
dtype: float64

>>> s = ps.Series([2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5])
>>> s.groupby(s).expanding(3).quantile(0.5).sort_index()
2  0     NaN
   1     NaN
3  2     NaN
   3     NaN
   4     3.0
4  5     NaN
   6     NaN
   7     4.0
   8     4.0
5  9     NaN
   10    NaN
dtype: float64

```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
CI passed

Closes #37836 from Yikun/SPARK-40339.

Authored-by: Yikun Jiang <yikunkero@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/groupby.py', 'python/pyspark/pandas/missing/window.py', 'python/pyspark/pandas/tests/test_expanding.py', 'python/pyspark/pandas/tests/test_rolling.py', 'python/pyspark/pandas/window.py']","Quantile functionality in Rolling/RollingGroupby/Expanding/ExpandingGroupby operations is missing, limiting PySpark API coverage."
fc385dafabe3c609b38b81deaaf36e5eb6ee341b,1655339930,"[SPARK-39061][SQL] Set nullable correctly for `Inline` output attributes

### What changes were proposed in this pull request?

Change `Inline#elementSchema` to make each struct field nullable when the containing array has a null element.

### Why are the changes needed?

This query returns incorrect results (the last row should be `NULL NULL`):
```
spark-sql> select inline(array(named_struct('a', 1, 'b', 2), null));
1	2
-1	-1
Time taken: 4.053 seconds, Fetched 2 row(s)
spark-sql>
```
And this query gets a NullPointerException:
```
spark-sql> select inline(array(named_struct('a', '1', 'b', '2'), null));
22/04/28 16:51:54 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NullPointerException: null
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(Buffere
```
When an array of structs is created by `CreateArray`, and no struct field contains a literal null value, the schema for the struct will have non-nullable fields, even if the array itself has a null entry (as in the example above). As a result, the output attributes for the generator will be non-nullable.

When the output attributes for `Inline` are non-nullable, `GenerateUnsafeProjection#writeExpressionsToBuffer` generates incorrect code for null structs.

In more detail, the issue is this: `GenerateExec#codeGenCollection` generates code that will check if the struct instance (i.e., array element) is null and, if so, set a boolean for each struct field to indicate that the field contains a null. However, unless the generator's output attributes are nullable, `GenerateUnsafeProjection#writeExpressionsToBuffer` will not generate any code to check those booleans. Instead it will generate code to write out whatever is in the variables that normally hold the struct values (which will be garbage if the array element is null).

Arrays of structs from file sources do not have this issue. In that case, each `StructField` will have nullable=true due to [this](https://github.com/apache/spark/blob/fe85d7912f86c3e337aa93b23bfa7e7e01c0a32e/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L417).

(Note: the eval path for `Inline` has a different bug with null array elements that occurs even when `nullable` is set correctly in the schema, but I will address that in a separate PR).

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New unit test.

Closes #36883 from bersprockets/inline_struct_nullability_issue.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala', 'sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala']","Array queries with `Inline` cannot handle null elements within arrays as they have been set to be non-nullable, leading to incorrect results or NullPointerExceptions."
004aea8155d3b768684ab20cd29b3a820a204f22,1594514718,"[SPARK-32154][SQL] Use ExpressionEncoder for the return type of ScalaUDF to convert to catalyst type

### What changes were proposed in this pull request?

This PR proposes to use `ExpressionEncoder` for the return type of ScalaUDF to convert to the catalyst type, instead of using `CatalystTypeConverters`.

Note, this change only takes effect for typed Scala UDF since its the only case where we know the type tag of the raw type.

### Why are the changes needed?

Users now could register a UDF with `Instant`/`LocalDate` as return types even with `spark.sql.datetime.java8API.enabled=false`. However, the UDF can not really be used.
For example, if we try:

```scala
scala> sql(""set spark.sql.datetime.java8API.enabled=false"")
scala> spark.udf.register(""buildDate"", udf{ d: String => java.time.LocalDate.parse(d) })
scala> Seq(""2020-07-02"").toDF(""d"").selectExpr(""CAST(buildDate(d) AS STRING)"").show
```
Then, we will hit the error:
```scala
java.lang.ClassCastException: java.time.LocalDate cannot be cast to java.sql.Date
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$DateConverter$.toCatalystImpl(CatalystTypeConverters.scala:304)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:107)
  at org.apache.spark.sql.catalyst.CatalystTypeConverters$.$anonfun$createToCatalystConverter$2(CatalystTypeConverters.scala:425)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1169)
...
```
as it actually requires enabling `spark.sql.datetime.java8API.enabled` when using the UDF. And I think this could make users get confused.

This happens because when registering the UDF,  Spark actually uses `ExpressionEncoder` to ser/deser types. However, when using UDF, Spark uses `CatalystTypeConverters`, which is under control of `spark.sql.datetime.java8API.enabled`, to ser/deser types. Therefore, Spark would fail to convert the Java8 date time types.

If we could also use `ExpressionEncoder` to ser/deser types for the return type, similar to what we do for the input parameter types, then, UDF could support Instant/LocalDate, event other combined complex types as well.

### Does this PR introduce _any_ user-facing change?

Yes. Before this PR, if users run the demo above, they would hit the error. After this PR, the demo will run successfully.

### How was this patch tested?

Updated 2 tests and added a new one for combined types of `Instant` and `LocalDate`.

Closes #28979 from Ngone51/udf-return-encoder.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala', 'sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala', 'sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala', 'sql/core/src/main/scala/org/apache/spark/sql/functions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala']","In Spark SQL, typed Scala UDF with 'Instant'/'LocalDate' as return types cause a ClassCastException when 'spark.sql.datetime.java8API.enabled' is set to false, hindering the UDF's application."
6e5bc39e17d4cf02806761170de6ddeb634aa343,1598940911,"[SPARK-32624][SQL][FOLLOWUP] Fix regression in CodegenContext.addReferenceObj on nested Scala types

### What changes were proposed in this pull request?

Use `CodeGenerator.typeName()` instead of `Class.getCanonicalName()` in `CodegenContext.addReferenceObj()` for getting the runtime class name for an object.

### Why are the changes needed?

https://github.com/apache/spark/pull/29439 fixed a bug in `CodegenContext.addReferenceObj()` for `Array[Byte]` (i.e. Spark SQL's `BinaryType`) objects, but unfortunately it introduced a regression for some nested Scala types.

For example, for `implicitly[Ordering[UTF8String]]`, after that PR `CodegenContext.addReferenceObj()` would return `((null) references[0] /* ... */)`. The actual type for `implicitly[Ordering[UTF8String]]` is `scala.math.LowPriorityOrderingImplicits$$anon$3` in Scala 2.12.10, and `Class.getCanonicalName()` returns `null` for that class.

On the other hand, `Class.getName()` is safe to use for all non-array types, and Janino will happily accept the type name returned from `Class.getName()` for nested types. `CodeGenerator.typeName()` happens to do the right thing by correctly handling arrays and otherwise use `Class.getName()`. So it's a better alternative than `Class.getCanonicalName()`.

Side note: rule of thumb for using Java reflection in Spark: it may be tempting to use `Class.getCanonicalName()`, but for functions that may need to handle Scala types, please avoid it due to potential issues with nested Scala types.
Instead, use `Class.getName()` or utility functions in `org.apache.spark.util.Utils` (e.g. `Utils.getSimpleName()` or `Utils.getFormattedClassName()` etc).

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added new unit test case for the regression case in `CodeGenerationSuite`.

Closes #29602 from rednaxelafx/spark-32624-followup.

Authored-by: Kris Mok <kris.mok@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala']","The use of `Class.getCanonicalName()` in `CodegenContext.addReferenceObj()` introduces regression for nested Scala types like `implicitly[Ordering[UTF8String]]`, returning null in places where actual nested class name is expected."
edd6076699c36a94c1bc1b9ca853f05e55ba9f2c,1665037078,"[SPARK-40670][SS][PYTHON] Fix NPE in applyInPandasWithState when the input schema has ""non-nullable"" column(s)

### What changes were proposed in this pull request?

This PR fixes a bug which occurs NPE when the input schema of applyInPandasWithState has ""non-nullable"" column(s).
This PR also leaves a code comment explaining the fix. Quoting:

```
  // See processTimedOutState: we create a row which contains the actual values for grouping key,
  // but all nulls for value side by intention. This technically changes the schema of input to
  // be ""nullable"", hence the schema information and the internal projection of row should take
  // this into consideration. Strictly saying, it's not applied to the part of grouping key, but
  // it doesn't hurt much even if we apply the same for grouping key as well.
```

### Why are the changes needed?

There's a bug which we didn't take the non-null columns into account. This PR fixes the bug.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New UT. The new test case failed with NPE without the fix, and succeeded with the fix.

Closes #38115 from HeartSaVioR/SPARK-40670.

Authored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasWithStateExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsInPandasWithStateSuite.scala']","A Null Pointer Exception (NPE) happens with the function applyInPandasWithState when the input schema includes ""non-nullable"" column(s)."
b6190a3db974c19b6a0c4fe7af75531d67755074,1694658224,"[SPARK-45056][PYTHON][SS][CONNECT] Termination tests for streamingQueryListener and foreachBatch

### What changes were proposed in this pull request?

Add termination tests for StreamingQueryListener and foreachBatch.

The behavior is mimicked by creating the same query on server side that would have been created if running the same python query is ran on client side. For example, in foreachBatch, a python foreachBatch function is serialized using cloudPickleSerializer and passed to the server side, here we start another python process on the server and call the same cloudPickleSerializer and pass the bytes to the server, and construct `SimplePythonFunction` accordingly.

Refactored the code a bit for testing purpose.

### Why are the changes needed?

Necessary tests

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Test only addition

### Was this patch authored or co-authored using generative AI tooling?

No

Closes #42779 from WweiL/SPARK-44435-followup-termination-tests.

Lead-authored-by: Wei Liu <wei.liu@databricks.com>
Co-authored-by: Wei Liu <z920631580@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala', 'connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/StreamingQueryListenerHelper.scala', 'connector/connect/server/src/test/scala/org/apache/spark/sql/connect/service/SparkConnectSessionHodlerSuite.scala', 'core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala', 'core/src/main/scala/org/apache/spark/api/python/StreamingPythonRunner.scala', 'python/pyspark/sql/tests/connect/streaming/test_parity_foreach_batch.py', 'python/pyspark/sql/tests/streaming/test_streaming_foreach_batch.py', 'sql/core/src/test/scala/org/apache/spark/sql/IntegratedUDFTestUtils.scala']","The streamingQueryListener and foreachBatch functionality lack termination tests, which may lead to unpredicted behavior during termination."
ceccda07076240a13759354dda35d929445a90e8,1675676047,"[SPARK-40819][SQL] Timestamp nanos behaviour regression

### What changes were proposed in this pull request?

Handle `TimeUnit.NANOS` for parquet `Timestamps` addressing a regression in behaviour since 3.2

### Why are the changes needed?

Since version 3.2 reading parquet files that contain attributes with type `TIMESTAMP(NANOS,true)` is not possible as ParquetSchemaConverter returns
```
Caused by: org.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,true))
```
https://issues.apache.org/jira/browse/SPARK-34661 introduced a change matching on the `LogicalTypeAnnotation` which only covers Timestamp cases for `TimeUnit.MILLIS` and `TimeUnit.MICROS` meaning `TimeUnit.NANOS` would return `illegalType()`

Prior to 3.2 the matching used the `originalType` which for `TIMESTAMP(NANOS,true)` return `null` and therefore resulted to a `LongType`, the change proposed is too consider `TimeUnit.NANOS` and return `LongType` making behaviour the same as before.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Added unit test covering this scenario.
Internally deployed to read parquet files that contain `TIMESTAMP(NANOS,true)`

Closes #38312 from awdavidson/ts-nanos-fix.

Lead-authored-by: alfreddavidson <alfie.davidson9@gmail.com>
Co-authored-by: Attila Zsolt Piros <2017933+attilapiros@users.noreply.github.com>
Co-authored-by: awdavidson <54780428+awdavidson@users.noreply.github.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala']","Reading Parquet files containing attributes of type `TIMESTAMP(NANOS,true)` is not possible since Spark version 3.2, throwing an `AnalysisException: Illegal Parquet type: INT64`."
a10b328dbc056aafaa696579f9a6e2b0cb8eb25f,1511902861,"[SPARK-22431][SQL] Ensure that the datatype in the schema for the table/view metadata is parseable by Spark before persisting it

## What changes were proposed in this pull request?
* JIRA:  [SPARK-22431](https://issues.apache.org/jira/browse/SPARK-22431)  : Creating Permanent view with illegal type

**Description:**
- It is possible in Spark SQL to create a permanent view that uses an nested field with an illegal name.
- For example if we create the following view:
```create view x as select struct('a' as `$q`, 1 as b) q```
- A simple select fails with the following exception:

```
select * from x;

org.apache.spark.SparkException: Cannot recognize hive type string: struct<$q:string,b:int>
  at org.apache.spark.sql.hive.client.HiveClientImpl$.fromHiveColumn(HiveClientImpl.scala:812)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:378)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:378)
...
```
**Issue/Analysis**: Right now, we can create a view with a schema that cannot be read back by Spark from the Hive metastore.  For more details, please see the discussion about the analysis and proposed fix options in comment 1 and comment 2 in the [SPARK-22431](https://issues.apache.org/jira/browse/SPARK-22431)

**Proposed changes**:
 - Fix the hive table/view codepath to check whether the schema datatype is parseable by Spark before persisting it in the metastore. This change is localized to HiveClientImpl to do the check similar to the check in FromHiveColumn. This is fail-fast and we will avoid the scenario where we write something to the metastore that we are unable to read it back.
- Added new unit tests
- Ran the sql related unit test suites ( hive/test, sql/test, catalyst/test) OK

With the fix:
```
create view x as select struct('a' as `$q`, 1 as b) q;
17/11/28 10:44:55 ERROR SparkSQLDriver: Failed in [create view x as select struct('a' as `$q`, 1 as b) q]
org.apache.spark.SparkException: Cannot recognize hive type string: struct<$q:string,b:int>
	at org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$getSparkSQLDataType(HiveClientImpl.scala:884)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$verifyColumnDataType$1.apply(HiveClientImpl.scala:906)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$verifyColumnDataType$1.apply(HiveClientImpl.scala:906)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
...
```
## How was this patch tested?
- New unit tests have been added.

hvanhovell, Please review and share your thoughts/comments.  Thank you so much.

Author: Sunitha Kambhampati <skambha@us.ibm.com>

Closes #19747 from skambha/spark22431.
","['sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala']","Creating a permanent view using an illegal type isn't prevented in current Spark SQL, leading to read failure from Hive metastore due to unparseable schema datatype."
24561caad4685b17e70596a4586773cff345bc40,1637771579,"[SPARK-37444][SQL] ALTER NAMESPACE ... SET LOCATION should handle empty location consistently across v1 and v2 command

### What changes were proposed in this pull request?

Currently, there is an inconsistency when handling an empty location for `ALTER NAMESPACE .. SET LOCATION` between v1 and v2 command. In v1 command, an empty string location will result in the `IllegalArgumentException` exception thrown whereas v2 uses the empty string as it is.

This PR proposes to make the behavior consistent by following the v1 command behavior.

### Why are the changes needed?

To make the behavior consistent and the reason for following v1 behavior is that ""Spark should be responsible to qualify the user-specified path using its spark/hadoop configs, before passing the path to v2 sources"": https://github.com/apache/spark/pull/34610#discussion_r754028045

### Does this PR introduce _any_ user-facing change?

Yes, now the empty string location will result in the `IllegalArgumentException` exception thrown even for v2 catalogs.

### How was this patch tested?

Added a new test

Closes #34686 from imback82/empty_location_fix.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala']","'ALTER NAMESPACE .. SET LOCATION' command handles empty string location inconsistently in v1 and v2 commands, resulting in undesired exceptions."
7d010b12257abc46bb253fbb7a5944139b176359,1664815313,"[SPARK-40481][CORE] Ignore stage fetch failure caused by decommissioned executor

### What changes were proposed in this pull request?
Add a config `spark.stage.ignoreDecommissionFetchFailure` to control whether ignore stage fetch failure caused by decommissioned executor when count `spark.stage.maxConsecutiveAttempts`

Fetch failure only be ignored when executors are in below condition:
1. Waiting for decommission start
2. Under decommission process

Fetch failure might not be ignored when executors are in below condition, but this is best effort approach based on current mechanism.
1. Stopped or terminated after finishing decommission
2. Under decommission process, then removed by driver with other reasons

### Why are the changes needed?
When executor decommission is enabled, there would be more stage failure caused by FetchFailed from decommissioned executor, further causing whole job's failure. One reason is decommissioning executor won't wait all FetchData requests to be finished, it will self-exit when no running tasks and migration finished. It would be better not to count such failure in `spark.stage.maxConsecutiveAttempts`. AWS EMR already supported this. Please refer https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html

### Does this PR introduce _any_ user-facing change?
Yes

### How was this patch tested?
Added test in `DAGSchedulerSuite`

Closes #37924 from warrenzhu25/deco-ignore.

Authored-by: Warren Zhu <warren.zhu25@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala', 'core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala']","Stage fetch failures caused by decommissioned executors lead to unnecessary stage failures, which can result in the job's failure when executor decommission is enabled."
07704c971cbc92bff15e15f8c42fab9afaab3ef7,1531429729,"[SPARK-23007][SQL][TEST] Add read schema suite for file-based data sources

## What changes were proposed in this pull request?

The reader schema is said to be evolved (or projected) when it changed after the data is written. The followings are already supported in file-based data sources. Note that partition columns are not maintained in files. In this PR, `column` means `non-partition column`.

   1. Add a column
   2. Hide a column
   3. Change a column position
   4. Change a column type (upcast)

This issue aims to guarantee users a backward-compatible read-schema test coverage on file-based data sources and to prevent future regressions by *adding read schema tests explicitly*.

Here, we consider safe changes without data loss. For example, data type change should be from small types to larger types like `int`-to-`long`, not vice versa.

As of today, in the master branch, file-based data sources have the following coverage.

File Format | Coverage  | Note
----------- | ---------- | ------------------------------------------------
TEXT          | N/A            | Schema consists of a single string column.
CSV            | 1, 2, 4        |
JSON          | 1, 2, 3, 4    |
ORC            | 1, 2, 3, 4    | Native vectorized ORC reader has the widest coverage among ORC formats.
PARQUET   | 1, 2, 3        |

## How was this patch tested?

Pass the Jenkins with newly added test suites.

Author: Dongjoon Hyun <dongjoon@apache.org>

Closes #20208 from dongjoon-hyun/SPARK-SCHEMA-EVOLUTION.
","['sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/ReadSchemaSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/ReadSchemaTest.scala']","Lack of comprehensive tests to cover read-schema evolution scenarios without data loss for file-based data sources, such as adding, hiding, changing column position and type."
de360e96d776f51c0fd7c31dcec142feabf3d543,1570540270,"[SPARK-29336][SQL] Fix the implementation of QuantileSummaries.merge (guarantee that the relativeError will be respected)

### What changes were proposed in this pull request?

Reimplement `org.apache.spark.sql.catalyst.util.QuantileSummaries#merge` and add a test-case showing the previous bug.

### Why are the changes needed?

The original Greenwald-Khanna paper, from which the algorithm behind `approxQuantile` was taken, does not cover how to merge the result of multiple parallel QuantileSummaries. The current implementation violates some invariants and therefore the effective error can be larger than the specified.

### Does this PR introduce any user-facing change?

Yes, for same cases, the results from `approxQuantile` (`percentile_approx` in SQL) will now be within the expected error margin. For example:

```scala
var values = (1 to 100).toArray
val all_quantiles = values.indices.map(i => (i+1).toDouble / values.length).toArray
for (n <- 0 until 5) {
  var df = spark.sparkContext.makeRDD(values).toDF(""value"").repartition(5)
  val all_answers = df.stat.approxQuantile(""value"", all_quantiles, 0.1)
  val all_answered_ranks = all_answers.map(ans => values.indexOf(ans)).toArray
  val error = all_answered_ranks.zipWithIndex.map({ case (answer, expected) => Math.abs(expected - answer) }).toArray
  val max_error = error.max
  print(max_error + ""\n"")
}
```

In the current build it returns:

```
16
12
10
11
17
```

I couldn't run the code with this patch applied to double check the implementation. Can someone please confirm it now outputs at most `10`, please?

### How was this patch tested?

A new unit test was added to uncover the previous bug.

Closes #26029 from sitegui/SPARK-29336.

Authored-by: Guilherme <sitegui@sitegui.com.br>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/QuantileSummaries.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/QuantileSummariesSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/ApproximatePercentileQuerySuite.scala']",The function approxQuantile in org.apache.spark.sql.catalyst.util.QuantileSummaries#merge violates invariants and the effective error is often larger than expected.
cb501771fad350a3f165354629f66c4a5e61557c,1570509222,"[SPARK-25668][SQL][TESTS] Refactor TPCDSQueryBenchmark to use main method

### What changes were proposed in this pull request?

This PR aims the followings.
- Refactor `TPCDSQueryBenchmark` to use main method to improve the usability.
- Reduce the number of iteration from 5 to 2 because it takes too long. (2 is okay because we have `Stdev` field now. If there is an irregular run, we can notice easily with that).
- Generate one result file for TPCDS scale factor 1. (Note that this test suite can be used for the other scale factors, too.)
  - AWS EC2 `r3.xlarge` with `ami-06f2f779464715dc5 (ubuntu-bionic-18.04-amd64-server-20190722.1)` is used.

This PR adds a JDK8 result based on the TPCDS ScaleFactor 1G data generated by the following.
```
# `spark-tpcds-datagen` needs this. (JDK8)
$ git clone https://github.com/apache/spark.git -b branch-2.4 --depth 1 spark-2.4
$ export SPARK_HOME=$PWD
$ ./build/mvn clean package -DskipTests

# Generate data. (JDK8)
$ git clone gitgithub.com:maropu/spark-tpcds-datagen.git
$ cd spark-tpcds-datagen/
$ build/mvn clean package
$ mkdir -p /data/tpcds
$ ./bin/dsdgen --output-location /data/tpcds/s1  // This need `Spark 2.4`
```

### Why are the changes needed?

Although the generated TPCDS data is random, we can keep the record.

### Does this PR introduce any user-facing change?

No. (This is dev-only test benchmark).

### How was this patch tested?

Manually run the benchmark. Please note that you need to have TPCDS data.
```
SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt ""sql/test:runMain org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark --data-location /data/tpcds/s1""
```

Closes #26049 from dongjoon-hyun/SPARK-25668.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/TPCDSQueryBenchmark.scala'],`TPCDSQueryBenchmark` lacks usability and requires too much time due to multiple iterations. There is also no result file for TPCDS scale factor 1.
8f6e439068281633acefb895f8c4bd9203868c24,1639028207,"[SPARK-37586][SQL] Add the `mode` and `padding` args to `aes_encrypt()`/`aes_decrypt()`

### What changes were proposed in this pull request?
In the PR, I propose to add new optional arguments to the `aes_encrypt()` and `aes_decrypt()` functions with default values:
1. `mode` - specifies which block cipher mode should be used to encrypt/decrypt messages. Only one valid value is `ECB` at the moment.
2. `padding` - specifies how to pad messages whose length is not a multiple of the block size. Currently, only `PKCS` is supported.

In this way, when an user doesn't pass `mode`/`padding` to the functions, the functions apply AES encryption/decryption in the `ECB` mode with the `PKCS5Padding` padding.

### Why are the changes needed?
1. For now, `aes_encrypt()` and `aes_decrypt()` rely on the jvm's configuration regarding which cipher mode to support, this is problematic as it is not fixed across versions and systems. By using default constants for new arguments, we can guarantee the same behaviour across all supported platforms.
2. We can consider new arguments as new point of extension in the current implementation of AES algorithm in Spark SQL. In the future in OSS or in a private Spark fork, devs can implement other modes (and paddings) like GCM. Other systems have already supported different AES modes, see:
   1. Snowflake: https://docs.snowflake.com/en/sql-reference/functions/encrypt.html
   2. BigQuery: https://cloud.google.com/bigquery/docs/reference/standard-sql/aead-encryption-concepts#block_cipher_modes
   3. MySQL: https://dev.mysql.com/doc/refman/8.0/en/encryption-functions.html#function_aes-encrypt
   4. Hive: https://cwiki.apache.org/confluence/display/hive/languagemanual+udf
   5. PostgreSQL: https://www.postgresql.org/docs/12/pgcrypto.html#id-1.11.7.34.8

### Does this PR introduce _any_ user-facing change?
No. This PR just extends existing APIs.

### How was this patch tested?
By running new checks:
```
$ build/sbt ""test:testOnly org.apache.spark.sql.DataFrameFunctionsSuite""
$ build/sbt ""sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite""
$ build/sbt ""sql/testOnly *ExpressionsSchemaSuite""
```

Closes #34837 from MaxGekk/aes-gsm-mode.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>
","['sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/misc.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala']","`aes_encrypt()` and `aes_decrypt()` functions in Spark SQL have inconsistent behavior across different systems due to reliance on JVM's configuration for the cipher mode. Additionally, they lack provision to specify padding for messages with length not multiple of block size."
a9fbd310300e57ed58818d7347f3c3172701c491,1576373946,"[SPARK-30240][CORE] Support HTTP redirects directly to a proxy server

### What changes were proposed in this pull request?

The PR adds a new config option to configure an address for the
proxy server, and a new handler that intercepts redirects and replaces
the URL with one pointing at the proxy server. This is needed on top
of the ""proxy base path"" support because redirects use full URLs, not
just absolute paths from the server's root.

### Why are the changes needed?

Spark's web UI has support for generating links to paths with a
prefix, to support a proxy server, but those do not apply when
the UI is responding with redirects. In that case, Spark is sending
its own URL back to the client, and if it's behind a dumb proxy
server that doesn't do rewriting (like when using stunnel for HTTPS
support) then the client will see the wrong URL and may fail.

### Does this PR introduce any user-facing change?

Yes. It's a new UI option.

### How was this patch tested?

Tested with added unit test, with Spark behind stunnel, and in a
more complicated app using a different HTTPS proxy.

Closes #26873 from vanzin/SPARK-30240.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['core/src/main/scala/org/apache/spark/TestUtils.scala', 'core/src/main/scala/org/apache/spark/internal/config/UI.scala', 'core/src/main/scala/org/apache/spark/ui/JettyUtils.scala', 'core/src/test/scala/org/apache/spark/ui/UISuite.scala']","HTTP redirects from Spark's web UI fail when Spark is behind a dumb proxy server, resulting in the client seeing the wrong URL which may cause failure."
0ae3ff60c4679495d376d2418450cfe93cac5590,1581959754,"[SPARK-30806][SQL] Evaluate once per group in UnboundedWindowFunctionFrame

### What changes were proposed in this pull request?
We only need to do aggregate evaluation once per group in `UnboundedWindowFunctionFrame`

### Why are the changes needed?
Currently, in `UnboundedWindowFunctionFrame.write`，it re-evaluate the processor for each row in a group, which is not necessary in fact which I'll address later. It hurts performance when the evaluation is time-consuming (for example, Percentile's eval need to sort its buffer and do some calculation). In our production, there is a percentile with window operation sql,  it costs more than 10 hours in SparkSQL while 10min in Hive.

In fact, `UnboundedWindowFunctionFrame` can be treated as `SlidingWindowFunctionFrame` with `lbound = UnboundedPreceding` and `ubound = UnboundedFollowing`, just as its comments. In that case, `SlidingWindowFunctionFrame` also only do evaluation once for each group.

The performance issue can be reproduced by running the follow scripts in local spark-shell
```
spark.range(100*100).map(i => (i, ""India"")).toDF(""uv"", ""country"").createOrReplaceTempView(""test"")
sql(""select uv, country, percentile(uv, 0.95) over (partition by country) as ptc95 from test"").collect.foreach(println)
```
Before this patch, the sql costs **128048 ms**.
With this patch,  the sql costs **3485 ms**.

If we increase the data size to 1000*1000 for example, then spark cannot even produce result without this patch(I'v waited for several hours).

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
Existing UT

Closes #27558 from WangGuangxin/windows.

Authored-by: wangguangxin.cn <wangguangxin.cn@gmail.com>
Signed-off-by: herman <herman@databricks.com>
",['sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala'],"Excessive time spent re-evaluating aggregate functions for each row in `UnboundedWindowFunctionFrame` groups, particularly noticeable with time-consuming evaluations like Percentile, causing significant performance issues."
647422685292cd1a46766afa9b07b6fcfc181bbd,1625470550,"[SPARK-35982][SQL] Allow from_json/to_json for map types where value types are year-month intervals

### What changes were proposed in this pull request?

This PR fixes two issues. One is that `to_json` doesn't support `map` types where value types are `year-month` interval types like:
```
spark-sql> select to_json(map('a', interval '1-2' year to  month));
21/07/02 11:38:15 ERROR SparkSQLDriver: Failed in [select to_json(map('a', interval '1-2' year to  month))]
java.lang.RuntimeException: Failed to convert value 14 (class of class java.lang.Integer) with the type of YearMonthIntervalType(0,1) to JSON.
```
The other issue is that even if the issue of `to_json` is resolved, `from_json` doesn't support to convert `year-month` interval string to JSON. So the result of following query will be `null`.
```
spark-sql> select from_json(to_json(map('a', interval '1-2' year to month)), 'a interval year to month');
{""a"":null}
```

### Why are the changes needed?

There should be no reason why year-month intervals cannot used as map value types.
`CalendarIntervalTypes` can do it.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New tests.

Closes #33181 from sarutak/map-json-yminterval.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala', 'sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala']","The `to_json` function fails when used on `map` types where value types are `year-month` intervals. Similarly, `from_json` does not support converting `year-month` interval strings to JSON."
50d9a56f824ae51d10543f4573753ff60dc9053b,1693245220,"[SPARK-44832][CONNECT] Make transitive dependencies work properly for Scala Client

### What changes were proposed in this pull request?
This PR cleans up the Maven build for the Spark Connect Client and Spark Connect Common. The most important change is that we move `sql-api` from a `provided` to `compile` dependency. The net effect of this is that when a user takes a dependency on the client, all of its required (transitive) dependencies are automatically added.

Please note that this does not address concerns around creating an überjar and shading. That is for a different day :)

### Why are the changes needed?
When you take a dependency on the connect scala client you need to manually add the `sql-api` module as a dependency. This is rather poor UX.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Manually running maven, checking dependency tree, ...

Closes #42518 from hvanhovell/SPARK-44832.

Authored-by: Herman van Hovell <herman@databricks.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
",['connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/CheckConnectJvmClientCompatibility.scala'],"When adding a dependency on the Spark Connect Scala client, the `sql-api` module needs to be manually added which leads to suboptimal user experience."
2c6290205928521e8d7404bb9a9cbccff0d35674,1665041347,"[SPARK-40311][SQL][PYTHON] Add `withColumnsRenamed` to scala and pyspark API

### What changes were proposed in this pull request?
This change adds an ability for code to rename multiple columns in a single call.
**Scala:**
```scala
withColumnsRenamed(colsMap: Map[String, String]): DataFrame
```
**Java:**
```java
withColumnsRenamed(colsMap: java.util.Map[String, String]): DataFrame
```
**Python:**
```python
withColumnsRenamed(self, *colsMap: Dict[str, Column]) -> ""DataFrame""
```

### Why are the changes needed?
We have seen that catalyst optimiser struggles with bigger plans. The larger contribution to these plans in our setup comes from `withColumnRenamed`, `drop` and `withColumn` being called in for loop by unknowing users. `master` branch of spark already has a version for handling `withColumns` and `drop` for multiple columns. The missing bit of the puzzle is `withColumnRenamed`.

With large amount of columns, either JVM gets killed or StackOverflowError occurs. I am skipping those for the following benchmark and focus on number of columns which work in both old and new implementation. Following example shows the performance impact with 100 columns.:
**Old fashioned with 100 columns**
```python
import datetime
import numpy as np
import pandas as pd

num_rows = 2
num_columns = 100
data = np.zeros((num_rows, num_columns))
columns = map(str, range(num_columns))
raw = spark.createDataFrame(pd.DataFrame(data, columns=columns))

a = datetime.datetime.now()

for col in raw.columns:
    raw = raw.withColumnRenamed(col, f""prefix_{col}"")

b = datetime.datetime.now()
for col in raw.columns:
    raw = raw.withColumnRenamed(col, f""prefix_{col}"")

c = datetime.datetime.now()
for col in raw.columns:
    raw = raw.withColumnRenamed(col, f""prefix_{col}"")

d = datetime.datetime.now()
for col in raw.columns:
    raw = raw.withColumnRenamed(col, f""prefix_{col}"")

e = datetime.datetime.now()
for col in raw.columns:
    raw = raw.withColumnRenamed(col, f""prefix_{col}"")

f = datetime.datetime.now()
for col in raw.columns:
    raw = raw.withColumnRenamed(col, f""prefix_{col}"")

g = datetime.datetime.now()
g-a
datetime.timedelta(seconds=12, microseconds=480021)
```

**New implementation with 100 columns**
```python
import datetime
import numpy as np
import pandas as pd

num_rows = 2
num_columns = 100
data = np.zeros((num_rows, num_columns))
columns = map(str, range(num_columns))
raw = spark.createDataFrame(pd.DataFrame(data, columns=columns))

a = datetime.datetime.now()
raw = raw.withColumnsRenamed({col: f""prefix_{col}"" for col in raw.columns})
b = datetime.datetime.now()
raw = raw.withColumnsRenamed({col: f""prefix_{col}"" for col in raw.columns})
c = datetime.datetime.now()
raw = raw.withColumnsRenamed({col: f""prefix_{col}"" for col in raw.columns})
d = datetime.datetime.now()
raw = raw.withColumnsRenamed({col: f""prefix_{col}"" for col in raw.columns})
e = datetime.datetime.now()
raw = raw.withColumnsRenamed({col: f""prefix_{col}"" for col in raw.columns})
f = datetime.datetime.now()
raw = raw.withColumnsRenamed({col: f""prefix_{col}"" for col in raw.columns})
g = datetime.datetime.now()
g-a
datetime.timedelta(microseconds=210400)
```

### Does this PR introduce _any_ user-facing change?
Yes, adds a method to efficiently rename columns in a single batch.

### How was this patch tested?
Added unit tests

Closes #37761 from santosh-d3vpl3x/master.

Lead-authored-by: santosh <3813695+santosh-d3vpl3x@users.noreply.github.com>
Co-authored-by: Santosh Pingale <3813695+santosh-d3vpl3x@users.noreply.github.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['python/pyspark/sql/dataframe.py', 'python/pyspark/sql/tests/test_dataframe.py', 'sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala']",Renaming multiple DataFrame columns individually using `withColumnRenamed` results in large plans that causes JVM to crash or StackOverflowError to occur.
b11e42663be680a0357f2bf7bd8b16afe313eb5e,1587415104,"[SPARK-31381][SPARK-29245][SQL] Upgrade built-in Hive 2.3.6 to 2.3.7

### What changes were proposed in this pull request?

**Hive 2.3.7** fixed these issues:
- HIVE-21508: ClassCastException when initializing HiveMetaStoreClient on JDK10 or newer
- HIVE-21980:Parsing time can be high in case of deeply nested subqueries
- HIVE-22249: Support Parquet through HCatalog

### Why are the changes needed?
Fix CCE during creating HiveMetaStoreClient in JDK11 environment: [SPARK-29245](https://issues.apache.org/jira/browse/SPARK-29245).

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?

- [x] Test Jenkins with Hadoop 2.7 (https://github.com/apache/spark/pull/28148#issuecomment-616757840)
- [x] Test Jenkins with Hadoop 3.2 on JDK11 (https://github.com/apache/spark/pull/28148#issuecomment-616294353)
- [x] Manual test with remote hive metastore.

Hive side:

```
export JAVA_HOME=/usr/lib/jdk1.8.0_221
export PATH=$JAVA_HOME/bin:$PATH
cd /usr/lib/hive-2.3.6 # Start Hive metastore with Hive 2.3.6
bin/schematool -dbType derby -initSchema --verbose
bin/hive --service metastore
```

Spark side:

```
export JAVA_HOME=/usr/lib/jdk-11.0.3
export PATH=$JAVA_HOME/bin:$PATH
build/sbt clean package -Phive -Phadoop-3.2 -Phive-thriftserver
export SPARK_PREPEND_CLASSES=true
bin/spark-sql --conf spark.hadoop.hive.metastore.uris=thrift://localhost:9083
```

Closes #28148 from wangyum/SPARK-31381.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/package.scala']","Issues with Hive 2.3.6 on JDK10 or newer include ClassCastException when initializing HiveMetaStoreClient, slow parsing time for deeply nested subqueries and lack of support for Parquet through HCatalog."
d8c0599e542976ef70b60bc673e7c9732fce49e5,1581988946,"[SPARK-30791][SQL][PYTHON] Add 'sameSemantics' and 'sementicHash' methods in Dataset

### What changes were proposed in this pull request?
This PR added two DeveloperApis to the Dataset[T] class. Both methods are just exposing lower-level methods to the Dataset[T] class.

### Why are the changes needed?
They are useful for checking whether two dataframes are the same when implementing dataframe caching in python, and also get a unique ID. It's easier to use if we wrap the lower-level APIs.

### Does this PR introduce any user-facing change?
```
scala> val df1 = Seq((1,2),(4,5)).toDF(""col1"", ""col2"")
df1: org.apache.spark.sql.DataFrame = [col1: int, col2: int]

scala> val df2 = Seq((1,2),(4,5)).toDF(""col1"", ""col2"")
df2: org.apache.spark.sql.DataFrame = [col1: int, col2: int]

scala> val df3 = Seq((0,2),(4,5)).toDF(""col1"", ""col2"")
df3: org.apache.spark.sql.DataFrame = [col1: int, col2: int]

scala> val df4 = Seq((0,2),(4,5)).toDF(""col0"", ""col2"")
df4: org.apache.spark.sql.DataFrame = [col0: int, col2: int]

scala> df1.semanticHash
res0: Int = 594427822

scala> df2.semanticHash
res1: Int = 594427822

scala> df1.sameSemantics(df2)
res2: Boolean = true

scala> df1.sameSemantics(df3)
res3: Boolean = false

scala> df3.semanticHash
res4: Int = -1592702048

scala> df4.semanticHash
res5: Int = -1592702048

scala> df4.sameSemantics(df3)
res6: Boolean = true
```

### How was this patch tested?
Unit test in scala and doctest in python.

Note: comments are copied from the corresponding lower-level APIs.
Note: There are some issues to be fixed that would improve the hash collision rate: https://github.com/apache/spark/pull/27565#discussion_r379881028

Closes #27565 from liangz1/df-same-result.

Authored-by: Liang Zhang <liang.zhang@databricks.com>
Signed-off-by: WeichenXu <weichen.xu@databricks.com>
","['python/pyspark/sql/dataframe.py', 'python/pyspark/sql/tests/test_dataframe.py', 'sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala']",No method available in Dataset[T] to check if two dataframes are the same in Python when implementing dataframe caching or to get a unique ID.
653fe02415a537299e15f92b56045569864b6183,1523458165,"[SPARK-6951][CORE] Speed up parsing of event logs during listing.

This change introduces two optimizations to help speed up generation
of listing data when parsing events logs.

The first one allows the parser to be stopped when enough data to
create the listing entry has been read. This is currently the start
event plus environment info, to capture UI ACLs. If the end event is
needed, the code will skip to the end of the log to try to find that
information, instead of parsing the whole log file.

Unfortunately this works better with uncompressed logs. Skipping bytes
on compressed logs only saves the work of parsing lines and some events,
so not a lot of gains are observed.

The second optimization deals with in-progress logs. It works in two
ways: first, it completely avoids parsing the rest of the log for
these apps when enough listing data is read. This, unlike the above,
also speeds things up for compressed logs, since only the very beginning
of the log has to be read.

On top of that, the code that decides whether to re-parse logs to get
updated listing data will ignore in-progress applications until they've
completed.

Both optimizations can be disabled but are enabled by default.

I tested this on some fake event logs to see the effect. I created
500 logs of about 60M each (so ~30G uncompressed; each log was 1.7M
when compressed with zstd). Below, C = completed, IP = in-progress,
the size means the amount of data re-parsed at the end of logs
when necessary.

```
            none/C   none/IP   zstd/C   zstd/IP
On / 16k      2s       2s       22s       2s
On / 1m       3s       2s       24s       2s
Off          1.1m     1.1m      26s      24s
```

This was with 4 threads on a single local SSD. As expected from the
previous explanations, there are considerable gains for in-progress
logs, and for uncompressed logs, but not so much when looking at the
full compressed log.

As a side note, I removed the custom code to get the scan time by
creating a file on HDFS; since file mod times are not used to detect
changed logs anymore, local time is enough for the current use of
the SHS.

Author: Marcelo Vanzin <vanzin@cloudera.com>

Closes #20952 from vanzin/SPARK-6951.
","['core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala', 'core/src/main/scala/org/apache/spark/deploy/history/config.scala', 'core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala', 'core/src/main/scala/org/apache/spark/util/ListenerBus.scala', 'core/src/test/scala/org/apache/spark/deploy/history/FsHistoryProviderSuite.scala']","Parsing event logs for listing data is highly time-consuming, especially for in-progress logs and those that are compressed. The process currently parses the entire log file, leading to significant performance issues."
dee596e3efe54651aa1e7c467b4f987f662e60b0,1610977959,"[SPARK-34027][SQL] Refresh cache in `ALTER TABLE .. RECOVER PARTITIONS`

### What changes were proposed in this pull request?
Invoke `refreshTable()` from `CatalogImpl` which refreshes the cache in v1 `ALTER TABLE .. RECOVER PARTITIONS`.

### Why are the changes needed?
This fixes the issues portrayed by the example:
```sql
spark-sql> create table tbl (col int, part int) using parquet partitioned by (part);
spark-sql> insert into tbl partition (part=0) select 0;
spark-sql> cache table tbl;
spark-sql> select * from tbl;
0	0
spark-sql> show table extended like 'tbl' partition(part=0);
default	tbl	false	Partition Values: [part=0]
Location: file:/Users/maximgekk/proj/recover-partitions-refresh-cache/spark-warehouse/tbl/part=0
...
```
Create new partition by copying the existing one:
```
$ cp -r /Users/maximgekk/proj/recover-partitions-refresh-cache/spark-warehouse/tbl/part=0 /Users/maximgekk/proj/recover-partitions-refresh-cache/spark-warehouse/tbl/part=1
```
```sql
spark-sql> alter table tbl recover partitions;
spark-sql> select * from tbl;
0	0
```

The last query must return `0	1` since it has been recovered by `ALTER TABLE .. RECOVER PARTITIONS`.

### Does this PR introduce _any_ user-facing change?
Yes. After the changes for the example above:
```sql
...
spark-sql> alter table tbl recover partitions;
spark-sql> select * from tbl;
0	0
0	1
```

### How was this patch tested?
By running the affected test suite:
```
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *CachedTableSuite""
```

Closes #31066 from MaxGekk/recover-partitions-refresh-cache.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala', 'sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSchemaInferenceSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/PartitionedTablePerfStatsSuite.scala']","The 'ALTER TABLE .. RECOVER PARTITIONS' command in cached, partitioned parquet tables does not refresh the cache, causing the newly created partitions to be not visible in subsequent queries."
673c67046598d33b9ecf864024ca7a937c1998d6,1510659261,"[SPARK-17310][SQL] Add an option to disable record-level filter in Parquet-side

## What changes were proposed in this pull request?

There is a concern that Spark-side codegen row-by-row filtering might be faster than Parquet's one in general due to type-boxing and additional fuction calls which Spark's one tries to avoid.

So, this PR adds an option to disable/enable record-by-record filtering in Parquet side.

It sets the default to `false` to take the advantage of the improvement.

This was also discussed in https://github.com/apache/spark/pull/14671.
## How was this patch tested?

Manually benchmarks were performed. I generated a billion (1,000,000,000) records and tested equality comparison concatenated with `OR`. This filter combinations were made from 5 to 30.

It seem indeed Spark-filtering is faster in the test case and the gap increased as the filter tree becomes larger.

The details are as below:

**Code**

``` scala
test(""Parquet-side filter vs Spark-side filter - record by record"") {
  withTempPath { path =>
    val N = 1000 * 1000 * 1000
    val df = spark.range(N).toDF(""a"")
    df.write.parquet(path.getAbsolutePath)

    val benchmark = new Benchmark(""Parquet-side vs Spark-side"", N)
    Seq(5, 10, 20, 30).foreach { num =>
      val filterExpr = (0 to num).map(i => s""a = $i"").mkString("" OR "")

      benchmark.addCase(s""Parquet-side filter - number of filters [$num]"", 3) { _ =>
        withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> false.toString,
          SQLConf.PARQUET_RECORD_FILTER_ENABLED.key -> true.toString) {

          // We should strip Spark-side filter to compare correctly.
          stripSparkFilter(
            spark.read.parquet(path.getAbsolutePath).filter(filterExpr)).count()
        }
      }

      benchmark.addCase(s""Spark-side filter - number of filters [$num]"", 3) { _ =>
        withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> false.toString,
          SQLConf.PARQUET_RECORD_FILTER_ENABLED.key -> false.toString) {

          spark.read.parquet(path.getAbsolutePath).filter(filterExpr).count()
        }
      }
    }

    benchmark.run()
  }
}
```

**Result**

```
Parquet-side vs Spark-side:              Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------
Parquet-side filter - number of filters [5]      4268 / 4367        234.3           4.3       0.8X
Spark-side filter - number of filters [5]      3709 / 3741        269.6           3.7       0.9X
Parquet-side filter - number of filters [10]      5673 / 5727        176.3           5.7       0.6X
Spark-side filter - number of filters [10]      3588 / 3632        278.7           3.6       0.9X
Parquet-side filter - number of filters [20]      8024 / 8440        124.6           8.0       0.4X
Spark-side filter - number of filters [20]      3912 / 3946        255.6           3.9       0.8X
Parquet-side filter - number of filters [30]    11936 / 12041         83.8          11.9       0.3X
Spark-side filter - number of filters [30]      3929 / 3978        254.5           3.9       0.8X
```

Author: hyukjinkwon <gurwls223@gmail.com>

Closes #15049 from HyukjinKwon/SPARK-17310.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala']","In Parquet-side filtering, potential performance degradation occurs due to type-boxing and extra function calls, especially noticeable with larger filter trees when compared with Spark-side row-by-row filtering."
8b2a2d1b2f08791fd8cbd9f6c6ec341e17733f7a,1671092037,"[SPARK-41525][K8S] Improve `onNewSnapshots` to use unique lists of known executor IDs and PVC names

### What changes were proposed in this pull request?

This PR improve `ExecutorPodsAllocator.onNewSnapshots` by removing duplications at `k8sKnownExecIds` and `k8sKnownPVCNames`. In the large cluster, this causes inefficiency.

### Why are the changes needed?

The existing variables have lots of duplications because `snapshots` is `Seq[ExecutorPodsSnapshot]`.
```
val k8sKnownExecIds = snapshots.flatMap(_.executorPods.keys)
```

For example, if we print out the values, it looks like the following.
```
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 3
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 3
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 3
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 3
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 1
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 2
22/12/15 07:09:37 INFO ExecutorPodsAllocator: 3
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Manual review because this is an improvement on the local variable computation.

Closes #39070 from dongjoon-hyun/SPARK-41525.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala'],ExecutorPodsAllocator.onNewSnapshots is inefficient in large clusters due to duplication in 'k8sKnownExecIds' and 'k8sKnownPVCNames'.
32d4a2b06220861efda1058b26d9a2ed3a1b2c74,1608797428,"[SPARK-33861][SQL] Simplify conditional in predicate

### What changes were proposed in this pull request?

This pr simplify conditional in predicate, after this change we can push down the filter to datasource:

Expression | After simplify
-- | --
IF(cond, trueVal, false)                   | AND(cond, trueVal)
IF(cond, trueVal, true)                    | OR(NOT(cond), trueVal)
IF(cond, false, falseVal)                  | AND(NOT(cond), elseVal)
IF(cond, true, falseVal)                   | OR(cond, elseVal)
CASE WHEN cond THEN trueVal ELSE false END | AND(cond, trueVal)
CASE WHEN cond THEN trueVal END            | AND(cond, trueVal)
CASE WHEN cond THEN trueVal ELSE null END  | AND(cond, trueVal)
CASE WHEN cond THEN trueVal ELSE true END  | OR(NOT(cond), trueVal)
CASE WHEN cond THEN false ELSE elseVal END | AND(NOT(cond), elseVal)
CASE WHEN cond THEN false END              | false
CASE WHEN cond THEN true ELSE elseVal END  | OR(cond, elseVal)
CASE WHEN cond THEN true END               | cond

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #30865 from wangyum/SPARK-33861.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala']","Difficulty in pushing down filters to datasource due to complex conditionals in predicates, negatively affecting query performance."
48b1a283a2eba9f70149d5980d074fad2743c4ff,1695183284,"[SPARK-44622][SQL][CONNECT] Implement FetchErrorDetails RPC

### What changes were proposed in this pull request?

- Introduced the FetchErrorDetails RPC to retrieve comprehensive error details. FetchErrorDetails is used for enriching the error by issuing a separate RPC call based on the `errorId` field in the ErrorInfo.
- Introduced error enrichment that utilizes an additional RPC to fetch untruncated exception messages and server-side stack traces. This enrichment can be enabled or disabled using the flag `spark.sql.connect.enrichError.enabled`, and it's true by default.
- Implemented setting server-side stack traces for exceptions on the client side via FetchErrorDetails RPC for debugging. The feature is enabled or disabled using the flag `spark.sql.connect.serverStacktrace.enabled` and it's true by default

### Why are the changes needed?

- Attaching full exception messages to the error details protobuf can quickly hit the 8K GRPC Netty header limit. Utilizing a separate RPC to fetch comprehensive error information is more dependable.
- Providing server-side stack traces aids in effectively diagnosing server-related issues.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

- `build/sbt ""connect/testOnly *FetchErrorDetailsHandlerSuite""`

### Was this patch authored or co-authored using generative AI tooling?

No

Closes #42377 from heyihong/SPARK-44622.

Authored-by: Yihong He <yihong.he@databricks.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
","['connector/connect/server/src/main/scala/org/apache/spark/sql/connect/config/Connect.scala', 'connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SessionHolder.scala', 'connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectFetchErrorDetailsHandler.scala', 'connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectService.scala', 'connector/connect/server/src/main/scala/org/apache/spark/sql/connect/utils/ErrorUtils.scala', 'connector/connect/server/src/test/scala/org/apache/spark/sql/connect/service/FetchErrorDetailsHandlerSuite.scala', 'python/pyspark/sql/connect/proto/base_pb2.py', 'python/pyspark/sql/connect/proto/base_pb2_grpc.py']","Errors are currently unable to convey comprehensive details due to hitting the 8K GRPC Netty header limit, inhibiting effective debugging of server-related issues."
e0ecb66f53058f999d0de6005d08b744b020fa7f,1595255151,"[SPARK-31869][SQL] BroadcastHashJoinExec can utilize the build side for its output partitioning

### What changes were proposed in this pull request?

Currently, the `BroadcastHashJoinExec`'s `outputPartitioning` only uses the streamed side's `outputPartitioning`. However, if the join type of `BroadcastHashJoinExec` is an inner-like join, the build side's info (the join keys) can be added to `BroadcastHashJoinExec`'s `outputPartitioning`.

 For example,
```Scala
spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", ""500"")
val t1 = (0 until 100).map(i => (i % 5, i % 13)).toDF(""i1"", ""j1"")
val t2 = (0 until 100).map(i => (i % 5, i % 13)).toDF(""i2"", ""j2"")
val t3 = (0 until 20).map(i => (i % 7, i % 11)).toDF(""i3"", ""j3"")
val t4 = (0 until 100).map(i => (i % 5, i % 13)).toDF(""i4"", ""j4"")

// join1 is a sort merge join.
val join1 = t1.join(t2, t1(""i1"") === t2(""i2""))

// join2 is a broadcast join where t3 is broadcasted.
val join2 = join1.join(t3, join1(""i1"") === t3(""i3""))

// Join on the column from the broadcasted side (i3).
val join3 = join2.join(t4, join2(""i3"") === t4(""i4""))

join3.explain
```
You see that `Exchange hashpartitioning(i2#103, 200)` is introduced because there is no output partitioning info from the build side.
```
== Physical Plan ==
*(6) SortMergeJoin [i3#29], [i4#40], Inner
:- *(4) Sort [i3#29 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(i3#29, 200), true, [id=#55]
:     +- *(3) BroadcastHashJoin [i1#7], [i3#29], Inner, BuildRight
:        :- *(3) SortMergeJoin [i1#7], [i2#18], Inner
:        :  :- *(1) Sort [i1#7 ASC NULLS FIRST], false, 0
:        :  :  +- Exchange hashpartitioning(i1#7, 200), true, [id=#28]
:        :  :     +- LocalTableScan [i1#7, j1#8]
:        :  +- *(2) Sort [i2#18 ASC NULLS FIRST], false, 0
:        :     +- Exchange hashpartitioning(i2#18, 200), true, [id=#29]
:        :        +- LocalTableScan [i2#18, j2#19]
:        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint))), [id=#34]
:           +- LocalTableScan [i3#29, j3#30]
+- *(5) Sort [i4#40 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(i4#40, 200), true, [id=#39]
      +- LocalTableScan [i4#40, j4#41]
```
This PR proposes to introduce output partitioning for the build side for `BroadcastHashJoinExec` if the streamed side has a `HashPartitioning` or a collection of `HashPartitioning`s.

There is a new internal config `spark.sql.execution.broadcastHashJoin.outputPartitioningExpandLimit`, which can limit the number of partitioning a `HashPartitioning` can expand to. It can be set to ""0"" to disable this feature.

### Why are the changes needed?

To remove unnecessary shuffle.

### Does this PR introduce _any_ user-facing change?

Yes, now the shuffle in the above example can be eliminated:
```
== Physical Plan ==
*(5) SortMergeJoin [i3#108], [i4#119], Inner
:- *(3) Sort [i3#108 ASC NULLS FIRST], false, 0
:  +- *(3) BroadcastHashJoin [i1#86], [i3#108], Inner, BuildRight
:     :- *(3) SortMergeJoin [i1#86], [i2#97], Inner
:     :  :- *(1) Sort [i1#86 ASC NULLS FIRST], false, 0
:     :  :  +- Exchange hashpartitioning(i1#86, 200), true, [id=#120]
:     :  :     +- LocalTableScan [i1#86, j1#87]
:     :  +- *(2) Sort [i2#97 ASC NULLS FIRST], false, 0
:     :     +- Exchange hashpartitioning(i2#97, 200), true, [id=#121]
:     :        +- LocalTableScan [i2#97, j2#98]
:     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint))), [id=#126]
:        +- LocalTableScan [i3#108, j3#109]
+- *(4) Sort [i4#119 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(i4#119, 200), true, [id=#130]
      +- LocalTableScan [i4#119, j4#120]
```

### How was this patch tested?

Added new tests.

Closes #28676 from imback82/broadcast_join_output.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala']","The output partitioning of `BroadcastHashJoinExec` only considers the streamed side's `outputPartitioning`, leading to unnecessary shuffle operations when executing inner-like joins."
8c51fd85d6e2e84795c03e9e3e1673f62243456a,1632830973,"[SPARK-36866][SQL] Pushdown filters with ANSI interval values to parquet

### What changes were proposed in this pull request?
In the PR, I propose to pushdown filters with ANSI intervals as filter values to the parquet datasource. After the changes, filter values are pushed down with the following values via Filter API:
- `java.time.Period` for year-month filters
- `java.time.Duration` for day-time filters.

Since at the Parquet filter level, we don't have info about Catalyst's types (`YearMonthIntervalType` and `DayTimeIntervalType`) but only the info about primitive parquet types `INT32` and `INT64`. As a consequence of that, Spark has to convert filters values ""dynamically"" to `Int`/`Long` while building Parquet filters.

### Why are the changes needed?
The PR https://github.com/apache/spark/pull/34057 supported ANSI intervals in the Parquet datasource as INT32 (year-month interval) and INT64 (day-time interval) but filters with such values are not pushed down. So, comparing to primitive types, ANSI intervals can suffer from worse performance in read. This PR aims to solve the issue, and achieve feature parity with other types.

### Does this PR introduce _any_ user-facing change?
No, the changes might influence on performance of the parquet datasource only.

### How was this patch tested?
By running new tests in `ParquetFilterSuite`:
```
$ build/sbt clean ""test:testOnly *ParquetV1FilterSuite""
$ build/sbt clean ""test:testOnly *ParquetV2FilterSuite""
```

Closes #34115 from MaxGekk/interval-parquet-filter-pushdown.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetTable.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala']","Filters with ANSI intervals as filter values are not being pushed down to the Parquet data source, resulting in potential performance issues during read operations."
23e9920b3910e4f05269853429c7f18888cdc7b5,1606122041,"[SPARK-33511][SQL] Respect case sensitivity while resolving V2 partition specs

### What changes were proposed in this pull request?
1. Pre-process partition specs in `ResolvePartitionSpec`, and convert partition names according to the partition schema and the SQL config `spark.sql.caseSensitive`. In the PR, I propose to invoke `normalizePartitionSpec` for that. The function is used in DSv1 commands, so, the behavior will be similar to DSv1.
2. Move `normalizePartitionSpec()` from `sql/core/.../datasources/PartitioningUtils` to `sql/catalyst/.../util/PartitioningUtils` to use it in Catalyst's rule `ResolvePartitionSpec`

### Why are the changes needed?
DSv1 commands like `ALTER TABLE .. ADD PARTITION` and `ALTER TABLE .. DROP PARTITION` respect the SQL config `spark.sql.caseSensitive` while resolving partition specs. For example:
```sql
spark-sql> CREATE TABLE tbl1 (id bigint, data string) USING parquet PARTITIONED BY (id);
spark-sql> ALTER TABLE tbl1 ADD PARTITION (ID=1);
spark-sql> SHOW PARTITIONS tbl1;
id=1
```
The same command fails on V2 Table catalog with error:
```
AnalysisException: Partition key ID not exists
```

### Does this PR introduce _any_ user-facing change?
Yes. After the changes, partition spec resolution works as for DSv1 (without the exception showed above).

### How was this patch tested?
By running `AlterTablePartitionV2SQLSuite`.

Closes #30454 from MaxGekk/partition-spec-case-sensitivity.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolvePartitionSpec.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/util/PartitioningUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTablePartitionV2SQLSuite.scala']","V2 Table catalog commands like `ALTER TABLE .. ADD PARTITION` and `ALTER TABLE .. DROP PARTITION` fail to respect the SQL config `spark.sql.caseSensitive` while resolving partition specs, resulting in AnalysisException errors."
b0f9978ec08caa9302c7340951b5c2979315ca13,1693537971,"[SPARK-45026][CONNECT] `spark.sql` should support datatypes not compatible with arrow

### What changes were proposed in this pull request?

Move the arrow batch creation to the `isCommand` branch

### Why are the changes needed?

https://github.com/apache/spark/pull/42736 and https://github.com/apache/spark/pull/42743 introduced the `CalendarIntervalType` in Spark Connect Python Client, however, there is a failure

```
spark.sql(""SELECT make_interval(100, 11, 1, 1, 12, 30, 01.001001)"")

...

pyspark.errors.exceptions.connect.UnsupportedOperationException: [UNSUPPORTED_DATATYPE] Unsupported data type ""INTERVAL"".
```

The root causes is that `handleSqlCommand` always create an arrow batch while `ArrowUtils` doesn't accept `CalendarIntervalType` now.

this PR mainly focus on enabling `schema` with datatypes not compatible with arrow.
In the future, we should make `ArrowUtils` accept `CalendarIntervalType` to make `collect/toPandas` works

### Does this PR introduce _any_ user-facing change?
yes

after this PR
```
In [1]: spark.sql(""SELECT make_interval(100, 11, 1, 1, 12, 30, 01.001001)"")
Out[1]: DataFrame[make_interval(100, 11, 1, 1, 12, 30, 1.001001): interval]

In [2]: spark.sql(""SELECT make_interval(100, 11, 1, 1, 12, 30, 01.001001)"").schema
Out[2]: StructType([StructField('make_interval(100, 11, 1, 1, 12, 30, 1.001001)', CalendarIntervalType(), True)])
```

### How was this patch tested?
enabled ut

### Was this patch authored or co-authored using generative AI tooling?
no

Closes #42754 from zhengruifeng/connect_sql_types.

Authored-by: Ruifeng Zheng <ruifengz@apache.org>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala', 'python/pyspark/sql/tests/connect/test_parity_types.py']","The `spark.sql` fails when dealing with `CalendarIntervalType` data type that is not compatible with arrow, resulting in `UnsupportedOperationException` errors."
98c0ca78610ccf62784081353584717c62285485,1545286664,"[SPARK-26308][SQL] Avoid cast of decimals for ScalaUDF

## What changes were proposed in this pull request?

Currently, when we infer the schema for scala/java decimals, we return as data type the `SYSTEM_DEFAULT` implementation, ie. the decimal type with precision 38 and scale 18. But this is not right, as we know nothing about the right precision and scale and these values can be not enough to store the data. This problem arises in particular with UDF, where we cast all the input of type `DecimalType` to a `DecimalType(38, 18)`: in case this is not enough, null is returned as input for the UDF.

The PR defines a custom handling for casting to the expected data types for ScalaUDF: the decimal precision and scale is picked from the input, so no casting to different and maybe wrong percision and scale happens.

## How was this patch tested?

added UTs

Closes #23308 from mgaido91/SPARK-26308.

Authored-by: Marco Gaido <marcogaido91@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala', 'sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala']","Inference of the schema for scala/java decimals defaults to `SYSTEM_DEFAULT` (precision 38, scale 18). This is causing an issue with UDFs where all inputs of `DecimalType` are cast to `DecimalType(38, 18)`, potentially leading to null inputs if the precision and scale are inadequate."
8b497046c647a21bbed1bdfbdcb176745a1d5cd5,1514565609,"[SPARK-20654][CORE] Add config to limit disk usage of the history server.

This change adds a new configuration option and support code that limits
how much disk space the SHS will use. The default value is pretty generous
so that applications will, hopefully, only rarely need to be replayed
because of their disk stored being evicted.

This works by keeping track of how much data each application is using.
Also, because it's not possible to know, before replaying, how much space
will be needed, it's possible that usage will exceed the configured limit
temporarily. The code uses the concept of a ""lease"" to try to limit how
much the SHS will exceed the limit in those cases.

Active UIs are also tracked, so they're never deleted. This works in
tandem with the existing option of how many active UIs are loaded; because
unused UIs will be unloaded, their disk stores will also become candidates
for deletion. If the data is not deleted, though, re-loading the UI is
pretty quick.

Author: Marcelo Vanzin <vanzin@cloudera.com>

Closes #20011 from vanzin/SPARK-20654.
","['core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala', 'core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala', 'core/src/main/scala/org/apache/spark/deploy/history/config.scala', 'core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala', 'core/src/test/scala/org/apache/spark/deploy/history/HistoryServerDiskManagerSuite.scala']","History server may exceed configured disk space limit, causing potential application replay due to the eviction of their stored disk data. Inability to foresee space needed prior to replaying data exacerbates issue."
9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e,1577763923,"[SPARK-30379][CORE] Avoid OOM when using collection accumulator

### What changes were proposed in this pull request?

This patch proposes to only convert first few elements of collection accumulators in `LiveEntityHelpers.newAccumulatorInfos`.

### Why are the changes needed?

One Spark job on our cluster uses collection accumulator to collect something and has encountered an exception like:

```
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)
    at java.lang.StringBuilder.append(StringBuilder.java:136)
    at java.lang.StringBuilder.append(StringBuilder.java:131)
    at java.util.AbstractCollection.toString(AbstractCollection.java:462)
    at java.util.Collections$UnmodifiableCollection.toString(Collections.java:1035)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2$$anonfun$apply$3.apply(LiveEntity.scala:596)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2$$anonfun$apply$3.apply(LiveEntity.scala:596)
    at scala.Option.map(Option.scala:146)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2.apply(LiveEntity.scala:596)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2.apply(LiveEntity.scala:591)
```

`LiveEntityHelpers.newAccumulatorInfos` converts `AccumulableInfo`s to `v1.AccumulableInfo` by calling `toString` on accumulator's value. For collection accumulator, it might take much more memory when in string representation, for example, collection accumulator of long values, and cause OOM (in this job, the driver memory is 6g).

Looks like the results of `newAccumulatorInfos` are used in api and ui. For such usage, it also does not make sense to have very long string of complete collection accumulators.

### Does this PR introduce any user-facing change?

Yes. Collection accumulator now only shows first few elements in api and ui.

### How was this patch tested?

Unit test.

Manual test. Launched a Spark shell, ran:
```scala
val accum = sc.collectionAccumulator[Long](""Collection Accumulator Example"")
sc.range(0, 10000, 1, 1).foreach(x => accum.add(x))
accum.value
```

<img width=""2533"" alt=""Screen Shot 2019-12-30 at 2 03 43 PM"" src=""https://user-images.githubusercontent.com/68855/71602488-6eb2c400-2b0d-11ea-8725-dba36478198f.png"">

Closes #27038 from viirya/partial-collect-accu.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['core/src/main/scala/org/apache/spark/status/LiveEntity.scala', 'core/src/test/scala/org/apache/spark/status/LiveEntitySuite.scala']","Collection accumulator's conversion to string representation in `LiveEntityHelpers.newAccumulatorInfos` can cause OutOfMemoryError, especially with larger collections such as long value sets."
6d45e6ea1507943f6ee833af8ad7969294b0356a,1545068810,"[SPARK-26255][YARN] Apply user provided UI filters  to SQL tab in yarn mode

## What changes were proposed in this pull request?

User specified filters are not applied to SQL tab in yarn mode, as it is overridden by the yarn AmIp filter.
So we need to append user provided filters (spark.ui.filters) with yarn filter.

## How was this patch tested?

【Test step】：

1)  Launch spark sql with authentication filter as below:

2)  spark-sql --master yarn --conf spark.ui.filters=org.apache.hadoop.security.authentication.server.AuthenticationFilter --conf spark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params=""type=simple""

3)  Go to Yarn application list UI link

4) Launch the application master for the Spark-SQL app ID and access all the tabs by appending tab name.

5) It will display an error for all tabs including SQL tab.(before able to access SQL tab,as Authentication filter is not applied for SQL tab)

6) Also can be verified with info logs,that Authentication filter applied to SQL tab.(before it is not applied).

I have attached the behaviour below in following order..

1) Command used
2) Before fix (logs and UI)
3) After fix (logs and UI)

**1) COMMAND USED**:

launching spark-sql with authentication filter.

![image](https://user-images.githubusercontent.com/45845595/49947295-e7e97400-ff16-11e8-8c9a-10659487ddee.png)

**2) BEFORE FIX:**

**UI result:**
able to access SQL tab.

![image](https://user-images.githubusercontent.com/45845595/49948398-62b38e80-ff19-11e8-95dc-e74f9e3c2ba7.png)

 **logs**:
authentication filter not applied to SQL tab.

![image](https://user-images.githubusercontent.com/45845595/49947343-ff286180-ff16-11e8-9de0-3f8db140bc32.png)

**3) AFTER FIX:**

**UI result**:

Not able to access SQL tab.

![image](https://user-images.githubusercontent.com/45845595/49947360-0d767d80-ff17-11e8-9e9e-a95311949164.png)

**in logs**:

Both yarn filter and Authentication filter applied to SQL tab.

![image](https://user-images.githubusercontent.com/45845595/49947377-1a936c80-ff17-11e8-9f44-700eb3dc0ded.png)

Closes #23312 from chakravarthiT/SPARK-26255_ui.

Authored-by: chakravarthi <tcchakra@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
",['resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala'],"User-specified filters are not applied to the SQL tab in YARN mode, causing an override by the YARN AmIp filter and possible access errors."
c1a02e7e304934a7e671c0ae2f5a1fcedd6806c0,1681868526,"[SPARK-43098][SQL] Fix correctness COUNT bug when scalar subquery has group by clause

### What changes were proposed in this pull request?
Fix a correctness bug for scalar subqueries with COUNT and a GROUP BY clause, for example:
```
create view t1(c1, c2) as values (0, 1), (1, 2);
create view t2(c1, c2) as values (0, 2), (0, 3);

select c1, c2, (select count(*) from t2 where t1.c1 = t2.c1 group by c1) from t1;

-- Correct answer: [(0, 1, 2), (1, 2, null)]
+---+---+------------------+
|c1 |c2 |scalarsubquery(c1)|
+---+---+------------------+
|0  |1  |2                 |
|1  |2  |0                 |
+---+---+------------------+
```

This is due to a bug in our ""COUNT bug"" handling for scalar subqueries. For a subquery with COUNT aggregate but no GROUP BY clause, 0 is the correct output on empty inputs, and we use the COUNT bug handling to construct the plan that  yields 0 when there were no matched rows.

But when there is a GROUP BY clause then NULL is the correct output (i.e. there is no COUNT bug), but we still incorrectly use the COUNT bug handling and therefore incorrectly output 0. Instead, we need to only apply the COUNT bug handling when the scalar subquery had no GROUP BY clause.

To fix this, we need to track whether the scalar subquery has a GROUP BY, i.e. a non-empty groupingExpressions for the Aggregate node. This need to be checked before subquery decorrelation, because that adds the correlated outer refs to the group-by list so after that the group-by is always non-empty. We save it in a boolean in the ScalarSubquery node until later when we rewrite the subquery into a join in constructLeftJoins.

This is a long-standing bug. This bug affected both the current DecorrelateInnerQuery framework and the old code (with spark.sql.optimizer.decorrelateInnerQuery.enabled = false), and this PR fixes both.

### Why are the changes needed?
Fix a correctness bug.

### Does this PR introduce _any_ user-facing change?
Yes, fix incorrect query results.

### How was this patch tested?
Add SQL tests and unit tests. (Note that there were 2 existing unit tests for queries of this shape, which had the incorrect results as golden results.)

Closes #40811 from jchen5/count-bug.

Authored-by: Jack Chen <jack.chen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InsertAdaptiveSparkPlan.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala', 'sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-count-bug.sql', 'sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala']","Scalar subqueries with COUNT and GROUP BY clause are incorrectly returning a 0 instead of NULL when no rows are matched, leading to incorrect query results."
abc88deeed164bf48eff1d1504e141c1cf5afade,1569021990,"[SPARK-29063][SQL] Modify fillValue approach to support joined dataframe

### What changes were proposed in this pull request?
Modify the approach in `DataFrameNaFunctions.fillValue`, the new one uses `df.withColumns` which only address the columns need to be filled. After this change, there are no more ambiguous fileds detected for joined dataframe.

### Why are the changes needed?
Before this change, when you have a joined table that has the same field name from both original table, fillna will fail even if you specify a subset that does not include the 'ambiguous' fields.
```
scala> val df1 = Seq((""f1-1"", ""f2"", null), (""f1-2"", null, null), (""f1-3"", ""f2"", ""f3-1""), (""f1-4"", ""f2"", ""f3-1"")).toDF(""f1"", ""f2"", ""f3"")
scala> val df2 = Seq((""f1-1"", null, null), (""f1-2"", ""f2"", null), (""f1-3"", ""f2"", ""f4-1"")).toDF(""f1"", ""f2"", ""f4"")
scala> val df_join = df1.alias(""df1"").join(df2.alias(""df2""), Seq(""f1""), joinType=""left_outer"")
scala> df_join.na.fill("""", cols=Seq(""f4""))

org.apache.spark.sql.AnalysisException: Reference 'f2' is ambiguous, could be: df1.f2, df2.f2.;
```

### Does this PR introduce any user-facing change?
Yes, fillna operation will pass and give the right answer for a joined table.

### How was this patch tested?
Local test and newly added UT.

Closes #25768 from xuanyuanking/SPARK-29063.

Lead-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Co-authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameNaFunctionsSuite.scala']","'fillna' operation on a joined dataframe with identical field names in both original tables fails, even when 'ambiguous' fields aren't included in the subset to be filled."
23f580beae7c82c14b0c3a2e821a0382dde99223,1644415887,"[SPARK-37585][SQL] Update InputMetric in DataSourceRDD with TaskCompletionListener

### What changes were proposed in this pull request?
Before this change, Spark only updates `InputMetrics.bytesRead` in `DataSourceRDD` once every 1000 records, or at the end of the iterator (when `MetricsIterator.hasNext` returns false). So when the output is limited in a query but there is still data in the datasource, `InputMetrics.bytesRead` will not be updated at the end of the iterator, leading to incorrect metric results. This is more pronounced when the total number of records is less than 1000 (which will lead to `InputMetrics.bytesRead == 0`).

This change fixes this bug by adding a force metric update in `TaskCompletionListener`.

### Why are the changes needed?
This is to fix the bug that `InputMetrics.bytesRead` is not updated when there is still data in the datasource but the output is limited.

### Does this PR introduce _any_ user-facing change?
Users will see more accurate `InputMetrics.bytesRead` with this change.

### How was this patch tested?
Added a unit test.

Closes #35432 from bozhang2820/spark-37585.

Authored-by: Bo Zhang <bo.zhang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala', 'sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/DataSourceScanExecRedactionSuite.scala']","The `InputMetrics.bytesRead` in `DataSourceRDD` is not updated accurately when a query's output is limited, and data remains in the datasource, causing incorrect metric results. The issue is particularly noticeable when total record count is less than 1000, leading to `InputMetrics.bytesRead == 0`."
32cfd3e75a5ca65696fedfa4d49681e6fc3e698d,1530794935,"[SPARK-24361][SQL] Polish code block manipulation API

## What changes were proposed in this pull request?

Current code block manipulation API is immature and hacky. We need a formal API to manipulate code blocks.

The basic idea is making `JavaCode`  as `TreeNode`. So we can use familiar `transform` API to manipulate code blocks and expressions in code blocks.

For example, we can replace `SimpleExprValue` in a code block like this:

```scala
code.transformExprValues {
  case SimpleExprValue(""1 + 1"", _) => aliasedParam
}
```

The example use case is splitting code to methods.

For example, we have an `ExprCode` containing generated code. But it is too long and we need to split it as method. Because statement-based expressions can't be directly passed into. We need to transform them as variables first:

```scala

def getExprValues(block: Block): Set[ExprValue] = block match {
  case c: CodeBlock =>
    c.blockInputs.collect {
      case e: ExprValue => e
    }.toSet
  case _ => Set.empty
}

def currentCodegenInputs(ctx: CodegenContext): Set[ExprValue] = {
  // Collects current variables in ctx.currentVars and ctx.INPUT_ROW.
  // It looks roughly like...
  ctx.currentVars.flatMap { v =>
    getExprValues(v.code) ++ Set(v.value, v.isNull)
  }.toSet + ctx.INPUT_ROW
}

// A code block of an expression contains too long code, making it as method
if (eval.code.length > 1024) {
  val setIsNull = if (!eval.isNull.isInstanceOf[LiteralValue]) {
    ...
  } else {
    """"
  }

  // Pick up variables and statements necessary to pass in.
  val currentVars = currentCodegenInputs(ctx)
  val varsPassIn = getExprValues(eval.code).intersect(currentVars)
  val aliasedExprs = HashMap.empty[SimpleExprValue, VariableValue]

  // Replace statement-based expressions which can't be directly passed in the method.
  val newCode = eval.code.transform {
    case block =>
      block.transformExprValues {
        case s: SimpleExprValue(_, javaType) if varsPassIn.contains(s) =>
          if (aliasedExprs.contains(s)) {
            aliasedExprs(s)
          } else {
            val aliasedVariable = JavaCode.variable(ctx.freshName(""aliasedVar""), javaType)
            aliasedExprs += s -> aliasedVariable
            varsPassIn += aliasedVariable
            aliasedVariable
          }
      }
  }

  val params = varsPassIn.filter(!_.isInstanceOf[SimpleExprValue])).map { variable =>
    s""${variable.javaType.getName} ${variable.variableName}""
  }.mkString("", "")

  val funcName = ctx.freshName(""nodeName"")
  val javaType = CodeGenerator.javaType(dataType)
  val newValue = JavaCode.variable(ctx.freshName(""value""), dataType)
  val funcFullName = ctx.addNewFunction(funcName,
    s""""""
      |private $javaType $funcName($params) {
      |  $newCode
      |  $setIsNull
      |  return ${eval.value};
      |}
    """""".stripMargin))

  eval.value = newValue
  val args = varsPassIn.filter(!_.isInstanceOf[SimpleExprValue])).map { variable =>
    s""${variable.variableName}""
  }

  // Create a code block to assign statements to aliased variables.
  val createVariables = aliasedExprs.foldLeft(EmptyBlock) { (block, (statement, variable)) =>
    block + code""${statement.javaType.getName} $variable = $statement;""
  }
  eval.code = createVariables + code""$javaType $newValue = $funcFullName($args);""
}
```

## How was this patch tested?

Added unite tests.

Author: Liang-Chi Hsieh <viirya@gmail.com>

Closes #21405 from viirya/codeblock-api.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/javaCode.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeBlockSuite.scala']","The current code block manipulation API is immature and hacky, requiring a more robust and formal API to properly manipulate code blocks and expressions within them. This problem arises particularly when attempting to split a long code block into smaller methods."
f4ff2d16483f7da2c7ab73c7cfec75bb9e91064d,1662041038,"[SPARK-40297][SQL] CTE outer reference nested in CTE main body cannot be resolved

### What changes were proposed in this pull request?

This PR fixes a bug where a CTE reference cannot be resolved if this reference occurs in an inner CTE definition nested in the outer CTE's main body FROM clause. E.g.,
```
WITH cte_outer AS (
  SELECT 1
)
SELECT * FROM (
  WITH cte_inner AS (
    SELECT * FROM cte_outer
  )
  SELECT * FROM cte_inner
)
```

This fix is to change the `CTESubstitution`'s traverse order from `resolveOperatorsUpWithPruning` to `resolveOperatorsDownWithPruning` and also to recursively call `traverseAndSubstituteCTE` for CTE main body.

### Why are the changes needed?

Bug fix. Without the fix an `AnalysisException` would be thrown for CTE queries mentioned above.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added UTs.

Closes #37751 from maryannxue/spark-40297.

Authored-by: Maryann Xue <maryann.xue@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala', 'sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql', 'sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala']","CTE references nested within the main body FROM clause of an outer CTE definition are not resolving correctly, leading to an AnalysisException being thrown."
05fc3497f00a0aad9240f14637ea21d271b2bbe4,1687914333,"[SPARK-44161][CONNECT] Handle Row input for UDFs

### What changes were proposed in this pull request?
If the client passes Rows as inputs to UDFs, the Spark connect planner will fail to create the RowEncoder for the Row input.

The Row encoder sent by the client contains no field or schema information. The real input schema should be obtained from the plan's output.

This PR ensures if the server planner failed to create the encoder for the UDF input using reflection, then it will fall back to use RowEncoders created from the plan.output schema.

This PR fixed [SPARK-43761](https://issues.apache.org/jira/browse/SPARK-43761) using the same logic.
This PR resolved [SPARK-43796](https://issues.apache.org/jira/browse/SPARK-43796). The error is just caused by the case class defined in the test.

### Why are the changes needed?
Fix the bug where the Row cannot be used as UDF inputs.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
E2E tests.

Closes #41704 from zhenlineo/rowEncoder.

Authored-by: Zhen Li <zhenlineo@users.noreply.github.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
","['connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala', 'connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala', 'connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/KeyValueGroupedDatasetE2ETestSuite.scala', 'connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/UserDefinedFunctionE2ETestSuite.scala', 'connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala', 'connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala']","Spark Connect's planner fails to create a RowEncoder when Rows are passed as inputs to UDFs, causing UDFs to malfunction."
f6ff7d0cf8c0e562f3b086180d5418e6996055bb,1585062237,"[SPARK-30127][SQL] Support case class parameter for typed Scala UDF

### What changes were proposed in this pull request?

To support  case class parameter for typed Scala UDF, e.g.

```
case class TestData(key: Int, value: String)
val f = (d: TestData) => d.key * d.value.toInt
val myUdf = udf(f)
val df = Seq((""data"", TestData(50, ""2""))).toDF(""col1"", ""col2"")
checkAnswer(df.select(myUdf(Column(""col2""))), Row(100) :: Nil)
```

### Why are the changes needed?

Currently, Spark UDF can only work on data types like java.lang.String, o.a.s.sql.Row, Seq[_], etc. This is inconvenient if user want to apply an operation on one column, and the column is struct type. You must access data from a Row object, instead of domain object like Dataset operations. It will be great if UDF can work on types that are supported by Dataset, e.g. case class.

And here's benchmark result of using case class comparing to row:

```scala

// case class:  58ms 65ms 59ms 64ms 61ms
// row:         59ms 64ms 73ms 84ms 69ms
val f1 = (d: TestData) => s""${d.key}, ${d.value}""
val f2 = (r: Row) => s""${r.getInt(0)}, ${r.getString(1)}""
val udf1 = udf(f1)
// set spark.sql.legacy.allowUntypedScalaUDF=true
val udf2 = udf(f2, StringType)

val df = spark.range(100000).selectExpr(""cast (id as int) as id"")
    .select(struct('id, lit(""str"")).as(""col""))
df.cache().collect()

// warmup to exclude some extra influence
df.select(udf1('col)).write.mode(SaveMode.Overwrite).format(""noop"").save()
df.select(udf2('col)).write.mode(SaveMode.Overwrite).format(""noop"").save()

start = System.currentTimeMillis()
df.select(udf1('col)).write.mode(SaveMode.Overwrite).format(""noop"").save()
println(System.currentTimeMillis() - start)

start = System.currentTimeMillis()
df.select(udf2('col)).write.mode(SaveMode.Overwrite).format(""noop"").save()
println(System.currentTimeMillis() - start)

```

### Does this PR introduce any user-facing change?

Yes. User now could be able to use typed Scala UDF with case class as input parameter.

### How was this patch tested?

Added unit tests.

Closes #27937 from Ngone51/udf_caseclass_support.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDFSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatDataWriter.scala', 'sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala', 'sql/core/src/main/scala/org/apache/spark/sql/functions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/IntegratedUDFTestUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala']","Spark UDF currently does not support case class parameters for typed Scala UDF, causing inconvenience when applying an operation on a struct type column. Operations must access data from a Row object rather than domain object like Dataset operations."
9a566f83a0e126742473574476c6381f58394aed,1613180563,"[SPARK-34380][SQL] Support ifExists for ALTER TABLE ... UNSET TBLPROPERTIES for v2 command

### What changes were proposed in this pull request?

This PR proposes to support `ifExists` flag for v2 `ALTER TABLE ... UNSET TBLPROPERTIES` command. Currently, the flag is not respected and the command behaves as `ifExists = true` where the command always succeeds when the properties do not exist.

### Why are the changes needed?

To support `ifExists` flag and align with v1 command behavior.

### Does this PR introduce _any_ user-facing change?

Yes, now if the property does not exist and `IF EXISTS` is not specified, the command will fail:
```
ALTER TABLE t UNSET TBLPROPERTIES ('unknown') // Fails with ""Attempted to unset non-existent property 'unknown'""
ALTER TABLE t UNSET TBLPROPERTIES IF EXISTS ('unknown') // OK
```

### How was this patch tested?

Added new test

Closes #31494 from imback82/AlterTableUnsetPropertiesIfExists.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveTableProperties.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala']","`ALTER TABLE ... UNSET TBLPROPERTIES` for v2 command does not respect the `ifExists` flag, the command succeeds even when the properties do not exist."
5adcddb87a052ce8e3b3c917c61f019bea5532ae,1657182161,"[SPARK-39695][SQL] Add the `REGEXP_SUBSTR` function

### What changes were proposed in this pull request?
In the PR, I propose to add new expression `RegExpSubStr` as a runtime replaceable expression of `NullIf` and `RegExpExtract`. And bind the expression to the function name `REGEXP_SUBSTR`. The `REGEXP_SUBSTR` function returns the substring that matches a regular expression within a string. It takes two parameters:
1. An expression that specifies the string in which the search is to take place.
2. An expression that specifies the regular expression string that is the pattern for the search.

If the regular expression is not found, the result is **null** (this behaviour is similar to other DBMSs). When any of the input parameters are NULL, the function returns NULL too.

For example:
```sql
spark-sql> CREATE TABLE log (logs string);
spark-sql> INSERT INTO log (logs) VALUES
         > ('127.0.0.1 - - [10/Jan/2022:16:55:36 -0800] ""GET / HTTP/1.0"" 200 2217'),
         > ('192.168.1.99 - - [14/Feb/2022:10:27:10 -0800] ""GET /cgi-bin/try/ HTTP/1.0"" 200 3396');
spark-sql> SELECT REGEXP_SUBSTR (logs,'\\b\\d{1,3}\.\\d{1,3}\.\\d{1,3}\.\\d{1,3}\\b') AS IP, REGEXP_SUBSTR (logs,'([\\w:\/]+\\s[+\-]\\d{4})') AS DATE FROM log;
127.0.0.1	10/Jan/2022:16:55:36 -0800
192.168.1.99	14/Feb/2022:10:27:10 -0800
```

### Why are the changes needed?
To make the migration process from other systems to Spark SQL easier, and achieve feature parity to such systems. For example, the systems below support the `REGEXP_SUBSTR` function, see:
- Oracle: https://docs.oracle.com/cd/B12037_01/server.101/b10759/functions116.htm
- DB2: https://www.ibm.com/docs/en/db2/11.5?topic=functions-regexp-substr
- Snowflake: https://docs.snowflake.com/en/sql-reference/functions/regexp_substr.html
- BigQuery: https://cloud.google.com/bigquery/docs/reference/standard-sql/string_functions#regexp_substr
- Redshift: https://docs.aws.amazon.com/redshift/latest/dg/REGEXP_SUBSTR.html
- MariaDB: https://mariadb.com/kb/en/regexp_substr/
- Exasol DB: https://docs.exasol.com/db/latest/sql_references/functions/alphabeticallistfunctions/regexp_substr.htm

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
By running new tests:
```
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z regexp-functions.sql""
$ build/sbt ""sql/testOnly *ExpressionsSchemaSuite""
$ build/sbt ""sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite""
```

Closes #37101 from MaxGekk/regexp_substr.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala', 'sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql']","Spark SQL lacks the `REGEXP_SUBSTR` function present in other SQL systems, making it difficult to extract substrings matching a regular expression and impeding the migration process from other SQL databases to Spark SQL."
4758dc78a2de1afa31b108bd3ec5c9eec63a5b94,1624174341,"[SPARK-35771][SQL][FOLLOWUP] IntervalUtils.toYearMonthIntervalString should consider the case year-month type is casted as month type

### What changes were proposed in this pull request?

This PR fixes an issue that `IntervalUtils.toYearMonthIntervalString` doesn't consider the case that year-month interval type is casted as month interval type.
If a year-month interval data is casted as month interval, the value of the year is multiplied by `12` and added to the value of month. For example, `INTERVAL '1-2' YEAR TO MONTH` will be `INTERVAL '14' MONTH` if  it's casted.
If this behavior is intended, it's stringified to be `'INTERVAL 14' MONTH` but currently, it will be `INTERVAL '2' MONTH`

### Why are the changes needed?

It's a bug if the behavior of cast is intended.

### Does this PR introduce _any_ user-facing change?

No, because this feature is not released yet.

### How was this patch tested?

Modified the tests added in SPARK-35771 (#32924).

Closes #32982 from sarutak/fix-toYearMonthIntervalString.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala']",Casting year-month interval type as month interval type in `IntervalUtils.toYearMonthIntervalString` yields incorrect string representation.
db6247faa8780bca8f8d3ba71b568ea63b162973,1584944484,"[SPARK-31211][SQL] Fix rebasing of 29 February of Julian leap years

### What changes were proposed in this pull request?
In the PR, I propose to fix the issue of rebasing leap years in Julian calendar to Proleptic Gregorian calendar in which the years are not leap years. In the Julian calendar, every four years is a leap year, with a leap day added to the month of February. In Proleptic Gregorian calendar, every year that is exactly divisible by four is a leap year, except for years that are exactly divisible by 100, but these centurial years are leap years, if they are exactly divisible by 400. In this ways, the date **1000-02-29** exists in the Julian calendar but not in Proleptic Gregorian calendar.

I modified the `rebaseJulianToGregorianMicros()` and `rebaseJulianToGregorianDays()` in `DateTimeUtils` by passing 1 as a day number of month while forming `LocalDate` or `LocalDateTime`, and adding the number of days using the `plusDays()` method. For example, **1000-02-29** doesn't exist in Proleptic Gregorian calendar, and `LocalDate.of(1000, 2, 29)` throws an exception. To avoid the issue, I build the `LocalDate.of(1000, 2, 1)` date and add 28 days. The `plusDays(28)` method produces the next valid date after `1000-02-28` which is **1000-03-01**.

### Why are the changes needed?
Before the changes, the `java.time.DateTimeException` exception is raised while loading the date `1000-02-29` from parquet files saved by Spark 2.4.5:
```scala
scala> spark.conf.set(""spark.sql.legacy.parquet.rebaseDateTime.enabled"", true)
scala> spark.read.parquet(""/Users/maxim/tmp/before_1582/2_4_5_date_leap"").show
20/03/21 03:03:59 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.time.DateTimeException: Invalid date 'February 29' as '1000' is not a leap year
```
The parquet files were saved via the commands:
```shell
$ export TZ=""America/Los_Angeles""
```
```scala
scala> scala> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
scala> val df = Seq(java.sql.Date.valueOf(""1000-02-29"")).toDF(""dateS"").select($""dateS"".as(""date""))
df: org.apache.spark.sql.DataFrame = [date: date]
scala> df.write.mode(""overwrite"").parquet(""/Users/maxim/tmp/before_1582/2_4_5_date_leap"")
scala> spark.read.parquet(""/Users/maxim/tmp/before_1582/2_4_5_date_leap"").show
+----------+
|      date|
+----------+
|1000-02-29|
+----------+
```

### Does this PR introduce any user-facing change?
Yes, after the fix:
```scala
scala> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
scala> spark.conf.set(""spark.sql.legacy.parquet.rebaseDateTime.enabled"", true)
scala> spark.read.parquet(""/Users/maxim/tmp/before_1582/2_4_5_date_leap"").show
+----------+
|      date|
+----------+
|1000-03-01|
+----------+
```

### How was this patch tested?
Added tests to `DateTimeUtilsSuite`.

Closes #27974 from MaxGekk/julian-date-29-feb.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeUtilsSuite.scala']","When loading leap year data from parquet files, the system throws a java.time.DateTimeException for dates like '1000-02-29' due to differences in leap year calculation between Julian and Proleptic Gregorian calendars."
35fa5e6716e59b004851b61f7fbfbdace15f46b7,1671084886,"[SPARK-41271][SQL] Support parameterized SQL queries by `sql()`

### What changes were proposed in this pull request?
In the PR, I propose to extend SparkSession API and override the `sql` method by:
```scala
  def sql(sqlText: String, args: Map[String, String]): DataFrame
```
which accepts a map with:
- keys are parameters names,
- values are SQL literal values.

And the first argument `sqlText` might have named parameters in the positions of constants like literal values.

For example:
```scala
  spark.sql(
    sqlText = ""SELECT * FROM tbl WHERE date > :startDate LIMIT :maxRows"",
    args = Map(
      ""startDate"" -> ""DATE'2022-12-01'"",
      ""maxRows"" -> ""100""))
```
The new `sql()` method parses the input SQL statement and provided parameter values, and replaces the named parameters by the literal values. And then it eagerly runs DDL/DML commands, but not for SELECT queries.

Closes #38712

### Why are the changes needed?
1. To improve user experience with Spark SQL via
    - Using Spark as remote service (microservice).
    - Write SQL code that will power reports, dashboards, charts and other data presentation solutions that need to account for criteria modifiable by users through an interface.
    - Build a generic integration layer based on the SQL API. The goal is to expose managed data to a wide application ecosystem with a microservice architecture. It is only natural in such a setup to ask for modular and reusable SQL code, that can be executed repeatedly with different parameter values.

2. To achieve feature parity with other systems that support named parameters:
    - Redshift: https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html#data-api-calling
    - BigQuery: https://cloud.google.com/bigquery/docs/parameterized-queries#api
    - MS DBSQL: https://learn.microsoft.com/en-us/azure/databricks/sql/user/queries/query-parameters

### Does this PR introduce _any_ user-facing change?
No, this is an extension of the existing APIs.

### How was this patch tested?
By running new tests:
```
$ build/sbt ""core/testOnly *SparkThrowableSuite""
$ build/sbt ""test:testOnly *PlanParserSuite""
$ build/sbt ""test:testOnly *AnalysisSuite""
$ build/sbt ""test:testOnly *ParametersSuite""

```

Closes #38864 from MaxGekk/parameterized-sql-2.

Lead-authored-by: Max Gekk <max.gekk@gmail.com>
Co-authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/parameters.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala', 'sql/core/src/test/scala/org/apache/spark/sql/ParametersSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestUtils.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/execution/benchmark/InsertIntoHiveTableBenchmark.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/execution/benchmark/ObjectHashAggregateExecBenchmark.scala']","Spark SQL's current `sql()` method doesn't support parameterized queries, limiting its usefulness for data presentation solutions, remote service applications, and achieving feature parity with other systems that support named parameters."
2af2da5a4b1f5dbf0b55afd0b2514a52f03ffa94,1606795874,"[SPARK-30900][SS] FileStreamSource: Avoid reading compact metadata log twice if the query restarts from compact batch

### What changes were proposed in this pull request?

This patch addresses the case where compact metadata file is read twice in FileStreamSource during restarting query.

When restarting the query, there is a case which the query starts from compaction batch, and the batch has source metadata file to read. One case is that the previous query succeeded to read from inputs, but not finalized the batch for various reasons.

The patch finds the latest compaction batch when restoring from metadata log, and put entries for the batch into the file entry cache which would avoid reading compact batch file twice.

FileStreamSourceLog doesn't know about offset / commit metadata in checkpoint so doesn't know which exactly batch to start from, but in practice, only couple of latest batches are candidates to
be started from when restarting query. This patch leverages the fact to skip calculation if possible.

### Why are the changes needed?

Spark incurs unnecessary cost on reading the compact metadata file twice on some case, which may not be ignorable when the query has been processed huge number of files so far.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

New UT.

Closes #27649 from HeartSaVioR/SPARK-30900.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala']","During query restarts in FileStreamSource, compact metadata log is being read twice, causing unnecessary computational overhead especially for queries that have processed large number of files."
e46e487b0831b39afa12ef9cff9b9133f111921b,1573714034,"[SPARK-29682][SQL] Resolve conflicting attributes in Expand correctly

### What changes were proposed in this pull request?

This PR addresses issues where conflicting attributes in `Expand` are not correctly handled.

### Why are the changes needed?

```Scala
val numsDF = Seq(1, 2, 3, 4, 5, 6).toDF(""nums"")
val cubeDF = numsDF.cube(""nums"").agg(max(lit(0)).as(""agcol""))
cubeDF.join(cubeDF, ""nums"").show
```
fails with the following exception:
```
org.apache.spark.sql.AnalysisException:
Failure when resolving conflicting references in Join:
'Join Inner
:- Aggregate [nums#38, spark_grouping_id#36], [nums#38, max(0) AS agcol#35]
:  +- Expand [List(nums#3, nums#37, 0), List(nums#3, null, 1)], [nums#3, nums#38, spark_grouping_id#36]
:     +- Project [nums#3, nums#3 AS nums#37]
:        +- Project [value#1 AS nums#3]
:           +- LocalRelation [value#1]
+- Aggregate [nums#38, spark_grouping_id#36], [nums#38, max(0) AS agcol#58]
   +- Expand [List(nums#3, nums#37, 0), List(nums#3, null, 1)], [nums#3, nums#38, spark_grouping_id#36]
                                                                         ^^^^^^^
      +- Project [nums#3, nums#3 AS nums#37]
         +- Project [value#1 AS nums#3]
            +- LocalRelation [value#1]

Conflicting attributes: nums#38
```
As you can see from the above plan, `num#38`, the output of `Expand` on the right side of `Join`, should have been handled to produce new attribute. Since the conflict is not resolved in `Expand`, the failure is happening upstream at `Aggregate`. This PR addresses handling conflicting attributes in `Expand`.

### Does this PR introduce any user-facing change?

Yes, the previous example now shows the following output:
```
+----+-----+-----+
|nums|agcol|agcol|
+----+-----+-----+
|   1|    0|    0|
|   6|    0|    0|
|   4|    0|    0|
|   2|    0|    0|
|   5|    0|    0|
|   3|    0|    0|
+----+-----+-----+
```
### How was this patch tested?

Added new unit test.

Closes #26441 from imback82/spark-29682.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala']","Conflicting attributes in `Expand` aren't resolved correctly, leading to failure when resolving these conflicts in Join. This can be observed when attempting to join the same DataFrame in Spark SQL."
46ebed2b3714c56661577336653f716a440f0c77,1695386199,"[SPARK-44112][BUILD][INFRA][DOCS] Drop support for Java 8 and Java 11

### What changes were proposed in this pull request?
The main purpose of this pr is to remove support for Java 8 and Java 11 in Apache Spark 4.0, the specific work includes:
1. `pom.xml`: change `java.version` from 1.8 to 17, change `target:jvm-1.8` to `target:17`
2. `SparkBuild.scala`: change `-target:jvm-${javaVersion.value}` to `-target:${javaVersion.value}`
3. workflow files: change the default Java version from 8 to 17, and ensure that branch-3.x still uses Java 8. Removed the daily job for Java 11 and Java 17.
4. docs: replace parts of Java 8 and 11 with Java 17.

### Why are the changes needed?
The minimum supported Java version for Apache Spark 4.0 is Java 17

### Does this PR introduce _any_ user-facing change?
Yes,  Apache will no longer support Java 8 and Java 11

### How was this patch tested?
- Pass Github Actions

### Was this patch authored or co-authored using generative AI tooling?
No

Closes #43005 from LuciferYang/SPARK-44112.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['project/SparkBuild.scala'],Apache Spark 4.0 fails to run on systems using Java versions lower than 17. Current version supports deprecated Java versions 8 and 11.
c3c443ca8c5c0bed9de5abb57a0bd1bfabe9ed82,1558495645,"[SPARK-27698][SQL] Add new method `convertibleFilters` for getting pushed down filters in Parquet file reader

## What changes were proposed in this pull request?

To return accurate pushed filters in Parquet file scan(https://github.com/apache/spark/pull/24327#pullrequestreview-234775673), we can process the original data source filters in the following way:
1. For ""And"" operators, split the conjunctive predicates and try converting each of them. After that
1.1 if partially predicate pushed down is allowed, return convertible results;
1.2 otherwise, return the whole predicate if convertible, or empty result if not convertible.

2. For ""Or"" operators, if both children can be  pushed down, it is partially or totally convertible; otherwise, return empty result

3. For other operators, they are not able to be partially pushed down.
2.1 if the entire predicate is convertible, return itself
2.2 otherwise, return an empty result.

This PR also contains code refactoring. Currently `ParquetFilters. createFilter ` accepts parameter `schema: MessageType` and create field mapping for every input filter. We can make it a class member and avoid creating the `nameToParquetField` mapping for every input filter.

## How was this patch tested?

Unit test

Closes #24597 from gengliangwang/refactorParquetFilters.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala']","Current Parquet file reader does not accurately return pushed down filters, making it difficult to handle ""And"" and ""Or"" operators as well as other non-partially pushable operators."
2036a8cca7a428672310ae11e71d0f1f51074cac,1571752692,"[SPARK-29488][WEBUI] In Web UI, stage page has js error when sort table

### What changes were proposed in this pull request?
In Web UI, stage page has js error when sort table.
https://issues.apache.org/jira/browse/SPARK-29488

### Why are the changes needed?
In Web UI, follow the steps below, get js error ""Uncaught TypeError: Failed to execute 'removeChild' on 'Node': parameter 1 is not of type 'Node'."".
 1) Click ""Summary Metrics..."" 's tablehead ""Min""
 2) Click ""Aggregated Metrics by Executor"" 's tablehead ""Task Time""
 3) Click ""Summary Metrics..."" 's tablehead ""Min""（the same as step 1.）

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
In Web UI, follow the steps below, no error occur.
 1) Click ""Summary Metrics..."" 's tablehead ""Min""
 2) Click ""Aggregated Metrics by Executor"" 's tablehead ""Task Time""
 3) Click ""Summary Metrics..."" 's tablehead ""Min""（the same as step 1.）
![image](https://user-images.githubusercontent.com/7802338/66899878-464b1b80-f02e-11e9-9660-6cdaab283491.png)

Closes #26136 from cjn082030/SPARK-1.

Authored-by: chenjuanni <chenjuanni@inspur.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
",['core/src/main/resources/org/apache/spark/ui/static/sorttable.js'],"Javascript error ""Uncaught TypeError: Failed to execute 'removeChild' on 'Node': parameter 1 is not of type 'Node'"" occurs in Web UI when sorting tables by ""Min"" in ""Summary Metrics"" and ""Task Time"" in ""Aggregated Metrics by Executor""."
1fd9a91c662a368e348ef96604a79929f814041c,1583412583,"[SPARK-31005][SQL] Support time zone ids in casting strings to timestamps

### What changes were proposed in this pull request?
In the PR, I propose to change `DateTimeUtils.stringToTimestamp` to support any valid time zone id at the end of input string. After the changes, the function accepts zone ids in the formats:
- no zone id. In that case, the function uses the local session time zone from the SQL config `spark.sql.session.timeZone`
- -[h]h:[m]m
- +[h]h:[m]m
- Z
- Short zone id, see https://docs.oracle.com/javase/8/docs/api/java/time/ZoneId.html#SHORT_IDS
- Zone ID starts with 'UTC+', 'UTC-', 'GMT+', 'GMT-', 'UT+' or 'UT-'. The ID is split in two, with a two or three letter prefix and a suffix starting with the sign. The suffix must be in the formats:
  - +|-h[h]
  - +|-hh[:]mm
  - +|-hh:mm:ss
  - +|-hhmmss
- Region-based zone IDs in the form `{area}/{city}`, such as `Europe/Paris` or `America/New_York`. The default set of region ids is supplied by the IANA Time Zone Database (TZDB).

### Why are the changes needed?
- To use `stringToTimestamp` as a substitution of removed `stringToTime`, see https://github.com/apache/spark/pull/27710#discussion_r385020173
- Improve UX of Spark SQL by allowing flexible formats of zone ids. Currently, Spark accepts only `Z` and zone offsets that can be inconvenient when a time zone offset is shifted due to daylight saving rules. For instance:
```sql
spark-sql> select cast('2015-03-18T12:03:17.123456 Europe/Moscow' as timestamp);
NULL
```

### Does this PR introduce any user-facing change?
Yes. After the changes, casting strings to timestamps allows time zone id at the end of the strings:
```sql
spark-sql> select cast('2015-03-18T12:03:17.123456 Europe/Moscow' as timestamp);
2015-03-18 12:03:17.123456
```

### How was this patch tested?
- Added new test cases to the `string to timestamp` test in `DateTimeUtilsSuite`.
- Run `CastSuite` and `AnsiCastSuite`.

Closes #27753 from MaxGekk/stringToTimestamp-uni-zoneId.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeUtilsSuite.scala']","Casting strings to timestamps fails to consider different time zone ids, returning null results and limiting the flexibility and usability of Spark SQL."
2656c9d304b59584c331b923e8536e4093d83f81,1564480454,"[SPARK-28071][SQL][TEST] Port strings.sql

## What changes were proposed in this pull request?

This PR is to port strings.sql from PostgreSQL regression tests. https://github.com/postgres/postgres/blob/REL_12_BETA2/src/test/regress/sql/strings.sql

The expected results can be found in the link: https://github.com/postgres/postgres/blob/REL_12_BETA2/src/test/regress/expected/strings.out

When porting the test cases, found nine PostgreSQL specific features that do not exist in Spark SQL:
[SPARK-28076](https://issues.apache.org/jira/browse/SPARK-28076): Support regular expression substring
[SPARK-28078](https://issues.apache.org/jira/browse/SPARK-28078):  Add support other 4 REGEXP functions
[SPARK-28412](https://issues.apache.org/jira/browse/SPARK-28412): OVERLAY function support byte array
[SPARK-28083](https://issues.apache.org/jira/browse/SPARK-28083):  ANSI SQL: LIKE predicate: ESCAPE clause
[SPARK-28087](https://issues.apache.org/jira/browse/SPARK-28087):  Add support split_part
[SPARK-28122](https://issues.apache.org/jira/browse/SPARK-28122): Missing `sha224`/`sha256 `/`sha384 `/`sha512 ` functions
[SPARK-28123](https://issues.apache.org/jira/browse/SPARK-28123): Add support string functions: btrim
[SPARK-28448](https://issues.apache.org/jira/browse/SPARK-28448): Implement ILIKE operator
[SPARK-28449](https://issues.apache.org/jira/browse/SPARK-28449): Missing escape_string_warning and standard_conforming_strings config

Also, found five inconsistent behavior:
[SPARK-27952](https://issues.apache.org/jira/browse/SPARK-27952): String Functions: regexp_replace is not compatible
[SPARK-28121](https://issues.apache.org/jira/browse/SPARK-28121): decode can not accept 'escape' as charset
[SPARK-27930](https://issues.apache.org/jira/browse/SPARK-27930): Replace `strpos` with `locate` or `position` in Spark SQL
[SPARK-27930](https://issues.apache.org/jira/browse/SPARK-27930): Replace `to_hex` with `hex ` or in Spark SQL
[SPARK-28451](https://issues.apache.org/jira/browse/SPARK-28451): `substr` returns different values

## How was this patch tested?

N/A

Closes #24923 from wangyum/SPARK-28071.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
",['sql/core/src/test/resources/sql-tests/inputs/pgSQL/strings.sql'],"Porting strings.sql from PostgreSQL regression tests into Spark SQL reveals several missing or inconsistently behaving functions, causing compatibility issues and function inconsistencies."
df27350142d81a3e8941939870bfc0ab50e37a43,1587058268,"[SPARK-31420][WEBUI][FOLLOWUP] Make locale of timeline-view 'en'

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue.
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This change explicitly set locale of timeline view to 'en' to be the same appearance as before upgrading vis-timeline.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
We upgraded vis-timeline in #28192 and the upgraded version is different from before we used in the notation of dates.
The notation seems to be dependent on locale. The following is appearance in my Japanese environment.
<img width=""557"" alt=""locale-changed"" src=""https://user-images.githubusercontent.com/4736016/79265314-de886700-7ed0-11ea-8641-fa76b993c0d9.png"">

Although the notation is in Japanese, the default format is a little bit unnatural (e.g. 4月9日 05:39 is natural rather than 9 四月 05:39).

I found we can get the same appearance as before by explicitly set locale to 'en'.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
I visited JobsPage, JobPage and StagePage and confirm that timeline view shows dates with 'en' locale.
<img width=""735"" alt=""fix-date-appearance"" src=""https://user-images.githubusercontent.com/4736016/79267107-8bfc7a00-7ed3-11ea-8a25-f6681d04a83c.png"">

NOTE: #28192 will be backported to branch-2.4 and branch-3.0 so this PR should be follow #28214 and #28213 .

Closes #28218 from sarutak/fix-locale-issue.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>
",['core/src/main/resources/org/apache/spark/ui/static/timeline-view.js'],"Timeline view on JobsPage, JobPage, and StagePage display dates in an incorrect format due to locale settings, especially noticeable in non-English environments."
3fde0ba6e67ca45e25e8a19e9bc8f8371f12cb71,1657631990,"[SPARK-39744][SQL] Add the `REGEXP_INSTR` function

### What changes were proposed in this pull request?
In the PR, I propose to add new expression `RegExpInStr`, and bind the expression to the function name `REGEXP_INSTR`. The `REGEXP_INSTR` function searches a string for a regular expression and returns an integer that indicates the beginning position matched substring. Positions are 1-based, not 0-based. If no match is found, returns 0. It takes two parameters:
1. An expression that specifies the string in which the search is to take place.
2. An expression that specifies the regular expression string that is the pattern for the search.

If the regular expression is not found, the result is **0** (this behaviour is similar to other DBMSs). When any of the input parameters are NULL, the function returns NULL too.

For example:
```sql
spark-sql> CREATE TABLE log (logs string);
spark-sql> INSERT INTO log (logs) VALUES
         > ('127.0.0.1 - - [10/Jan/2022:16:55:36 -0800] ""GET / HTTP/1.0"" 200 2217'),
         > ('192.168.1.99 - - [14/Feb/2022:10:27:10 -0800] ""GET /cgi-bin/try/ HTTP/1.0"" 200 3396');
spark-sql>  SELECT REGEXP_INSTR (logs,'\\b\\d{1,3}\.\\d{1,3}\.\\d{1,3}\.\\d{1,3}\\b') AS IP, REGEXP_INSTR (logs,'([\\w:\/]+\\s[+\-]\\d{4})') AS DATE FROM log;
1	19
1	16
```

### Why are the changes needed?
To make the migration process from other systems to Spark SQL easier, and achieve feature parity to such systems. For example, the systems below support the `REGEXP_INSTR` function, see:
 - MariaDB: https://mariadb.com/kb/en/regexp_instr/
 - Oracle: https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions129.htm
 - DB2: https://www.ibm.com/docs/en/db2/11.5?topic=functions-regexp-instr
 - Snowflake: https://docs.snowflake.com/en/sql-reference/functions/regexp_instr.html
 - BigQuery: https://cloud.google.com/bigquery/docs/reference/standard-sql/string_functions#regexp_instr
 - Redshift: https://docs.aws.amazon.com/redshift/latest/dg/REGEXP_INSTR.html
 - Exasol DB: https://docs.exasol.com/db/latest/sql_references/functions/alphabeticallistfunctions/regexp_instr.htm
- Vertica: https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/RegularExpressions/REGEXP_INSTR.htm

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
By running new tests:
```
$ build/sbt ""sql/testOnly *ExpressionsSchemaSuite""
$ build/sbt ""sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite""
$ build/sbt ""test:testOnly *.RegexpExpressionsSuite""
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z regexp-functions.sql""
```

Closes #37154 from MaxGekk/regexp_instr.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql']","Spark SQL lacks the REGEXP_INSTR function that searches a string for a regular expression and indicates the position of the matched substring, which is supported by many other DBMSs impacting migration efforts and feature parity."
578b90cdec37503c0d6db48e4c2ed4f3654faafc,1594388189,"[SPARK-32091][CORE] Ignore timeout error when remove blocks on the lost executor

### What changes were proposed in this pull request?

This PR adds the check to see whether the executor is lost (by asking the `CoarseGrainedSchedulerBackend`) after timeout error raised in `BlockManagerMasterEndponit` due to removing blocks(e.g. RDD, broadcast, shuffle). If the executor is lost, we will ignore the error. Otherwise, throw the error.

### Why are the changes needed?

When removing blocks(e.g. RDD, broadcast, shuffle), `BlockManagerMaserEndpoint` will make RPC calls to each known `BlockManagerSlaveEndpoint` to remove the specific blocks. The PRC call sometimes could end in a timeout when the executor has been lost, but only notified the `BlockManagerMasterEndpoint` after the removing call has already happened. The timeout error could therefore fail the whole job.

In this case, we actually could just ignore the error since those blocks on the lost executor could be considered as removed already.

### Does this PR introduce _any_ user-facing change?

Yes. In case of users hits this issue, they will have the job executed successfully instead of throwing the exception.

### How was this patch tested?

Added unit tests.

Closes #28924 from Ngone51/ignore-timeout-error-for-inactive-executor.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala', 'core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala', 'core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala', 'core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala', 'core/src/main/scala/org/apache/spark/util/RpcUtils.scala', 'core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala']","Timeout error occurs when removing blocks like RDD, broadcast, shuffle from a lost executor can halt the entire job, leading to job failure."
1439d9b275e844b5b595126bc97d2b44f6e859ed,1663055693,"[SPARK-38734][SQL] Remove the error class `INDEX_OUT_OF_BOUNDS`

### What changes were proposed in this pull request?
In the PR, I propose to remove the error class `INDEX_OUT_OF_BOUNDS` from `error-classes.json` and the exception `SparkIndexOutOfBoundsException`. And replace the last one by a SparkException w/ the error class `INTERNAL_ERROR` because the exception should not be raised in regular cases.

`ArrayDataIndexedSeq` throws the exception from `apply()`, and `ArrayDataIndexedSeq` can be created from `ArrayData.toSeq` only. The last one is invoked from 2 places:

1. The `Slice` expression ( or `slice` function):
https://github.com/apache/spark/blob/443eea97578c41870c343cdb88cf69bfdf27033a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L1600-L1601

where any access to the produced array is guarded:
```sql
spark-sql> set spark.sql.ansi.enabled=true;
spark.sql.ansi.enabled	true
Time taken: 2.415 seconds, Fetched 1 row(s)
spark-sql> SELECT slice(array(1, 2, 3, 4), 2, 2)[4];
...
org.apache.spark.SparkArrayIndexOutOfBoundsException: [INVALID_ARRAY_INDEX] The index 4 is out of bounds. The array has 2 elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
== SQL(line 1, position 8) ==
SELECT slice(array(1, 2, 3, 4), 2, 2)[4]
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidArrayIndexError(QueryExecutionErrors.scala:239)
	at org.apache.spark.sql.catalyst.expressions.GetArrayItem.nullSafeEval(complexTypeExtractors.scala:271)
```
see
https://github.com/apache/spark/blob/a9bb924480e4953457dad680c15ca346f71a26c8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala#L268-L271

2. `MapObjects.convertToSeq`:
https://github.com/apache/spark/blob/5b96e82ad6a4f5d5e4034d9d7112077159cf5044/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala#L886

where any access to the produced IndexedSeq is guarded via map-way access in
https://github.com/apache/spark/blob/5b96e82ad6a4f5d5e4034d9d7112077159cf5044/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala#L864-L867

### Why are the changes needed?
To improve code maintenance.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
By running the affected test suite:
```
$ build/sbt ""core/testOnly *SparkThrowableSuite""
$ build/sbt ""test:testOnly *ArrayDataIndexedSeqSuite""
```

Closes #37857 from MaxGekk/rm-INDEX_OUT_OF_BOUNDS.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['core/src/main/scala/org/apache/spark/SparkException.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayData.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ArrayDataIndexedSeqSuite.scala']","'INDEX_OUT_OF_BOUNDS' error class throws an exception from `apply()` causing inconsistencies, should not occur in normal scenarios."
39dfaf2fd167cafc84ec9cc637c114ed54a331e3,1529676974,"[SPARK-24519] Make the threshold for highly compressed map status configurable

**Problem**
MapStatus uses hardcoded value of 2000 partitions to determine if it should use highly compressed map status. We should make it configurable to allow users to more easily tune their jobs with respect to this without having for them to modify their code to change the number of partitions.  Note we can leave this as an internal/undocumented config for now until we have more advise for the users on how to set this config.
Some of my reasoning:
The config gives you a way to easily change something without the user having to change code, redeploy jar, and then run again. You can simply change the config and rerun. It also allows for easier experimentation. Changing the # of partitions has other side affects, whether good or bad is situation dependent. It can be worse are you could be increasing # of output files when you don't want to be, affects the # of tasks needs and thus executors to run in parallel, etc.
There have been various talks about this number at spark summits where people have told customers to increase it to be 2001 partitions. Note if you just do a search for spark 2000 partitions you will fine various things all talking about this number.  This shows that people are modifying their code to take this into account so it seems to me having this configurable would be better.
Once we have more advice for users we could expose this and document information on it.

**What changes were proposed in this pull request?**
I make the hardcoded value mentioned above to be configurable under the name _SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS_, which has default value to be 2000. Users can set it to the value they want by setting the property name _spark.shuffle.minNumPartitionsToHighlyCompress_

**How was this patch tested?**
I wrote a unit test to make sure that the default value is 2000, and  _IllegalArgumentException_ will be thrown if user set it to a non-positive value. The unit test also checks that highly compressed map status is correctly used when the number of partition is greater than _SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS_.

Author: Hieu Huynh <“Hieu.huynh@oath.com”>

Closes #21527 from hthuynh2/spark_branch_1.
","['core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala', 'core/src/test/scala/org/apache/spark/scheduler/MapStatusSuite.scala']","Hardcoded value of 2000 partitions is used to determine usage of highly compressed map status in MapStatus, leading to a lack of flexibility for users to easily tune their jobs."
ab33028957443189efc4106afd9d65dddf8f9c98,1535090335,"[SPARK-25178][SQL] Directly ship the StructType objects of the keySchema / valueSchema for xxxHashMapGenerator

## What changes were proposed in this pull request?

This PR generates the code that to refer a `StructType` generated in the scala code instead of generating `StructType` in Java code.

The original code has two issues.
1. Avoid to used the field name such as `key.name`
1. Support complicated schema (e.g. nested DataType)

At first, [the JIRA entry](https://issues.apache.org/jira/browse/SPARK-25178) proposed to change the generated field name of the keySchema / valueSchema to a dummy name in `RowBasedHashMapGenerator` and `VectorizedHashMapGenerator.scala`. This proposal can addresse issue 1.

Ueshin suggested an approach to refer to a `StructType` generated in the scala code using `ctx.addReferenceObj()`. This approach can address issues 1 and 2. Finally, this PR uses this approach.

## How was this patch tested?

Existing UTs

Closes #22187 from kiszk/SPARK-25178.

Authored-by: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
Signed-off-by: Takuya UESHIN <ueshin@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala']",Existing Java code generates StructType for keySchema/valueSchema and fails to handle complex schemas like nested DataType. Field names such as key.name cannot be used which may affect some operations.
27bb40b6297361985e3590687f0332a72b71bc85,1605004758,"[SPARK-33339][PYTHON] Pyspark application will hang due to non Exception error

### What changes were proposed in this pull request?

When a system.exit exception occurs during the process, the python worker exits abnormally, and then the executor task is still waiting for the worker for reading from socket, causing it to hang.
The system.exit exception may be caused by the user's error code, but spark should at least throw an error to remind the user, not get stuck
we can run a simple test to reproduce this case:

```
from pyspark.sql import SparkSession
def err(line):
  raise SystemExit
spark = SparkSession.builder.appName(""test"").getOrCreate()
spark.sparkContext.parallelize(range(1,2), 2).map(err).collect()
spark.stop()
```

### Why are the changes needed?

to make sure pyspark application won't hang if there's non-Exception error in python worker

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

added a new test and also manually tested the case above

Closes #30248 from li36909/pyspark.

Lead-authored-by: lrz <lrz@lrzdeMacBook-Pro.local>
Co-authored-by: Hyukjin Kwon <gurwls223@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['python/pyspark/tests/test_worker.py', 'python/pyspark/worker.py']",PySpark applications hang when a SystemExit exception occurs during processing due to the executor task still awaiting a response from an abnormally exited python worker.
6361467bde9db0a8094922ead2c6e5d6206df179,1564110345,"[SPARK-28289][SQL][PYTHON][TESTS] Convert and port 'union.sql' into UDF test base

## What changes were proposed in this pull request?
This PR adds some tests converted from 'union.sql' to test UDFs

<details><summary>Diff comparing to 'union.sql'</summary>
<p>

```diff
diff --git a/sql/core/src/test/resources/sql-tests/results/union.sql.out b/sql/core/src/test/resources/sql-tests/results/udf/udf-union.sql.out
index b023df825d..84b5e10dbe 100644
--- a/sql/core/src/test/resources/sql-tests/results/union.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/udf/udf-union.sql.out
 -19,10 +19,10  struct<>

 -- !query 2
-SELECT *
-FROM   (SELECT * FROM t1
+SELECT udf(c1) as c1, udf(c2) as c2
+FROM   (SELECT udf(c1) as c1, udf(c2) as c2 FROM t1
         UNION ALL
-        SELECT * FROM t1)
+        SELECT udf(c1) as c1, udf(c2) as c2 FROM t1)
 -- !query 2 schema
 struct<c1:int,c2:string>
 -- !query 2 output
 -33,12 +33,12  struct<c1:int,c2:string>

 -- !query 3
-SELECT *
-FROM   (SELECT * FROM t1
+SELECT udf(c1) as c1, udf(c2) as c2
+FROM   (SELECT udf(c1) as c1, udf(c2) as c2 FROM t1
         UNION ALL
-        SELECT * FROM t2
+        SELECT udf(c1) as c1, udf(c2) as c2 FROM t2
         UNION ALL
-        SELECT * FROM t2)
+        SELECT udf(c1) as c1, udf(c2) as c2 FROM t2)
 -- !query 3 schema
 struct<c1:decimal(11,1),c2:string>
 -- !query 3 output
 -51,11 +51,11  struct<c1:decimal(11,1),c2:string>

 -- !query 4
-SELECT a
-FROM (SELECT 0 a, 0 b
+SELECT udf(udf(a)) as a
+FROM (SELECT udf(0) a, udf(0) b
       UNION ALL
-      SELECT SUM(1) a, CAST(0 AS BIGINT) b
-      UNION ALL SELECT 0 a, 0 b) T
+      SELECT udf(SUM(1)) a, udf(CAST(0 AS BIGINT)) b
+      UNION ALL SELECT udf(0) a, udf(0) b) T
 -- !query 4 schema
 struct<a:bigint>
 -- !query 4 output
 -89,13 +89,13  struct<>

 -- !query 8
-SELECT 1 AS x,
-       col
-FROM   (SELECT col AS col
-        FROM (SELECT p1.col AS col
+SELECT udf(1) AS x,
+       udf(col) as col
+FROM   (SELECT udf(col) AS col
+        FROM (SELECT udf(p1.col) AS col
               FROM   p1 CROSS JOIN p2
               UNION ALL
-              SELECT col
+              SELECT udf(col)
               FROM p3) T1) T2
 -- !query 8 schema
 struct<x:int,col:int>
 -105,9 +105,9  struct<x:int,col:int>

 -- !query 9
-SELECT map(1, 2), 'str'
+SELECT map(1, 2), udf('str') as str
 UNION ALL
-SELECT map(1, 2, 3, NULL), 1
+SELECT map(1, 2, 3, NULL), udf(1)
 -- !query 9 schema
 struct<map(1, 2):map<int,int>,str:string>
 -- !query 9 output
 -116,9 +116,9  struct<map(1, 2):map<int,int>,str:string>

 -- !query 10
-SELECT array(1, 2), 'str'
+SELECT array(1, 2), udf('str') as str
 UNION ALL
-SELECT array(1, 2, 3, NULL), 1
+SELECT array(1, 2, 3, NULL), udf(1)
 -- !query 10 schema
 struct<array(1, 2):array<int>,str:string>
 -- !query 10 output
```

</p>
</details>

## How was this patch tested?
Tested as guided in SPARK-27921.

Closes #25202 from yiheng/fix_28289.

Authored-by: Yiheng Wang <yihengw@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['sql/core/src/test/resources/sql-tests/inputs/udf/udf-union.sql'],The 'union.sql' testing lacks uniform function descriptor (UDF) testing leading to possible uncaught errors in union operations with UDFs in different SQL queries.
3946de773498621f88009c309254b019848ed490,1539393253,"[SPARK-20327][CORE][YARN] Add CLI support for YARN custom resources, like GPUs

## What changes were proposed in this pull request?

This PR adds CLI support for YARN custom resources, e.g. GPUs and any other resources YARN defines.
The custom resources are defined with Spark properties, no additional CLI arguments were introduced.

The properties can be defined in the following form:

**AM resources, client mode:**
Format: `spark.yarn.am.resource.<resource-name>`
The property name follows the naming convention of YARN AM cores / memory properties: `spark.yarn.am.memory and spark.yarn.am.cores
`

**Driver resources, cluster mode:**
Format: `spark.yarn.driver.resource.<resource-name>`
The property name follows the naming convention of driver cores / memory properties: `spark.driver.memory and spark.driver.cores.`

**Executor resources:**
Format: `spark.yarn.executor.resource.<resource-name>`
The property name follows the naming convention of executor cores / memory properties: `spark.executor.memory / spark.executor.cores`.

For the driver resources (cluster mode) and executor resources properties, we use the `yarn` prefix here as custom resource types are specific to YARN, currently.

**Validation:**
Please note that a validation logic is added to avoid having requested resources defined in 2 ways, for example defining the following configs:
```
""--conf"", ""spark.driver.memory=2G"",
""--conf"", ""spark.yarn.driver.resource.memory=1G""
```

will not start execution and will print an error message.

## How was this patch tested?
Unit tests + manual execution with Hadoop2 and Hadoop 3 builds.

Testing have been performed on a real cluster with Spark and YARN configured:
Cluster and client mode
Request Resource Types with lowercase and uppercase units
Start Spark job with only requesting standard resources (mem / cpu)
Error handling cases:
- Request unknown resource type
- Request Resource type (either memory / cpu) with duplicate configs at the same time (e.g. with this config:
```
--conf spark.yarn.am.resource.memory=1G \
  --conf spark.yarn.driver.resource.memory=2G \
  --conf spark.yarn.executor.resource.memory=3G \
```
), ResourceTypeValidator handles these cases well, so it is not permitted
- Request standard resource (memory / cpu) with the new style configs, e.g. --conf spark.yarn.am.resource.memory=1G,  this is not permitted and handled well.

An example about how I ran the testcases:
```
cd ~;export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop/;
./spark-2.4.0-SNAPSHOT-bin-custom-spark/bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 1G \
  --driver-cores 1 \
  --executor-memory 1G \
  --executor-cores 1 \
  --conf spark.logConf=true \
  --conf spark.yarn.executor.resource.gpu=3G \
  --verbose \
  ./spark-2.4.0-SNAPSHOT-bin-custom-spark/examples/jars/spark-examples_2.11-2.4.0-SNAPSHOT.jar \
  10;
```

Closes #20761 from szyszy/SPARK-20327.

Authored-by: Szilard Nemeth <snemeth@cloudera.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala', 'resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ResourceRequestHelper.scala', 'resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala', 'resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestHelperSuite.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestTestHelper.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala']","The Spark CLI lacks the ability to support YARN custom resources like GPUs, causing custom resources defined with Spark properties to not be correctly identified and utilized."
286d3364ab258e68b4dd0df8bc0980645119d222,1675736518,"[SPARK-40149][SQL][FOLLOWUP] Avoid adding extra Project in AddMetadataColumns

### What changes were proposed in this pull request?
This PR is a follow-up for #37758. It updates the rule `AddMetadataColumns` to avoid introducing extra `Project`.

### Why are the changes needed?

To fix an issue introduced by #37758.
```sql
-- t1: [key, value] t2: [key, value]
select t1.key, t2.key from t1 full outer join t2 using (key)
```
Before this PR, the rule `AddMetadataColumns` will add a new Project between the using join and the select list:
```
Project [key, key]
+- Project [key, key, key, key] <--- extra project
   +- Project [coalesce(key, key) AS key, value, value, key, key]
      +- Join FullOuter, (key = key)
         :- LocalRelation <empty>, [key#0, value#0]
         +- LocalRelation <empty>, [key#0, value#0]
```
After this PR, this extra Project will be removed.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Add a new UT.

Closes #39895 from allisonwang-db/spark-40149-follow-up.

Authored-by: allisonwang-db <allison.wang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala']","Excessive Project layers are introduced due to the AddMetadataColumns rule, which leads to unnecessary complexity when executing SQL full outer joins."
e76ae9a47b66d57510b526424326154d08d05478,1640254462,"[SPARK-37721][PYTHON] Fix SPARK_HOME key error when running PySpark test

### What changes were proposed in this pull request?
Replace `os.environ[""SPARK_HOME""]` direct access with `_find_spark_home` in pyspark testing utils.

### Why are the changes needed?
When running PySpark unit test in WSL with PyCharm, all test raise the `KeyError: 'SPARK_HOME'`
```
Launching unittests with arguments python -m unittest test_rdd.RDDTests.test_range in /home/yikun/spark/python/pyspark/testsTraceback (most recent call last):
  File ""/mnt/d/Program Files/JetBrains/PyCharm 2021.1.3/plugins/python/helpers/pycharm/_jb_unittest_runner.py"", line 35, in <module>
    sys.exit(main(argv=args, module=None, testRunner=unittestpy.TeamcityTestRunner, buffer=not JB_DISABLE_BUFFERING))
  File ""/usr/lib/python3.8/unittest/main.py"", line 100, in __init__
    self.parseArgs(argv)
  File ""/usr/lib/python3.8/unittest/main.py"", line 147, in parseArgs
    self.createTests()
  File ""/usr/lib/python3.8/unittest/main.py"", line 158, in createTests
    self.test = self.testLoader.loadTestsFromNames(self.testNames,
  File ""/usr/lib/python3.8/unittest/loader.py"", line 220, in loadTestsFromNames
    suites = [self.loadTestsFromName(name, module) for name in names]
  File ""/usr/lib/python3.8/unittest/loader.py"", line 220, in <listcomp>
    suites = [self.loadTestsFromName(name, module) for name in names]
  File ""/usr/lib/python3.8/unittest/loader.py"", line 154, in loadTestsFromName
    module = __import__(module_name)
  File ""/home/yikun/spark/python/pyspark/tests/test_rdd.py"", line 37, in <module>
    from pyspark.testing.utils import ReusedPySparkTestCase, SPARK_HOME, QuietTest
  File ""/home/yikun/spark/python/pyspark/testing/utils.py"", line 47, in <module>
    SPARK_HOME = os.environ[""SPARK_HOME""]#_find_spark_home()
  File ""/usr/lib/python3.8/os.py"", line 675, in __getitem__
    raise KeyError(key) from None
KeyError: 'SPARK_HOME'
```

We should use `_find_spark_home` to get SPARK_HOME rather than access `os.environ`.

### Does this PR introduce _any_ user-facing change?
No, test only

### How was this patch tested?
- UT passed.
- Run test in Win WSL.

Closes #34993 from Yikun/SPARK-WSL-TEST.

Authored-by: Yikun Jiang <yikunkero@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/sql/utils.py', 'python/pyspark/testing/utils.py', 'python/pyspark/tests/test_appsubmit.py']","Running PySpark unit tests on WSL with PyCharm results in a KeyError for 'SPARK_HOME', hindering the execution of the tests."
03c9e8adeece2c371cef9e442f8630c52e09ecfb,1558496151,"[SPARK-24586][SQL] Upcast should not allow casting from string to other types

## What changes were proposed in this pull request?

When turning a Dataset to another Dataset, Spark will up cast the fields in the original Dataset to the type of corresponding fields in the target DataSet.

However, the current upcast behavior is a little weird, we don't allow up casting from string to numeric, but allow non-numeric types as the target, like boolean, date, etc.

As a result, `Seq(""str"").toDS.as[Int]` fails, but `Seq(""str"").toDS.as[Boolean]` works and throw NPE during execution.

The motivation of the up cast is to prevent things like runtime NPE, it's more reasonable to make up cast stricter.

This PR does 2 things:
1. rename `Cast.canSafeCast` to `Cast.canUpcast`, and support complex typres
2. remove `Cast.mayTruncate` and replace it with `!Cast.canUpcast`

Note that, the up cast change also affects persistent view resolution. But since we don't support changing column types of an existing table, there is no behavior change here.

## How was this patch tested?

new tests

Closes #21586 from cloud-fan/cast.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/view.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/types/DataTypeWriteCompatibilitySuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala']",Inconsistent upcast behaviour in Spark SQL allows non-numeric transformations from string to non-numeric types while not allowing numeric transformations – leads to misleading error occurrences.
b14bfc3f8e97479ac5927c071b00ed18f2104c95,1491970681,"[SPARK-19993][SQL] Caching logical plans containing subquery expressions does not work.

## What changes were proposed in this pull request?
The sameResult() method does not work when the logical plan contains subquery expressions.

**Before the fix**
```SQL
scala> val ds = spark.sql(""select * from s1 where s1.c1 in (select s2.c1 from s2 where s1.c1 = s2.c1)"")
ds: org.apache.spark.sql.DataFrame = [c1: int]

scala> ds.cache
res13: ds.type = [c1: int]

scala> spark.sql(""select * from s1 where s1.c1 in (select s2.c1 from s2 where s1.c1 = s2.c1)"").explain(true)
== Analyzed Logical Plan ==
c1: int
Project [c1#86]
+- Filter c1#86 IN (list#78 [c1#86])
   :  +- Project [c1#87]
   :     +- Filter (outer(c1#86) = c1#87)
   :        +- SubqueryAlias s2
   :           +- Relation[c1#87] parquet
   +- SubqueryAlias s1
      +- Relation[c1#86] parquet

== Optimized Logical Plan ==
Join LeftSemi, ((c1#86 = c1#87) && (c1#86 = c1#87))
:- Relation[c1#86] parquet
+- Relation[c1#87] parquet
```
**Plan after fix**
```SQL
== Analyzed Logical Plan ==
c1: int
Project [c1#22]
+- Filter c1#22 IN (list#14 [c1#22])
   :  +- Project [c1#23]
   :     +- Filter (outer(c1#22) = c1#23)
   :        +- SubqueryAlias s2
   :           +- Relation[c1#23] parquet
   +- SubqueryAlias s1
      +- Relation[c1#22] parquet

== Optimized Logical Plan ==
InMemoryRelation [c1#22], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   +- *BroadcastHashJoin [c1#1, c1#1], [c1#2, c1#2], LeftSemi, BuildRight
      :- *FileScan parquet default.s1[c1#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/dbiswal/mygit/apache/spark/bin/spark-warehouse/s1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c1:int>
      +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[0, int, true] as bigint), 32) | (cast(input[0, int, true] as bigint) & 4294967295))))
         +- *FileScan parquet default.s2[c1#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/dbiswal/mygit/apache/spark/bin/spark-warehouse/s2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c1:int>
```
## How was this patch tested?
New tests are added to CachedTableSuite.

Author: Dilip Biswal <dbiswal@us.ibm.com>

Closes #17330 from dilipbiswal/subquery_cache_final.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScanExec.scala']","Caching logical plans with subquery expressions fails due to ineffective sameResult() method, leading to inappropriate query optimization."
9c8592058cd1a5cc530fb30e6dc7c5c759ad528d,1686703102,"[SPARK-43684][SPARK-43685][SPARK-43686][SPARK-43691][CONNECT][PS] Fix `(NullOps|NumOps).(eq|ne)` for Spark Connect

### What changes were proposed in this pull request?

This PR proposes to fix `NullOps.(eq|ne)` and `NumOps.(eq|ne)` for pandas API on Spark with Spark Connect.

This includes SPARK-43684, SPARK-43685, SPARK-43686, SPARK-43691 at once, because they are all related similar modifications in single file.

This PR also introduce new util function `_is_extension_dtypes` to check whether the given object is a type of extension dtype or not, and apply to all related functions.

### Why are the changes needed?

The reason is that pandas API on Spark with Spark Connect operates differently from pandas as below:

**For `ne`:**
```python
>>> pser = pd.Series([1.0, 2.0, np.nan])
>>> psser = ps.from_pandas(pser)
>>> pser.ne(pser)
0    False
1    False
2     True
dtype: bool
>>> psser.ne(psser)
0    False
1    False
2     None
dtype: bool
```

We expect `True` for non-equal case, but it returns `None` in Spark Connect. So we should cast `None` to `True` for `ne`.

**For `eq`:**
```python
>>> pser = pd.Series([1.0, 2.0, np.nan])
>>> psser = ps.from_pandas(pser)
>>> pser.eq(pser)
0     True
1     True
2    False
dtype: bool
>>> psser.eq(psser)
0     True
1     True
2     None
dtype: bool
```

We expect `False` for non-equal case, but it returns `None` in Spark Connect. So we should cast `None` to `False` for `eq`.

### Does this PR introduce _any_ user-facing change?

Yes, `NullOps.eq`, `NullOps.ne`, `NumOps.eq`, `NumOps.ne` are now working as expected on Spark Connect.

### How was this patch tested?

Uncomment the UTs, tested manually for vanilla PySpark.

Closes #41514 from itholic/SPARK-43684.

Authored-by: itholic <haejoon.lee@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/data_type_ops/base.py', 'python/pyspark/pandas/data_type_ops/binary_ops.py', 'python/pyspark/pandas/data_type_ops/categorical_ops.py', 'python/pyspark/pandas/data_type_ops/datetime_ops.py', 'python/pyspark/pandas/data_type_ops/null_ops.py', 'python/pyspark/pandas/data_type_ops/num_ops.py', 'python/pyspark/pandas/data_type_ops/string_ops.py', 'python/pyspark/pandas/data_type_ops/timedelta_ops.py', 'python/pyspark/pandas/tests/connect/data_type_ops/test_parity_null_ops.py', 'python/pyspark/pandas/tests/connect/data_type_ops/test_parity_num_ops.py', 'python/pyspark/pandas/tests/data_type_ops/test_null_ops.py', 'python/pyspark/sql/utils.py']","Pandas API on Spark with Spark Connect behaves incorrectly when applying 'ne' and 'eq' operations on NullOps and NumOps, returning 'None' instead of 'True' or 'False'."
8a3815f7226f91a78c72c2f24450c21336f40de6,1618460572,"[SPARK-34789][TEST] Introduce Jetty based construct for integration tests where HTTP server is used

### What changes were proposed in this pull request?

Introducing a new test construct:
```
  withHttpServer() { baseURL =>
    ...
  }
```
Which starts and stops a Jetty server to serve files via HTTP.

Moreover this PR uses this new construct in the test `Run SparkRemoteFileTest using a remote data file`.

### Why are the changes needed?

Before this PR github URLs was used like ""https://raw.githubusercontent.com/apache/spark/master/data/mllib/pagerank_data.txt"".
This connects two Spark version in an unhealthy way like connecting the ""master"" branch which is moving part with the committed test code which is a non-moving (as it might be even released).
So this way a test running for an earlier version of Spark expects something (filename, content, path) from a the latter release and what is worse when the moving version is changed the earlier test will break.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing unit test.

Closes #31935 from attilapiros/SPARK-34789.

Authored-by: “attilapiros” <piros.attila.zsolt@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['core/src/main/scala/org/apache/spark/TestUtils.scala', 'resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala']","Integration tests using fixed URLs for data files, such as ""https://raw.githubusercontent.com/apache/spark/master/data/mllib/pagerank_data.txt"", may lead to test failures if the content, filename or path at these URLs change."
549e8aec24dbf5cc5fde20c46251cd63c1b1f262,1673915901,"[SPARK-42088][PYTHON][WINDOWS][BUILD] Improve setup.py adaptation for Windows

### What changes were proposed in this pull request?

Update the supports symlinks method in setup.py

### Why are the changes needed?

My system version is windows 10, and I can run setup.py with administrator permissions, so there will be no error. However, it may be troublesome for us to upgrade permissions with Windows Server, so we need to modify the code of setup.py to ensure no error. To avoid the hassle of compiling for the user, I suggest modifying the following code to enable the out-of-the-box effect

### Does this PR introduce _any_ user-facing change?

Such an update would allow users to build spark without dealing with windows specific situations

### How was this patch tested?

need to run setup.py with and without administrator permissions in windows to eventually build python packages properly

Closes #39603 from zekai-li/master.

Authored-by: zekai-li <58294989+zekai-li@users.noreply.github.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['python/setup.py'],"The current setup.py causes issues when run without administrator permissions on Windows, potentially impacting the building of spark and the out-of-the-box effect."
601fac2cb30b7dc2d033e7be5bbd2f6c3a3c5fc1,1554817546,"[SPARK-27411][SQL] DataSourceV2Strategy should not eliminate subquery

## What changes were proposed in this pull request?

In DataSourceV2Strategy, it seems we eliminate the subqueries by mistake after normalizing filters.
We have a sql with a scalar subquery:

``` scala
val plan = spark.sql(""select * from t2 where t2a > (select max(t1a) from t1)"")
plan.explain(true)
```

And we get the log info of DataSourceV2Strategy:
```
Pushing operators to csv:examples/src/main/resources/t2.txt
Pushed Filters:
Post-Scan Filters: isnotnull(t2a#30)
Output: t2a#30, t2b#31
```

The `Post-Scan Filters` should contain the scalar subquery, but we eliminate it by mistake.
```
== Parsed Logical Plan ==
'Project [*]
+- 'Filter ('t2a > scalar-subquery#56 [])
   :  +- 'Project [unresolvedalias('max('t1a), None)]
   :     +- 'UnresolvedRelation `t1`
   +- 'UnresolvedRelation `t2`

== Analyzed Logical Plan ==
t2a: string, t2b: string
Project [t2a#30, t2b#31]
+- Filter (t2a#30 > scalar-subquery#56 [])
   :  +- Aggregate [max(t1a#13) AS max(t1a)#63]
   :     +- SubqueryAlias `t1`
   :        +- RelationV2[t1a#13, t1b#14] csv:examples/src/main/resources/t1.txt
   +- SubqueryAlias `t2`
      +- RelationV2[t2a#30, t2b#31] csv:examples/src/main/resources/t2.txt

== Optimized Logical Plan ==
Filter (isnotnull(t2a#30) && (t2a#30 > scalar-subquery#56 []))
:  +- Aggregate [max(t1a#13) AS max(t1a)#63]
:     +- Project [t1a#13]
:        +- RelationV2[t1a#13, t1b#14] csv:examples/src/main/resources/t1.txt
+- RelationV2[t2a#30, t2b#31] csv:examples/src/main/resources/t2.txt

== Physical Plan ==
*(1) Project [t2a#30, t2b#31]
+- *(1) Filter isnotnull(t2a#30)
   +- *(1) BatchScan[t2a#30, t2b#31] class org.apache.spark.sql.execution.datasources.v2.csv.CSVScan
```
## How was this patch tested?

ut

Closes #24321 from francis0407/SPARK-27411.

Authored-by: francis0407 <hanmingcong123@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/v2/DataSourceV2Suite.scala']","DataSourceV2Strategy mistakenly eliminates subqueries after normalizing filters, causing scalar subqueries to be missing in the Post-Scan filters."
989eb6884d77226ab4f494a4237e09aea54a032d,1612514249,"[SPARK-34331][SQL] Speed up DS v2 metadata col resolution

### What changes were proposed in this pull request?

This is a follow-up of https://github.com/apache/spark/pull/28027

https://github.com/apache/spark/pull/28027 added a DS v2 API that allows data sources to produce metadata/hidden columns that can only be seen when it's explicitly selected. The way we integrate this API into Spark is:
1. The v2 relation gets normal output and metadata output from the data source, and the metadata output is excluded from the plan output by default.
2. column resolution can resolve `UnresolvedAttribute` with metadata columns, even if the child plan doesn't output metadata columns.
3. An analyzer rule searches the query plan, trying to find a node that has missing inputs. If such node is found, transform the sub-plan of this node, and update the v2 relation to include the metadata output.

The analyzer rule in step 3 brings a perf regression, for queries that do not read v2 tables at all. This rule will calculate `QueryPlan.inputSet` (which builds an `AttributeSet` from outputs of all children) and `QueryPlan.missingInput` (which does a set exclusion and creates a new `AttributeSet`) for every plan node in the query plan. In our benchmark, the TPCDS query compilation time gets increased by more than 10%

This PR proposes a simple way to improve it: we add a special metadata entry to the metadata attribute, which allows us to quickly check if a plan needs to add metadata columns: we just check all the references of this plan, and see if the attribute contains the special metadata entry, instead of calculating `QueryPlan.missingInput`.

This PR also fixes one bug: we should not change the final output schema of the plan, if we only use metadata columns in operators like filter, sort, etc.

### Why are the changes needed?

Fix perf regression in SQL query compilation, and fix a bug.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Run `org.apache.spark.sql.TPCDSQuerySuite`, before this PR, `AddMetadataColumns` is the top 4 rule ranked by running time
```
=== Metrics of Analyzer/Optimizer Rules ===
Total number of runs: 407641
Total time: 47.257239779 seconds

Rule                                  Effective Time / Total Time                     Effective Runs / Total Runs

OptimizeSubqueries                      4157690003 / 8485444626                         49 / 2778
Analyzer$ResolveAggregateFunctions      1238968711 / 3369351761                         49 / 2141
ColumnPruning                           660038236 / 2924755292                          338 / 6391
Analyzer$AddMetadataColumns             0 / 2918352992                                  0 / 2151
```
after this PR:
```
Analyzer$AddMetadataColumns             0 / 122885629                                   0 / 2151
```
This rule is 20 times faster and is negligible to the total compilation time.

This PR also add new tests to verify the bug fix.

Closes #31440 from cloud-fan/metadata-col.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryTable.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala']","Performance regression in SQL query compilation due to analyzer rule in DS v2 API metadata column resolution. It notably increases TPCDS query compilation time. Final output schema incorrectly changes when metadata columns used in operators like filter, sort, etc."
5b7f89cee8e9e95ca015fc88df4af4a5d3ff1096,1669708462,"[MINOR][PYTHON][DOCS] Fix types and docstring in DataFrame.toDF

### What changes were proposed in this pull request?

`df.toDF` cannot take `Column`s:

```python
>>> df.toDF(df.id)
```
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 4606, in toDF
    jdf = self._jdf.toDF(self._jseq(cols))
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 2413, in _jseq
    return _to_seq(self.sparkSession._sc, cols, converter)
  File ""/.../spark/python/pyspark/sql/column.py"", line 88, in _to_seq
    return sc._jvm.PythonUtils.toSeq(cols)
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1314, in __call__
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1277, in _build_args
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1264, in _get_args
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_collections.py"", line 511, in convert
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1314, in __call__
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1277, in _build_args
  File ""/.../forked/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1264, in _get_args
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_collections.py"", line 510, in convert
  File ""/.../spark/python/pyspark/sql/column.py"", line 622, in __iter__
    raise TypeError(""Column is not iterable"")
TypeError: Column is not iterable
```

This PR fixes the type and docstrings to remove the mention about `Column`

### Why are the changes needed?

To provide the correct documentation to the end users.

### Does this PR introduce _any_ user-facing change?

No for the main codes.
Yes for the docs.

### How was this patch tested?

CI in this PR should verify it via Python linters.

Closes #38834 from HyukjinKwon/minor-docs-todf.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['python/pyspark/sql/dataframe.py'],"The `df.toDF` function cannot handle `Column`s as input, causing a TypeError, and its documentation incorrectly states that it can."
d015eff16dbd56a7a9104e2d66885e2bb21eb111,1624063729,"[SPARK-35796][TESTS] Fix SparkSubmitSuite failure on MacOS 10.15+

### What changes were proposed in this pull request?
Change primaryResource assertion from exact match to suffix match in case SparkSubmitSuite.`handles k8s cluster mode`

### Why are the changes needed?
When I run SparkSubmitSuite on MacOs 10.15.7, I got AssertionError for `handles k8s cluster mode` test after pr [SPARK-35691](https://issues.apache.org/jira/browse/SPARK-35691), due to `File(path).getCanonicalFile().toURI()` function  with absolute path as parameter will return path begin with `/System/Volumes/Data` on MacOs higher tha 10.15.
eg.  `/home/testjars.jar` will get `file:/System/Volumes/Data/home/testjars.jar`

In order to pass UT on MacOs higher than 10.15, we change the assertion into suffix match

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
1. Pass the GitHub Action
2. Manually test
    - environment: MacOs > 10.15
    - commad: `build/mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -pl core -am -DwildcardSuites=org.apache.spark.deploy.SparkSubmitSuite -Dtest=none`
    - Test result:
        - before this pr, case failed with following exception:
        `- handles k8s cluster mode *** FAILED ***
  Some(""file:/System/Volumes/Data/home/thejar.jar"") was not equal to Some(""file:/home/thejar.jar"") (SparkSubmitSuite.scala:485)
  Analysis:
  Some(value: ""file:/[System/Volumes/Data/]home/thejar.jar"" -> ""file:/[]home/thejar.jar"")`
        - after this pr, run all test successfully

Closes #32948 from toujours33/SPARK-35796.

Authored-by: toujours33 <wangyazhi@baidu.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala'],"On MacOS 10.15 and higher, SparkSubmitSuite's `handles k8s cluster mode` test fails due to discrepancies in file path resolution. The problem arises when using absolute paths with the `File(path).getCanonicalFile().toURI()` function."
4a6005c795926929a7963ae8cc75cf553f69a9ed,1571516637,"[SPARK-29235][ML][PYSPARK] Support avgMetrics in read/write of CrossValidatorModel

### What changes were proposed in this pull request?
 Currently pyspark doesn't write/read `avgMetrics` in `CrossValidatorModel`, whereas scala supports it.

### Why are the changes needed?
 Test step to reproduce it:

```
dataset = spark.createDataFrame([(Vectors.dense([0.0]), 0.0),
     (Vectors.dense([0.4]), 1.0),
     (Vectors.dense([0.5]), 0.0),
      (Vectors.dense([0.6]), 1.0),
      (Vectors.dense([1.0]), 1.0)] * 10,
     [""features"", ""label""])
lr = LogisticRegression()
grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()
evaluator = BinaryClassificationEvaluator()
cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator,parallelism=2)
cvModel = cv.fit(dataset)
cvModel.write().save(""/tmp/model"")
cvModel2 = CrossValidatorModel.read().load(""/tmp/model"")
print(cvModel.avgMetrics) # prints non empty result as expected
print(cvModel2.avgMetrics) # Bug: prints an empty result.
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually tested

Before patch:
```
>>> cvModel.write().save(""/tmp/model_0"")
>>> cvModel2 = CrossValidatorModel.read().load(""/tmp/model_0"")
>>> print(cvModel2.avgMetrics)
[]
```

After patch:
```
>>> cvModel2 = CrossValidatorModel.read().load(""/tmp/model_2"")
>>> print(cvModel2.avgMetrics[0])
0.5
```

Closes #26038 from shahidki31/avgMetrics.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
",['python/pyspark/ml/tuning.py'],"'avgMetrics' in 'CrossValidatorModel' is not being properly read/written in PySpark, resulting in an inconsistency between PySpark and Scala. This results in empty results when attempting to print 'avgMetrics'."
b04330cd38e2817748ff50a7bf63b7145ea85cd4,1628531223,"[SPARK-36454][SQL] Not push down partition filter to ORCScan for DSv2

### What changes were proposed in this pull request?
not push down partition filter to `ORCScan` for DSv2

### Why are the changes needed?
Seems to me that partition filter is only used for partition pruning and shouldn't be pushed down to `ORCScan`. We don't push down partition filter to ORCScan in DSv1
```
== Physical Plan ==
*(1) Filter (isnotnull(value#19) AND NOT (value#19 = a))
+- *(1) ColumnarToRow
   +- FileScan orc [value#19,p1#20,p2#21] Batched: true, DataFilters: [isnotnull(value#19), NOT (value#19 = a)], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/private/var/folders/pt/_5f4sxy56x70dv9zpz032f0m0000gn/T/spark-c1..., PartitionFilters: [isnotnull(p1#20), isnotnull(p2#21), (p1#20 = 1), (p2#21 = 2)], PushedFilters: [IsNotNull(value), Not(EqualTo(value,a))], ReadSchema: struct<value:string>
```
Also, we don't push down partition filter for parquet in DSv2.
https://github.com/apache/spark/pull/30652

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing test suites

Closes #33680 from huaxingao/orc_filter.

Authored-by: Huaxin Gao <huaxin_gao@apple.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScanBuilder.scala', 'sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala']","Partition filters are being pushed down to `ORCScan` for DSv2 incorrectly, differing from DSv1 and Parquet's behaviour in DSv2. This might lead to inconsistent results across different data sources."
e3de6ab30d52890eb08578e55eb4a5d2b4e7aa35,1525825940,"[SPARK-24068] Propagating DataFrameReader's options to Text datasource on schema inferring

## What changes were proposed in this pull request?

While reading CSV or JSON files, DataFrameReader's options are converted to Hadoop's parameters, for example there:
https://github.com/apache/spark/blob/branch-2.3/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L302

but the options are not propagated to Text datasource on schema inferring, for instance:
https://github.com/apache/spark/blob/branch-2.3/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala#L184-L188

The PR proposes propagation of user's options to Text datasource on scheme inferring in similar way as user's options are converted to Hadoop parameters if schema is specified.

## How was this patch tested?
The changes were tested manually by using https://github.com/twitter/hadoop-lzo:

```
hadoop-lzo> mvn clean package
hadoop-lzo> ln -s ./target/hadoop-lzo-0.4.21-SNAPSHOT.jar ./hadoop-lzo.jar
```
Create 2 test files in JSON and CSV format and compress them:
```shell
$ cat test.csv
col1|col2
a|1
$ lzop test.csv
$ cat test.json
{""col1"":""a"",""col2"":1}
$ lzop test.json
```
Run `spark-shell` with hadoop-lzo:
```
bin/spark-shell --jars ~/hadoop-lzo/hadoop-lzo.jar
```
reading compressed CSV and JSON without schema:
```scala
spark.read.option(""io.compression.codecs"", ""com.hadoop.compression.lzo.LzopCodec"").option(""inferSchema"",true).option(""header"",true).option(""sep"",""|"").csv(""test.csv.lzo"").show()
+----+----+
|col1|col2|
+----+----+
|   a|   1|
+----+----+
```
```scala
spark.read.option(""io.compression.codecs"", ""com.hadoop.compression.lzo.LzopCodec"").option(""multiLine"", true).json(""test.json.lzo"").printSchema
root
 |-- col1: string (nullable = true)
 |-- col2: long (nullable = true)
```

Author: Maxim Gekk <maxim.gekk@databricks.com>
Author: Maxim Gekk <max.gekk@gmail.com>

Closes #21182 from MaxGekk/text-options.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala']",DataFrameReader's options are not being propagated to Text datasource during schema inferencing for CSV or JSON files.
af20474c67a61190829fa100a17ded58cb9a2102,1624221586,"[SPARK-35827][SQL] Show proper error message when update column types to year-month/day-time interval

### What changes were proposed in this pull request?

This PR fixes error message shown when changing a column type to year-month/day-time interval type is attempted.

### Why are the changes needed?

It's for consistent behavior.
Updating column types to interval types are prohibited for V2 source tables.
So, if we attempt to update the type of a column to the conventional interval type, an error message like `Error in query: Cannot update <table> field <column> to interval type;`.

But, for year-month/day-time interval types, another error message like `Error in query: Cannot update <table> field <column>:<type> cannot be cast to interval year;`.

You can reproduce with the following procedure.
```
$ bin/spark-sql
spark-sql> SET spark.sql.catalog.mycatalog=<a catalog implementation class>;
spark-sql> CREATE TABLE mycatalog.t1(c1 int) USING <V2 datasource implementation class>;
spark-sql> ALTER TABLE mycatalog.t1 ALTER COLUMN c1 TYPE interval year to month;
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Modified an existing test.

Closes #32978 from sarutak/err-msg-interval.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala']","When attempting to update column types to year-month/day-time interval for V2 source tables, the error message shown is inconsistent with that shown for conventional interval types."
c770daa9d468cf2c3cda117904478996cd88ebb9,1652283759,"[SPARK-39079][SQL] Forbid dot in V2 catalog name

### What changes were proposed in this pull request?

Forbid dot in V2 catalog name.

### Why are the changes needed?

In the following configuration, we define 2 catalogs `test`, `test.cat`.
```
spark.sql.catalog.test=CatalogTestImpl
spark.sql.catalog.test.k1=v1
spark.sql.catalog.test.cat.k1=CatalogTestImpl
spark.sql.catalog.test.cat.k1=v1
```

In the current implementation, three keys will be treated to `test`'s configuration, it's a little bit weird.
```
k1=v1
cat.k1=CatalogTestImpl
cat.k1=v1
```

Besides, the current implementation of `SHOW CATALOGS` use lazy load strategy, it's not friendly for GUI client like DBeaver to display available catalog list, w/o this restriction, it is hard to find the catalog key in configuration to load catalogs eagerly.

### Does this PR introduce _any_ user-facing change?

Yes, previously Spark allows the catalog name contain `.` but not after this change.

### How was this patch tested?

New UT.

Closes #36418 from pan3793/catalog.

Authored-by: Cheng Pan <chengpan@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala', 'sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java']","The use of a dot in V2 catalog name causes confusion in configuration, leading to improper key allocation and loading issues, particularly unfriendly for GUI client operations."
324275ae8350ec15844ce384f40f1ecc4acdc072,1604601522,"[SPARK-33185][YARN] Set up yarn.Client to print direct links to driver stdout/stderr

### What changes were proposed in this pull request?
Currently when run in `cluster` mode on YARN, the Spark `yarn.Client` will print out the application report into the logs, to be easily viewed by users. For example:
```
INFO yarn.Client:
 	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
 	 diagnostics: N/A
 	 ApplicationMaster host: X.X.X.X
 	 ApplicationMaster RPC port: 0
 	 queue: default
 	 start time: 1602782566027
 	 final status: UNDEFINED
 	 tracking URL: http://hostname:8888/proxy/application_<id>/
 	 user: xkrogen
```

I propose adding, alongside the application report, some additional lines like:
```
         Driver Logs (stdout): http://hostname:8042/node/containerlogs/container_<id>/xkrogen/stdout?start=-4096
         Driver Logs (stderr): http://hostname:8042/node/containerlogs/container_<id>/xkrogen/stderr?start=-4096
```

This information isn't contained in the `ApplicationReport`, so it's necessary to query the ResourceManager REST API. For now I have added this as an always-on feature, but if there is any concern about adding this REST dependency, I think hiding this feature behind an off-by-default flag is reasonable.

### Why are the changes needed?
Typically, the tracking URL can be used to find the logs of the ApplicationMaster/driver while the application is running. Later, the Spark History Server can be used to track this information down, using the stdout/stderr links on the Executors page.

However, in the situation when the driver crashed _before_ writing out a history file, the SHS may not be aware of this application, and thus does not contain links to the driver logs. When this situation arises, it can be difficult for users to debug further, since they can't easily find their driver logs.

It is possible to reach the logs by using the `yarn logs` commands, but the average Spark user isn't aware of this and shouldn't have to be.

With this information readily available in the logs, users can quickly jump to their driver logs, even if it crashed before the SHS became aware of the application. This has the additional benefit of providing a quick way to access driver logs, which often contain useful information, in a single click (instead of navigating through the Spark UI).

### Does this PR introduce _any_ user-facing change?
Yes, some additional print statements will be created in the application report when using YARN in cluster mode.

### How was this patch tested?
Added unit tests for the parsing logic in `yarn.ClientSuite`. Also tested against a live cluster. When the driver is running:
```
INFO Client: Application report for application_XXXXXXXXX_YYYYYY (state: RUNNING)
INFO Client:
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: host.example.com
         ApplicationMaster RPC port: ######
         queue: queue_name
         start time: 1604529046091
         final status: UNDEFINED
         tracking URL: http://host.example.com:8080/proxy/application_XXXXXXXXX_YYYYYY/
         user: xkrogen
         Driver Logs (stdout): http://host.example.com:8042/node/containerlogs/container_e07_XXXXXXXXX_YYYYYY_01_000001/xkrogen/stdout?start=-4096
         Driver Logs (stderr): http://host.example.com:8042/node/containerlogs/container_e07_XXXXXXXXX_YYYYYY_01_000001/xkrogen/stderr?start=-4096
INFO Client: Application report for application_XXXXXXXXX_YYYYYY (state: RUNNING)
```
I confirmed that when the driver has not yet launched, the report does not include the two Driver Logs items. Will omit the output here for brevity since it looks the same.

Closes #30096 from xkrogen/xkrogen-SPARK-33185-yarn-client-print.

Authored-by: Erik Krogen <xkrogen@apache.org>
Signed-off-by: Mridul Muralidharan <mridul<at>gmail.com>
","['resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala', 'resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala', 'resource-managers/yarn/src/main/scala/org/apache/spark/util/YarnContainerInfoHelper.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala']","When running in YARN cluster mode, in the event of a driver crash prior to writing out a history file, it becomes difficult for users to locate their driver logs for debugging since Spark History Server doesn't record this information."
e08af6d753ea8478a56dde24f9fef678ac9cb9dd,1673266306,"[SPARK-41945][CONNECT][PYTHON] Python: connect client lost column data with pyarrow.Table.to_pylist

### What changes were proposed in this pull request?
Python: connect client should not use pyarrow.Table.to_pylist to transform fetched data.
For example:
the data in pyarrow.Table show below.

```
pyarrow.Table
key: string
order: int64
nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW): string
nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW): string
nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW): string
----
key: [[""a"",""a"",""a"",""a"",""a"",""b"",""b""]]
order: [[0,1,2,3,4,1,2]]
nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW): [[null,""x"",""x"",""x"",""x"",null,null]]
nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW): [[null,""x"",""x"",""x"",""x"",null,null]]
nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW): [[null,null,""y"",""y"",""y"",null,null]]
```

The table have five columns show above.
But the data after call pyarrow.Table.to_pylist() show below.

```
[{
	'key': 'a',
	'order': 0,
	'nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': None,
	'nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': None
}, {
	'key': 'a',
	'order': 1,
	'nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': 'x',
	'nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': None
}, {
	'key': 'a',
	'order': 2,
	'nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': 'x',
	'nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': 'y'
}, {
	'key': 'a',
	'order': 3,
	'nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': 'x',
	'nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': 'y'
}, {
	'key': 'a',
	'order': 4,
	'nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': 'x',
	'nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': 'y'
}, {
	'key': 'b',
	'order': 1,
	'nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': None,
	'nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': None
}, {
	'key': 'b',
	'order': 2,
	'nth_value(value, 2) OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': None,
	'nth_value(value, 2) ignore nulls OVER (PARTITION BY key ORDER BY order ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)': None
}]
```

There are only four columns left.

### Why are the changes needed?
Fix the bug that connect client lost column data with `pyarrow.Table.to_pylist`.

### Does this PR introduce _any_ user-facing change?
'No'.
New feature.

### How was this patch tested?
Manual tests.

Closes #39461 from beliefer/SPARK-41945.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/sql/connect/dataframe.py', 'python/pyspark/sql/tests/connect/test_connect_basic.py', 'python/pyspark/sql/tests/connect/test_parity_functions.py']",Python's connect client using `pyarrow.Table.to_pylist` to transform fetched data leads to loss of column data.
2fa98a6a0971cafeb98a2148a962cf8d79381842,1655363123,"[SPARK-39490][K8S] Support `ipFamilyPolicy` and `ipFamilies` in Driver Service

### What changes were proposed in this pull request?

This PR aims to support `iFamilyPolicy` and `ipFamilies` K8s Service feature in Spark Driver Service in order to support `IPv6`-only environment. After this PR, we can control `ipFamilyPolicy` and `ipFamilies`.

```yaml
$ kubectl get svc spark-xxx-driver-svc -oyaml
apiVersion: v1
kind: Service
metadata:
  ...
spec:
  clusterIP: None
  ipFamilyPolicy: SingleStack
  ipFamilies:
  - IPv4
...
```

### Why are the changes needed?

K8s IPv4/IPv6 dual-stack Feature reached `Stable` stage at v1.23.
- https://kubernetes.io/docs/concepts/services-networking/dual-stack/
  - v1.16 [alpha]
  - v1.21 [beta]
  - v1.23 [stable]

According to [EKS milestone](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html), K8s v1.23 will be GA on August.

Kubernetes version | Upstream release | Amazon EKS release | Amazon EKS end of support
-- | -- | -- | --
1.20 | December 8, 2020 | May 18, 2021 | September 2022
1.21 | April 8, 2021 | July 19, 2021 | February 2023
1.22 | August 4, 2021 | April 4, 2022 | May 2023
1.23 | December 7, 2021 | August 2022 | October 2023

Note that
- [EKS started IPv6 since January 2022.](https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/)
- [Docker Desktop supports IPv6 only on Linux.](https://docs.docker.com/config/daemon/ipv6/)
- [Minikube doesn't support IPv6 yet](https://github.com/kubernetes/minikube/issues/8535) unfortunately.

### Does this PR introduce _any_ user-facing change?

Yes, this is a new feature.

### How was this patch tested?

Pass the CIs with newly added test cases.

Closes #36887 from dongjoon-hyun/SPARK-39490.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala', 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala', 'resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStepSuite.scala']","Spark Driver Service in K8s doesn't support `ipFamilyPolicy` and `ipFamilies` Service feature, limiting the functionality in IPv6-only environments."
8e5f9995cad409799f3646b3d03761a771ea1664,1551409468,"[SPARK-27008][SQL] Support java.time.LocalDate as an external type of DateType

## What changes were proposed in this pull request?

In the PR, I propose to add new Catalyst type converter for `DateType`. It should be able to convert `java.time.LocalDate` to/from `DateType`.

Main motivations for the changes:
- Smoothly support Java 8 time API
- Avoid inconsistency of calendars used inside of Spark 3.0 (Proleptic Gregorian calendar) and `java.sql.Date` (hybrid calendar - Julian + Gregorian).
- Make conversion independent from current system timezone.

By default, Spark converts values of `DateType` to `java.sql.Date` instances but the SQL config `spark.sql.datetime.java8API.enabled` can change the behavior. If it is set to `true`, Spark uses `java.time.LocalDate` as external type for `DateType`.

## How was this patch tested?

Added new testes to `CatalystTypeConvertersSuite` to check conversion of `DateType` to/from `java.time.LocalDate`, `JavaUDFSuite`/ `UDFSuite` to test usage of `LocalDate` type in Scala/Java UDFs.

Closes #23913 from MaxGekk/date-localdate.

Authored-by: Maxim Gekk <maxim.gekk@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/DeserializerBuildHelper.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/CatalystTypeConvertersSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/RowEncoderSuite.scala', 'sql/core/src/test/java/test/org/apache/spark/sql/JavaUDFSuite.java', 'sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala']","The current Catalyst type converter for `DateType` struggles with the new Java 8 time API and the hybrid calendar system used in `java.sql.Date`, resulting in inconsistencies and timezone-dependent conversions."
42721120f3c7206a9fc22db5d0bb7cf40f0cacfd,1673631636,"[SPARK-42045][SQL] ANSI SQL mode: Round/Bround should return an error on integer overflow

### What changes were proposed in this pull request?

In ANSI SQL mode, Round/Bround should return an error on integer overflow.
Note this PR is for integer only. Once it is merge, I will create one follow-up PR for all the rest integral types: byte, short, and long.
Also, the function ceil and floor accepts decimal type input, so there is no need to change them.

### Why are the changes needed?

In ANSI SQL mode, integer overflow should cause error instead of returning an unreasonable result.
For example, `round(2147483647, -1)` should return error instead of returning `-2147483646`

### Does this PR introduce _any_ user-facing change?

Yes, in ANSI SQL mode, SQL function Round and Bround will return an error on integer overflow

### How was this patch tested?

UT

Closes #39546 from gengliangwang/fixRound.

Authored-by: Gengliang Wang <gengliang@apache.org>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MathUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/PhysicalAggregationSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/ansi/math.sql', 'sql/core/src/test/resources/sql-tests/inputs/math.sql']","In ANSI SQL mode, functions like Round and Bround are not returning an error on integer overflow resulting in unreasonable results such as `round(2147483647, -1)` returning `-2147483646`."
aad2125475dcdeb4a0410392b6706511db17bac4,1504744680,"Fixed pandoc dependency issue in python/setup.py

## Problem Description

When pyspark is listed as a dependency of another package, installing
the other package will cause an install failure in pyspark. When the
other package is being installed, pyspark's setup_requires requirements
are installed including pypandoc. Thus, the exception handling on
setup.py:152 does not work because the pypandoc module is indeed
available. However, the pypandoc.convert() function fails if pandoc
itself is not installed (in our use cases it is not). This raises an
OSError that is not handled, and setup fails.

The following is a sample failure:
```
$ which pandoc
$ pip freeze | grep pypandoc
pypandoc==1.4
$ pip install pyspark
Collecting pyspark
  Downloading pyspark-2.2.0.post0.tar.gz (188.3MB)
    100% |████████████████████████████████| 188.3MB 16.8MB/s
    Complete output from command python setup.py egg_info:
    Maybe try:

        sudo apt-get install pandoc
    See http://johnmacfarlane.net/pandoc/installing.html
    for installation options
    ---------------------------------------------------------------

    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-build-mfnizcwa/pyspark/setup.py"", line 151, in <module>
        long_description = pypandoc.convert('README.md', 'rst')
      File ""/home/tbeck/.virtualenvs/cem/lib/python3.5/site-packages/pypandoc/__init__.py"", line 69, in convert
        outputfile=outputfile, filters=filters)
      File ""/home/tbeck/.virtualenvs/cem/lib/python3.5/site-packages/pypandoc/__init__.py"", line 260, in _convert_input
        _ensure_pandoc_path()
      File ""/home/tbeck/.virtualenvs/cem/lib/python3.5/site-packages/pypandoc/__init__.py"", line 544, in _ensure_pandoc_path
        raise OSError(""No pandoc was found: either install pandoc and add it\n""
    OSError: No pandoc was found: either install pandoc and add it
    to your PATH or or call pypandoc.download_pandoc(...) or
    install pypandoc wheels with included pandoc.

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-mfnizcwa/pyspark/
```

## What changes were proposed in this pull request?

This change simply adds an additional exception handler for the OSError
that is raised. This allows pyspark to be installed client-side without requiring pandoc to be installed.

## How was this patch tested?

I tested this by building a wheel package of pyspark with the change applied. Then, in a clean virtual environment with pypandoc installed but pandoc not available on the system, I installed pyspark from the wheel.

Here is the output

```
$ pip freeze | grep pypandoc
pypandoc==1.4
$ which pandoc
$ pip install --no-cache-dir ../spark/python/dist/pyspark-2.3.0.dev0-py2.py3-none-any.whl
Processing /home/tbeck/work/spark/python/dist/pyspark-2.3.0.dev0-py2.py3-none-any.whl
Requirement already satisfied: py4j==0.10.6 in /home/tbeck/.virtualenvs/cem/lib/python3.5/site-packages (from pyspark==2.3.0.dev0)
Installing collected packages: pyspark
Successfully installed pyspark-2.3.0.dev0
```

Author: Tucker Beck <tucker.beck@rentrakmail.com>

Closes #18981 from dusktreader/dusktreader/fix-pandoc-dependency-issue-in-setup_py.
",['python/setup.py'],'pyspark installation fails when listed as a dependency of another package due to an unhandled OSError raised from pypandoc.convert() function if pandoc is not installed.'
03161055de0c132070354407160553363175c4d7,1613444006,"[SPARK-34424][SQL][TESTS] Fix failures of HiveOrcHadoopFsRelationSuite

### What changes were proposed in this pull request?
Modify `RandomDataGenerator.forType()` to allow generation of dates/timestamps that are valid in both Julian and Proleptic Gregorian calendars. Currently, the function can produce a date (for example `1582-10-06`) which is valid in the Proleptic Gregorian calendar. Though it cannot be saved to ORC files AS IS since ORC format (ORC libs in fact) assumes Julian calendar. So, Spark shifts `1582-10-06` to the next valid date `1582-10-15` while saving it to ORC files. And as a consequence of that, the test fails because it compares original date `1582-10-06` and the date `1582-10-15` loaded back from the ORC files.

In this PR, I propose to generate valid dates/timestamps in both calendars for ORC datasource till SPARK-34440 is resolved.

### Why are the changes needed?
The changes fix failures of `HiveOrcHadoopFsRelationSuite`. For instance, the test ""test all data types"" fails with the seed **610710213676**:
```
== Results ==
!== Correct Answer - 20 ==    == Spark Answer - 20 ==
 struct<index:int,col:date>   struct<index:int,col:date>
...
![9,1582-10-06]               [9,1582-10-15]
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
By running the modified test suite:
```
$ build/sbt -Phive -Phive-thriftserver ""test:testOnly *HiveOrcHadoopFsRelationSuite""
```

Closes #31552 from MaxGekk/fix-HiveOrcHadoopFsRelationSuite.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/test/scala/org/apache/spark/sql/RandomDataGenerator.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/sources/HadoopFsRelationTest.scala']","RandomDataGenerator.forType() generates dates that are only valid in the Proleptic Gregorian calendar, causing an error when writing and reading ORC files due to calendar incompatibilities. This results in test failures in the suite 'HiveOrcHadoopFsRelationSuite'."
9b875ceada60732899053fbd90728b4944d1c03d,1612979926,"[SPARK-32953][PYTHON][SQL] Add Arrow self_destruct support to toPandas

### What changes were proposed in this pull request?

Creating a Pandas dataframe via Apache Arrow currently can use twice as much memory as the final result, because during the conversion, both Pandas and Arrow retain a copy of the data. Arrow has a ""self-destruct"" mode now (Arrow >= 0.16) to avoid this, by freeing each column after conversion. This PR integrates support for this in toPandas, handling a couple of edge cases:

self_destruct has no effect unless the memory is allocated appropriately, which is handled in the Arrow serializer here. Essentially, the issue is that self_destruct frees memory column-wise, but Arrow record batches are oriented row-wise:

```
Record batch 0: allocation 0: column 0 chunk 0, column 1 chunk 0, ...
Record batch 1: allocation 1: column 0 chunk 1, column 1 chunk 1, ...
```

In this scenario, Arrow will drop references to all of column 0's chunks, but no memory will actually be freed, as the chunks were just slices of an underlying allocation. The PR copies each column into its own allocation so that memory is instead arranged as so:

```
Record batch 0: allocation 0 column 0 chunk 0, allocation 1 column 1 chunk 0, ...
Record batch 1: allocation 2 column 0 chunk 1, allocation 3 column 1 chunk 1, ...
```

The optimization is disabled by default, and can be enabled with the Spark SQL conf ""spark.sql.execution.arrow.pyspark.selfDestruct.enabled"" set to ""true"". We can't always apply this optimization because it's more likely to generate a dataframe with immutable buffers, which Pandas doesn't always handle well, and because it is slower overall (since it only converts one column at a time instead of in parallel).

### Why are the changes needed?

This lets us load larger datasets - in particular, with N bytes of memory, before we could never load a dataset bigger than N/2 bytes; now the overhead is more like N/1.25 or so.

### Does this PR introduce _any_ user-facing change?

Yes - it adds a new SQL conf ""spark.sql.execution.arrow.pyspark.selfDestruct.enabled""

### How was this patch tested?

See the [mailing list](http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Reducing-memory-usage-of-toPandas-with-Arrow-quot-self-destruct-quot-option-td30149.html) - it was tested with Python memory_profiler. Unit tests added to check memory within certain bounds and correctness with the option enabled.

Closes #29818 from lidavidm/spark-32953.

Authored-by: David Li <li.davidm96@gmail.com>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>
","['python/pyspark/sql/pandas/conversion.py', 'python/pyspark/sql/tests/test_arrow.py', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala']","Creating a Pandas dataframe via Apache Arrow results in twice the memory consumption during the conversion, as both Pandas and Arrow retain a copy of the data. This inhibits the loading of datasets larger than half of available memory."
7ef6a2e948062eb5fd30bf8154f1c5d71f5f18a3,1636351301,"[SPARK-37231][SQL] Dynamic writes/reads of ANSI interval partitions

### What changes were proposed in this pull request?
In the PR, I propose to check that ANSI intervals can be written as partitions dynamically, and read back as intervals if the  schema is set correctly. The last one requires a fix in `PartitioningUtils. AnsiIntervalType` to handle ANSI interval types and cast partition value strings to desired interval types.

### Why are the changes needed?
1. To check that ANSI intervals can be saved as partitions values
2. To fix loading of ANSI intervals as partitions values. The example below demonstrates the issue:
```scala
scala> sql(""SELECT INTERVAL '1' YEAR AS i, 0 as id"").write.partitionBy(""i"").json(""/Users/maximgekk/tmp/ansi_interval_json"")

scala> spark.read.schema(""i INTERVAL YEAR, id INT"").json(""/Users/maximgekk/tmp/ansi_interval_json"")
java.lang.RuntimeException: Failed to cast value `INTERVAL %271%27 YEAR` to `YearMonthIntervalType(0,0)` for partition column `i`
  at org.apache.spark.sql.errors.QueryExecutionErrors$.failedToCastValueToDataTypeForPartitionColumnError(QueryExecutionErrors.scala:593)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$.$anonfun$parsePartitions$13(PartitioningUtils.scala:204)
```

### Does this PR introduce _any_ user-facing change?
No, this PR fixes the issues above. After the changes, the code works as expected:
```scala
scala> spark.read.schema(""i INTERVAL YEAR, id INT"").json(""/Users/maximgekk/tmp/ansi_interval_json"").show(false)
+---+-----------------+
|id |i                |
+---+-----------------+
|0  |INTERVAL '1' YEAR|
+---+-----------------+
```

### How was this patch tested?
By running new test:
```
$ build/sbt ""test:testOnly *PartitionedWriteSuite""
```

Closes #34506 from MaxGekk/ansi-intervals-as-part-values.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/PartitionedWriteSuite.scala']",Writing and reading ANSI intervals as partition values dynamically causes a failure in casting the partition value strings to the expected interval types.
664a1719de2855d913c3bb1d2a94bd8681bc1a0d,1600435473,"[SPARK-32936][SQL] Pass all `external/avro` module UTs in Scala 2.13

### What changes were proposed in this pull request?
This pr fix all 14 failed cases in `external/avro` module in Scala 2.13, the main change of this pr as follow:

- Manual call `toSeq` in `AvroDeserializer#newWriter` and `SchemaConverters#toSqlTypeHelper` method because the object  type for case match is `ArrayBuffer` not `Seq` in Scala 2.13

- Specified `Seq` to `s.c.Seq` when we call `Row.get(i).asInstanceOf[Seq]` because the data maybe `mutable.ArraySeq` but `Seq` is `immutable.Seq` in Scala 2.13

### Why are the changes needed?
We need to support a Scala 2.13 build.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

- Scala 2.12: Pass the Jenkins or GitHub Action

- Scala 2.13: Pass 2.13 Build GitHub Action and do the following:

```
dev/change-scala-version.sh 2.13
mvn clean install -DskipTests  -pl external/avro -Pscala-2.13 -am
mvn clean test -pl external/avro -Pscala-2.13
```

**Before**
```
Tests: succeeded 197, failed 14, canceled 0, ignored 2, pending 0
*** 14 TESTS FAILED ***
```

**After**

```
Tests: succeeded 211, failed 0, canceled 0, ignored 2, pending 0
All tests passed.
```

Closes #29801 from LuciferYang/fix-external-avro-213.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala', 'external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala', 'external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala']",The `external/avro` unit tests are failing in Scala 2.13 due to mismatched expectations of object types and sequence mutability.
3359bc4d24e988320956ae3d79ee34dd3fe7559f,1677716293,"[SPARK-42613][CORE][PYTHON][YARN] PythonRunner should set OMP_NUM_THREADS to task cpus instead of executor cores by default

### What changes were proposed in this pull request?

Set OMP_NUM_THREADS to `spark.task.cpus` instead of `spark.executor.cores` by default.

### Why are the changes needed?

If OMP_NUM_THREADS is set to executor cores, we might still have issues when the number of executer cores is large but the number of task cpus is 1.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Run the following PySpark UDF script with these 2 Spark properties:

- spark.executor.cores=3
- spark.task.cpus=1

```python
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf

spark = SparkSession.builder.getOrCreate()

var_name = 'OMP_NUM_THREADS'

def get_env_var():
  return os.getenv(var_name)

udf_get_env_var = udf(get_env_var)
spark.range(1).toDF(""id"").withColumn(f""env_{var_name}"", udf_get_env_var()).show(truncate=False)
```

Output with release `3.3.0`:

```
+---+-------------------+
|id |env_OMP_NUM_THREADS|
+---+-------------------+
|0  |3                  |
+---+-------------------+
```

After the fix:

```
+---+-------------------+
|id |env_OMP_NUM_THREADS|
+---+-------------------+
|0  |1                  |
+---+-------------------+
```

Closes #40212 from jzhuge/SPARK-42613.

Authored-by: John Zhuge <jzhuge@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala'],OMP_NUM_THREADS is incorrectly being set to the number of executor cores which is causing issues when the number of executor cores is large but the number of task cpus is 1.
a418548dad57775fbb10b4ea690610bad1a8bfb0,1597214350,"[SPARK-31703][SQL] Parquet RLE float/double are read incorrectly on big endian platforms

### What changes were proposed in this pull request?
This PR fixes the issue introduced during SPARK-26985.

SPARK-26985 changes the `putDoubles()` and `putFloats()` methods to respect the platform's endian-ness.  However, that causes the RLE paths in VectorizedRleValuesReader.java to read the RLE entries in parquet as BIG_ENDIAN on big endian platforms (i.e., as is), even though parquet data is always in little endian format.

The comments in `WriteableColumnVector.java` say those methods are used for ""ieee formatted doubles in platform native endian"" (or floats), but since the data in parquet is always in little endian format, use of those methods appears to be inappropriate.

To demonstrate the problem with spark-shell:

```scala
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._

var data = Seq(
  (1.0, 0.1),
  (2.0, 0.2),
  (0.3, 3.0),
  (4.0, 4.0),
  (5.0, 5.0))

var df = spark.createDataFrame(data).write.mode(SaveMode.Overwrite).parquet(""/tmp/data.parquet2"")
var df2 = spark.read.parquet(""/tmp/data.parquet2"")
df2.show()
```

result:

```scala
+--------------------+--------------------+
|                  _1|                  _2|
+--------------------+--------------------+
|           3.16E-322|-1.54234871366845...|
|         2.0553E-320|         2.0553E-320|
|          2.561E-320|          2.561E-320|
|4.66726145843124E-62|         1.0435E-320|
|        3.03865E-319|-1.54234871366757...|
+--------------------+--------------------+
```

Also tests in ParquetIOSuite that involve float/double data would fail, e.g.,

- basic data types (without binary)
- read raw Parquet file

/examples/src/main/python/mllib/isotonic_regression_example.py would fail as well.

Purposed code change is to add `putDoublesLittleEndian()` and `putFloatsLittleEndian()` methods for parquet to invoke, just like the existing `putIntsLittleEndian()` and `putLongsLittleEndian()`.  On little endian platforms they would call `putDoubles()` and `putFloats()`, on big endian they would read the entries as little endian like pre-SPARK-26985.

No new unit-test is introduced as the existing ones are actually sufficient.

### Why are the changes needed?
RLE float/double data in parquet files will not be read back correctly on big endian platforms.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
All unit tests (mvn test) were ran and OK.

Closes #29383 from tinhto-000/SPARK-31703.

Authored-by: Tin Hang To <tinto@us.ibm.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java', 'sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java', 'sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java', 'sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java', 'sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala']","On big endian platforms, RLE float/double data in parquet files is read incorrectly due to platform's endian-ness being respected in `putDoubles()` and `putFloats()` methods."
a0c029c18b0e01cd593cb4dc47c7869d36599592,1692763991,"[SPARK-44878][SS] Disable strict limit for RocksDB write manager to avoid insertion exception on cache full

### What changes were proposed in this pull request?
Disable strict limit for RocksDB write manager to avoid insertion exception on cache full

### Why are the changes needed?
In some cases, if the memory limit is reached, on insert/get, we are seeing the following exception

```
org.apache.spark.SparkException: Job aborted due to stage failure: Task 42 in stage 9.0 failed 4 times, most recent failure: Lost task 42.3 in stage 9.0 (TID 2950) (96.104.176.55 executor 0): org.rocksdb.RocksDBException: Insert failed due to LRU cache being full.
	at org.rocksdb.RocksDB.get(Native Method)
	at org.rocksdb.RocksDB.get(RocksDB.java:2053)
	at org.apache.spark.sql.execution.streaming.state.RocksDB.get(RocksDB.scala:299)
	at org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider$RocksDBStateStore.get(RocksDBStateStoreProvider.scala:55)
```

It seems this is being thrown with strict memory limit within RocksDB here - https://github.com/facebook/rocksdb/blob/0fa0c97d3e9ac5dfc2e7ae94834b0850cdef5df7/cache/lru_cache.cc#L394

It seems this issue can only happen with the strict mode as described here - https://github.com/facebook/rocksdb/issues/5048#issuecomment-470788792

Seems like there is a pending issue for RocksDB around this as well - https://github.com/facebook/rocksdb/issues/8670

There is probably a relevant fix, but not sure whether this addresses the issue completely - https://github.com/facebook/rocksdb/pull/6619
(cc - siying )

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing tests

Closes #42567 from anishshri-db/task/SPARK-44878.

Authored-by: Anish Shrigondekar <anish.shrigondekar@databricks.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
",['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBMemoryManager.scala'],"Reaching memory limit in RocksDB triggers an exception on insert/get during execution, causing task failures due to full LRU cache.
"
0094b5fe727823e8f90c21825edf910e63364281,1565977152,"[SPARK-28722][ML] Change sequential label sorting in StringIndexer fit to parallel

## What changes were proposed in this pull request?

The `fit` method in `StringIndexer` sorts given labels in a sequential approach, if there are multiple input columns. When the number of input column increases, the time of label sorting dramatically increases too so it is hard to use in practice if dealing with hundreds of input columns.

This patch tries to make the label sorting parallel.

This runs benchmark like:
```scala
import org.apache.spark.ml.feature.StringIndexer

val numCol = 300

val data = (0 to 100).map { i =>
  (i, 100 * i)
}
var df = data.toDF(""id"", ""label0"")
(1 to numCol).foreach { idx =>
  df = df.withColumn(s""label$idx"", col(""label0"") + 1)
}
val inputCols = (0 to numCol).map(i => s""label$i"").toArray
val outputCols = (0 to numCol).map(i => s""labelIndex$i"").toArray
val t0 = System.nanoTime()
val indexer = new StringIndexer().setInputCols(inputCols).setOutputCols(outputCols).setStringOrderType(""alphabetDesc"").fit(df)
val t1 = System.nanoTime()
println(""Elapsed time: "" + (t1 - t0) / 1000000000.0 + ""s"")
```

| numCol  | 20 | 50  | 100  | 200  | 300 |
|--:|---|---|---|---|---|
|  Before |  9.85 |  28.62 | 64.35  | 167.17  | 431.60 |
| After  | 2.44  | 2.71  | 3.34  | 4.83  | 6.90 |

Unit: second

## How was this patch tested?

Passed existing tests. Manually test for performance.

Closes #25442 from viirya/improve_stringindexer2.

Authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
",['mllib/src/main/scala/org/apache/spark/ml/feature/StringIndexer.scala'],"StringIndexer's fit method sorts labels sequentially resulting in dramatic increase in sort time as the number of input columns increases, affecting usability when handling hundreds of input columns."
c0d84e6cf1046b7944796038414ef21fe9c7e3b5,1626797816,"[SPARK-36222][SQL] Step by days in the Sequence expression for dates

### What changes were proposed in this pull request?
The current implement of `Sequence` expression not support step by days for dates.
```
spark-sql> select sequence(date'2021-07-01', date'2021-07-10', interval '3' day);
Error in query: cannot resolve 'sequence(DATE '2021-07-01', DATE '2021-07-10', INTERVAL '3' DAY)' due to data type mismatch:
sequence uses the wrong parameter type. The parameter type must conform to:
1. The start and stop expressions must resolve to the same type.
2. If start and stop expressions resolve to the 'date' or 'timestamp' type
then the step expression must resolve to the 'interval' or
'interval year to month' or 'interval day to second' type,
otherwise to the same type as the start and stop expressions.
         ; line 1 pos 7;
'Project [unresolvedalias(sequence(2021-07-01, 2021-07-10, Some(INTERVAL '3' DAY), Some(Europe/Moscow)), None)]
+- OneRowRelation
```

### Why are the changes needed?
`DayTimeInterval` has day granularity should as step for dates.

### Does this PR introduce _any_ user-facing change?
'Yes'.
Sequence expression will supports step by `DayTimeInterval` has day granularity for dates.

### How was this patch tested?
New tests.

Closes #33439 from beliefer/SPARK-36222.

Authored-by: gengjiaan <gengjiaan@360.cn>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala']",The 'Sequence' expression currently does not support 'step by days' for dates resulting in a data type mismatch error upon execution.
c1a5f94973213b1cad15388f3ef8a488424c34a7,1575652936,"[SPARK-30112][SQL] Allow insert overwrite same table if using dynamic partition overwrite

### What changes were proposed in this pull request?

This patch proposes to allow insert overwrite same table if using dynamic partition overwrite.

### Why are the changes needed?

Currently, Insert overwrite cannot overwrite to same table even it is dynamic partition overwrite. But for dynamic partition overwrite, we do not delete partition directories ahead. We write to staging directories and move data to final partition directories. We should be able to insert overwrite to same table under dynamic partition overwrite.

This enables users to read data from a table and insert overwrite to same table by using dynamic partition overwrite. Because this is not allowed for now, users need to write to other temporary location and move it back to the table.

### Does this PR introduce any user-facing change?

Yes. Users can insert overwrite same table if using dynamic partition overwrite.

### How was this patch tested?

Unit test.

Closes #26752 from viirya/dynamic-overwrite-same-table.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala']","Dynamic partition overwrite doesn't allow overwriting to the same table, forcing users to first write to a temporary location and then move the data back to the original table."
e79e8797e4467c85e5ff5ad5b49631a3177a461b,1692922166,"[SPARK-44827][PYTHON][TESTS] Fix test when ansi mode enabled

### What changes were proposed in this pull request?
The pr aims to fix some UT when SPARK_ANSI_SQL_MODE=true, include:
- test_assert_approx_equal_decimaltype_custom_rtol_pass
- functions.to_unix_timestamp
- DataFrame.union

### Why are the changes needed?
Make pyspark test happy.
When Ansi workflow daily ga runs, the following error occurs, eg: https://github.com/apache/spark/actions/runs/5873530086/job/15926967006
![image](https://github.com/apache/spark/assets/15246973/36b35ef3-9fc3-4e7c-8a06-abe4052a7071)

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
- Pass GA.
- Manually test:
```
(base) panbingkun:~/Developer/spark/spark-community$export SPARK_ANSI_SQL_MODE=true
(base) panbingkun:~/Developer/spark/spark-community$python/run-tests --testnames 'pyspark.sql.tests.connect.test_utils ConnectUtilsTests.test_assert_approx_equal_decimaltype_custom_rtol_pass'
Running PySpark tests. Output is in /Users/panbingkun/Developer/spark/spark-community/python/unit-tests.log
Will test against the following Python executables: ['python3.9']
Will test the following Python tests: ['pyspark.sql.tests.connect.test_utils ConnectUtilsTests.test_assert_approx_equal_decimaltype_custom_rtol_pass']
python3.9 python_implementation is CPython
python3.9 version is: Python 3.9.13
Starting test(python3.9): pyspark.sql.tests.connect.test_utils ConnectUtilsTests.test_assert_approx_equal_decimaltype_custom_rtol_pass (temp output: /Users/panbingkun/Developer/spark/spark-community/python/target/b59e563b-ac28-4279-ae95-462cde8f19c3/python3.9__pyspark.sql.tests.connect.test_utils_ConnectUtilsTests.test_assert_approx_equal_decimaltype_custom_rtol_pass__9ypt4lse.log)
Finished test(python3.9): pyspark.sql.tests.connect.test_utils ConnectUtilsTests.test_assert_approx_equal_decimaltype_custom_rtol_pass (8s)
Tests passed in 8 seconds
```

Closes #42513 from panbingkun/SPARK-44827.

Lead-authored-by: panbingkun <pbk1982@gmail.com>
Co-authored-by: panbingkun <84731559@qq.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/sql/dataframe.py', 'python/pyspark/sql/functions.py', 'python/pyspark/sql/tests/test_utils.py', 'python/pyspark/testing/utils.py']","Unit tests in PySpark fail when SPARK_ANSI_SQL_MODE is set to true. This impacts the 'test_assert_approx_equal_decimaltype_custom_rtol_pass', 'functions.to_unix_timestamp', and 'DataFrame.union' tests."
f2d35427eedeacceb6edb8a51974a7e8bbb94bc2,1535085070,"[SPARK-4502][SQL] Parquet nested column pruning - foundation

(Link to Jira: https://issues.apache.org/jira/browse/SPARK-4502)

_N.B. This is a restart of PR #16578 which includes a subset of that code. Relevant review comments from that PR should be considered incorporated by reference. Please avoid duplication in review by reviewing that PR first. The summary below is an edited copy of the summary of the previous PR._

## What changes were proposed in this pull request?

One of the hallmarks of a column-oriented data storage format is the ability to read data from a subset of columns, efficiently skipping reads from other columns. Spark has long had support for pruning unneeded top-level schema fields from the scan of a parquet file. For example, consider a table, `contacts`, backed by parquet with the following Spark SQL schema:

```
root
 |-- name: struct
 |    |-- first: string
 |    |-- last: string
 |-- address: string
```

Parquet stores this table's data in three physical columns: `name.first`, `name.last` and `address`. To answer the query

```SQL
select address from contacts
```

Spark will read only from the `address` column of parquet data. However, to answer the query

```SQL
select name.first from contacts
```

Spark will read `name.first` and `name.last` from parquet.

This PR modifies Spark SQL to support a finer-grain of schema pruning. With this patch, Spark reads only the `name.first` column to answer the previous query.

### Implementation

There are two main components of this patch. First, there is a `ParquetSchemaPruning` optimizer rule for gathering the required schema fields of a `PhysicalOperation` over a parquet file, constructing a new schema based on those required fields and rewriting the plan in terms of that pruned schema. The pruned schema fields are pushed down to the parquet requested read schema. `ParquetSchemaPruning` uses a new `ProjectionOverSchema` extractor for rewriting a catalyst expression in terms of a pruned schema.

Second, the `ParquetRowConverter` has been patched to ensure the ordinals of the parquet columns read are correct for the pruned schema. `ParquetReadSupport` has been patched to address a compatibility mismatch between Spark's built in vectorized reader and the parquet-mr library's reader.

### Limitation

Among the complex Spark SQL data types, this patch supports parquet column pruning of nested sequences of struct fields only.

## How was this patch tested?

Care has been taken to ensure correctness and prevent regressions. A more advanced version of this patch incorporating optimizations for rewriting queries involving aggregations and joins has been running on a production Spark cluster at VideoAmp for several years. In that time, one bug was found and fixed early on, and we added a regression test for that bug.

We forward-ported this patch to Spark master in June 2016 and have been running this patch against Spark 2.x branches on ad-hoc clusters since then.

Closes #21320 from mallman/spark-4502-parquet_column_pruning-foundation.

Lead-authored-by: Michael Allman <msa@allman.ms>
Co-authored-by: Adam Jacques <adam@technowizardry.net>
Co-authored-by: Michael Allman <michael@videoamp.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/SchemaPruningTest.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/GetStructFieldObject.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/ProjectionOverSchema.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SelectedField.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruning.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/SelectedFieldSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala']","When selecting a nested column from a Parquet file, Spark unnecessarily reads additional related columns, leading to inefficiency."
f1ad34558cc4b2b7d4e5aab679c9cd5eedf52724,1624600436,"[SPARK-35883][SQL] Migrate ALTER TABLE RENAME COLUMN command to use UnresolvedTable to resolve the identifier

### What changes were proposed in this pull request?

This PR proposes to migrate the following `ALTER TABLE ... RENAME COLUMN` command to use `UnresolvedTable` as a `child` to resolve the table identifier. This allows consistent resolution rules (temp view first, etc.) to be applied for both v1/v2 commands. More info about the consistent resolution rule proposal can be found in [JIRA](https://issues.apache.org/jira/browse/SPARK-29900) or [proposal doc](https://docs.google.com/document/d/1hvLjGA8y_W_hhilpngXVub1Ebv8RsMap986nENCFnrg/edit?usp=sharing).

### Why are the changes needed?

This is a part of effort to make the relation lookup behavior consistent: [SPARK-29900](https://issues.apache.org/jira/browse/SPARK-29900).

### Does this PR introduce _any_ user-facing change?

After this PR, the above `ALTER TABLE ... RENAME COLUMN` commands will have a consistent resolution behavior.

### How was this patch tested?

Updated existing tests.

Closes #33066 from imback82/alter_rename.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTableCatalogSuite.scala']","The 'ALTER TABLE ... RENAME COLUMN' command in Spark SQL is not using UnresolvedTable to resolve table identifiers, leading to inconsistent resolution rules being applied across v1/v2 commands."
82b33a304160e4f950de613c3d17f88fa3e75e5e,1613832356,"[SPARK-34379][SQL] Map JDBC RowID to StringType rather than LongType

### What changes were proposed in this pull request?

This PR fix an issue that `java.sql.RowId` is mapped to `LongType` and prefer `StringType`.

In the current implementation, JDBC RowID type is mapped to `LongType` except for `OracleDialect`, but there is no guarantee to be able to convert RowID to long.
`java.sql.RowId` declares `toString` and the specification of `java.sql.RowId` says

> _all methods on the RowId interface must be fully implemented if the JDBC driver supports the data type_
(https://docs.oracle.com/javase/8/docs/api/java/sql/RowId.html)

So, we should prefer StringType to LongType.

### Why are the changes needed?

This seems to be a potential bug.

### Does this PR introduce _any_ user-facing change?

Yes. RowID is mapped to StringType rather than LongType.

### How was this patch tested?

New test and  the existing test case `SPARK-32992: map Oracle's ROWID type to StringType` in `OracleIntegrationSuite` passes.

Closes #31491 from sarutak/rowid-type.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala', 'sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala']","The mapping of JDBC RowID to LongType can cause issues as there's no guaranteed conversion from RowID to long, potentially causing data type mismatches or conversion errors."
e751bc66a02997aaca792cd06fa6c65a8792425c,1579114051,"[SPARK-30479][SQL] Apply compaction of event log to SQL events

### What changes were proposed in this pull request?

This patch addresses adding event filter to handle SQL related events. This patch is next task of SPARK-29779 (#27085), please refer the description of PR #27085 to see overall rationalization of this patch.

Below functionalities will be addressed in later parts:

* integrate compaction into FsHistoryProvider
* documentation about new configuration

### Why are the changes needed?

One of major goal of SPARK-28594 is to prevent the event logs to become too huge, and SPARK-29779 achieves the goal. We've got another approach in prior, but the old approach required models in both KVStore and live entities to guarantee compatibility, while they're not designed to do so.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UTs.

Closes #27164 from HeartSaVioR/SPARK-30479.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['core/src/test/scala/org/apache/spark/status/ListenerEventsTestHelper.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/history/SQLEventFilterBuilder.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/history/SQLEventFilterBuilderSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/history/SQLLiveEntitiesEventFilterSuite.scala']",SQL related events are not being compacted which leads to excessively large event logs.
4559a82a1de289093064490ef2d39c3c535fb3d4,1568778884,"[SPARK-28930][SQL] Last Access Time value shall display 'UNKNOWN' in all clients

**What changes were proposed in this pull request?**
Issue 1 : modifications not required as these are different formats for the same info. In the case of a Spark DataFrame, null is correct.

Issue 2 mentioned in JIRA Spark SQL ""desc formatted tablename"" is not showing the header # col_name,data_type,comment , seems to be the header has been removed knowingly as part of SPARK-20954.

Issue 3:
Corrected the Last Access time, the value shall display 'UNKNOWN' as currently system wont support the last access time evaluation, since hive was setting Last access time as '0' in metastore even though spark CatalogTable last access time value set as -1. this will make the validation logic of LasAccessTime where spark sets 'UNKNOWN' value if last access time value set as -1 (means not evaluated).

**Does this PR introduce any user-facing change?**
No

**How was this patch tested?**
Locally and corrected a ut.
Attaching the test report below
![SPARK-28930](https://user-images.githubusercontent.com/12999161/64484908-83a1d980-d236-11e9-8062-9facf3003e5e.PNG)

Closes #25720 from sujith71955/master_describe_info.

Authored-by: s71955 <sujithchacko.2010@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala']","Spark SQL ""desc formatted tablename"" not showing the Last Access Time correctly, displaying '0' in metastore even when CatalogTable Last Access Time set as -1. This misrepresents 'not evaluated' state."
9c7250fa736b29714a30ec515ceb48fd14e33e24,1623465150,"[SPARK-35321][SQL] Don't register Hive permanent functions when creating Hive client

### What changes were proposed in this pull request?

Instantiate a new Hive client through `Hive.getWithoutRegisterFns(conf, false)` instead of `Hive.get(conf)`, if `Hive` version is >= '2.3.9' (the built-in version).

### Why are the changes needed?

[HIVE-10319](https://issues.apache.org/jira/browse/HIVE-10319) introduced a new API `get_all_functions` which is only supported in Hive 1.3.0/2.0.0 and up. As result, when Spark 3.x talks to a HMS service of version 1.2 or lower, the following error will occur:
```
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: Invalid method name: 'get_all_functions'
        at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3897)
        at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248)
        at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)
        ... 96 more
Caused by: org.apache.thrift.TApplicationException: Invalid method name: 'get_all_functions'
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_functions(ThriftHiveMetastore.java:3845)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_functions(ThriftHiveMetastore.java:3833)
```

The `get_all_functions` is called only when `doRegisterAllFns` is set to true:
```java
  private Hive(HiveConf c, boolean doRegisterAllFns) throws HiveException {
    conf = c;
    if (doRegisterAllFns) {
      registerAllFunctionsOnce();
    }
  }
```

what this does is to register all Hive permanent functions defined in HMS in Hive's `FunctionRegistry` class, via iterating through results from `get_all_functions`. To Spark, this seems unnecessary as it loads Hive permanent (not built-in) UDF via directly calling the HMS API, i.e., `get_function`. The `FunctionRegistry` is only used in loading Hive's built-in function that is not supported by Spark. At this time, it only applies to `histogram_numeric`.

[HIVE-21563](https://issues.apache.org/jira/browse/HIVE-21563) introduced a new API `getWithoutRegisterFns` which skips the above registration and is available in Hive 2.3.9. Therefore, Spark should adopt it to avoid the cost.

### Does this PR introduce _any_ user-facing change?

Yes with this fix Spark now should be able to talk to HMS server with Hive 1.2.x and lower.

### How was this patch tested?

Manually started a HMS server of Hive version 1.2.2. Without the PR it failed with the above exception. With the PR the error disappeared and I can successfully perform common operations such as create table, create database, list tables, etc.

Closes #32887 from sunchao/SPARK-35321-new.

Authored-by: Chao Sun <sunchao@apple.com>
Signed-off-by: Yuming Wang <yumwang@ebay.com>
",['sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala'],"While using Spark to communicate with a Hive Metadata Store (HMS) service version 1.2 or lower, the `get_all_functions` method triggers an error because it's only supported in Hive 1.3.0/2.0.0 and up."
6506616b978066a078ad866b3251cdf8fb95af42,1558960799,"[SPARK-27803][SQL][PYTHON] Fix column pruning for Python UDF

## What changes were proposed in this pull request?

In https://github.com/apache/spark/pull/22104 , we create the python-eval nodes at the end of the optimization phase, which causes a problem.

After the main optimization batch, Filter and Project nodes are usually pushed to the bottom, near the scan node. However, if we extract Python UDFs from Filter/Project, and create a python-eval node under Filter/Project, it will break column pruning/filter pushdown of the scan node.

There are some hacks in the `ExtractPythonUDFs` rule, to duplicate the column pruning and filter pushdown logic. However, it has some bugs as demonstrated in the new test case(only column pruning is broken). This PR removes the hacks and re-apply the column pruning and filter pushdown rules explicitly.

**Before:**

```
...
== Analyzed Logical Plan ==
a: bigint
Project [a#168L]
+- Filter dummyUDF(a#168L)
   +- Relation[a#168L,b#169L] parquet

== Optimized Logical Plan ==
Project [a#168L]
+- Project [a#168L, b#169L]
   +- Filter pythonUDF0#174: boolean
      +- BatchEvalPython [dummyUDF(a#168L)], [a#168L, b#169L, pythonUDF0#174]
         +- Relation[a#168L,b#169L] parquet

== Physical Plan ==
*(2) Project [a#168L]
+- *(2) Project [a#168L, b#169L]
   +- *(2) Filter pythonUDF0#174: boolean
      +- BatchEvalPython [dummyUDF(a#168L)], [a#168L, b#169L, pythonUDF0#174]
         +- *(1) FileScan parquet [a#168L,b#169L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/private/var/folders/_1/bzcp960d0hlb988k90654z2w0000gp/T/spark-798bae3c-a2..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:bigint,b:bigint>
```

**After:**

```
...
== Analyzed Logical Plan ==
a: bigint
Project [a#168L]
+- Filter dummyUDF(a#168L)
   +- Relation[a#168L,b#169L] parquet

== Optimized Logical Plan ==
Project [a#168L]
+- Filter pythonUDF0#174: boolean
   +- BatchEvalPython [dummyUDF(a#168L)], [pythonUDF0#174]
      +- Project [a#168L]
         +- Relation[a#168L,b#169L] parquet

== Physical Plan ==
*(2) Project [a#168L]
+- *(2) Filter pythonUDF0#174: boolean
   +- BatchEvalPython [dummyUDF(a#168L)], [pythonUDF0#174]
      +- *(1) FileScan parquet [a#168L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/private/var/folders/_1/bzcp960d0hlb988k90654z2w0000gp/T/spark-9500cafb-78..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:bigint>
```

## How was this patch tested?

new test

Closes #24675 from cloud-fan/python.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/pythonLogicalOperators.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFsSuite.scala']","Python UDF extraction in Filter/Project nodes breaks column pruning/filter pushdown in scan node, resulting in incorrect optimized logical plans."
a088a801ed8c17171545c196a3f26ce415de0cd1,1606681087,"[SPARK-33585][SQL][DOCS] Fix the comment for `SQLContext.tables()` and mention the `database` column

### What changes were proposed in this pull request?
Change the comments for `SQLContext.tables()` to ""The returned DataFrame has three columns, database, tableName and isTemporary"".

### Why are the changes needed?
Currently, the comment mentions only 2 columns but `tables()` returns 3 columns actually:
```scala
scala> spark.range(10).createOrReplaceTempView(""view1"")
scala> val tables = spark.sqlContext.tables()
tables: org.apache.spark.sql.DataFrame = [database: string, tableName: string ... 1 more field]

scala> tables.printSchema
root
 |-- database: string (nullable = false)
 |-- tableName: string (nullable = false)
 |-- isTemporary: boolean (nullable = false)

scala> tables.show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|       t1|      false|
| default|       t2|      false|
| default|      ymd|      false|
|        |    view1|       true|
+--------+---------+-----------+
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
By running `./dev/scalastyle`

Closes #30526 from MaxGekk/sqlcontext-tables-doc.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala'],"The comment for `SQLContext.tables()` inaccurately states that it returns only two columns, while it actually returns three: database, tableName, and isTemporary."
fac469e2e0298a08ae3144bf887ff76554896c1c,1566382226,"[SPARK-28774][SQL] Fix exchange reuse for columnar data

### What changes were proposed in this pull request?
The rule ReuseExchange optimization rule will look for instances of Exchange that have the same plan and convert dedupe them to them to a ReuseExchangeExec instance. In the current Spark codebase all Exchange instances are row based, but if we use the spark.sql.extensions config to put in our own columnar based exchange implementation reuse will throw an exception saying that there was a columnar mismatch.

### Why are the changes needed?
Without it Reused Columnar Exchanges throw an exception

### Does this PR introduce any user-facing change?
No

### How was this patch tested?

I tested this patch by running it against a query that was showing this exact issue and it fixed it.

I also added a very simple unit test that shows the issue.

Closes #25499 from revans2/reused-columnar-exchange.

Authored-by: Robert (Bobby) Evans <bobby@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/ExchangeSuite.scala']","ReuseExchange optimization rule triggers an exception when encountering a columnar-based exchange implementation, effectively failing to reuse columnar exchanges."
c8604de735ec76c3ec4c249409d16150c8994136,1652227496,"[SPARK-39109][PYTHON] Adjust `GroupBy.mean/median` to match pandas 1.4

### What changes were proposed in this pull request?
The PR is proposed to
- Implement `numeric_only` of `SeriesGroupBy.mean` and `DataFrameGroupBy.mean`
- Adjust `SeriesGroupBy.mean/median` to match pandas 1.4

### Why are the changes needed?
Improve API compatibility with pandas.

### Does this PR introduce _any_ user-facing change?
Yes. `GroupBy.mean/median` behaves the same as pandas 1.4's.

Take `GroupBy.mean` as an example:
- DataFrameGroupBy.mean
```py
>>> psdf = ps.DataFrame({""A"": [1, 2, 1, 2],""B"": [3.1, 4.1, 4.1, 3.1],""C"": [""a"", ""b"", ""b"", ""a""],""D"": [True, False, False, True]})
>>> psdf
   A    B  C      D
0  1  3.1  a   True
1  2  4.1  b  False
2  1  4.1  b  False
3  2  3.1  a   True

>>> psdf.groupby('A').mean(numeric_only=False)
...: FutureWarning: Dropping invalid columns in GroupBy.mean is deprecated. In a future version, a TypeError will be raised. Before calling .mean, select only columns which should be valid for the function.
  warnings.warn(
     B    D
A
1  3.6  0.5
2  3.6  0.5

>>> psdf.groupby('A').mean(numeric_only=None)
...: FutureWarning: Dropping invalid columns in GroupBy.mean is deprecated. In a future version, a TypeError will be raised. Before calling .mean, select only columns which should be valid for the function.
  warnings.warn(
     B    D
A
1  3.6  0.5
2  3.6  0.5

>>> psdf.groupby('A').mean(numeric_only=True)
     B    D
A
1  3.6  0.5
2  3.6  0.5

# Not raise the FutureWarning when there is no non-numeric aggregation column
>>> psdf[['A', 'B', 'D']].groupby('A').mean(numeric_only=False)
     B    D
A
1  3.6  0.5
2  3.6  0.5
```
- SeriesGroupBy.mean
```py
>>> psdf.groupby(""A"")[""C""].mean()
Traceback (most recent call last):
...
TypeError: Only numeric aggregation column is accepted.
```
### How was this patch tested?
Unit tests.

Closes #36452 from xinrong-databricks/groupby.numeric_only.

Authored-by: Xinrong Meng <xinrong.meng@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/groupby.py', 'python/pyspark/pandas/tests/test_groupby.py']","`GroupBy.mean/median` methods in SPARK's Python API don't have `numeric_only` argument, resulting in an inconsistent behavior with pandas 1.4."
5d393dde751181287a7fa8cc8f0f56bdb9d1d1ac,1655078796,"[SPARK-39406][PYTHON] Accept NumPy array in createDataFrame

### What changes were proposed in this pull request?
Accept NunPy array in createDataFrame, with existing dtypes support.

Note that
-  by the constraint of Spark DataFrame, we support 1-dimensional and 2-dimensional arrays only.
- full NunPy <> PySpark mappings will be introduced as a follow-up.

### Why are the changes needed?
As part of SPARK-39405 for NumPy support in SQL.

### Does this PR introduce _any_ user-facing change?
Yes, NumPy array is accepted in createDataFrame now:

Before:
```py
>>> spark.createDataFrame(np.array([[1, 2], [3, 4]]))
Traceback (most recent call last):
...
TypeError: Can not infer schema for type: <class 'numpy.ndarray'>

>>> spark.createDataFrame(np.array([0.1, 0.2]))
Traceback (most recent call last):
...
TypeError: Can not infer schema for type: <class 'numpy.float64'>
```

After:
```py
>>> spark.createDataFrame(np.array([[1, 2], [3, 4]])).show()
+---+---+
| _1| _2|
+---+---+
|  1|  2|
|  3|  4|
+---+---+

>>> spark.createDataFrame(np.array([0.1, 0.2])).show()
+-----+
|value|
+-----+
|  0.1|
|  0.2|
+-----+
```

### How was this patch tested?
Unit tests.

Closes #36793 from xinrong-databricks/createDataFrame2.

Authored-by: Xinrong Meng <xinrong.meng@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/sql/session.py', 'python/pyspark/sql/tests/test_arrow.py', 'python/setup.py', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala']","'createDataFrame' does not support NumPy arrays, leading to a TypeError when attempting to infer schema for these types. The issue occurs for both 1-dimensional and 2-dimensional NumPy arrays."
89bad267d4b8cf451d1906aa57764d97e1a565f4,1569139199,"[SPARK-29200][SQL] Optimize `extract`/`date_part` for epoch

### What changes were proposed in this pull request?

Refactoring of the `DateTimeUtils.getEpoch()` function by avoiding decimal operations that are pretty expensive, and converting the final result to the decimal type at the end.

### Why are the changes needed?
The changes improve performance of the `getEpoch()` method at least up to **20 times**.
Before:
```
Invoke extract for timestamp:             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
cast to timestamp                                   256            277          33         39.0          25.6       1.0X
EPOCH of timestamp                                23455          23550         131          0.4        2345.5       0.0X
```
After:
```
Invoke extract for timestamp:             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
cast to timestamp                                   255            294          34         39.2          25.5       1.0X
EPOCH of timestamp                                 1049           1054           9          9.5         104.9       0.2X
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?

By existing test from `DateExpressionSuite`.

Closes #25881 from MaxGekk/optimize-extract-epoch.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala'],"The `DateTimeUtils.getEpoch()` method is performing poorly, causing decimal operations to be too expensive and slowing down the 'extract' or 'date_part' operations for epoch."
2f3cb36cb99a4b8d1ddec74696c4ed036c5df5b2,1695332651,"[SPARK-45261][CORE] Fix `EventLogFileWriters` to handle `none` as a codec

### What changes were proposed in this pull request?

This PR aims to support `none` as a codec instead of throwing exception.

Currrently, our unit test is supposed to test it, but actually it's not tested at all.

https://github.com/apache/spark/blob/892fdc532696e703b353c4758320d69162fffe8c/core/src/test/scala/org/apache/spark/deploy/history/EventLogFileReadersSuite.scala#L120-L124

```
$ build/sbt ""core/testOnly *EventLogFileReaderSuite*""
...
[info] - get information, list event log files, zip log files - with codec None (33 milliseconds)
[info] - get information, list event log files, zip log files - with codec Some(lz4) (125 milliseconds)
...
```

### Why are the changes needed?

```
$ bin/spark-shell \
-c spark.eventLog.enabled=true \
-c spark.eventLog.compress=true \
-c spark.eventLog.compression.codec=none
...
23/09/21 13:26:45 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkIllegalArgumentException: [CODEC_SHORT_NAME_NOT_FOUND] Cannot find a short name for the codec none.
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs with the revised CIs.

### Was this patch authored or co-authored using generative AI tooling?

No.

Closes #43038 from dongjoon-hyun/SPARK-45261.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['core/src/main/scala/org/apache/spark/SparkContext.scala', 'core/src/main/scala/org/apache/spark/deploy/history/EventLogFileWriters.scala', 'core/src/main/scala/org/apache/spark/internal/config/package.scala', 'core/src/test/scala/org/apache/spark/deploy/history/EventLogTestHelper.scala']","The 'none' codec in `EventLogFileWriters` is throwing an exception instead of being properly supported, causing an error upon initializing SparkContext."
aed977c4682b6f378a26050ffab51b9b2075cae4,1633514769,"[SPARK-36919][SQL] Make BadRecordException fields transient

### What changes were proposed in this pull request?
Migrating a Spark application from 2.4.x to 3.1.x and finding a difference in the exception chaining behavior. In a case of parsing a malformed CSV, where the root cause exception should be Caused by: java.lang.RuntimeException: Malformed CSV record, only the top level exception is kept, and all lower level exceptions and root cause are lost. Thus, when we call ExceptionUtils.getRootCause on the exception, we still get itself.
The reason for the difference is that RuntimeException is wrapped in BadRecordException, which has unserializable fields. When we try to serialize the exception from tasks and deserialize from scheduler, the exception is lost.
This PR makes unserializable fields of BadRecordException transient, so the rest of the exception could be serialized and deserialized properly.

### Why are the changes needed?
Make BadRecordException serializable

### Does this PR introduce _any_ user-facing change?
User could get root cause of BadRecordException

### How was this patch tested?
Unit testing

Closes #34167 from tianhanhu/master.

Authored-by: tianhanhu <adrianhu96@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/BadRecordException.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala']","When migrating a Spark application, the exception chaining behavior for parsing bad CSV records is inconsistent, losing lower level exceptions and root causes due to unserializable fields in BadRecordException."
02eecfec9938404f16545dc921b4275e157b4249,1572932559,"[SPARK-29695][SQL] ALTER TABLE (SerDe properties) should look up catalog/table like v2 commands

### What changes were proposed in this pull request?
Add AlterTableSerDePropertiesStatement and make ALTER TABLE ... SET SERDE/SERDEPROPERTIES go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC t // success and describe the table t from my_catalog
ALTER TABLE t SET SERDE 'org.apache.class' // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?
Yes. When running ALTER TABLE ... SET SERDE/SERDEPROPERTIES, Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?
Unit tests.

Closes #26374 from huaxingao/spark_29695.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala']",`ALTER TABLE ... SET SERDE/SERDEPROPERTIES` commands have inconsistent table resolution behavior causing failed commands when the current catalog is set to a v2 catalog or the table name specifies a v2 catalog.
5bc0d76591b46f0c1c9ec283ee8e1c5da76e67d6,1584457398,"[SPARK-31170][SQL] Spark SQL Cli should respect hive-site.xml and spark.sql.warehouse.dir

### What changes were proposed in this pull request?

In Spark CLI, we create a hive `CliSessionState` and it does not load the `hive-site.xml`. So the configurations in `hive-site.xml` will not take effects like other spark-hive integration apps.

Also, the warehouse directory is not correctly picked. If the `default` database does not exist, the `CliSessionState` will create one during the first time it talks to the metastore. The `Location` of the default DB will be neither the value of `spark.sql.warehousr.dir` nor the user-specified value of `hive.metastore.warehourse.dir`, but the default value of `hive.metastore.warehourse.dir `which will always be `/user/hive/warehouse`.

### Why are the changes needed?

fix bug for Spark SQL cli to pick right confs

### Does this PR introduce any user-facing change?

yes, the non-exists default database will be created in the location specified by the users via `spark.sql.warehouse.dir` or `hive.metastore.warehouse.dir`, or the default value of `spark.sql.warehouse.dir` if none of them specified.

### How was this patch tested?

add cli ut

Closes #27933 from yaooqinn/SPARK-31170.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala', 'sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSharedStateSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala']","The Spark SQL CLI is not considering `hive-site.xml` settings and `spark.sql.warehouse.dir`, causing the creation of the default database in incorrect locations."
1af19a7b6836f87a3b34189a8a13b6d21d3a37d8,1593108287,"[SPARK-32098][PYTHON] Use iloc for positional slicing instead of direct slicing in createDataFrame with Arrow

### What changes were proposed in this pull request?

When you use floats are index of pandas, it creates a Spark DataFrame with a wrong results as below when Arrow is enabled:

```bash
./bin/pyspark --conf spark.sql.execution.arrow.pyspark.enabled=true
```

```python
>>> import pandas as pd
>>> spark.createDataFrame(pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])).show()
+---+
|  a|
+---+
|  1|
|  1|
|  2|
+---+
```

This is because direct slicing uses the value as index when the index contains floats:

```python
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:]
     a
2.0  1
3.0  2
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:]
     a
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:]
   a
4  3
```

This PR proposes to explicitly use `iloc` to positionally slide when we create a DataFrame from a pandas DataFrame with Arrow enabled.

FWIW, I was trying to investigate why direct slicing refers the index value or the positional index sometimes but I stopped investigating further after reading this https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#selection

> While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, `.at`, `.iat`, `.loc` and `.iloc`.

### Why are the changes needed?

To create the correct Spark DataFrame from a pandas DataFrame without a data loss.

### Does this PR introduce _any_ user-facing change?

Yes, it is a bug fix.

```bash
./bin/pyspark --conf spark.sql.execution.arrow.pyspark.enabled=true
```
```python
import pandas as pd
spark.createDataFrame(pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])).show()
```

Before:

```
+---+
|  a|
+---+
|  1|
|  1|
|  2|
+---+
```

After:

```
+---+
|  a|
+---+
|  1|
|  2|
|  3|
+---+
```

### How was this patch tested?

Manually tested and unittest were added.

Closes #28928 from HyukjinKwon/SPARK-32098.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>
","['python/pyspark/sql/pandas/conversion.py', 'python/pyspark/sql/tests/test_arrow.py']","When Arrow is enabled, creating a Spark DataFrame from a pandas DataFrame using floats as index results in incorrect data allocation."
8c6748f69166b828ca076359332b479659ea963e,1616655486,"[SPARK-34817][SQL] Read parquet unsigned types that stored as int32 physical type in parquet

### What changes were proposed in this pull request?

Unsigned types may be used to produce smaller in-memory representations of the data. These types used by frameworks(e.g. hive, pig) using parquet. And parquet will map them to its base types.

see more https://github.com/apache/parquet-format/blob/master/LogicalTypes.md
https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift

```thrift
  /**
   * An unsigned integer value.
   *
   * The number describes the maximum number of meaningful data bits in
   * the stored value. 8, 16 and 32 bit values are stored using the
   * INT32 physical type.  64 bit values are stored using the INT64
   * physical type.
   *
   */
  UINT_8 = 11;
  UINT_16 = 12;
  UINT_32 = 13;
  UINT_64 = 14;
```

```
UInt8-[0:255]
UInt16-[0:65535]
UInt32-[0:4294967295]
UInt64-[0:18446744073709551615]
```

In this PR, we support read UINT_8 as ShortType, UINT_16 as IntegerType, UINT_32 as LongType to fit their range. Support for UINT_64 will be in another PR.

### Why are the changes needed?

better parquet support

### Does this PR introduce _any_ user-facing change?

yes, we can read unit[8/16/32] from parquet files

### How was this patch tested?

new tests

Closes #31921 from yaooqinn/SPARK-34817.

Authored-by: Kent Yao <yao@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java', 'sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java', 'sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java', 'sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java', 'sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala']","Inability to read UINT_8, UINT_16, and UINT_32 unsigned types from Parquet files, which are stored as INT32 physical types in Parquet."
2c7f20151e99c212443a1f8762350d0a96a26440,1636425423,"[SPARK-37252][PYTHON][TESTS] Ignore `test_memory_limit` on non-Linux environment

### What changes were proposed in this pull request?

This PR aims to ignore `test_memory_limit` on non-Linux environment.

### Why are the changes needed?

Like the documentation https://github.com/apache/spark/pull/23664, it fails on non-Linux environment like the following MacOS example.

**BEFORE**
```
$ build/sbt -Phadoop-cloud -Phadoop-3.2 test:package
$ python/run-tests --modules pyspark-core
...
======================================================================
FAIL: test_memory_limit (pyspark.tests.test_worker.WorkerMemoryTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/dongjoon/APACHE/spark-merge/python/pyspark/tests/test_worker.py"", line 212, in test_memory_limit
    self.assertEqual(soft_limit, 2 * 1024 * 1024 * 1024)
AssertionError: 9223372036854775807 != 2147483648

----------------------------------------------------------------------
```

**AFTER**
```
...
Tests passed in 104 seconds

Skipped tests in pyspark.tests.test_serializers with /Users/dongjoon/.pyenv/versions/3.8.12/bin/python3:
    test_serialize (pyspark.tests.test_serializers.SciPyTests) ... skipped 'SciPy not installed'

Skipped tests in pyspark.tests.test_worker with /Users/dongjoon/.pyenv/versions/3.8.12/bin/python3:
    test_memory_limit (pyspark.tests.test_worker.WorkerMemoryTest) ... skipped ""Memory limit feature in Python worker is dependent on Python's 'resource' module on Linux; however, not found or not on Linux.""
```

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Manual.

Closes #34527 from dongjoon-hyun/SPARK-37252.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['python/pyspark/tests/test_worker.py'],The `test_memory_limit` causes failure in non-Linux environments due to differences in how Python's 'resource' module operates.
174d9104cf3915f1098bcf0fdbe4fec5dbddbadb,1568434157,"[SPARK-29003][CORE] Add `start` method to ApplicationHistoryProvider to avoid deadlock on startup

### What changes were proposed in this pull request?

During Spark History Server startup, there are two things happening simultaneously that call into `java.nio.file.FileSystems.getDefault()` and we sometime hit [JDK-8194653](https://bugs.openjdk.java.net/browse/JDK-8194653).
1) start jetty server
2) start ApplicationHistoryProvider (which reads files from HDFS)

We should do these two things sequentially instead of in parallel.
We introduce a start() method in ApplicationHistoryProvider (and its subclass FsHistoryProvider), and we do initialize inside the start() method instead of the constructor.
In HistoryServer, we explicitly call provider.start() after we call bind() which starts the Jetty server.

### Why are the changes needed?
It is a bug that occasionally starting Spark History Server results in process hang due to deadlock among threads.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
I stress tested this PR with a bash script to stop and start Spark History Server more than 1000 times, it worked fine. Previously I can only do the stop/start loop less than 10 times before I hit the deadlock issue.

Closes #25705 from shanyu/shanyu-29003.

Authored-by: Shanyu Zhao <shzhao@microsoft.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala', 'core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala', 'core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala', 'core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala']",Occasional deadlock on Spark History Server startup due to simultaneous invocation of `java.nio.file.FileSystems.getDefault()` during Jetty server start and ApplicationHistoryProvider initialization.
dc153f525c8c895b9ceac8dfb3516b601c86a462,1645829307,"[SPARK-38237][SQL][SS] Allow `ClusteredDistribution` to require full clustering keys

### What changes were proposed in this pull request?

This PR is to allow`ClusteredDistribution` (such as operator with window, aggregate, etc) to require full clustering keys. Traditionally operator with `ClusteredDistribution` can be satisfied with `HashPartitioning` on subset of clustering keys. This behavior could potentially lead to data skewness (comments raised from https://github.com/apache/spark/pull/35552). Although we have various way to deal with the data skewness in this case, such as adding `repartition()`, disabling bucketing, adding custom AQE rule etc. There's still case we cannot handle e.g. data skewness in the same stage - (`join(t1.x = t2.x)` followed by `window(t1.x, t1.y)`). With the newly introduced config `spark.sql.requireAllClusterKeysForDistribution`.

### Why are the changes needed?

Allow users to work around data skewness issue when partitioned on subset of keys.

### Does this PR introduce _any_ user-facing change?

Yes, the added config, but disable by default.

### How was this patch tested?

Added unit test in `DataFrameWindowFunctionsSuite.scala` and `DistributionSuite.scala`

Closes #35574 from c21/exact-partition.

Authored-by: Cheng Su <chengsu@fb.com>
Signed-off-by: Chao Sun <sunchao@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala']","Using `HashPartitioning` on a subset of clustering keys in `ClusteredDistribution` can potentially lead to data skewness, including in same stage operations such as (`join(t1.x = t2.x)` followed by `window(t1.x, t1.y)`)."
a2f71a8d852be166561c208fad0c9a74ae304dd3,1563162105,"[SPARK-28133][SQL] Add acosh/asinh/atanh functions to SQL

## What changes were proposed in this pull request?

Adding support to hyperbolic functions like asinh\acosh\atanh in spark SQL.
Feature parity: https://www.postgresql.org/docs/12/functions-math.html#FUNCTIONS-MATH-HYP-TABLE

The followings are the diffence from PostgreSQL.
```
spark-sql> SELECT acosh(0);     (PostgreSQL returns `ERROR:  input is out of range`)
NaN

spark-sql> SELECT atanh(2);     (PostgreSQL returns `ERROR:  input is out of range`)
NaN
```

Teradata has similar behavior as PostgreSQL with out of range input float values - It outputs **Invalid Input: numeric value within range only.**

These newly added asinh/acosh/atanh handles special input(NaN, +-Infinity) in the same way as existing cos/sin/tan/acos/asin/atan in spark. For which input value range is not (-∞, ∞)):
out of range float values: Spark returns NaN and PostgreSQL shows input is out of range
NaN: Spark returns NaN, PostgreSQL also returns NaN
Infinity: Spark return NaN, PostgreSQL shows input is out of range

## How was this patch tested?

```
spark.sql(""select asinh(xx)"")
spark.sql(""select acosh(xx)"")
spark.sql(""select atanh(xx)"")

./build/sbt ""testOnly org.apache.spark.sql.MathFunctionsSuite""
./build/sbt ""testOnly org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite""
```

Closes #25041 from Tonix517/SPARK-28133.

Authored-by: Tony Zhang <tony.zhang@uber.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala']","Spark SQL is lacking support for hyperbolic functions such as asinh, acosh, and atanh, causing inconsistency with platforms like PostgreSQL and Teradata."
59741887e272be92ebd6e61783f99f7d8fc05456,1543524994,"[SPARK-25905][CORE] When getting a remote block, avoid forcing a conversion to a ChunkedByteBuffer

## What changes were proposed in this pull request?

In `BlockManager`, `getRemoteValues` gets a `ChunkedByteBuffer` (by calling `getRemoteBytes`) and creates an `InputStream` from it. `getRemoteBytes`, in turn, gets a `ManagedBuffer` and converts it to a `ChunkedByteBuffer`.
Instead, expose a `getRemoteManagedBuffer` method so `getRemoteValues` can just get this `ManagedBuffer` and use its `InputStream`.
When reading a remote cache block from disk, this reduces heap memory usage significantly.
Retain `getRemoteBytes` for other callers.

## How was this patch tested?

Imran Rashid wrote an application (https://github.com/squito/spark_2gb_test/blob/master/src/main/scala/com/cloudera/sparktest/LargeBlocks.scala), that among other things, tests reading remote cache blocks. I ran this application, using 2500MB blocks, to test reading a cache block on disk. Without this change, with `--executor-memory 5g`, the test fails with `java.lang.OutOfMemoryError: Java heap space`. With the change, the test passes with `--executor-memory 2g`.
I also ran the unit tests in core. In particular, `DistributedSuite` has a set of tests that exercise the `getRemoteValues` code path. `BlockManagerSuite` has several tests that call `getRemoteBytes`; I left these unchanged, so `getRemoteBytes` still gets exercised.

Closes #23058 from wypoon/SPARK-25905.

Authored-by: Wing Yew Poon <wypoon@cloudera.com>
Signed-off-by: Imran Rashid <irashid@cloudera.com>
","['core/src/main/scala/org/apache/spark/storage/BlockManager.scala', 'core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala', 'core/src/test/scala/org/apache/spark/DistributedSuite.scala']","When retrieving a remote block in `BlockManager`, forced conversion to a `ChunkedByteBuffer` leads to significant heap memory usage increase."
9cfc3ee6253bed21924424ccaadea0287a6f15f4,1543550455,"[SPARK-26188][SQL] FileIndex: don't infer data types of partition columns if user specifies schema

## What changes were proposed in this pull request?

This PR is to fix a regression introduced in: https://github.com/apache/spark/pull/21004/files#r236998030

If user specifies schema, Spark don't need to infer data type for of partition columns, otherwise the data type might not match with the one user provided.
E.g. for partition directory `p=4d`, after data type inference  the column value will be `4.0`.
See https://issues.apache.org/jira/browse/SPARK-26188 for more details.

Note that user specified schema **might not cover all the data columns**:
```
val schema = new StructType()
  .add(""id"", StringType)
  .add(""ex"", ArrayType(StringType))
val df = spark.read
  .schema(schema)
  .format(""parquet"")
  .load(src.toString)

assert(df.schema.toList === List(
  StructField(""ex"", ArrayType(StringType)),
  StructField(""part"", IntegerType), // inferred partitionColumn dataType
  StructField(""id"", StringType))) // used user provided partitionColumn dataType
```
For the missing columns in user specified schema, Spark still need to infer their data types if `partitionColumnTypeInferenceEnabled` is enabled.

To implement the partially inference, refactor `PartitioningUtils.parsePartitions`  and pass the user specified schema as parameter to cast partition values.

## How was this patch tested?

Add unit test.

Closes #23165 from gengliangwang/fixFileIndex.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileIndexSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala']","If the user specifies a schema in Spark, data type of partition columns may still be inferred incorrectly leading to potential data type mismatch issues."
3d3e366aa836cb7d2295f54e78e544c7b15c9c08,1583931236,"[SPARK-31076][SQL] Convert Catalyst's DATE/TIMESTAMP to Java Date/Timestamp via local date-time

### What changes were proposed in this pull request?
In the PR, I propose to change conversion of java.sql.Timestamp/Date values to/from internal values of Catalyst's TimestampType/DateType before cutover day `1582-10-15` of Gregorian calendar. I propose to construct local date-time from microseconds/days since the epoch. Take each date-time component `year`, `month`, `day`, `hour`, `minute`, `second` and `second fraction`, and construct java.sql.Timestamp/Date using the extracted components.

### Why are the changes needed?
This will rebase underlying time/date offset in the way that collected java.sql.Timestamp/Date values will have the same local time-date component as the original values in Gregorian calendar.

Here is the example which demonstrates the issue:
```sql
scala> sql(""select date '1100-10-10'"").collect()
res1: Array[org.apache.spark.sql.Row] = Array([1100-10-03])
```

### Does this PR introduce any user-facing change?
Yes, after the changes:
```sql
scala> sql(""select date '1100-10-10'"").collect()
res0: Array[org.apache.spark.sql.Row] = Array([1100-10-10])
```

### How was this patch tested?
By running `DateTimeUtilsSuite`, `DateFunctionsSuite` and `DateExpressionsSuite`.

Closes #27807 from MaxGekk/rebase-timestamp-before-1582.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/HiveResultSuite.scala', 'sql/core/v1.2/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java', 'sql/core/v2.3/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala']","Conversion of java.sql.Timestamp/Date values to Catalyst's TimestampType/DateType before the Gregorian calendar's cutover day results in incorrect date representation, causing disparities in the time/date offset."
502a1ec4cf819187e966b4f1462189242b80e9ee,1635388681,"[SPARK-37036][PYTHON] Add util function to raise advice warning for pandas API on Spark

### What changes were proposed in this pull request?

This PR proposes to add an util function to raise advice warning for pandas API on Spark.

Apart from the existing warnings recognized by general Python, PySpark and pandas users, these warnings are things to pay special attention to in the pandas API on Spark, so I think it is better to manage warnings separately.

This PR basically do:
- Add util function, `log_advice` to raise `UserWarning` with proper message.
- Add `log_advice` to the APIs where needs showing the advice.

### Why are the changes needed?

The pandas API on Spark has functions that the existing pandas users who are not familiar with distributed environment should aware for avoiding confusion (not only confusion, it also could cause the serious performance degradation).

For example:
- `sort_index`, `len`, `sort_values`: such functions can cause the performance degradation since it goes through the entire data set
- `to_xxx`, `read_xxx`: if the `index_col` is not specified for some I/O functions, the default index is attached which is expensive (and also the existing index will be lost)
- `to_list`, `to_pandas`, `to_markdown`: such functions load the whole data into the driver's memory, so potentially could cause the OOM.

### Does this PR introduce _any_ user-facing change?

Yes, the pandas-on-Spark users can see the warning when they're using inefficient or potentially dangerous functions.

### How was this patch tested?

Manually check the behavior one-by-one.

```python
>>> import pyspark.pandas as ps
>>> psser = ps.Series([1, 2, 3, 4])
>>> psser.to_list()
.../spark/python/pyspark/pandas/utils.py:968: PandasAPIOnSparkAdviceWarning: `to_list` loads the all data into the driver's memory. It should only be used if the resulting list is expected to be small.
  warnings.warn(message, PandasAPIOnSparkAdviceWarning)
[1, 2, 3, 4]
```

Closes #34389 from itholic/SPARK-37036.

Lead-authored-by: itholic <haejoon.lee@databricks.com>
Co-authored-by: Haejoon Lee <44108233+itholic@users.noreply.github.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/pandas/accessors.py', 'python/pyspark/pandas/frame.py', 'python/pyspark/pandas/generic.py', 'python/pyspark/pandas/groupby.py', 'python/pyspark/pandas/indexes/base.py', 'python/pyspark/pandas/namespace.py', 'python/pyspark/pandas/series.py', 'python/pyspark/pandas/utils.py']",Some pandas API on Spark functions may lead to performance degradation or potential out-of-memory issues due to full dataset processing or loading all data into the driver's memory. Warning messages for such scenarios are not managed separately.
972e23d18186c73026ebed95b37a886ca6eecf3e,1584029189,"[SPARK-31130][BUILD] Use the same version of `commons-io` in SBT

### What changes were proposed in this pull request?

This PR (SPARK-31130) aims to pin `Commons IO` version to `2.4` in SBT build like Maven build.

### Why are the changes needed?

[HADOOP-15261](https://issues.apache.org/jira/browse/HADOOP-15261) upgraded `commons-io` from 2.4 to 2.5 at Apache Hadoop 3.1.

In `Maven`, Apache Spark always uses `Commons IO 2.4` based on `pom.xml`.
```
$ git grep commons-io.version
pom.xml:    <commons-io.version>2.4</commons-io.version>
pom.xml:        <version>${commons-io.version}</version>
```

However, `SBT` choose `2.5`.

**branch-3.0**
```
$ build/sbt -Phadoop-3.2 ""core/dependencyTree"" | grep commons-io:commons-io | head -n1
[info]   | | +-commons-io:commons-io:2.5
```

**branch-2.4**
```
$ build/sbt -Phadoop-3.1 ""core/dependencyTree"" | grep commons-io:commons-io | head -n1
[info]   | | +-commons-io:commons-io:2.5
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with `[test-hadoop3.2]` (the default PR Builder is `SBT`) and manually do the following locally.
```
build/sbt -Phadoop-3.2 ""core/dependencyTree"" | grep commons-io:commons-io | head -n1
```

Closes #27886 from dongjoon-hyun/SPARK-31130.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['project/SparkBuild.scala'],"SBT and Maven builds are using two different versions of `Commons IO` (2.5 vs 2.4 respectively), despite the pinning in `pom.xml`.
"
f8c544b9ef78de37b1d149405ff6ead3285eac3e,1655199978,"[SPARK-39459][CORE] `local*HostName*` methods should support `IPv6`

### What changes were proposed in this pull request?

This PR aims to
- Support `IPv6`-only environment in `localHostName`, `localHostNameForUri` and `localCanonicalHostName` methods
- have no side-effects in `IPv4` environment.

### Why are the changes needed?

Currently, Apache Spark fails on pure-IPv6 environment (which doesn't have IPv4 address).

**BEFORE**
```
$ SPARK_LOCAL_IP=::1 build/sbt ""core/testOnly *.DistributedSuite"" -Djava.net.preferIPv6Addresses=true
...
Using SPARK_LOCAL_IP=::1
...
[info] *** 45 TESTS FAILED ***
[error] Failed tests:
[error] 	org.apache.spark.DistributedSuite
[error] (core / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 9 s, completed Jun 13, 2022, 8:38:03 PM
```

**AFTER**
```
$ SPARK_LOCAL_IP=::1 build/sbt ""core/testOnly *.DistributedSuite"" -Djava.net.preferIPv6Addresses=true
...
Using SPARK_LOCAL_IP=::1
...
[info] Tests: succeeded 46, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
```

### Does this PR introduce _any_ user-facing change?

This will help IPv6-only environment users.

### How was this patch tested?

Since we don't have IPv6 test CI, this should be tested in IPv6 environment manually with `DistributedSuite` and `Spark-Shell`.

**DistributedSuite**
```
$ SPARK_LOCAL_IP=::1 build/sbt ""core/testOnly *.DistributedSuite"" -Djava.net.preferIPv6Addresses=true
```

**SPARK-SHELL**
```
$ SPARK_LOCAL_IP=2600:...:...:c26a bin/spark-shell
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/06/13 20:17:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://unknown1498774f7f18.attlocal.net:4040
Spark context available as 'sc' (master = local[*], app id = local-1655176664558).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.0-SNAPSHOT
      /_/

Using Scala version 2.12.16 (OpenJDK 64-Bit Server VM, Java 17.0.3)
Type in expressions to have them evaluated.
Type :help for more information.
```

Closes #36863 from dongjoon-hyun/SPARK-39459.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['core/src/main/scala/org/apache/spark/util/Utils.scala'],"Apache Spark fails in pure-IPv6 environment due to lack of support for IPv6 within the `localHostName`, `localHostNameForUri`, and `localCanonicalHostName` methods."
2e54d68eb94cf39b59166f2b1bbb8f6c317760b8,1614677233,"[SPARK-34547][SQL] Only use metadata columns for resolution as last resort

### What changes were proposed in this pull request?

Today, child expressions may be resolved based on ""real"" or metadata output attributes. We should prefer the real attribute during resolution if one exists.

### Why are the changes needed?

Today, attempting to resolve an expression when there is a ""real"" output attribute and a metadata attribute with the same name results in resolution failure. This is likely unexpected, as the user may not know about the metadata attribute.

### Does this PR introduce _any_ user-facing change?

Yes. Previously, the user would see an error message when resolving a column with the same name as a ""real"" output attribute and a metadata attribute as below:
```
org.apache.spark.sql.AnalysisException: Reference 'index' is ambiguous, could be: testcat.ns1.ns2.tableTwo.index, testcat.ns1.ns2.tableOne.index.; line 1 pos 71
at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:363)
at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:107)
```

Now, resolution succeeds and provides the ""real"" output attribute.

### How was this patch tested?

Added a unit test.

Closes #31654 from karenfeng/fallback-resolve-metadata.

Authored-by: Karen Feng <karen.feng@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala']","When trying to resolve an expression and there is both a ""real"" output attribute and a metadata attribute sharing the same name, it results in a resolution failure with an ""ambiguous reference"" error."
3d1dce75d96373130e27b3809c73d3796b5b77be,1597854202,"[SPARK-32621][SQL] 'path' option can cause issues while inferring schema in CSV/JSON datasources

### What changes were proposed in this pull request?

When CSV/JSON datasources infer schema (e.g, `def inferSchema(files: Seq[FileStatus])`, they use the `files` along with the original options. `files` in `inferSchema` could have been deduced from the ""path"" option if the option was present, so this can cause issues (e.g., reading more data, listing the path again) since the ""path"" option is **added** to the `files`.

### Why are the changes needed?

The current behavior can cause the following issue:
```scala
class TestFileFilter extends PathFilter {
  override def accept(path: Path): Boolean = path.getParent.getName != ""p=2""
}

val path = ""/tmp""
val df = spark.range(2)
df.write.json(path + ""/p=1"")
df.write.json(path + ""/p=2"")

val extraOptions = Map(
  ""mapred.input.pathFilter.class"" -> classOf[TestFileFilter].getName,
  ""mapreduce.input.pathFilter.class"" -> classOf[TestFileFilter].getName
)

// This works fine.
assert(spark.read.options(extraOptions).json(path).count == 2)

// The following with ""path"" option fails with the following:
// assertion failed: Conflicting directory structures detected. Suspicious paths
//	file:/tmp
//	file:/tmp/p=1
assert(spark.read.options(extraOptions).format(""json"").option(""path"", path).load.count() === 2)
```

### Does this PR introduce _any_ user-facing change?

Yes, the above failure doesn't happen and you get the consistent experience when you use `spark.read.csv(path)` or `spark.read.format(""csv"").option(""path"", path).load`.

### How was this patch tested?

Updated existing tests.

Closes #29437 from imback82/path_bug.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['external/avro/src/main/scala/org/apache/spark/sql/v2/avro/AvroDataSourceV2.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileDataSourceV2.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVDataSourceV2.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/json/JsonDataSourceV2.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcDataSourceV2.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetDataSourceV2.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/text/TextDataSourceV2.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala']","Inferred schema in CSV/JSON data sources has issues when 'path' option is used, potentially leading to data reading errors and path relisting."
3fabbc576203c7fd63808a259adafc5c3cea1838,1526405129,"[SPARK-24040][SS] Support single partition aggregates in continuous processing.

## What changes were proposed in this pull request?

Support aggregates with exactly 1 partition in continuous processing.

A few small tweaks are needed to make this work:

* Replace currentEpoch tracking with an ThreadLocal. This means that current epoch is scoped to a task rather than a node, but I think that's sustainable even once we add shuffle.
* Add a new testing-only flag to disable the UnsupportedOperationChecker whitelist of allowed continuous processing nodes. I think this is preferable to writing a pile of custom logic to enforce that there is in fact only 1 partition; we plan to support multi-partition aggregates before the next Spark release, so we'd just have to tear that logic back out.
* Restart continuous processing queries from the first available uncommitted epoch, rather than one that's guaranteed to be unused. This is required for stateful operators to overwrite partial state from the previous attempt at the epoch, and there was no specific motivation for the original strategy. In another PR before stabilizing the StreamWriter API, we'll need to narrow down and document more precise semantic guarantees for the epoch IDs.
* We need a single-partition ContinuousMemoryStream. The way MemoryStream is constructed means it can't be a text option like it is for rate source, unfortunately.

## How was this patch tested?

new unit tests

Author: Jose Torres <torres.joseph.f+github@gmail.com>

Closes #21239 from jose-torres/withAggr.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousQueuedDataReader.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousWriteRDD.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochTracker.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ContinuousMemoryStream.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala', 'sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousAggregationSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousQueuedDataReaderSuite.scala']","Continuous processing queries fail to support aggregates with exactly one partition, restricting functionality and causing potential issues in stateful operations."
1b9368f7d4c1d5c0df49204f48515d3b4ffe3e13,1530066460,"[SPARK-24659][SQL] GenericArrayData.equals should respect element type differences

## What changes were proposed in this pull request?

Fix `GenericArrayData.equals`, so that it respects the actual types of the elements.
e.g. an instance that represents an `array<int>` and another instance that represents an `array<long>` should be considered incompatible, and thus should return false for `equals`.

`GenericArrayData` doesn't keep any schema information by itself, and rather relies on the Java objects referenced by its `array` field's elements to keep track of their own object types. So, the most straightforward way to respect their types is to call `equals` on the elements, instead of using Scala's `==` operator, which can have semantics that are not always desirable:
```
new java.lang.Integer(123) == new java.lang.Long(123L) // true in Scala
new java.lang.Integer(123).equals(new java.lang.Long(123L)) // false in Scala
```

## How was this patch tested?

Added unit test in `ComplexDataSuite`

Author: Kris Mok <kris.mok@databricks.com>

Closes #21643 from rednaxelafx/fix-genericarraydata-equals.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/GenericArrayData.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala']","GenericArrayData.equals method considers different data types as equivalent, leading to inaccurate comparisons between instances of different array types."
9d84369ade670737a4ccda166e452e5208eb8253,1693466861,"[SPARK-45018][PYTHON][CONNECT] Add CalendarIntervalType to Python Client

### What changes were proposed in this pull request?
Add CalendarIntervalType to Python Client

### Why are the changes needed?
for feature parity

### Does this PR introduce _any_ user-facing change?
yes

before this PR:
```
In [1]: from pyspark.sql import functions as sf

In [2]: spark.range(1).select(sf.make_interval(sf.lit(1))).schema
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
Cell In[2], line 1
----> 1 spark.range(1).select(sf.make_interval(sf.lit(1))).schema

File ~/Dev/spark/python/pyspark/sql/connect/dataframe.py:1687, in DataFrame.schema(self)
   1685     if self._session is None:
   1686         raise Exception(""Cannot analyze without SparkSession."")
-> 1687     return self._session.client.schema(query)
   1688 else:
   1689     raise Exception(""Empty plan."")

...

Exception: Unsupported data type calendar_interval
```

after this PR:
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 4.0.0.dev0
      /_/

Using Python version 3.10.11 (main, May 17 2023 14:30:36)
Client connected to the Spark Connect server at localhost
SparkSession available as 'spark'.

In [1]: from pyspark.sql import functions as sf

In [2]: spark.range(1).select(sf.make_interval(sf.lit(1))).schema
Out[2]: StructType([StructField('make_interval(1, 0, 0, 0, 0, 0, 0)', CalendarIntervalType(), True)])
```

### How was this patch tested?
added UT

### Was this patch authored or co-authored using generative AI tooling?
NO

Closes #42743 from zhengruifeng/py_connect_cal_interval.

Authored-by: Ruifeng Zheng <ruifengz@apache.org>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/sql/connect/types.py', 'python/pyspark/sql/tests/connect/test_parity_types.py', 'python/pyspark/sql/tests/test_types.py']","The Python client raises an exception when attempting to handle the calendar interval data type, failing to achieve feature parity."
434319e73f8cb6e080671bdde42a72228bd814ef,1532359524,"[SPARK-24802][SQL] Add a new config for Optimization Rule Exclusion

## What changes were proposed in this pull request?

Since Spark has provided fairly clear interfaces for adding user-defined optimization rules, it would be nice to have an easy-to-use interface for excluding an optimization rule from the Spark query optimizer as well.

This would make customizing Spark optimizer easier and sometimes could debugging issues too.

- Add a new config spark.sql.optimizer.excludedRules, with the value being a list of rule names separated by comma.
- Modify the current batches method to remove the excluded rules from the default batches. Log the rules that have been excluded.
- Split the existing default batches into ""post-analysis batches"" and ""optimization batches"" so that only rules in the ""optimization batches"" can be excluded.

## How was this patch tested?

Add a new test suite: OptimizerRuleExclusionSuite

Author: maryannxue <maryannxue@apache.org>

Closes #21764 from maryannxue/rule-exclusion.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OptimizerRuleExclusionSuite.scala']","Lack of a configuration option in Spark SQL optimizer to exclude certain optimization rules, making customization and debugging more difficult."
2514163366feb91de665b704a6fb703855db6bee,1548974372,"[SPARK-26799][BUILD] Make ANTLR v4 version consistent between Maven and SBT

## What changes were proposed in this pull request?
Currently ANTLR v4 versions used by Maven and SBT are slightly different. Maven uses `4.7.1` while SBT uses `4.7`.

* Maven(`pom.xml`): `<antlr4.version>4.7.1</antlr4.version>`
* SBT(`project/SparkBuild.scala`): `antlr4Version in Antlr4 := ""4.7""`

We should make Maven and SBT use a single version. Furthermore we'd better specify antlr4 version in one place to avoid mismatch between Maven and SBT in the future.

This PR lets SBT use antlr4 version specified in Maven POM file, rather than specify its own antlr4 version. This is in the same as how `hadoop.version` is specified in `project/SparkBuild.scala`

## How was this patch tested?
Test locally.

After run `sbt compile`, Java files generated by ANTLR are located at:

```
sql/catalyst/target/scala-2.12/src_managed/main/antlr4/org/apache/spark/sql/catalyst/parser/*.java
```

These Java files have a comment at the head. We can see now SBT uses ANTLR `4.7.1`.

```
// Generated from .../spark/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4 by ANTLR 4.7.1
```

Closes #23713 from seancxmao/antlr4-version-consistent.

Authored-by: seancxmao <seancxmao@gmail.com>
Signed-off-by: gatorsmile <gatorsmile@gmail.com>
",['project/SparkBuild.scala'],"Different versions of ANTLR v4 are being used in Maven and SBT, causing inconsistencies in ANTLR's performance."
392f8d80c8cc4823ea513e78d452bba7f1a7d76c,1689755369,"[SPARK-44264][ML][PYTHON] Support Distributed Training of Functions Using Deepspeed

### What changes were proposed in this pull request?
Made the DeepspeedTorchDistributor run() method use the _run() function as the backbone.
### Why are the changes needed?
It allows the user to run distributed training of a function with deepspeed easily.

### Does this PR introduce _any_ user-facing change?
This adds the ability for the user to pass in a function as the train_object when calling DeepspeedTorchDistributor.run(). The user must have all necessary imports within the function itself, and the function must be picklable. An example use case can be found in the python file linked in the JIRA ticket.

### How was this patch tested?
Notebook/file linked in the JIRA ticket. Formal e2e tests will come in future PR.

### Next Steps/Timeline

- [ ] Add more e2e tests for both running a regular pytorch file and running a function for training
- [ ] Write more documentation

Closes #42067 from mathewjacob1002/add_func_deepspeed.

Authored-by: Mathew Jacob <mathew.jacob@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['python/pyspark/ml/deepspeed/deepspeed_distributor.py'],"DeepspeedTorchDistributor's run() method does not support distributed training of functions in Deepspeed, limiting the ability of users to run distributed training easily."
e43b9e8520bd4ea5bc3693beb496893b17e79054,1631669638,"[SPARK-36733][SQL] Fix a perf issue in SchemaPruning when a struct has many fields

### What changes were proposed in this pull request?

This PR fixes a perf issue in `SchemaPruning` when a struct has many fields (e.g. >10K fields).
The root cause is `SchemaPruning.sortLeftFieldsByRight` does N * M order searching.
```
 val filteredRightFieldNames = rightStruct.fieldNames
    .filter(name => leftStruct.fieldNames.exists(resolver(_, name)))
```

To fix this issue, this PR proposes to use `HashMap` to expect a constant order searching.
This PR also adds `case _ if left == right => left` to the method as a short-circuit code.

### Why are the changes needed?

To fix a perf issue.

### Does this PR introduce _any_ user-facing change?

No. The logic should be identical.

### How was this patch tested?

I confirmed that the following micro benchmark finishes within a few seconds.
```
import org.apache.spark.sql.catalyst.expressions.SchemaPruning
import org.apache.spark.sql.types._

var struct1 = new StructType()
(1 to 50000).foreach { i =>
  struct1 = struct1.add(new StructField(i + """", IntegerType))
}

var struct2 = new StructType()
(50001 to 100000).foreach { i =>
  struct2 = struct2.add(new StructField(i + """", IntegerType))
}

SchemaPruning.sortLeftFieldsByRight(struct1, struct2)
SchemaPruning.sortLeftFieldsByRight(struct2, struct2)
```

The correctness should be checked by existing tests.

Closes #33981 from sarutak/improve-schemapruning-performance.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala'],"Performance issue observed in SchemaPruning when dealing with structures that contain a significant number of fields (e.g., >10K fields). The underlying order searching operation exhibits an N*M complexity, leading to efficiency problems."
0811666ab104b41cf189233439f4158b18bc8282,1600143306,"[SPARK-32878][CORE] Avoid scheduling TaskSetManager which has no pending tasks

### What changes were proposed in this pull request?

This PR proposes to avoid scheduling the (non-zombie) TaskSetManager which has no pending tasks.

### Why are the changes needed?

Currently, Spark always tries to schedule a (non-zombie) TaskSetManager even if it has no pending tasks. This causes notable problems for the barrier TaskSetManager: 1. `calculateAvailableSlots` can be called for multiple times for a launched barrier TaskSetManager; 2. user would see ""Skip current round of resource offers for barrier stage"" log message for
a launched barrier TaskSetManager all the time until the barrier TaskSetManager finishes, which is quite confused.

Besides, scheduling a TaskSetManager always involves many function invocations even if there're no pending tasks.

Therefore, I think we can skip those un-schedulable TasksetManagers to avoid the potential overhead.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass existing tests.

Closes #29750 from Ngone51/filter-out-unschedulable-stage.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/scheduler/Pool.scala', 'core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala', 'core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala', 'core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala']","Spark attempts to schedule a TaskSetManager, even if it has no pending tasks, causing issues and potential overhead, particularly for barrier TaskSetManagers."
2f700773c2e8fac26661d0aa8024253556a921ba,1627917594,"[SPARK-36224][SQL] Use Void as the type name of NullType

### What changes were proposed in this pull request?
Change the `NullType.simpleString` to ""void"" to set ""void"" as the formal type name of `NullType`

### Why are the changes needed?
This PR is intended to address the type name discussion in PR #28833. Here are the reasons:
1. The type name of NullType is displayed everywhere, e.g. schema string, error message, document. Hence it's not possible to hide it from users, we have to choose a proper name
2. The ""void"" is widely used as the type name of ""NULL"", e.g. Hive, pgSQL
3. Changing to ""void"" can enable the round trip of `toDDL`/`fromDDL` for NullType. (i.e. make `from_json(col, schema.toDDL)`) work

### Does this PR introduce _any_ user-facing change?
Yes, the type name of ""NULL"" is changed from ""null"" to ""void"". for example:
```
scala> sql(""select null as a, 1 as b"").schema.catalogString
res5: String = struct<a:void,b:int>
```

### How was this patch tested?
existing test cases

Closes #33437 from linhongliu-db/SPARK-36224-void-type-name.

Authored-by: Linhong Liu <linhong.liu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['python/pyspark/sql/tests/test_types.py', 'python/pyspark/sql/types.py', 'sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/types/NullType.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/types/DataTypeSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcSourceSuite.scala']","The type name of NullType is displayed as ""null"", but ""void"" is a more universally recognized terminology for NullType across other systems like Hive and pgSQL. This discrepancy also hinders round trip of `toDDL`/`fromDDL` for NullType."
72af2c0fbc6673a5e49f1fd6693fe2c90141a84f,1690583387,"[SPARK-44585][MLLIB] Fix warning condition in MLLib RankingMetrics ndcgAk

### What changes were proposed in this pull request?

This PR fixes the condition to raise the following warning in MLLib's RankingMetrics ndcgAk function: ""# of ground truth set and # of relevance value set should be equal, check input data""

The logic for raising warnings is faulty at the moment: it raises a warning if the `rel` input is empty and `lab.size` and `rel.size` are not equal.

The logic should be to raise a warning if `rel` input is **not empty** and `lab.size` and `rel.size` are not equal.

This warning was added in the following PR: https://github.com/apache/spark/pull/36843

### Why are the changes needed?

With the current logic, RankingMetrics will:
- raise incorrect warning when a user is using it in the ""binary"" mode (i.e. no relevance values in the input)
- not raise warning (that could be necessary) when the user is using it in the ""non-binary"" model (i.e. with relevance values in the input)

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
No change made to the test suite for RankingMetrics: https://github.com/uchiiii/spark/blob/a172172329cc78b50f716924f2a344517deb71fc/mllib/src/test/scala/org/apache/spark/mllib/evaluation/RankingMetricsSuite.scala

Closes #42207 from guilhem-depop/patch-1.

Authored-by: Guilhem Vuillier <101632595+guilhem-depop@users.noreply.github.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
",['mllib/src/main/scala/org/apache/spark/mllib/evaluation/RankingMetrics.scala'],Faulty logic in MLLib's RankingMetrics ndcgAk function results in incorrect raising of warnings about unequal ground truth set and relevance value set.
8c8801cf501ddbdeb4a4a869bc27c8a2331531fe,1656337613,"[SPARK-37753][FOLLOWUP][SQL] Fix unit tests sometimes failing

### What changes were proposed in this pull request?
This unit test sometimes fails to run. for example, https://github.com/apache/spark/pull/35715#discussion_r892247619

When the left side is completed first, and then the right side is completed, since it is known that there are many empty partitions on the left side, the broadcast on the right side is demoted.

However, if the right side is completed first and the left side is still being executed, the right side does not know whether there are many empty partitions on the left side, so there is no demote, and then the right side is broadcast in the planning stage.

This PR does this：
When it is found that the other side is QueryStage, if the QueryStage has not been materialized, demote it first. When the other side is completed, judge again whether demote is needed.

### Why are the changes needed?
Fix small problems in logic

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
manual testing

Closes #36966 from mcdull-zhang/wait_other_side.

Authored-by: mcdull-zhang <work4dong@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala'],"Inconsistent completion order between left and right sides in unit tests results in broadcast demotion issue, affecting demotion judgement during execution."
bef5828e12500630d7efc8e0c005182b25ef2b7f,1588687913,"[SPARK-31630][SQL] Fix perf regression by skipping timestamps rebasing after some threshold

### What changes were proposed in this pull request?
Skip timestamps rebasing after a global threshold when there is no difference between Julian and Gregorian calendars. This allows to avoid checking hash maps of switch points, and fixes perf regressions in `toJavaTimestamp()` and `fromJavaTimestamp()`.

### Why are the changes needed?
The changes fix perf regressions of conversions to/from external type `java.sql.Timestamp`.

Before (see the PR's results https://github.com/apache/spark/pull/28440):
```
================================================================================================
Conversion from/to external types
================================================================================================

OpenJDK 64-Bit Server VM 1.8.0_252-8u252-b09-1~18.04-b09 on Linux 4.15.0-1063-aws
Intel(R) Xeon(R) CPU E5-2670 v2  2.50GHz
To/from Java's date-time:                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
From java.sql.Timestamp                             376            388          10         13.3          75.2       1.1X
Collect java.sql.Timestamp                         1878           1937          64          2.7         375.6       0.2X
```

After:
```
================================================================================================
Conversion from/to external types
================================================================================================

OpenJDK 64-Bit Server VM 1.8.0_252-8u252-b09-1~18.04-b09 on Linux 4.15.0-1063-aws
Intel(R) Xeon(R) CPU E5-2670 v2  2.50GHz
To/from Java's date-time:                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
From java.sql.Timestamp                             249            264          24         20.1          49.8       1.7X
Collect java.sql.Timestamp                         1503           1523          24          3.3         300.5       0.3X
```

Perf improvements in average of:

1. From java.sql.Timestamp is ~ 34%
2. To java.sql.Timestamps is ~16%

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
By existing test suites `DateTimeUtilsSuite` and `RebaseDateTimeSuite`.

Closes #28441 from MaxGekk/opt-rebase-common-threshold.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala'],Performance regression observed when converting to/from external type java.sql.Timestamp due to unnecessary timestamps rebasing check after a certain threshold.
dba525c997b0033ac1b6fd24236cd72938f94bbf,1585712546,"[SPARK-31313][K8S][TEST] Add `m01` node name to support Minikube 1.8.x

### What changes were proposed in this pull request?

This PR aims to add `m01` as a node name additionally to `PVTestsSuite`.

### Why are the changes needed?

minikube 1.8.0 ~ 1.8.2 generate a cluster with a nodename `m01` while all the other versions have `minikube`. This causes `PVTestSuite` failure.
```
$ minikube --vm-driver=hyperkit start --memory 6000 --cpus 8
* minikube v1.8.2 on Darwin 10.15.3
  - MINIKUBE_ACTIVE_DOCKERD=minikube
* Using the hyperkit driver based on user configuration
* Creating hyperkit VM (CPUs=8, Memory=6000MB, Disk=20000MB) ...
* Preparing Kubernetes v1.18.0 on Docker 19.03.6 ...
* Launching Kubernetes ...
* Enabling addons: default-storageclass, storage-provisioner
* Waiting for cluster to come online ...
* Done! kubectl is now configured to use ""minikube""

$ kubectl get nodes
NAME   STATUS   ROLES    AGE   VERSION
m01    Ready    master   22s   v1.17.3
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This only adds a new node name. So, K8S Jenkins job should passed.
In addition, `K8s` integration test suite should be tested on `minikube 1.8.2` manually.

```
KubernetesSuite:
- Run SparkPi with no resources
- Run SparkPi with a very long application name.
- Use SparkLauncher.NO_RESOURCE
- Run SparkPi with a master URL without a scheme.
- Run SparkPi with an argument.
- Run SparkPi with custom labels, annotations, and environment variables.
- All pods have the same service account by default
- Run extraJVMOptions check on driver
- Run SparkRemoteFileTest using a remote data file
- Run SparkPi with env and mount secrets.
- Run PySpark on simple pi.py example
- Run PySpark with Python2 to test a pyfiles example
- Run PySpark with Python3 to test a pyfiles example
- Run PySpark with memory customization
- Run in client mode.
- Start pod creation from template
- PVs with local storage
- Launcher client dependencies
- Test basic decommissioning
- Run SparkR on simple dataframe.R example
Run completed in 10 minutes, 23 seconds.
Total number of tests run: 20
Suites: completed 2, aborted 0
Tests: succeeded 20, failed 0, canceled 0, ignored 0, pending 0
All tests passed.
```

For the above test, Minikube 1.8.2 and K8s v1.18.0 is used.
```
$ minikube version
minikube version: v1.8.2
commit: eb13446e786c9ef70cb0a9f85a633194e62396a1

$ kubectl version --short
Client Version: v1.18.0
Server Version: v1.18.0
```

Closes #28080 from dongjoon-hyun/SPARK-31313.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: DB Tsai <d_tsai@apple.com>
",['resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/PVTestsSuite.scala'],"`PVTestsSuite` fails in Minikube versions 1.8.0 - 1.8.2 due to difference in node name, 'm01', as opposed to 'minikube' in other versions."
6051755bfe23a0e4564bf19476ec34cd7fd6008d,1600225995,"[SPARK-32688][SQL][TEST] Add special values to LiteralGenerator for float and double

### What changes were proposed in this pull request?

The `LiteralGenerator` for float and double datatypes was supposed to yield special values (NaN, +-inf) among others, but the `Gen.chooseNum` method does not yield values that are outside the defined range. The `Gen.chooseNum` for a wide range of floats and doubles does not yield values in the ""everyday"" range as stated in https://github.com/typelevel/scalacheck/issues/113 .

There is an similar class `RandomDataGenerator` that is used in some other tests. Added `-0.0` and `-0.0f` as special values to there too.

These changes revealed an inconsistency with the equality check between `-0.0` and `0.0`.

### Why are the changes needed?

The `LiteralGenerator` is mostly used in the `checkConsistencyBetweenInterpretedAndCodegen` method in `MathExpressionsSuite`. This change would have caught the bug fixed in #29495 .

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Locally reverted #29495 and verified that the existing test cases caught the bug.

Closes #29515 from tanelk/SPARK-32688.

Authored-by: Tanel Kiis <tanel.kiis@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
","['sql/catalyst/src/test/scala/org/apache/spark/sql/RandomDataGenerator.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/LiteralGenerator.scala']",`LiteralGenerator` for float and double data types doesn't yield special values outside the defined range and inconsistencies detected in equality check between `-0.0` and `0.0`.
83673c8e28a6429386419770e8ef6f09d49cba85,1647223376,"[SPARK-38528][SQL] Eagerly iterate over aggregate sequence when building project list in `ExtractGenerator`

### What changes were proposed in this pull request?

When building the project list from an aggregate sequence in `ExtractGenerator`, convert the aggregate sequence to an `IndexedSeq` before performing the flatMap operation.

### Why are the changes needed?

This query fails with a `NullPointerException`:
```
val df = Seq(1, 2, 3).toDF(""v"")
df.select(Stream(explode(array(min($""v""), max($""v""))), sum($""v"")): _*).collect
```
If you change `Stream` to `Seq`, then it succeeds.

`ExtractGenerator` uses a flatMap operation over `aggList` for two purposes:

- To produce a new aggregate list
- to update `projectExprs` (which is initialized as an array of nulls).

When `aggList` is a `Stream`, the flatMap operation evaluates lazily, so all entries in `projectExprs` after the first will still be null when the rule completes.

Changing `aggList` to an `IndexedSeq` forces the flatMap to evaluate eagerly.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

New unit test

Closes #35837 from bersprockets/generator_aggregate_issue.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala']",'NullPointerException' occurs in queries due to lazy evaluation in `ExtractGenerator` when building the project list from an aggregate sequence resulting in null entries in `projectExprs`.
718191276efe91dedb4366e25fe8a573ad6f187f,1663622251,"[SPARK-39991][SQL][AQE] Use available column statistics from completed query stages

### What changes were proposed in this pull request?

AQE uses statistics from completed query stages and feeds them back into the logical optimizer. AQE currently only uses `dataSize` and `numOutputRows` and ignores any available `attributeMap` (column statistics).

This PR updates AQE to also populate `attributeMap` in the statistics that it uses for re-optimizing the plan.

### Why are the changes needed?

These changes are needed so that Spark plugins that provide custom implementations of the `ShuffleExchangeLike` trait can leverage column statistics for better plan optimization during AQE execution.

### Does this PR introduce _any_ user-facing change?

No. The current Spark implementation of `ShuffleExchangeLike` (`ShuffleExchangeExec`) does not populate `attributeMap`, so this PR is a no-op for regular Spark.

### How was this patch tested?

New unit test added.

Closes #37424 from andygrove/aqe-column-stats.

Authored-by: Andy Grove <andygrove73@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SparkSessionExtensionSuite.scala']","Adaptive Query Execution (AQE) in Spark SQL does not use available `attributeMap` (column statistics) from completed query stages while re-optimizing the plan, leading to suboptimal query plans in custom implementations of `ShuffleExchangeLike`."
c7b46d4d8aa8da24131d79d2bfa36e8db19662e4,1507300727,"[SPARK-21877][DEPLOY, WINDOWS] Handle quotes in Windows command scripts

## What changes were proposed in this pull request?

All the windows command scripts can not handle quotes in parameter.

Run a windows command shell with parameter which has quotes can reproduce the bug:

```
C:\Users\meng\software\spark-2.2.0-bin-hadoop2.7> bin\spark-shell --driver-java-options "" -Dfile.encoding=utf-8 ""
'C:\Users\meng\software\spark-2.2.0-bin-hadoop2.7\bin\spark-shell2.cmd"" --driver-java-options ""' is not recognized as an internal or external command,
operable program or batch file.
```

Windows recognize ""--driver-java-options"" as part of the command.
All the Windows command script has the following code have the bug.

```
cmd /V /E /C ""<other command>"" %*
```

We should quote command and parameters like

```
cmd /V /E /C """"<other command>"" %*""
```

## How was this patch tested?

Test manually on Windows 10 and Windows 7

We can verify it by the following demo:

```
C:\Users\meng\program\demo>cat a.cmd
echo off
cmd /V /E /C ""b.cmd"" %*

C:\Users\meng\program\demo>cat b.cmd
echo off
echo %*

C:\Users\meng\program\demo>cat c.cmd
echo off
cmd /V /E /C """"b.cmd"" %*""

C:\Users\meng\program\demo>a.cmd ""123""
'b.cmd"" ""123' is not recognized as an internal or external command,
operable program or batch file.

C:\Users\meng\program\demo>c.cmd ""123""
""123""
```

With the spark-shell.cmd example, change it to the following code will make the command execute succeed.

```
cmd /V /E /C """"%~dp0spark-shell2.cmd"" %*""
```

```
C:\Users\meng\software\spark-2.2.0-bin-hadoop2.7> bin\spark-shell  --driver-java-options "" -Dfile.encoding=utf-8 ""
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
...

```

Author: minixalpha <xkzalpha@gmail.com>

Closes #19090 from minixalpha/master.
","['bin/beeline.cmd', 'bin/pyspark.cmd', 'bin/run-example.cmd', 'bin/spark-class.cmd', 'bin/spark-shell.cmd', 'bin/spark-submit.cmd', 'bin/sparkR.cmd']","Windows command scripts are unable to handle parameters that include quotes, leading to misinterpretation and failure of script execution."
7952f44dacd18891e4f78c91146c1cf37dda6a46,1589359600,"[SPARK-31697][WEBUI] HistoryServer should set Content-Type

### What changes were proposed in this pull request?

This PR changes HistoryServer to set Content-Type.

I noticed that we will get html as plain text when we access to wrong URLs which represent non-existence appId on HistoryServer.

```
<html>
      <head>
        <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8""/><meta name=""viewport"" content=""width=device-width, initial-scale=1""/><link rel=""stylesheet"" href=""/static/bootstrap.min.css"" type=""text/css""/><link rel=""stylesheet"" href=""/static/vis-timeline-graph2d.min.css"" type=""text/css""/><link rel=""stylesheet"" href=""/static/webui.css"" type=""text/css""/><link rel=""stylesheet"" href=""/static/timeline-view.css"" type=""text/css""/><script src=""/static/sorttable.js""></script><script src=""/static/jquery-3.4.1.min.js""></script><script src=""/static/vis-timeline-graph2d.min.js""></script><script src=""/static/bootstrap.bundle.min.js""></script><script src=""/static/initialize-tooltips.js""></script><script src=""/static/table.js""></script><script src=""/static/timeline-view.js""></script><script src=""/static/log-view.js""></script><script src=""/static/webui.js""></script><script>setUIRoot('')</script>

        <link rel=""shortcut icon"" href=""/static/spark-logo-77x50px-hd.png""></link>
        <title>Not Found</title>
      </head>
      <body>
        <div class=""container-fluid"">
          <div class=""row"">
            <div class=""col-12"">
              <h3 style=""vertical-align: middle; display: inline-block;"">
                <a style=""text-decoration: none"" href=""/"">
                  <img src=""/static/spark-logo-77x50px-hd.png""/>
                  <span class=""version"" style=""margin-right: 15px;"">3.1.0-SNAPSHOT</span>
                </a>
                Not Found
              </h3>
            </div>
          </div>
          <div class=""row"">
            <div class=""col-12"">
              <div class=""row"">Application local-1589239 not found.</div>
            </div>
          </div>
        </div>
      </body>
    </html>
```
The reason is Content-Type not set. I confirmed it with `curl -I http://localhost:18080/history/<wrong-appId>`
```
HTTP/1.1 404 Not Found
Date: Wed, 13 May 2020 06:59:29 GMT
Cache-Control: no-cache, no-store, must-revalidate
X-Frame-Options: SAMEORIGIN
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
Content-Length: 1778
Server: Jetty(9.4.18.v20190429)
```

### Why are the changes needed?

This is a bug.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

I added a test case for this issue.

Closes #28519 from sarutak/fix-content-type.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala', 'core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala']",Accessing non-existent appId URLs on HistoryServer returns HTML as plain text due to missing Content-Type in the response header.
d01594e8d186e63a6c3ce361e756565e830d5237,1600697140,"[SPARK-32886][WEBUI] fix 'undefined' link in event timeline view

### What changes were proposed in this pull request?

Fix "".../jobs/undefined"" link from ""Event Timeline"" in jobs page. Job page link in ""Event Timeline"" view is constructed by fetching job page link defined in job list below. when job count exceeds page size of job table, only links of jobs in job table can be fetched from page. Other jobs' link would be 'undefined', and links of them in ""Event Timeline"" are broken, they are redirected to some wired URL like "".../jobs/undefined"". This PR is fixing this wrong link issue. With this PR, job link in ""Event Timeline"" view would always redirect to correct job page.

### Why are the changes needed?

Wrong link ("".../jobs/undefined"") in ""Event Timeline"" of jobs page. for example, the first job in below page is not in table below, as job count(116) exceeds page size(100). When clicking it's item in ""Event Timeline"", page is redirected to "".../jobs/undefined"", which is wrong. Links in ""Event Timeline"" should always be correct.
![undefinedlink](https://user-images.githubusercontent.com/10524738/93184779-83fa6d80-f6f1-11ea-8a80-1a304ca9cbb2.JPG)

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Manually tested.

Closes #29757 from zhli1142015/fix-link-event-timeline-view.

Authored-by: Zhen Li <zhli@microsoft.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
",['core/src/main/resources/org/apache/spark/ui/static/timeline-view.js'],"The 'Event Timeline' on the jobs page redirects to an incorrect URL (e.g., "".../jobs/undefined"") when clicking on a job that isn't listed in the job table below because the job count exceeds the page size."
f1bc37e6244e959f1d950c450010dd6024b6ba5f,1611770517,"[SPARK-34221][WEBUI] Ensure if a stage fails in the UI page, the corresponding error message can be displayed correctly

### What changes were proposed in this pull request?
Ensure that if a stage fails in the UI page, the corresponding error message can be displayed correctly.

### Why are the changes needed?
errormessage is not handled properly in JavaScript. If the 'at' is not exist, the error message on the page will be blank.
I made wochanges,
1. `msg.indexOf(""at"")` => `msg.indexOf(""\n"")`

![image](https://user-images.githubusercontent.com/52202080/105663531-7362cb00-5f0d-11eb-87fd-008ed65c33ca.png)

  As shows ablove, truncated at the 'at' position will result in a strange abstract of the error message. If there is a `\n` worit is more reasonable to truncate at the '\n' position.

2. If the `\n` does not exist check whether the msg  is more than 100. If true, then truncate the display to avoid too long error message

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Manual test shows as belows, just a js change:

before modified:
![problem](https://user-images.githubusercontent.com/52202080/105712153-661cff00-5f54-11eb-80bf-e33c323c4e55.png)

after modified
![after mdified](https://user-images.githubusercontent.com/52202080/105712180-6c12e000-5f54-11eb-8998-ff8bc8a0a503.png)

Closes #31314 from akiyamaneko/error_message_display_empty.

Authored-by: neko <echohlne@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['core/src/main/resources/org/apache/spark/ui/static/stagepage.js'],"The UI page is not displaying error messages correctly for failed stages due to improper handling in JavaScript, leading to blank error messages or strange abstracts of them."
177bf672e47977cbb6ccfd88f3ec77687c1fdebe,1571900421,"[SPARK-29522][SQL] CACHE TABLE should look up catalog/table like v2 commands

### What changes were proposed in this pull request?

Add CacheTableStatement and make CACHE TABLE go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.

```
USE my_catalog
DESC t // success and describe the table t from my_catalog
CACHE TABLE t // report table not found as there is no table t in the session catalog
```
### Does this PR introduce any user-facing change?

yes. When running CACHE TABLE, Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26179 from viirya/SPARK-29522.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/CachedTableSuite.scala']","CACHE TABLE command is not using the same catalog/table resolution framework as v2 commands, causing inconsistency in table resolution behavior for end-users."
c24f2b2d6afb411fbfffb90fa87150f3b6912343,1606843666,"[SPARK-33612][SQL] Add dataSourceRewriteRules batch to Optimizer

### What changes were proposed in this pull request?

This PR adds a new batch to the optimizer for executing rules that rewrite plans for data sources.

### Why are the changes needed?

Right now, we have a special place in the optimizer where we construct v2 scans. As time shows, we need more rewrite rules that would be executed after the operator optimization and before any stats-related rules for v2 tables. Not all rules will be specific to reads. One option is to rename the current batch into something more generic but it would require changing quite some places. That's why it seems better to introduce a new batch and use it for all rewrites. The name is generic so that we don't limit ourselves to v2 data sources only.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

The change is trivial and SPARK-23889 will depend on it.

Closes #30558 from aokolnychyi/spark-33612.

Authored-by: Anton Okolnychyi <aokolnychyi@apple.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala']",The Spark SQL optimizer requires additional rewrite rules that should be executed after operator optimization and before any stats-related rules for v2 tables. This is not properly accommodated in the current architecture.
f9180f8752b7cddec463faf82cdb09af707ae402,1552845496,"[SPARK-26979][PYTHON] Add missing string column name support for some SQL functions

## What changes were proposed in this pull request?

Most SQL functions defined in `spark.sql.functions` have two calling patterns, one with a Column object as input, and another with a string representing a column name, which is then converted into a Column object internally.

There are, however, a few notable exceptions:

- lower()
- upper()
- abs()
- bitwiseNOT()
- ltrim()
- rtrim()
- trim()
- ascii()
- base64()
- unbase64()

While this doesn't break anything, as you can easily create a Column object yourself prior to passing it to one of these functions, it has two undesirable consequences:

1. It is surprising - it breaks coder's expectations when they are first starting with Spark. Every API should be as consistent as possible, so as to make the learning curve smoother and to reduce causes for human error;

2. It gets in the way of stylistic conventions. Most of the time it makes Python code more readable to use literal names, and the API provides ample support for that, but these few exceptions prevent this pattern from being universally applicable.

This patch is meant to fix the aforementioned problem.

### Effect

This patch **enables** support for passing column names as input to those functions mentioned above.

### Side effects

This PR also **fixes** an issue with some functions being defined multiple times by using `_create_function()`.

### How it works

`_create_function()` was redefined to always convert the argument to a Column object. The old implementation has been kept under `_create_name_function()`, and is still being used to generate the following special functions:

- lit()
- col()
- column()
- asc()
- desc()
- asc_nulls_first()
- asc_nulls_last()
- desc_nulls_first()
- desc_nulls_last()

This is because these functions can only take a column name as their argument. This is not a problem, as their semantics require so.

## How was this patch tested?

Ran ./dev/run-tests and tested it manually.

Closes #23882 from asmello/col-name-support-pyspark.

Authored-by: André Sá de Mello <amello@palantir.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
",['python/pyspark/sql/functions.py'],"Several SQL functions (lower, upper, abs, bitwiseNOT, ltrim, rtrim, trim, ascii, base64, unbase64) in spark.sql.functions only support Column object as input, breaking conventions and disrupting coder's expectations as usually column names can also be passed to these functions."
dcaf62afea8791e49a44c2062fe14bafdcc0e92f,1612751543,"[SPARK-34346][CORE][TESTS][FOLLOWUP] Fix UT by removing core-site.xml

### What changes were proposed in this pull request?

This is a follow-up for SPARK-34346 which causes a flakiness due to `core-site.xml` test resource file addition. This PR aims to remove the test resource `core/src/test/resources/core-site.xml` from `core` module.

### Why are the changes needed?

Due to the test resource `core-site.xml`, YARN UT becomes flaky in GitHub Action and Jenkins.
```
$ build/sbt ""yarn/testOnly *.YarnClusterSuite -- -z SPARK-16414"" -Pyarn
...
[info] YarnClusterSuite:
[info] - yarn-cluster should respect conf overrides in SparkHadoopUtil (SPARK-16414, SPARK-23630) *** FAILED *** (20 seconds, 209 milliseconds)
[info]   FAILED did not equal FINISHED (stdout/stderr was not captured) (BaseYarnClusterSuite.scala:210)
```

To isolate more, we may use `SPARK_TEST_HADOOP_CONF_DIR` like `yarn` module's `yarn/Client`, but it seems an overkill in `core` module.
```
// SPARK-23630: during testing, Spark scripts filter out hadoop conf dirs so that user's
// environments do not interfere with tests. This allows a special env variable during
// tests so that custom conf dirs can be used by unit tests.
val confDirs = Seq(""HADOOP_CONF_DIR"", ""YARN_CONF_DIR"") ++
  (if (Utils.isTesting) Seq(""SPARK_TEST_HADOOP_CONF_DIR"") else Nil)
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs.

Closes #31515 from dongjoon-hyun/SPARK-34346-2.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['core/src/test/scala/org/apache/spark/SparkContextSuite.scala'],"The addition of `core-site.xml` test resource file is causing flakiness in YARN unit tests on GitHub Actions and Jenkins, indicating improper configuration handling."
cbb41a0c5b01579c85f06ef42cc0585fbef216c5,1523662299,"[SPARK-23966][SS] Refactoring all checkpoint file writing logic in a common CheckpointFileManager interface

## What changes were proposed in this pull request?

Checkpoint files (offset log files, state store files) in Structured Streaming must be written atomically such that no partial files are generated (would break fault-tolerance guarantees). Currently, there are 3 locations which try to do this individually, and in some cases, incorrectly.

1. HDFSOffsetMetadataLog - This uses a FileManager interface to use any implementation of `FileSystem` or `FileContext` APIs. It preferably loads `FileContext` implementation as FileContext of HDFS has atomic renames.
1. HDFSBackedStateStore (aka in-memory state store)
  - Writing a version.delta file - This uses FileSystem APIs only to perform a rename. This is incorrect as rename is not atomic in HDFS FileSystem implementation.
  - Writing a snapshot file - Same as above.

#### Current problems:
1. State Store behavior is incorrect - HDFS FileSystem implementation does not have atomic rename.
1. Inflexible - Some file systems provide mechanisms other than write-to-temp-file-and-rename for writing atomically and more efficiently. For example, with S3 you can write directly to the final file and it will be made visible only when the entire file is written and closed correctly. Any failure can be made to terminate the writing without making any partial files visible in S3. The current code does not abstract out this mechanism enough that it can be customized.

#### Solution:

1. Introduce a common interface that all 3 cases above can use to write checkpoint files atomically.
2. This interface must provide the necessary interfaces that allow customization of the write-and-rename mechanism.

This PR does that by introducing the interface `CheckpointFileManager` and modifying `HDFSMetadataLog` and `HDFSBackedStateStore` to use the interface. Similar to earlier `FileManager`, there are implementations based on `FileSystem` and `FileContext` APIs, and the latter implementation is preferred to make it work correctly with HDFS.

The key method this interface has is `createAtomic(path, overwrite)` which returns a `CancellableFSDataOutputStream` that has the method `cancel()`. All users of this method need to either call `close()` to successfully write the file, or `cancel()` in case of an error.

## How was this patch tested?
New tests in `CheckpointFileManagerSuite` and slightly modified existing tests.

Author: Tathagata Das <tathagata.das1565@gmail.com>

Closes #21048 from tdas/SPARK-23966.
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CheckpointFileManager.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/CheckpointFileManagerSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLogSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLogSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreSuite.scala']","Checkpoint file writing in Structured Streaming lacks atomicity, leading to generation of partial files and incorrect behavior with HDFS. Also, current implementation does not support customization for different file systems."
f4073020adf9752c7d7b39631ec3fa36d6345902,1505669684,"[SPARK-22032][PYSPARK] Speed up StructType conversion

## What changes were proposed in this pull request?

StructType.fromInternal is calling f.fromInternal(v) for every field.
We can use precalculated information about type to limit the number of function calls. (its calculated once per StructType and used in per record calculations)

Benchmarks (Python profiler)
```
df = spark.range(10000000).selectExpr(""id as id0"", ""id as id1"", ""id as id2"", ""id as id3"", ""id as id4"", ""id as id5"", ""id as id6"", ""id as id7"", ""id as id8"", ""id as id9"", ""struct(id) as s"").cache()
df.count()
df.rdd.map(lambda x: x).count()
```

Before
```
310274584 function calls (300272456 primitive calls) in 1320.684 seconds

Ordered by: internal time, cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
 10000000  253.417    0.000  486.991    0.000 types.py:619(<listcomp>)
 30000000  192.272    0.000 1009.986    0.000 types.py:612(fromInternal)
100000000  176.140    0.000  176.140    0.000 types.py:88(fromInternal)
 20000000  156.832    0.000  328.093    0.000 types.py:1471(_create_row)
    14000  107.206    0.008 1237.917    0.088 {built-in method loads}
 20000000   80.176    0.000 1090.162    0.000 types.py:1468(<lambda>)
```

After
```
210274584 function calls (200272456 primitive calls) in 1035.974 seconds

Ordered by: internal time, cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
 30000000  215.845    0.000  698.748    0.000 types.py:612(fromInternal)
 20000000  165.042    0.000  351.572    0.000 types.py:1471(_create_row)
    14000  116.834    0.008  946.791    0.068 {built-in method loads}
 20000000   87.326    0.000  786.073    0.000 types.py:1468(<lambda>)
 20000000   85.477    0.000  134.607    0.000 types.py:1519(__new__)
 10000000   65.777    0.000  126.712    0.000 types.py:619(<listcomp>)
```

Main difference is types.py:619(<listcomp>) and types.py:88(fromInternal) (which is removed in After)
The number of function calls is 100 million less. And performance is 20% better.

Benchmark (worst case scenario.)

Test
```
df = spark.range(1000000).selectExpr(""current_timestamp as id0"", ""current_timestamp as id1"", ""current_timestamp as id2"", ""current_timestamp as id3"", ""current_timestamp as id4"", ""current_timestamp as id5"", ""current_timestamp as id6"", ""current_timestamp as id7"", ""current_timestamp as id8"", ""current_timestamp as id9"").cache()
df.count()
df.rdd.map(lambda x: x).count()
```

Before
```
31166064 function calls (31163984 primitive calls) in 150.882 seconds
```

After
```
31166064 function calls (31163984 primitive calls) in 153.220 seconds
```

IMPORTANT:
The benchmark was done on top of https://github.com/apache/spark/pull/19246.
Without https://github.com/apache/spark/pull/19246 the performance improvement will be even greater.

## How was this patch tested?

Existing tests.
Performance benchmark.

Author: Maciej Bryński <maciek-github@brynski.pl>

Closes #19249 from maver1ck/spark_22032.
",['python/pyspark/sql/types.py'],"StructType.fromInternal in PySpark is executing a high number of unnecessary function calls for every field, resulting in a noticeable slowdown in conversion performance."
bd7510bcb75bf6543ac4065f95723e2943114dcb,1578503741,"[SPARK-30281][SS] Consider partitioned/recursive option while verifying archive path on FileStreamSource

### What changes were proposed in this pull request?

This patch renews the verification logic of archive path for FileStreamSource, as we found the logic doesn't take partitioned/recursive options into account.

Before the patch, it only requires the archive path to have depth more than 2 (two subdirectories from root), leveraging the fact FileStreamSource normally reads the files where the parent directory matches the pattern or the file itself matches the pattern. Given 'archive' operation moves the files to the base archive path with retaining the full path, archive path is tend to be safe if the depth is more than 2, meaning FileStreamSource doesn't re-read archived files as new source files.

WIth partitioned/recursive options, the fact is invalid, as FileStreamSource can read any files in any depth of subdirectories for source pattern. To deal with this correctly, we have to renew the verification logic, which may not intuitive and simple but works for all cases.

The new verification logic prevents both cases:

1) archive path matches with source pattern as ""prefix"" (the depth of archive path > the depth of source pattern)

e.g.
* source pattern: `/hello*/spar?`
* archive path: `/hello/spark/structured/streaming`

Any files in archive path will match with source pattern when recursive option is enabled.

2) source pattern matches with archive path as ""prefix"" (the depth of source pattern > the depth of archive path)

e.g.
* source pattern: `/hello*/spar?/structured/hello2*`
* archive path: `/hello/spark/structured`

Some archive files will not match with source pattern, e.g. file path:  `/hello/spark/structured/hello2`, then final archived path: `/hello/spark/structured/hello/spark/structured/hello2`.

But some other archive files will still match with source pattern, e.g. file path: `/hello2/spark/structured/hello2`, then final archived path: `/hello/spark/structured/hello2/spark/structured/hello2` which matches with source pattern when recursive is enabled.

Implicitly it also prevents archive path matches with source pattern as full match (same depth).

We would want to prevent any source files to be archived and added to new source files again, so the patch takes most restrictive approach to prevent the possible cases.

### Why are the changes needed?

Without this patch, there's a chance archived files are included as new source files when partitioned/recursive option is enabled, as current condition doesn't take these options into account.

### Does this PR introduce any user-facing change?

Only for Spark 3.0.0-preview (only preview 1 for now, but possibly preview 2 as well) - end users are required to provide archive path with ensuring a bit complicated conditions, instead of simply higher than 2 depths.

### How was this patch tested?

New UT.

Closes #26920 from HeartSaVioR/SPARK-30281.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala']",FileStreamSource's current path verification logic doesn't take into account partitioned/recursive options leading to cases where archived files are included as new source files.
905b7f7fc7d887da5d7c366b191baf4beea0aadf,1567778799,"[SPARK-28967][CORE] Include cloned version of ""properties"" to avoid ConcurrentModificationException

### What changes were proposed in this pull request?

This patch fixes the bug which throws ConcurrentModificationException when job with 0 partition is submitted via DAGScheduler.

### Why are the changes needed?

Without this patch, structured streaming query throws ConcurrentModificationException, like below stack trace:

```
19/09/04 09:48:49 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception
java.util.ConcurrentModificationException
	at java.util.Hashtable$Enumerator.next(Hashtable.java:1387)
	at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)
	at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:237)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.util.JsonProtocol$.mapToJson(JsonProtocol.scala:514)
	at org.apache.spark.util.JsonProtocol$.$anonfun$propertiesToJson$1(JsonProtocol.scala:520)
	at scala.Option.map(Option.scala:163)
	at org.apache.spark.util.JsonProtocol$.propertiesToJson(JsonProtocol.scala:519)
	at org.apache.spark.util.JsonProtocol$.jobStartToJson(JsonProtocol.scala:155)
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:79)
	at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:149)
	at org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:217)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:99)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:84)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:102)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:102)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:93)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:93)
```

Please refer https://issues.apache.org/jira/browse/SPARK-28967 for detailed reproducer.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Newly added UT. Also manually tested via running simple structured streaming query in spark-shell.

Closes #25672 from HeartSaVioR/SPARK-28967.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
","['core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala', 'core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala']","ConcurrentModificationException is thrown during structured streaming query job submission via DAGScheduler, particularly when a job with 0 partition is submitted."
12d984c15f30c2f31b62c403738b40d8872b7c13,1663033575,"[SPARK-40376][PYTHON] Avoid Numpy deprecation warning

### What changes were proposed in this pull request?

Use `bool` instead of `np.bool` as `np.bool` will be deprecated (see: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations)

Using `np.bool` generates this warning:

```
UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.
3070E                     `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
3071E                   Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
```

### Why are the changes needed?
Deprecation soon: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations.

### Does this PR introduce _any_ user-facing change?
The warning will be suppressed

### How was this patch tested?
Existing tests should suffice.

Closes #37817 from ELHoussineT/patch-1.

Authored-by: ELHoussineT <elhoussinetalab@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
",['python/pyspark/sql/pandas/conversion.py'],Usage of `np.bool` in 'spark.sql.execution.arrow.pyspark.enabled' is causing a deprecation warning as it is set to be deprecated in an upcoming Numpy version.
dcd710d3e12f6cc540cea2b8c747bb6b61254504,1683327630,"[SPARK-43340][CORE] Handle missing stack-trace field in eventlogs

### What changes were proposed in this pull request?

This PR fixes a regression introduced by #36885 which broke JsonProtocol's ability to handle missing fields from exception field. old eventlogs missing a `Stack Trace` will raise a NPE.
As a result, SHS misinterprets  failed-jobs/SQLs as `Active/Incomplete`

This PR solves this problem by checking the JsonNode for null. If it is null, an empty array of `StackTraceElements`

### Why are the changes needed?

Fix a case which prevents the history server from identifying failed jobs if the stacktrace was not set.

Example eventlog

```
{
   ""Event"":""SparkListenerJobEnd"",
   ""Job ID"":31,
   ""Completion Time"":1616171909785,
   ""Job Result"":{
      ""Result"":""JobFailed"",
      ""Exception"":{
         ""Message"":""Job aborted""
      }
   }
}
```

**Original behavior:**

The job is marked as `incomplete`

Error from the SHS logs:

```
23/05/01 21:57:16 INFO FsHistoryProvider: Parsing file:/tmp/nds_q86_fail_test to re-build UI...
23/05/01 21:57:17 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/tmp/nds_q86_fail_test
java.lang.NullPointerException
    at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractElements(JsonProtocol.scala:1589)
    at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1558)
    at org.apache.spark.util.JsonProtocol$.exceptionFromJson(JsonProtocol.scala:1569)
    at org.apache.spark.util.JsonProtocol$.jobResultFromJson(JsonProtocol.scala:1423)
    at org.apache.spark.util.JsonProtocol$.jobEndFromJson(JsonProtocol.scala:967)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:878)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)
....
23/05/01 21:57:17 ERROR ReplayListenerBus: Malformed line #24368: {""Event"":""SparkListenerJobEnd"",""Job ID"":31,""Completion Time"":1616171909785,""Job Result"":{""Result"":""JobFailed"",""Exception"":
{""Message"":""Job aborted""}
}}
```

**After the fix:**

Job 31 is marked as `failedJob`

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Added new unit test in JsonProtocolSuite.

Closes #41050 from amahussein/aspark-43340-b.

Authored-by: Ahmed Hussein <ahussein@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/util/JsonProtocol.scala', 'core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala']","History server misinterprets failed jobs as 'Active/Incomplete' when the 'stack-trace' field in the eventlogs is missing, causing a NullPointerException error."
5864e8e47496c3a841b97632e5137de87a91efea,1549944993,"[SPARK-25158][SQL] Executor accidentally exit because ScriptTransformationWriterThread throw Exception.

## What changes were proposed in this pull request?

Run Spark-Sql job use transform features(`ScriptTransformationExec`) with config `spark.speculation = true`, sometimes job fails and we found many Executor Dead through `Executor Tab`, through analysis log and code we found :

`ScriptTransformationExec` start a new thread(`ScriptTransformationWriterThread`), the new thread is very likely to throw `TaskKilledException`(from iter.map.foreach part) when speculation is on, this exception will captured by `SparkUncaughtExceptionHandler` which registered during Executor start, `SparkUncaughtExceptionHandler` will call `System.exit (SparkExitCode.UNCAUGHT_EXCEPTION)` to shutdown `Executor`, this is unexpected.

We should not kill the executor just because `ScriptTransformationWriterThread` fails. log the error(not only `TaskKilledException`) instead of throwing it is enough, Exception already pass to `ScriptTransformationExec` and handle by `TaskRunner`.

## How was this patch tested?

Register `TestUncaughtExceptionHandler` to test case in `ScriptTransformationSuite`, then assert there is no Uncaught Exception handled.

Before this patch ""script transformation should not swallow errors from upstream operators (no serde)"" and ""script transformation should not swallow errors from upstream operators (with serde)""  throwing `IllegalArgumentException` and handle by `TestUncaughtExceptionHandler` .

Closes #22149 from LuciferYang/fix-transformation-task-kill.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/ScriptTransformationExec.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ScriptTransformationSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/TestUncaughtExceptionHandler.scala']","When using Spark-SQL's transform feature with speculation turned on, `ScriptTransformationWriterThread` might throw a `TaskKilledException`, leading to an unexpected Executor shutdown."
ef8f22e53d31225b3429e2f24ca588d113fc7462,1687300426,"[SPARK-44012][SS] KafkaDataConsumer to print some read status

### What changes were proposed in this pull request?
In the end of each KafkaDataConsumer, it logs some stats. Here is an sample log line:

23/06/08 23:48:14 INFO KafkaDataConsumer: From Kafka topicPartition=topic-121-2 groupId=spark-kafka-source-623fa0a8-04a5-4f34-a9ad-adbf31494e85-711383366-executor read 1 records, taking 504554479 nanos, during time span of 504620999 nanos

### Why are the changes needed?
For each task, Kafka source should report fraction of time spent in KafkaConsumer to fetch records. It should also report overall read bandwidth (bytes or records read / time spent fetching).

This will be useful in verifying if fetching is the bottleneck.

### Does this PR introduce _any_ user-facing change?
no

### How was this patch tested?
1. Run unit tests and validate log line is correct
2. Run some benchmarks and see it doesn't show up much in CPU profiling.

Closes #41525 from siying/kafka_logging2.

Authored-by: Siying Dong <siying.dong@databricks.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
",['connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/consumer/KafkaDataConsumer.scala'],KafkaDataConsumer's lack of detailed read status and time consumption is obfuscating potential bottlenecks in data fetching from Kafka topics.
969d2d4f62759f75125c0ee7e2324bda8927527c,1663814645,"[SPARK-40142][PYTHON][DOCS][FOLLOW-UP] Remove non-ANSI compliant example in element_at

### What changes were proposed in this pull request?

This PR is a followup of https://github.com/apache/spark/pull/37850 that removes non-ANSI compliant example in `element_at`.

### Why are the changes needed?

ANSI build fails to run the example.

https://github.com/apache/spark/actions/runs/3094607589/jobs/5008176959

```
    Caused by: org.apache.spark.SparkArrayIndexOutOfBoundsException: [INVALID_ARRAY_INDEX_IN_ELEMENT_AT] The index -4 is out of bounds. The array has 3 elements. Use `try_element_at` to tolerate accessing element at invalid index and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
    	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidElementAtIndexError(QueryExecutionErrors.scala:264)
    	...

/usr/local/pypy/pypy3.7/lib-python/3/runpy.py:125: RuntimeWarning: 'pyspark.sql.functions' found in sys.modules after import of package 'pyspark.sql', but prior to execution of 'pyspark.sql.functions'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
/__w/spark/spark/python/pyspark/context.py:310: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
  warnings.warn(""Python 3.7 support is deprecated in Spark 3.4."", FutureWarning)
**********************************************************************
   1 of   6 in pyspark.sql.functions.element_at

```

### Does this PR introduce _any_ user-facing change?

No. The example added is not exposed to end users yet.

### How was this patch tested?
Manually tested with enabling the ANSI configuration (`spark.sql.ansi.enabled`)

Closes #37959 from HyukjinKwon/SPARK-40142-followup.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['python/pyspark/sql/functions.py'],The ANSI build fails to run the example provided in `element_at` due to an out-of-bounds array index error.
0823aec4630e70323e66bea243871aaab761d9ca,1578908835,"[SPARK-30480][PYTHON][TESTS] Increases the memory limit being tested in 'WorkerMemoryTest.test_memory_limit'

### What changes were proposed in this pull request?

This PR proposes to increase the memory in `WorkerMemoryTest.test_memory_limit` in order to make the test pass with PyPy.

The test is currently failed only in PyPy as below in some PRs unexpectedly:

```
Current mem limits: 18446744073709551615 of max 18446744073709551615

Setting mem limits to 1048576 of max 1048576

RPython traceback:
  File ""pypy_module_pypyjit_interp_jit.c"", line 289, in portal_5
  File ""pypy_interpreter_pyopcode.c"", line 3468, in handle_bytecode__AccessDirect_None
  File ""pypy_interpreter_pyopcode.c"", line 5558, in dispatch_bytecode__AccessDirect_None
out of memory: couldn't allocate the next arena
ERROR
```

It seems related to how PyPy allocates the memory and GC works PyPy-specifically. There seems nothing wrong in this configuration implementation itself in PySpark side.

I roughly tested in higher PyPy versions on Ubuntu (PyPy v7.3.0) and this test seems passing fine so I suspect this might be an issue in old PyPy behaviours.

The change only increases the limit so it would not affect actual memory allocations. It just needs to test if the limit is properly set in worker sides. For clarification, the memory is unlimited in the machine if not set.

### Why are the changes needed?

To make the tests pass and unblock other PRs.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually and Jenkins should test it out.

Closes #27186 from HyukjinKwon/SPARK-30480.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['python/pyspark/tests/test_worker.py', 'python/run-tests.py']",WorkerMemoryTest.test_memory_limit fails unexpectedly in PyPy due to out of memory error. The issue seems specific to PyPy's memory allocation and GC work.
7d8a721278a007bccd0a3762d0d198129e4e52e2,1639371138,"[SPARK-37569][SQL] Don't mark nested view fields as nullable

### What changes were proposed in this pull request?

When analyzing a view, we should not unnecessarily mark nested fields as nullable. If the columns projected by the view define themselves as non-nullable, their nullability should be preserved.

### Why are the changes needed?

Consider a view as follows with all fields non-nullable (required)
```
spark.sql(""""""
    CREATE OR REPLACE VIEW v AS
    SELECT id, named_struct('a', id) AS nested
    FROM RANGE(10)
"""""")
```

When trying to read this view, it incorrectly marks nested column a as nullable
```
scala> spark.table(""v2"").printSchema
root
 |-- id: long (nullable = false)
 |-- nested: struct (nullable = false)
 |    |-- a: long (nullable = true)
```

However, we can see that the view schema has been correctly stored as non-nullable
```
scala> System.out.println(spark.sessionState.catalog.externalCatalog.getTable(""default"", ""v2""))
CatalogTable(
Database: default
Table: v2
.
.
.
Schema: root
 |-- id: long (nullable = false)
 |-- nested: struct (nullable = false)
 |    |-- a: long (nullable = false)
)
```

This is caused by [this line](https://github.com/apache/spark/blob/fb40c0e19f84f2de9a3d69d809e9e4031f76ef90/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3546) in Analyzer.scala. Going through the history of changes for this block of code, it seems like `asNullable` is a remnant of a time before we added [checks](https://github.com/apache/spark/blob/fb40c0e19f84f2de9a3d69d809e9e4031f76ef90/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3543) to ensure that the from and to types of the cast were compatible. As nullability is already checked, it should be safe to add a cast without converting the target datatype to nullable.

### Does this PR introduce _any_ user-facing change?
Yes. View analysis will preserve nullability of nested fields instead of marking all nested fields as nullable.

### How was this patch tested?
Added unit test

Closes #34839 from shardulm94/view-nullability.

Authored-by: Shardul Mahadik <smahadik@linkedin.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewTestSuite.scala']","Nested fields in views are erroneously being marked as nullable even when defined as non-nullable in the original columns, causing discrepancies in the schema interpretation."
882f54b0a323fb5cd827d600b3c3332e1fcdf65a,1574122301,"[SPARK-29870][SQL][FOLLOW-UP] Keep CalendarInterval's toString

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/26418. This PR removed `CalendarInterval`'s `toString` with an unfinished changes.

### Why are the changes needed?

1. Ideally we should make each PR isolated and separate targeting one issue without touching unrelated codes.

2. There are some other places where the string formats were exposed to users. For example:

    ```scala
    scala> sql(""select interval 1 days as a"").selectExpr(""to_csv(struct(a))"").show()
    ```
    ```
    +--------------------------+
    |to_csv(named_struct(a, a))|
    +--------------------------+
    |      ""CalendarInterval...|
    +--------------------------+
    ```

3.  Such fixes:

    ```diff
     private def writeMapData(
        map: MapData, mapType: MapType, fieldWriter: ValueWriter): Unit = {
      val keyArray = map.keyArray()
    + val keyString = mapType.keyType match {
    +   case CalendarIntervalType =>
    +    (i: Int) => IntervalUtils.toMultiUnitsString(keyArray.getInterval(i))
    +   case _ => (i: Int) => keyArray.get(i, mapType.keyType).toString
    + }
    ```

    can cause performance regression due to type dispatch for each map.

### Does this PR introduce any user-facing change?

Yes, see 2. case above.

### How was this patch tested?

Manually tested.

Closes #26572 from HyukjinKwon/SPARK-29783.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['common/unsafe/src/main/java/org/apache/spark/unsafe/types/CalendarInterval.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala']","The removal of `CalendarInterval`'s `toString` method unintentionally affects string formatting in certain contexts, exposing raw string formats like `CalendarInterval...` to users unexpectedly, and has the potential to cause a performance regression due to type dispatch for each map."
c98725a2b9574ba3c9a10567af740db7467df59d,1649041400,"[SPARK-32268][SQL][FOLLOWUP] Add ColumnPruning in injectBloomFilter

### What changes were proposed in this pull request?
Add `ColumnPruning` in `InjectRuntimeFilter.injectBloomFilter` to optimize the BoomFilter creation query.

### Why are the changes needed?
It seems BloomFilter subqueries injected by `InjectRuntimeFilter` will read as many columns as filterCreationSidePlan. This does not match ""Only scan the required columns"" as the design said. We can check this by a simple case in `InjectRuntimeFilterSuite`:
```scala
withSQLConf(SQLConf.RUNTIME_BLOOM_FILTER_ENABLED.key -> ""true"",
  SQLConf.RUNTIME_BLOOM_FILTER_APPLICATION_SIDE_SCAN_SIZE_THRESHOLD.key -> ""3000"",
  SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> ""2000"") {
  val query = ""select * from bf1 join bf2 on bf1.c1 = bf2.c2 where bf2.a2 = 62""
  sql(query).explain()
}
```
The reason is subqueries have not been optimized by `ColumnPruning`, and this pr will fix it.

### Does this PR introduce _any_ user-facing change?
No, not released

### How was this patch tested?
Improve the test by adding `columnPruningTakesEffect` to check the optimizedPlan of bloom filter join.

Closes #36047 from Flyangz/SPARK-32268-FOllOWUP.

Authored-by: Yang Liu <yintai@xiaohongshu.com>
Signed-off-by: Yuming Wang <yumwang@ebay.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala', 'sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala']","BloomFilter subqueries injected by `InjectRuntimeFilter` read as many columns as filterCreationSidePlan, not adhering to ""Only scan the required columns"" principle, which leads to inefficient BloomFilter creation."
1d1eacde9d1b6fb75a20e4b909d221e70ad737db,1591718842,"[SPARK-31220][SQL] repartition obeys initialPartitionNum when adaptiveExecutionEnabled

### What changes were proposed in this pull request?
This PR makes `repartition`/`DISTRIBUTE BY` obeys [initialPartitionNum](https://github.com/apache/spark/blob/af4248b2d661d04fec89b37857a47713246d9465/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L446-L455) when adaptive execution enabled.

### Why are the changes needed?
To make `DISTRIBUTE BY`/`GROUP BY` partitioned by same partition number.
How to reproduce:
```scala
spark.sql(""CREATE TABLE spark_31220(id int)"")
spark.sql(""set spark.sql.adaptive.enabled=true"")
spark.sql(""set spark.sql.adaptive.coalescePartitions.initialPartitionNum=1000"")
```

Before this PR:
```
scala> spark.sql(""SELECT id from spark_31220 GROUP BY id"").explain
== Physical Plan ==
AdaptiveSparkPlan(isFinalPlan=false)
+- HashAggregate(keys=[id#5], functions=[])
   +- Exchange hashpartitioning(id#5, 1000), true, [id=#171]
      +- HashAggregate(keys=[id#5], functions=[])
         +- FileScan parquet default.spark_31220[id#5]

scala> spark.sql(""SELECT id from spark_31220 DISTRIBUTE BY id"").explain
== Physical Plan ==
AdaptiveSparkPlan(isFinalPlan=false)
+- Exchange hashpartitioning(id#5, 200), false, [id=#179]
   +- FileScan parquet default.spark_31220[id#5]
```
After this PR:
```
scala> spark.sql(""SELECT id from spark_31220 GROUP BY id"").explain
== Physical Plan ==
AdaptiveSparkPlan(isFinalPlan=false)
+- HashAggregate(keys=[id#5], functions=[])
   +- Exchange hashpartitioning(id#5, 1000), true, [id=#171]
      +- HashAggregate(keys=[id#5], functions=[])
         +- FileScan parquet default.spark_31220[id#5]

scala> spark.sql(""SELECT id from spark_31220 DISTRIBUTE BY id"").explain
== Physical Plan ==
AdaptiveSparkPlan(isFinalPlan=false)
+- Exchange hashpartitioning(id#5, 1000), false, [id=#179]
   +- FileScan parquet default.spark_31220[id#5]
```

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Unit test.

Closes #27986 from wangyum/SPARK-31220.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala']","When adaptive execution is enabled, `repartition`/`DISTRIBUTE BY` commands don't abide by the set `initialPartitionNum`, causing inconsistent partition numbers in `GROUP BY` and `DISTRIBUTE BY` operations."
4bad4b6c4bb70a1a19c386466af134077e2106c1,1696782689,"[SPARK-45454][SQL] Set the table's default owner to current_user

### What changes were proposed in this pull request?

This PR sets the table's default owner to `CURRENT_USER`.

### Why are the changes needed?

In thrift server mode, the owner of the table is inconsistent with the `SELECT CURRENT_USER();`,  the owner of the table is always the user who started the thrift server.

### Does this PR introduce _any_ user-facing change?

The table owner may be changed to `CURRENT_USER`.

For example:
```
Before this PR:
yumwangG9L07H60PK spark-3.5.0-bin-hadoop3 % bin/beeline -u ""jdbc:hive2://localhost:10000/"" -n test_table_owner -e ""create table t(id int) using parquet; desc formatted t;"" | grep Owner
Connecting to jdbc:hive2://localhost:10000/
Connected to: Spark SQL (version 3.5.0)
Driver: Hive JDBC (version 2.3.9)
Transaction isolation: TRANSACTION_REPEATABLE_READ
No rows selected (0.36 seconds)
No rows selected (0.1 seconds)
| Owner                         | yumwang                                            |          |
16 rows selected (0.055 seconds)
Beeline version 2.3.9 by Apache Hive
Closing: 0: jdbc:hive2://localhost:10000/

After this PR:
yumwangG9L07H60PK spark-4.0.0-SNAPSHOT-bin-3.3.6 % bin/beeline -u ""jdbc:hive2://localhost:10000/"" -n test_table_owner -e ""create table t(id int) using parquet; desc formatted t;"" | grep Owner
Connecting to jdbc:hive2://localhost:10000/
Connected to: Spark SQL (version 4.0.0-SNAPSHOT)
Driver: Hive JDBC (version 2.3.9)
Transaction isolation: TRANSACTION_REPEATABLE_READ
No rows selected (0.719 seconds)
No rows selected (0.335 seconds)
| Owner                         | test_table_owner                                   |          |
16 rows selected (0.065 seconds)
Beeline version 2.3.9 by Apache Hive
Closing: 0: jdbc:hive2://localhost:10000/
```

### How was this patch tested?

Unit test and manual test.

### Was this patch authored or co-authored using generative AI tooling?

No.

Closes #43264 from wangyum/SPARK-45454.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CurrentUserContext.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/CatalogSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerWithSparkContextSuite.scala']","In thrift server mode, the owner of a table created is inconsistent with the current user, it defaults to the user starting the thrift server."
3a4afce96c6840431ed45280742f9e969be19639,1565797484,"[SPARK-28687][SQL] Support `epoch`, `isoyear`, `milliseconds` and `microseconds` at `extract()`

## What changes were proposed in this pull request?

In the PR, I propose new expressions `Epoch`, `IsoYear`, `Milliseconds` and `Microseconds`, and support additional parameters of `extract()` for feature parity with PostgreSQL (https://www.postgresql.org/docs/11/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT):

1. `epoch` - the number of seconds since 1970-01-01 00:00:00 local time in microsecond precision.
2. `isoyear` - the ISO 8601 week-numbering year that the date falls in. Each ISO 8601 week-numbering year begins with the Monday of the week containing the 4th of January.
3. `milliseconds` - the seconds field including fractional parts multiplied by 1,000.
4. `microseconds` - the seconds field including fractional parts multiplied by 1,000,000.

Here are examples:
```sql
spark-sql> SELECT EXTRACT(EPOCH FROM TIMESTAMP '2019-08-11 19:07:30.123456');
1565550450.123456
spark-sql> SELECT EXTRACT(ISOYEAR FROM DATE '2006-01-01');
2005
spark-sql> SELECT EXTRACT(MILLISECONDS FROM TIMESTAMP '2019-08-11 19:07:30.123456');
30123.456
spark-sql> SELECT EXTRACT(MICROSECONDS FROM TIMESTAMP '2019-08-11 19:07:30.123456');
30123456
```

## How was this patch tested?

Added new tests to `DateExpressionsSuite`, and uncommented existing tests in `extract.sql` and `pgSQL/date.sql`.

Closes #25408 from MaxGekk/extract-ext3.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/extract.sql', 'sql/core/src/test/resources/sql-tests/inputs/pgSQL/date.sql']","The `extract()` function doesn't support parameters like `epoch`, `isoyear`, `milliseconds`, and `microseconds`, breaking feature parity with PostgreSQL for date-time functions."
1d084513b977684027f78330780525856866bdaa,1617345932,"[SPARK-34938][SQL][TESTS] Benchmark only legacy interval in `ExtractBenchmark`

### What changes were proposed in this pull request?
In the PR, I propose to disable ANSI intervals as the result of dates/timestamp subtraction in `ExtractBenchmark` and benchmark only legacy intervals because `EXTRACT( .. FROM ..)` doesn't support ANSI intervals so far.

### Why are the changes needed?
This fixes the benchmark failure:
```
[info]   Running case: YEAR of interval
[error] Exception in thread ""main"" org.apache.spark.sql.AnalysisException: cannot resolve 'year((subtractdates(CAST(timestamp_seconds(id) AS DATE), DATE '0001-01-01') + subtracttimestamps(timestamp_seconds(id), TIMESTAMP '1000-01-01 01:02:03.123456')))' due to data type mismatch: argument 1 requires date type, however, '(subtractdates(CAST(timestamp_seconds(id) AS DATE), DATE '0001-01-01') + subtracttimestamps(timestamp_seconds(id), TIMESTAMP '1000-01-01 01:02:03.123456'))' is of day-time interval type.; line 1 pos 0;
[error] 'Project [extract(YEAR, (subtractdates(cast(timestamp_seconds(id#1456L) as date), 0001-01-01, false) + subtracttimestamps(timestamp_seconds(id#1456L), 1000-01-01 01:02:03.123456, false, Some(Europe/Moscow)))) AS YEAR#1458]
[error] +- Range (1262304000, 1272304000, step=1, splits=Some(1))
[error] 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[error] 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:194)
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
By running the `ExtractBenchmark` benchmark via:
```
$ build/sbt ""sql/test:runMain org.apache.spark.sql.execution.benchmark.ExtractBenchmark""
```

Closes #32035 from MaxGekk/fix-ExtractBenchmark.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/ExtractBenchmark.scala'],The `ExtractBenchmark` fails when attempting to execute the 'YEAR of interval' use case due to a data type mismatch error in argument 1.
c277afb12b61a91272568dd46380c0d0a9958989,1561579541,"[SPARK-27992][PYTHON] Allow Python to join with connection thread to propagate errors

## What changes were proposed in this pull request?

Currently with `toLocalIterator()` and `toPandas()` with Arrow enabled, if the Spark job being run in the background serving thread errors, it will be caught and sent to Python through the PySpark serializer.
This is not the ideal solution because it is only catch a SparkException, it won't handle an error that occurs in the serializer, and each method has to have it's own special handling to propagate the error.

This PR instead returns the Python Server object along with the serving port and authentication info, so that it allows the Python caller to join with the serving thread. During the call to join, the serving thread Future is completed either successfully or with an exception. In the latter case, the exception will be propagated to Python through the Py4j call.

## How was this patch tested?

Existing tests

Closes #24834 from BryanCutler/pyspark-propagate-server-error-SPARK-27992.

Authored-by: Bryan Cutler <cutlerb@gmail.com>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>
","['core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala', 'core/src/main/scala/org/apache/spark/api/r/RRDD.scala', 'core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala', 'core/src/main/scala/org/apache/spark/security/SocketAuthServer.scala', 'core/src/main/scala/org/apache/spark/util/Utils.scala', 'python/pyspark/rdd.py', 'python/pyspark/sql/dataframe.py', 'python/pyspark/sql/tests/test_arrow.py', 'sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala']",Errors occurring in the Spark job background serving thread or PySpark serializer aren't properly captured or propagated to Python when using `toLocalIterator()` or `toPandas()` with Arrow enabled.
ab8a9a0ceb74acbaee1db6289f1c6ef49d68818a,1615165962,"[SPARK-34545][SQL] Fix issues with valueCompare feature of pyrolite

### What changes were proposed in this pull request?

pyrolite 4.21 introduced and enabled value comparison by default (`valueCompare=true`) during object memoization and serialization: https://github.com/irmen/Pyrolite/blob/pyrolite-4.21/java/src/main/java/net/razorvine/pickle/Pickler.java#L112-L122
This change has undesired effect when we serialize a row (actually `GenericRowWithSchema`) to be passed to python: https://github.com/apache/spark/blob/branch-3.0/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala#L60. A simple example is that
```
new GenericRowWithSchema(Array(1.0, 1.0), StructType(Seq(StructField(""_1"", DoubleType), StructField(""_2"", DoubleType))))
```
and
```
new GenericRowWithSchema(Array(1, 1), StructType(Seq(StructField(""_1"", IntegerType), StructField(""_2"", IntegerType))))
```
are currently equal and the second instance is replaced to the short code of the first one during serialization.

### Why are the changes needed?
The above can cause nasty issues like the one in https://issues.apache.org/jira/browse/SPARK-34545 description:

```
>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.types import *
>>>
>>> def udf1(data_type):
        def u1(e):
            return e[0]
        return udf(u1, data_type)
>>>
>>> df = spark.createDataFrame([((1.0, 1.0), (1, 1))], ['c1', 'c2'])
>>>
>>> df = df.withColumn(""c3"", udf1(DoubleType())(""c1""))
>>> df = df.withColumn(""c4"", udf1(IntegerType())(""c2""))
>>>
>>> df.select(""c3"").show()
+---+
| c3|
+---+
|1.0|
+---+

>>> df.select(""c4"").show()
+---+
| c4|
+---+
|  1|
+---+

>>> df.select(""c3"", ""c4"").show()
+---+----+
| c3|  c4|
+---+----+
|1.0|null|
+---+----+
```
This is because during serialization from JVM to Python `GenericRowWithSchema(1.0, 1.0)` (`c1`) is memoized first and when `GenericRowWithSchema(1, 1)` (`c2`) comes next, it is replaced to some short code of the `c1` (instead of serializing `c2` out) as they are `equal()`. The python functions then runs but the return type of `c4` is expected to be `IntegerType` and if a different type (`DoubleType`) comes back from python then it is discarded: https://github.com/apache/spark/blob/branch-3.0/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala#L108-L113

After this PR:
```
>>> df.select(""c3"", ""c4"").show()
+---+---+
| c3| c4|
+---+---+
|1.0|  1|
+---+---+
```

### Does this PR introduce _any_ user-facing change?
Yes, fixes a correctness issue.

### How was this patch tested?
Added new UT + manual tests.

Closes #31682 from peter-toth/SPARK-34545-fix-row-comparison.

Authored-by: Peter Toth <peter.toth@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala', 'mllib/src/main/scala/org/apache/spark/mllib/api/python/PythonMLLibAPI.scala', 'python/pyspark/sql/tests/test_udf.py', 'sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala']",Pyrolite's value comparison during serialization incorrectly equates differing data types in GenericRowWithSchema leading to incorrect serializations that result in discarding unexpected types returning from Python.
f590f871b67e8ea7c9c73007c72953ca1537cba9,1670902991,"[SPARK-41461][BUILD][CORE][CONNECT][PROTOBUF] Unify the environment variable of *_PROTOC_EXEC_PATH

### What changes were proposed in this pull request?
This PR unify the environment variable of `*_PROTOC_EXEC_PATH` to support that users can use the same environment variable to build and test `core`, `connect`, `protobuf` module by use profile named `-Puser-defined-protoc` with specifying custom `protoc` executables.

### Why are the changes needed?
As described in [SPARK-41485](https://issues.apache.org/jira/browse/SPARK-41485), at present, there are 3 similar environment variable of `*_PROTOC_EXEC_PATH`, but they use the same `pb` version. Because they are consistent in compilation, so I unify the environment variable names to simplify.

### Does this PR introduce _any_ user-facing change?
No, the way to using official pre-release `protoc` binary files is activated by default.

### How was this patch tested?
- Pass GitHub Actions
- Manual test on CentOS6u3 and CentOS7u4
```bash
export SPARK_PROTOC_EXEC_PATH=/path-to-protoc-exe
./build/mvn clean install -pl core -Puser-defined-protoc -am -DskipTests -DskipDefaultProtoc
./build/mvn clean install -pl connector/connect/common -Puser-defined-protoc -am -DskipTests
./build/mvn clean install -pl connector/protobuf -Puser-defined-protoc -am -DskipTests
./build/mvn clean test -pl core -Puser-defined-protoc -DskipDefaultProtoc
./build/mvn clean test -pl connector/connect/common -Puser-defined-protoc
./build/mvn clean test -pl connector/protobuf -Puser-defined-protoc
```
and
```bash
export SPARK_PROTOC_EXEC_PATH=/path-to-protoc-exe
./build/sbt clean ""core/compile"" -Puser-defined-protoc
./build/sbt clean ""connect-common/compile"" -Puser-defined-protoc
./build/sbt clean ""protobuf/compile"" -Puser-defined-protoc
./build/sbt ""core/test"" -Puser-defined-protoc
./build/sbt  ""connect-common/test"" -Puser-defined-protoc
./build/sbt  ""protobuf/test"" -Puser-defined-protoc
```

Closes #39036 from WolverineJiang/master.

Authored-by: jianghaonan <jianghaonan@baidu.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['project/SparkBuild.scala'],"The usage of separate environment variable names for `*_PROTOC_EXEC_PATH` across `core`, `connect`, `protobuf` modules despite them using the same protobuf version makes building and testing cumbersome."
01b557d3121ccfc6ab7edeae70f40c52269c814a,1649737460,"[SPARK-38859][PYTHON] Validate value type before is_list_like to fix iloc setitem failed

### What changes were proposed in this pull request?
Move value type check before is_list_like check.

This is one of fixes to make pandas on spark work with pandas 1.4+.

### Why are the changes needed?
Since Pandas v1.4.0, pandas are using [`not (hasattr(obj, ""ndim"") and obj.ndim == 0)`](https://github.com/pandas-dev/pandas/blob/d228a781b4d7be6c753cee940ce3ee692e97697d/pandas/_libs/lib.pyx#L1114) to exclude zero-dimensional duck-arrays, effectively scalars: https://github.com/pandas-dev/pandas/commit/d228a781b4d7be6c753cee940ce3ee692e97697d

But for a `Column` instance, it has some problems, df doesn't allow apply `not` and raise a execption `ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.`

So we need to move the value type check before is_list_like check.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?
UT passed.
```
OpsOnDiffFramesEnabledTest.test_frame_iloc_setitem
OpsOnDiffFramesEnabledTest.test_series_iloc_setitem
IndexingTest.test_series_iloc_setitem
IndexingTest.test_frame_iloc_setitem
```
passed on 1.4.x pandas.

Closes #36142 from Yikun/SPARK-38859.

Authored-by: Yikun Jiang <yikunkero@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
",['python/pyspark/pandas/indexing.py'],Pandas on Spark's `is_list_like` check for `Column` instances fails with Pandas v1.4.0+ due to a change in Pandas excluding zero-dimensional duck-arrays. This leads to iloc setitem operation failures.
e1b3e9a3d25978dc0ad4609ecbc157ea1eebe2dd,1583295602,"[SPARK-29212][ML][PYSPARK] Add common classes without using JVM backend

### What changes were proposed in this pull request?

Implement common base ML classes (`Predictor`, `PredictionModel`, `Classifier`, `ClasssificationModel` `ProbabilisticClassifier`, `ProbabilisticClasssificationModel`, `Regressor`, `RegrssionModel`) for non-Java backends.

Note

- `Predictor` and `JavaClassifier` should be abstract as `_fit` method is not implemented.
- `PredictionModel` should be abstract as `_transform` is not implemented.

### Why are the changes needed?

To provide extensions points for non-JVM algorithms, as well as a public (as opposed to `Java*` variants, which are commonly described in docstrings as private) hierarchy which can be used to distinguish between different classes of predictors.

For longer discussion see [SPARK-29212](https://issues.apache.org/jira/browse/SPARK-29212) and / or https://github.com/apache/spark/pull/25776.

### Does this PR introduce any user-facing change?

It adds new base classes as listed above, but effective interfaces (method resolution order notwithstanding) stay the same.

Additionally ""private"" `Java*` classes in`ml.regression` and `ml.classification` have been renamed to follow PEP-8 conventions (added leading underscore).

It is for discussion if the same should be done to equivalent classes from `ml.wrapper`.

If we take `JavaClassifier` as an example, type hierarchy will change from

![old pyspark ml classification JavaClassifier](https://user-images.githubusercontent.com/1554276/72657093-5c0b0c80-39a0-11ea-9069-a897d75de483.png)

to

![new pyspark ml classification _JavaClassifier](https://user-images.githubusercontent.com/1554276/72657098-64fbde00-39a0-11ea-8f80-01187a5ea5a6.png)

Similarly the old model

![old pyspark ml classification JavaClassificationModel](https://user-images.githubusercontent.com/1554276/72657103-7513bd80-39a0-11ea-9ffc-59eb6ab61fde.png)

will become

![new pyspark ml classification _JavaClassificationModel](https://user-images.githubusercontent.com/1554276/72657110-80ff7f80-39a0-11ea-9f5c-fe408664e827.png)

### How was this patch tested?

Existing unit tests.

Closes #27245 from zero323/SPARK-29212.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>
","['python/pyspark/ml/__init__.py', 'python/pyspark/ml/base.py', 'python/pyspark/ml/classification.py', 'python/pyspark/ml/regression.py', 'python/pyspark/ml/tests/test_param.py', 'python/pyspark/ml/wrapper.py']","Current implementation of common base ML classes does not support non-JVM backends, limiting the extension points for non-JVM algorithms and does not provide a public hierarchy."
cfde117c6fec758c44f77fe9f9379ba38940b51a,1622731517,"[SPARK-35316][SQL] UnwrapCastInBinaryComparison support In/InSet predicate

### What changes were proposed in this pull request?

This pr add in/inset predicate support for `UnwrapCastInBinaryComparison`.

Current implement doesn't pushdown filters for `In/InSet` which contains `Cast`.

For instance:

```scala
spark.range(50).selectExpr(""cast(id as int) as id"").write.mode(""overwrite"").parquet(""/tmp/parquet/t1"")
spark.read.parquet(""/tmp/parquet/t1"").where(""id in (1L, 2L, 4L)"").explain
```

before this pr:

```
== Physical Plan ==
*(1) Filter cast(id#5 as bigint) IN (1,2,4)
+- *(1) ColumnarToRow
   +- FileScan parquet [id#5] Batched: true, DataFilters: [cast(id#5 as bigint) IN (1,2,4)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/parquet/t1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>
```

after this pr:

```
== Physical Plan ==
*(1) Filter id#95 IN (1,2,4)
+- *(1) ColumnarToRow
   +- FileScan parquet [id#95] Batched: true, DataFilters: [id#95 IN (1,2,4)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/parquet/t1], PartitionFilters: [], PushedFilters: [In(id, [1,2,4])], ReadSchema: struct<id:int>
```

### Does this PR introduce _any_ user-facing change?

No.
### How was this patch tested?

New test.

Closes #32488 from cfmcgrady/SPARK-35316.

Authored-by: Fu Chen <cfmcgrady@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala']",In/InSet predicates which contain Cast are not being pushed down filters efficiently by UnwrapCastInBinaryComparison.
4c58f12132bf10963102c7f3454e6ac886bbe966,1640870329,"[SPARK-37777][SQL] Update the SQL syntax of SHOW FUNCTIONS

### What changes were proposed in this pull request?

The SQL syntax of `SHOW FUNCTIONS` is very weird today. If you want to specify the database to list functions, you need to write `SHOW FUNCTIONS LIKE db.f1`. This is inconsistent with `SHOW TABLES` and `SHOW VIEWS`.

This PR proposes to follow `SHOW TABLES` and `SHOW VIEWS`, and support `SHOW FUNCTIONS FROM/IN db LIKE pattern`. To keep backward compatibility, the legacy syntax is still supported, but it can't be used together with the new `FROM/IN db` clause.

### Why are the changes needed?

Be consistent with `SHOW TABLES` and `SHOW VIEWS`

### Does this PR introduce _any_ user-facing change?

Yea, as it extends the `SHOW FUNCTIONS` SQL syntax.

### How was this patch tested?

updated tests

Closes #35056 from cloud-fan/show-functions.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveUDFSuite.scala']","The SQL syntax for `SHOW FUNCTIONS` inconsistent with `SHOW TABLES` and `SHOW VIEWS`, which results in a cumbersome requirement to specify the database to list functions."
6fb8b8606544f26dc2d9719a2d009eb5aea65ba2,1574170238,"[SPARK-29913][SQL] Improve Exception in postgreCastToBoolean

### What changes were proposed in this pull request?
Exception improvement.

### Why are the changes needed?
After selecting pgSQL dialect, queries which are failing because of wrong syntax will give long exception stack trace. For example,
`explain select cast (""abc"" as boolean);`

Current output:

> ERROR SparkSQLDriver: Failed in [explain select cast (""abc"" as boolean)]
> java.lang.IllegalArgumentException: invalid input syntax for type boolean: abc
> 	at org.apache.spark.sql.catalyst.expressions.postgreSQL.PostgreCastToBoolean.$anonfun$castToBoolean$2(PostgreCastToBoolean.scala:51)
> 	at org.apache.spark.sql.catalyst.expressions.CastBase.buildCast(Cast.scala:277)
> 	at org.apache.spark.sql.catalyst.expressions.postgreSQL.PostgreCastToBoolean.$anonfun$castToBoolean$1(PostgreCastToBoolean.scala:44)
> 	at org.apache.spark.sql.catalyst.expressions.CastBase.nullSafeEval(Cast.scala:773)
> 	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:460)
> 	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:52)
> 	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:45)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:286)
> 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:286)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:291)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
> 	at org.apache.spark.sql.catalyst.plans.QueryPlan.
>       .
>       .
>       .

### Does this PR introduce any user-facing change?
Yes. After this PR, output for above query will be:

> == Physical Plan ==
> org.apache.spark.sql.AnalysisException: invalid input syntax for type boolean: abc;
>
> Time taken: 0.044 seconds, Fetched 1 row(s)
> 19/11/15 15:38:57 INFO SparkSQLCLIDriver: Time taken: 0.044 seconds, Fetched 1 row(s)

### How was this patch tested?
Updated existing test cases.

Closes #26546 from jobitmathew/pgsqlexception.

Authored-by: Jobit Mathew <jobit.mathew@huawei.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala'],Queries failing due to wrong syntax after selecting PostgreSQL dialect produce long exception stack trace instead of user-friendly error messages.
f8bf72ed5d1c25cb9068dc80d3996fcd5aade3ae,1611445750,"[SPARK-34213][SQL] Refresh cached data of v1 table in `LOAD DATA`

### What changes were proposed in this pull request?
Invoke `CatalogImpl.refreshTable()` instead of `SessionCatalog.refreshTable` in v1 implementation of the `LOAD DATA` command. `SessionCatalog.refreshTable` just refreshes metadata comparing to `CatalogImpl.refreshTable()` which refreshes cached table data as well.

### Why are the changes needed?
The example below portraits the issue:

- Create a source table:
```sql
spark-sql> CREATE TABLE src_tbl (c0 int, part int) USING hive PARTITIONED BY (part);
spark-sql> INSERT INTO src_tbl PARTITION (part=0) SELECT 0;
spark-sql> SHOW TABLE EXTENDED LIKE 'src_tbl' PARTITION (part=0);
default	src_tbl	false	Partition Values: [part=0]
Location: file:/Users/maximgekk/proj/load-data-refresh-cache/spark-warehouse/src_tbl/part=0
...
```
- Load data from the source table to a cached destination table:
```sql
spark-sql> CREATE TABLE dst_tbl (c0 int, part int) USING hive PARTITIONED BY (part);
spark-sql> INSERT INTO dst_tbl PARTITION (part=1) SELECT 1;
spark-sql> CACHE TABLE dst_tbl;
spark-sql> SELECT * FROM dst_tbl;
1	1
spark-sql> LOAD DATA LOCAL INPATH '/Users/maximgekk/proj/load-data-refresh-cache/spark-warehouse/src_tbl/part=0' INTO TABLE dst_tbl PARTITION (part=0);
spark-sql> SELECT * FROM dst_tbl;
1	1
```
The last query does not return new loaded data.

### Does this PR introduce _any_ user-facing change?
Yes. After the changes, the example above works correctly:
```sql
spark-sql> LOAD DATA LOCAL INPATH '/Users/maximgekk/proj/load-data-refresh-cache/spark-warehouse/src_tbl/part=0' INTO TABLE dst_tbl PARTITION (part=0);
spark-sql> SELECT * FROM dst_tbl;
0	0
1	1
```

### How was this patch tested?
Added new test to `org.apache.spark.sql.hive.CachedTableSuite`:
```
$ build/sbt -Phive -Phive-thriftserver ""test:testOnly *CachedTableSuite""
```

Closes #31304 from MaxGekk/load-data-refresh-cache.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/CachedTableSuite.scala']","The `LOAD DATA` command in Spark's v1 implementation does not refresh cached table data, causing newly loaded data to be unavailable in subsequent queries."
5b61cc6d629d537a7e5bcbd5205d1c8a43b14d43,1500537762,"[MINOR][DOCS] Fix some missing notes for Python 2.6 support drop

## What changes were proposed in this pull request?

After SPARK-12661, I guess we officially dropped Python 2.6 support. It looks there are few places missing this notes.

I grepped ""Python 2.6"" and ""python 2.6"" and the results were below:

```
./core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala:  // Unpickle array.array generated by Python 2.6
./docs/index.md:Note that support for Java 7, Python 2.6 and old Hadoop versions before 2.6.5 were removed as of Spark 2.2.0.
./docs/rdd-programming-guide.md:Spark {{site.SPARK_VERSION}} works with Python 2.6+ or Python 3.4+. It can use the standard CPython interpreter,
./docs/rdd-programming-guide.md:Note that support for Python 2.6 is deprecated as of Spark 2.0.0, and may be removed in Spark 2.2.0.
./python/pyspark/context.py:            warnings.warn(""Support for Python 2.6 is deprecated as of Spark 2.0.0"")
./python/pyspark/ml/tests.py:        sys.stderr.write('Please install unittest2 to test with Python 2.6 or earlier')
./python/pyspark/mllib/tests.py:        sys.stderr.write('Please install unittest2 to test with Python 2.6 or earlier')
./python/pyspark/serializers.py:        # On Python 2.6, we can't write bytearrays to streams, so we need to convert them
./python/pyspark/sql/tests.py:        sys.stderr.write('Please install unittest2 to test with Python 2.6 or earlier')
./python/pyspark/streaming/tests.py:        sys.stderr.write('Please install unittest2 to test with Python 2.6 or earlier')
./python/pyspark/tests.py:        sys.stderr.write('Please install unittest2 to test with Python 2.6 or earlier')
./python/pyspark/tests.py:        # NOTE: dict is used instead of collections.Counter for Python 2.6
./python/pyspark/tests.py:        # NOTE: dict is used instead of collections.Counter for Python 2.6
```

This PR only proposes to change visible changes as below:

```
./docs/rdd-programming-guide.md:Spark {{site.SPARK_VERSION}} works with Python 2.6+ or Python 3.4+. It can use the standard CPython interpreter,
./docs/rdd-programming-guide.md:Note that support for Python 2.6 is deprecated as of Spark 2.0.0, and may be removed in Spark 2.2.0.
./python/pyspark/context.py:            warnings.warn(""Support for Python 2.6 is deprecated as of Spark 2.0.0"")
```

This one is already correct:

```
./docs/index.md:Note that support for Java 7, Python 2.6 and old Hadoop versions before 2.6.5 were removed as of Spark 2.2.0.
```

## How was this patch tested?

```bash
 grep -r ""Python 2.6"" .
 grep -r ""python 2.6"" .
 ```

Author: hyukjinkwon <gurwls223@gmail.com>

Closes #18682 from HyukjinKwon/minor-python.26.
",['python/pyspark/context.py'],Drop of Python 2.6 support is not uniformly noted in all the required places in the documentation and warning messages.
ed3ea6734c8459b4062789ef53fe792143a2011c,1566438558,"[SPARK-28837][SQL] CTAS/RTAS should use nullable schema

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue.
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
When running CTAS/RTAS, use the nullable schema of the input query to create the table.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
It's very likely to run CTAS/RTAS with non-nullable input query, e.g. `CREATE TABLE t AS SELECT 1`. However, it's surprising to users if they can't write null to this table later. Non-nullable is kind of a constraint of the column and should be specified by users explicitly.

For reference, Postgres also use nullable schema for CTAS:
```
> create table t1(i int not null);

> insert into t1 values (1);

> create table t2 as select i from t1;

> \d+ t1;
 Column |  Type   | Collation | Nullable | Default | Storage | Stats target | Description
--------+---------+-----------+----------+---------+---------+--------------+-------------
 i      | integer |           | not null |         | plain   |              |

> \d+ t2;
 Column |  Type   | Collation | Nullable | Default | Storage | Stats target | Description
--------+---------+-----------+----------+---------+---------+--------------+-------------
 i      | integer |           |          |         | plain   |              |

```

File source V1 has the same behavior.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
Yes, after this PR CTAS/RTAS creates tables with nullable schema, then users can insert null values later.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
new test

Closes #25536 from cloud-fan/ctas.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/v2/DataSourceV2SQLSuite.scala']","Running CTAS/RTAS with non-nullable input queries, like `CREATE TABLE t AS SELECT 1`, leads to issues as users cannot write null to this table later."
e5914e199c2dd629dcf3e7707c0215e3fd089b0e,1663895273,"[SPARK-40527][SQL] Keep struct field names or map keys in CreateStruct

### What changes were proposed in this pull request?

The PR improves field name resolution in `CreateStruct` when using `struct()` with fields from `named_struct` or `map` and makes ""index"" notation consistent with ""dot"" notation. Here is an example:

```sql
select struct(a['x'], a['y']), struct(a.x, a.y) from (select named_struct('x', 1, 'y', 2) as a)
```

As you can observe, the first struct has ""col1"" and ""col2"" names while the second struct has ""x"" and ""y"" which correspond to the parent struct fields.

Before:
```scala
root
 |-- struct(a.x, a.y): struct (nullable = false)
 |    |-- col1: integer (nullable = false)
 |    |-- col2: integer (nullable = false)
 |-- struct(a.x, a.y): struct (nullable = false)
 |    |-- x: integer (nullable = false)
 |    |-- y: integer (nullable = false)

+----------------+----------------+
|struct(a.x, a.y)|struct(a.x, a.y)|
+----------------+----------------+
|{1, 2}          |{1, 2}          |
+----------------+----------------+
```

This PR makes those two examples consistent and both structs will have the following schema.

After:
```scala
root
 |-- struct(a.x, a.y): struct (nullable = false)
 |    |-- x: integer (nullable = false)
 |    |-- y: integer (nullable = false)
 |-- struct(a.x, a.y): struct (nullable = false)
 |    |-- x: integer (nullable = false)
 |    |-- y: integer (nullable = false)
```

### Why are the changes needed?

Makes the behaviour consistent between `struct(a.x)` and `struct(a['x'])`.

### Does this PR introduce _any_ user-facing change?

Yes, the column names returned by `struct()` function will be different after this patch.

### How was this patch tested?

I added unit tests to verify the fix.

Closes #37965 from sadikovi/SPARK-40527.

Authored-by: Ivan Sadikov <ivan.sadikov@databricks.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala', 'sql/core/src/test/scala/org/apache/spark/sql/ComplexTypesSuite.scala']","Field name resolution in `CreateStruct` using `struct()` with fields from `named_struct` or `map` is inconsistent. Naming inconsistencies between ""index"" notation and ""dot"" notation are observed."
a57afd442cf7758d6db494f44429c33f55271b7b,1621946629,"[SPARK-29223][SQL][SS] New option to specify timestamp on all subscribing topic-partitions in Kafka source

### What changes were proposed in this pull request?

This patch is a follow-up of SPARK-26848 (#23747). In SPARK-26848, we decided to open possibility to let end users set individual timestamp per partition. But in many cases, specifying timestamp represents the intention that we would want to go back to specific timestamp and reprocess records, which should be applied to all topics and partitions.

This patch proposes to provide a way to set a global timestamp across topic-partitions which the source is subscribing to, so that end users can set all offsets by specific timestamp easily. To provide the way to config the timestamp easier, the new options only receive ""a"" timestamp for start/end timestamp.

New options introduced in this PR:

* startingTimestamp
* endingTimestamp

All two options receive timestamp as string.

There're priorities for options regarding starting/ending offset as we will have three options for start offsets and another three options for end offsets. Priorities are following:

* starting offsets: startingTimestamp -> startingOffsetsByTimestamp -> startingOffsets
* ending offsets: startingTimestamp -> startingOffsetsByTimestamp -> startingOffsets

### Why are the changes needed?

Existing option to specify timestamp as offset is quite verbose if there're a lot of partitions across topics. Suppose there're 100s of partitions in a topic, the json should contain 100s of times of the same timestamp.

Also, the number of partitions can also change, which requires either:

* fixing the code if the json is statically created
* introducing the dependencies on Kafka client and deal with Kafka API on crafting json programmatically

Both approaches are even not ""acceptable"" if we're dealing with ad-hoc query; anyone doesn't want to write the code more complicated than the query itself. Flink [provides the option](https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/datastream/kafka/#kafka-consumers-start-position-configuration) to specify a timestamp for all topic-partitions like this PR, and even doesn't provide the option to specify the timestamp per topic-partition.

With this PR, end users are only required to provide a single timestamp value. No more complicated JSON format end users need to know about the structure.

### Does this PR introduce _any_ user-facing change?

Yes, this PR introduces two new options, described in above section.

Doc changes are following:

![스크린샷 2021-05-21 오후 12 01 02](https://user-images.githubusercontent.com/1317309/119076244-3034e680-ba2d-11eb-8323-0e227932d2e5.png)
![스크린샷 2021-05-21 오후 12 01 12](https://user-images.githubusercontent.com/1317309/119076255-35923100-ba2d-11eb-9d79-538a7f9ee738.png)
![스크린샷 2021-05-21 오후 12 01 24](https://user-images.githubusercontent.com/1317309/119076264-39be4e80-ba2d-11eb-8265-ac158f55c360.png)
![스크린샷 2021-05-21 오후 12 06 01](https://user-images.githubusercontent.com/1317309/119076271-3d51d580-ba2d-11eb-98ea-35fd72b1bbfc.png)

### How was this patch tested?

New UTs covering new functionalities. Also manually tested via simple batch & streaming queries.

Closes #32609 from HeartSaVioR/SPARK-29223-v2.

Authored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousStream.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeLimit.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderAdmin.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderConsumer.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala', 'external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala', 'external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala', 'external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaRelationSuite.scala']",Setting timestamps as offsets in Kafka source is verbose and complicated with numerous partitions across topics and a potential change in number of partitions. A simpler means is required to specify a global timestamp.
c7a68a920df433f682b3716f761ff0a2208e18f7,1595124702,"[SPARK-32344][SQL] Unevaluable expr is set to FIRST/LAST ignoreNullsExpr in distinct aggregates

### What changes were proposed in this pull request?

This PR intends to fix a bug of distinct FIRST/LAST aggregates in v2.4.6/v3.0.0/master;
```
scala> sql(""SELECT FIRST(DISTINCT v) FROM VALUES 1, 2, 3 t(v)"").show()
...
Caused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: false#37
  at org.apache.spark.sql.catalyst.expressions.Unevaluable$class.eval(Expression.scala:258)
  at org.apache.spark.sql.catalyst.expressions.AttributeReference.eval(namedExpressions.scala:226)
  at org.apache.spark.sql.catalyst.expressions.aggregate.First.ignoreNulls(First.scala:68)
  at org.apache.spark.sql.catalyst.expressions.aggregate.First.updateExpressions$lzycompute(First.scala:82)
  at org.apache.spark.sql.catalyst.expressions.aggregate.First.updateExpressions(First.scala:81)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$15.apply(HashAggregateExec.scala:268)
```
A root cause of this bug is that the `Aggregation` strategy replaces a foldable boolean `ignoreNullsExpr` expr with a `Unevaluable` expr (`AttributeReference`) for distinct FIRST/LAST aggregate functions. But, this operation cannot be allowed because the `Analyzer` has checked that it must be foldabe;
https://github.com/apache/spark/blob/ffdbbae1d465fe2c710d020de62ca1a6b0b924d9/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/First.scala#L74-L76
So, this PR proposes to change a vriable for `IGNORE NULLS`  from `Expression` to `Boolean` to avoid the case.

### Why are the changes needed?

Bugfix.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added a test in `DataFrameAggregateSuite`.

Closes #29143 from maropu/SPARK-32344.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/First.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Last.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDistinctAggregates.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/FirstLastTestSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/functions.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala']",Distinct FIRST/LAST aggregates in Spark SQL v2.4.6/v3.0.0/master replacing a foldable boolean with 'Unevaluable' expression causing UnsupportedOperationException when trying to evaluate the expression.
ecb8b383af1cf1b67f3111c148229e00c9c17c40,1519758752,"[SPARK-23365][CORE] Do not adjust num executors when killing idle executors.

The ExecutorAllocationManager should not adjust the target number of
executors when killing idle executors, as it has already adjusted the
target number down based on the task backlog.

The name `replace` was misleading with DynamicAllocation on, as the target number
of executors is changed outside of the call to `killExecutors`, so I adjusted that name.  Also separated out the logic of `countFailures` as you don't always want that tied to `replace`.

While I was there I made two changes that weren't directly related to this:
1) Fixed `countFailures` in a couple cases where it was getting an incorrect value since it used to be tied to `replace`, eg. when killing executors on a blacklisted node.
2) hard error if you call `sc.killExecutors` with dynamic allocation on, since that's another way the ExecutorAllocationManager and the CoarseGrainedSchedulerBackend would get out of sync.

Added a unit test case which verifies that the calls to ExecutorAllocationClient do not adjust the number of executors.

Author: Imran Rashid <irashid@cloudera.com>

Closes #20604 from squito/SPARK-23365.
","['core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala', 'core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala', 'core/src/main/scala/org/apache/spark/SparkContext.scala', 'core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala', 'core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala', 'core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala', 'core/src/test/scala/org/apache/spark/deploy/StandaloneDynamicAllocationSuite.scala', 'core/src/test/scala/org/apache/spark/scheduler/BlacklistTrackerSuite.scala']",ExecutorAllocationManager adjusts the target number of execulators inaccurately when killing idle executors. Some unrelated changes also affect the value of `countFailures`.
527d936049842da917c4fafad32d64c127447e5c,1559940456,"[SPARK-27798][SQL] from_avro shouldn't produces same value when converted to local relation

## What changes were proposed in this pull request?

When using `from_avro` to deserialize avro data to catalyst StructType format, if `ConvertToLocalRelation` is applied at the time, `from_avro` produces only the last value (overriding previous values).

The cause is `AvroDeserializer` reuses output row for StructType. Normally, it should be fine in Spark SQL. But `ConvertToLocalRelation` just uses `InterpretedProjection` to project local rows. `InterpretedProjection` creates new row for each output thro, it includes the same nested row object from `AvroDeserializer`. By the end, converted local relation has only last value.

I think there're two possible options:

1. Make `AvroDeserializer` output new row for StructType.
2. Use `InterpretedMutableProjection` in `ConvertToLocalRelation` and call `copy()` on output rows.

Option 2 is chose because previously `ConvertToLocalRelation` also creates new rows, this `InterpretedMutableProjection` + `copy()` shoudn't bring too much performance penalty. `ConvertToLocalRelation` should be arguably less critical, compared with `AvroDeserializer`.

## How was this patch tested?

Added test.

Closes #24805 from viirya/SPARK-27798.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConvertToLocalRelationSuite.scala']","The `from_avro` function seems to be continuously overwriting values when 'ConvertToLocalRelation' is applied, leading to identical output values when converting Avro data to a Catalyst StructType format."
9aa42a970c4bd8e54603b1795a0f449bd556b11b,1689242280,"[SPARK-41811][PYTHON][CONNECT] Implement SparkSession.sql's string formatter

### What changes were proposed in this pull request?
Implement SparkSession.sql's string formatter

### Why are the changes needed?
for parity

### Does this PR introduce _any_ user-facing change?
yes

before:
```
In [1]: spark.createDataFrame([(""Alice"", 6), (""Bob"", 7), (""John"", 10)], ['name', 'age']).createOrReplaceTempView(""person"")

In [2]: spark.sql(""""""SELECT * FROM person WHERE age < {age}"""""", age = 9).show()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[2], line 1
----> 1 spark.sql(""""""SELECT * FROM person WHERE age < {age}"""""", age = 9).show()

TypeError: sql() got an unexpected keyword argument 'age'
```

after:
```
In [1]: spark.createDataFrame([(""Alice"", 6), (""Bob"", 7), (""John"", 10)], ['name', 'age']).createOrReplaceTempView(""person"")

In [2]: spark.sql(""""""SELECT * FROM person WHERE age < {age}"""""", age = 9).show()
+-----+---+
| name|age|
+-----+---+
|Alice|  6|
|  Bob|  7|
+-----+---+
```

### How was this patch tested?
enabled doc test

Closes #41980 from zhengruifeng/py_connect_sql_formatter.

Authored-by: Ruifeng Zheng <ruifengz@apache.org>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/pandas/sql_formatter.py', 'python/pyspark/sql/connect/session.py', 'python/pyspark/sql/connect/sql_formatter.py', 'python/pyspark/sql/sql_formatter.py', 'python/pyspark/sql/utils.py']","SparkSession.sql's string formatter is not working with unexpected keyword arguments, causing TypeError."
683bc46ff9a791ab6b9cd3cb95be6bbc368121e0,1645670992,"[SPARK-38286][SQL] Union's maxRows and maxRowsPerPartition may overflow

### What changes were proposed in this pull request?
check Union's maxRows and maxRowsPerPartition

### Why are the changes needed?
Union's maxRows and maxRowsPerPartition may overflow:

case 1:
```
scala> val df1 = spark.range(0, Long.MaxValue, 1, 1)
df1: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> val df2 = spark.range(0, 100, 1, 10)
df2: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> val union = df1.union(df2)
union: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> union.queryExecution.logical.maxRowsPerPartition
res19: Option[Long] = Some(-9223372036854775799)

scala> union.queryExecution.logical.maxRows
res20: Option[Long] = Some(-9223372036854775709)
```

case 2:
```
scala> val n = 2000000
n: Int = 2000000

scala> val df1 = spark.range(0, n, 1, 1).selectExpr(""id % 5 as key1"", ""id as value1"")
df1: org.apache.spark.sql.DataFrame = [key1: bigint, value1: bigint]

scala> val df2 = spark.range(0, n, 1, 2).selectExpr(""id % 3 as key2"", ""id as value2"")
df2: org.apache.spark.sql.DataFrame = [key2: bigint, value2: bigint]

scala> val df3 = spark.range(0, n, 1, 3).selectExpr(""id % 4 as key3"", ""id as value3"")
df3: org.apache.spark.sql.DataFrame = [key3: bigint, value3: bigint]

scala> val joined = df1.join(df2, col(""key1"") === col(""key2"")).join(df3, col(""key1"") === col(""key3""))
joined: org.apache.spark.sql.DataFrame = [key1: bigint, value1: bigint ... 4 more fields]

scala> val unioned = joined.select(col(""key1""), col(""value3"")).union(joined.select(col(""key1""), col(""value2"")))
unioned: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [key1: bigint, value3: bigint]

scala> unioned.queryExecution.optimizedPlan.maxRows
res32: Option[Long] = Some(-2446744073709551616)

scala> unioned.queryExecution.optimizedPlan.maxRows
res33: Option[Long] = Some(-2446744073709551616)
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
added testsuite

Closes #35609 from zhengruifeng/union_maxRows_validate.

Authored-by: Ruifeng Zheng <ruifengz@foxmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/LogicalPlanSuite.scala']","Union operation in Spark SQL is resulting in overflow for maximum rows and maximum rows per partition, producing erroneous calculations in certain conditions."
252e7a3bba6b5078120239c3e316dd6065c1d70a,1678101793,"[SPARK-42668][SS] Catch exception while trying to close compressed stream in HDFSStateStoreProvider abort

### What changes were proposed in this pull request?
We have seen some cases where the task exits as cancelled/failed which triggers the abort in the task completion listener for HDFSStateStoreProvider. As part of this, we cancel the backing stream and close the compressed stream. However, different stores such as Azure blob store could throw exceptions which are not caught in the current path, leading to job failures. This change proposes to fix this issue by catching all non fatal exceptions thrown by cancel/close.

### Why are the changes needed?
Changes are required to avoid job failures due to exceptions thrown by output stream handlers on abort with the HDFSStateStoreProvider.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Modified a test and simulated a NPE in the abort path and verified that the task and thereby the job fails before this change.
After the change, the test passes fine.

Closes #40273 from anishshri-db/task/SPARK-42668.

Authored-by: Anish Shrigondekar <anish.shrigondekar@databricks.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
",['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala'],Abort operation in HDFSStateStoreProvider may result in job failure due to unhandled exceptions thrown by output stream handlers when tasks are cancelled or failed.
e9cc1024df4d587a0f456842d495db91984ed9db,1659499162,"[SPARK-39867][SQL] Global limit should not inherit OrderPreservingUnaryNode

### What changes were proposed in this pull request?

Make GlobalLimit inherit UnaryNode rather than OrderPreservingUnaryNode

### Why are the changes needed?

Global limit can not promise the output ordering is same with child, it actually depend on the certain physical plan.

For all physical plan with gobal limits:
- CollectLimitExec: it does not promise output ordering
- GlobalLimitExec: it required all tuples so it can assume the child is shuffle or child is single partition. Then it can use output ordering of child
- TakeOrderedAndProjectExec: it do sort inside it's implementation

This bug get worse since we pull out v1 write require ordering.

### Does this PR introduce _any_ user-facing change?

yes, bug fix

### How was this patch tested?

fix test and add test

Closes #37284 from ulysses-you/sort.

Authored-by: ulysses-you <ulyssesyou18@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala']","GlobalLimit, inheriting from OrderPreservingUnaryNode, cannot guarantee the same output ordering as its child, causing issues depending on specific physical plans it uses."
56086cb6c6df241e811be96cdf8bc6bc713b649d,1647698089,"[SPARK-38541][BUILD] Upgrade Netty to 4.1.75

### What changes were proposed in this pull request?
This pr aims to upgrade netty to 4.1.75 and this pr add the explicit dependency on `netty-tcnative-classes` because the dependency on `netty-tcnative-classes` is changed to `Optional` in [netty](https://github.com/netty/netty/pull/12146).

### Why are the changes needed?
The release notes as follows:

- https://netty.io/news/2022/03/10/4-1-75-Final.html

There are 2 import changes in this version and both of these changes are to reduce memory-overhead:

- [Reduce the default PooledByteBufAllocator chunk size from 16 MiB to 4 MiB](https://github.com/netty/netty/pull/12108)
- [Change default of io.netty.allocator.useCacheForAllThreads to false](https://github.com/netty/netty/pull/12109)

### Does this PR introduce _any_ user-facing change?
Yes, all changes are caused by the change of Netty.
If the user wants to maintain the same behavior as the previous version, the user needs to make the following configuration:

- add `-Dio.netty.allocator.useCacheForAllThreads=true` to enable `useCacheForAllThreads `
- add `-Dio.netty.allocator.maxOrder=11` to keep the chunk size of `PooledByteBufAllocator` to 16m

### How was this patch tested?
Pass GA

Closes #35835 from LuciferYang/upgrade-netty-4175.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
",['common/network-common/src/main/java/org/apache/spark/network/util/NettyUtils.java'],Current Netty version in use is old and causing excessive memory overhead due to default PooledByteBufAllocator chunk size and cacheForAllThreads setting.
9b262f6a08c0c1b474d920d49b9fdd574c401d39,1509064793,"[SPARK-22356][SQL] data source table should support overlapped columns between data and partition schema

## What changes were proposed in this pull request?

This is a regression introduced by #14207. After Spark 2.1, we store the inferred schema when creating the table, to avoid inferring schema again at read path. However, there is one special case: overlapped columns between data and partition. For this case, it breaks the assumption of table schema that there is on ovelap between data and partition schema, and partition columns should be at the end. The result is, for Spark 2.1, the table scan has incorrect schema that puts partition columns at the end. For Spark 2.2, we add a check in CatalogTable to validate table schema, which fails at this case.

To fix this issue, a simple and safe approach is to fallback to old behavior when overlapeed columns detected, i.e. store empty schema in metastore.

## How was this patch tested?

new regression test

Author: Wenchen Fan <wenchen@databricks.com>

Closes #19579 from cloud-fan/bug2.
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala', 'sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveExternalCatalogVersionsSuite.scala']",Overlap between data and partition schema in data source table could lead to incorrect schema in table scan and fails the validation checks in CatalogTable.
6073d721933031a7c44086d1588ee7aa7b9be926,1691150089,"[SPARK-44675][INFRA] Increase ReservedCodeCacheSize for release build

### What changes were proposed in this pull request?

This PR increases `ReservedCodeCacheSize` to 1g for release build.

The current warning and cache size:
```
OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.
OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=
```
```
$ ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
spark-rm     1     0  0 07:47 pts/0    00:00:00 bash /opt/spark-rm/do-release.sh
spark-rm    13     1  0 07:47 ?        00:00:02 gpg-agent --homedir /home/spark-rm/.gnupg --use-standard-socket --daemon
spark-rm    15     1  0 07:47 pts/0    00:00:00 bash /opt/spark-rm/release-build.sh package
spark-rm  6491     0  0 09:56 pts/1    00:00:00 /bin/sh
spark-rm  7809    15  0 10:07 pts/0    00:00:00 bash ./dev/make-distribution.sh --name hadoop3 --mvn /opt/spark-rm/output/spark-3.3.3-bin-hadoop3/
spark-rm  7977  7809 99 10:07 pts/0    00:01:16 /usr/bin/java -Xss128m -Xmx12g -classpath /opt/spark-rm/output/spark-3.3.3-bin-hadoop3/build/apach
spark-rm  8205  6491  0 10:08 pts/1    00:00:00 ps -ef
$ jinfo -flag ReservedCodeCacheSize   7977
-XX:ReservedCodeCacheSize=251658240
```

### Why are the changes needed?

Reduce build time.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Manual test.

Closes #42344 from wangyum/SPARK-44675.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Yuming Wang <yumwang@ebay.com>
",['dev/create-release/release-build.sh'],"During the release build, warnings of code cache being full are observed, indicating that the ReservedCodeCacheSize is not sufficient. This results in increased build time."
9e73be38a53214780512d0cafedfae9d472cdd05,1558459341,"[SPARK-27726][CORE] Fix performance of ElementTrackingStore deletes when using InMemoryStore under high loads

The details of the PR are explored in-depth in the sub-tasks of the umbrella jira SPARK-27726.
Briefly:
  1. Stop issuing asynchronous requests to cleanup elements in the tracking store when a request is already pending
  2. Fix a couple of thread-safety issues (mutable state and mis-ordered updates)
  3. Move Summary deletion outside of Stage deletion loop like Tasks already are
  4. Reimplement multi-delete in a removeAllKeys call which allows InMemoryStore to implement it in a performant manner.
  5. Some generic typing and exception handling cleanup

We see about five orders of magnitude improvement in the deletion code, which for us is the difference between a server that needs restarting daily, and one that is stable over weeks.

Unit tests for the fire-once asynchronous code and the removeAll calls in both LevelDB and InMemoryStore are supplied.  It was noted that the testing code for the LevelDB and InMemoryStore is highly repetitive, and should probably be merged, but we did not attempt that in this PR.

A version of this code was run in our production 2.3.3 and we were able to sustain higher throughput without going into GC overload (which was happening on a daily basis some weeks ago).

A version of this code was also put under a purpose-built Performance Suite of tests to verify performance under both types of Store implementations for both before and after code streams and for both total and partial delete cases (this code is not included in this PR).

Closes #24616 from davidnavas/PentaBugFix.

Authored-by: David Navas <davidn@clearstorydata.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>
","['common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java', 'common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStore.java', 'common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreView.java', 'common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVTypeInfo.java', 'common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDB.java', 'common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java', 'common/kvstore/src/test/java/org/apache/spark/util/kvstore/InMemoryStoreSuite.java', 'common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java', 'core/src/main/scala/org/apache/spark/status/AppStatusListener.scala', 'core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala', 'core/src/test/scala/org/apache/spark/status/ElementTrackingStoreSuite.scala']","Under high loads, ElementTrackingStore deletion performance with InMemoryStore leads to server instability requiring frequent restarts, with problems arising from thread safety, asynchronous cleanup request issuance, and deletion implementations."
7d318bfe907ab22b904d118e4ff4970af32b0e44,1558719946,"[SPARK-26356][SQL] remove SaveMode from data source v2

## What changes were proposed in this pull request?

In data source v1, save mode specified in `DataFrameWriter` is passed to data source implementation directly, and each data source can define its own behavior about save mode. This is confusing and we want to get rid of save mode in data source v2.

For data source v2, we expect data source to implement the `TableCatalog` API, and end-users use SQL(or the new write API described in [this doc](https://docs.google.com/document/d/1gYm5Ji2Mge3QBdOliFV5gSPTKlX4q1DCBXIkiyMv62A/edit?ts=5ace0718#heading=h.e9v1af12g5zo)) to acess data sources. The SQL API has very clear semantic and we don't need save mode at all.

However, for simple data sources that do not have table management (like a JIRA data source, a noop sink, etc.), it's not ideal to ask them to implement the `TableCatalog` API, and throw exception here and there.

`TableProvider` API is created for simple data sources. It can only get tables, without any other table management methods. This means, it can only deal with existing tables.

`TableProvider` fits well with `DataStreamReader` and `DataStreamWriter`, as they can only read/write existing tables. However, `TableProvider` doesn't fit `DataFrameWriter` well, as the save mode requires more than just get table. More specifically, `ErrorIfExists` mode needs to check if table exists, and create table. `Ignore` mode needs to check if table exists. When end-users specify `ErrorIfExists` or `Ignore` mode and write data to `TableProvider` via `DataFrameWriter`, Spark fails the query and asks users to use `Append` or `Overwrite` mode.

The file source is in the middle of `TableProvider` and `TableCatalog`: it's simple but it can check table(path) exists and create table(path). That said, file source supports all the save modes.

Currently file source implements `TableProvider`, and it's not working because `TableProvider` doesn't support `ErrorIfExists` and `Ignore` modes. Ideally we should create a new API for path-based data sources, but to unblock the work of file source v2 migration, this PR proposes to special-case file source v2 in `DataFrameWriter`, to make it work.

This PR also removes `SaveMode` from data source v2, as now only the internal file source v2 needs it.

## How was this patch tested?

existing tests

Closes #24233 from cloud-fan/file.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: gatorsmile <gatorsmile@gmail.com>
","['sql/core/src/main/java/org/apache/spark/sql/sources/v2/TableProvider.java', 'sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsSaveMode.java', 'sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriteBuilder.java', 'sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/noop/NoopDataSource.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileWriteBuilder.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/v2/DataSourceV2Suite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/v2/FileDataSourceV2FallBackSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/v2/SimpleWritableDataSource.scala', 'sql/core/src/test/scala/org/apache/spark/sql/test/DataFrameReaderWriterSuite.scala']","In DataSource v2, SaveMode is causing confusion and is unnecessary due to the clear semantic of SQL API. However, DataSource v1 and simple data sources without table management require SaveMode for operations like 'ErrorIfExists' or 'Ignore', causing inconsistencies."
0d3a783cc57ed09650ee31851a19728d8f16cd0c,1566280079,"[SPARK-28699][CORE] Fix a corner case for aborting indeterminate stage

### What changes were proposed in this pull request?
Change the logic of collecting the indeterminate stage, we should look at stages from mapStage, not failedStage during handle FetchFailed.

### Why are the changes needed?
In the fetch failed error handle logic, the original logic of collecting indeterminate stage from the fetch failed stage. And in the scenario of the fetch failed happened in the first task of this stage, this logic will cause the indeterminate stage to resubmit partially. Eventually, we are capable of getting correctness bug.

### Does this PR introduce any user-facing change?
It makes the corner case of indeterminate stage abort as expected.

### How was this patch tested?
New UT in DAGSchedulerSuite.
Run below integrated test with `local-cluster[5, 2, 5120]`, and set `spark.sql.execution.sortBeforeRepartition`=false, it will abort the indeterminate stage as expected:
```
import scala.sys.process._
import org.apache.spark.TaskContext

val res = spark.range(0, 10000 * 10000, 1).map{ x => (x % 1000, x)}
// kill an executor in the stage that performs repartition(239)
val df = res.repartition(113).map{ x => (x._1 + 1, x._2)}.repartition(239).map { x =>
  if (TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId < 1 && TaskContext.get.stageAttemptNumber == 0) {
    throw new Exception(""pkill -f -n java"".!!)
  }
  x
}
val r2 = df.distinct.count()
```

Closes #25498 from xuanyuanking/SPARK-28699-followup.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala', 'core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala']","In specific scenarios, FetchFailed error handling leads to partial resubmission of indeterminate stages, causing possible correctness bugs."
dbba3a33bc6712bec6c6cfa3dbb872c42dd0fbff,1560193117,"[SPARK-27947][SQL] Enhance redactOptions to accept any Map type

## What changes were proposed in this pull request?

Handle the case when ParsedStatement subclass has a Map field but not of type Map[String, String].

In ParsedStatement.productIterator, `case mapArg: Map[_, _]` can match any Map type due to type erasure, thus causing `asInstanceOf[Map[String, String]]` to throw ClassCastException.

The following test reproduces the issue:
```
case class TestStatement(p: Map[String, Int]) extends ParsedStatement {
 override def output: Seq[Attribute] = Nil
 override def children: Seq[LogicalPlan] = Nil
}

TestStatement(Map(""abc"" -> 1)).toString
```
Changing the code to `case mapArg: Map[String, String]` will not help due to type erasure. As a matter of fact, compiler gives this warning:
```
Warning:(41, 18) non-variable type argument String in type pattern
 scala.collection.immutable.Map[String,String] (the underlying of Map[String,String])
 is unchecked since it is eliminated by erasure
case mapArg: Map[String, String] =>
```

## How was this patch tested?

Add 2 unit tests.

Closes #24800 from jzhuge/SPARK-27947.

Authored-by: John Zhuge <jzhuge@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['core/src/main/scala/org/apache/spark/util/Utils.scala', 'core/src/test/scala/org/apache/spark/util/UtilsSuite.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/sql/ParsedStatement.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala']","The `redactOptions` function throws a ClassCastException when a ParsedStatement subclass contains a Map field not of type Map[String, String] due to type erasure in Scala."
23f45f18223e3ca34bc5e43e2b1d952c844f0d19,1571434236,"[SPARK-29515][CORE] MapStatuses SerDeser Benchmark

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
-->

### What changes were proposed in this pull request?
Add benchmark code for MapStatuses serialization & deserialization performance.

### Why are the changes needed?
For comparing the performance differences against optimization.

### Does this PR introduce any user-facing change?
<!--
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
No test is required.

Closes #26169 from dbtsai/benchmark.

Lead-authored-by: DB Tsai <d_tsai@apple.com>
Co-authored-by: Dongjoon Hyun <dhyun@apple.com>
Co-authored-by: DB Tsai <dbtsai@dbtsai.com>
Signed-off-by: DB Tsai <d_tsai@apple.com>
","['core/src/main/scala/org/apache/spark/MapOutputTracker.scala', 'core/src/test/scala/org/apache/spark/MapStatusesSerDeserBenchmark.scala']","Performance of MapStatuses serialization and deserialization is untested, causing a lack of benchmark comparisons for potential optimizations."
690b09368d5566fa33a133df9e13ab57d2f54ecd,1659517977,"[SPARK-39949][SS] Use canonical host name for principals in KafkaTestUtils

### What changes were proposed in this pull request?

This PR proposes to use canonical host name for principals in KafkaTestUtils.

This is effectively reverting a part of SPARK-39530, as we used the canonical host name before SPARK-39530. (We used IP as ""127.0.0.1"" (loopback) and the host part of principal as the canonical host name of ""127.0.0.1"") Picking up the actual IP to use IPv6 if appropriate remains unchanged from SPARK-39530.

### Why are the changes needed?

KafkaDelegationTokenSuite fails on AWS EC2.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Ran KafkaDelegationTokenSuite in AWS EC2 via following command:

```
build/sbt ""sql-kafka-0-10/testOnly *KafkaDelegationTokenSuite""
```

This suite failed on current master branch and succeeds with this PR.

kerberos debug message on failure (only picked up the last error part as it's quite long and I need to redact):

```
>>>KRBError:
	 sTime is Tue Aug 02 02:30:29 PDT 2022 1659432629000
KrbException: Server not found in Kerberos database (7) - Server not found in Kerberos database
  | => s suSec is 100
	 error code is 7
	 error Message is Server not found in Kerberos database
	 crealm is EXAMPLE.COM
	 sname is zookeeper/ip-<ipv4-dot-replaced-with-bar>.<region>.compute.internalEXAMPLE.COM
	 msgType is 30
	at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73)
  | => sat sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:226)
	at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:237)
	at sun.security.krb5.internal.CredentialsUtil.serviceCredsSingle(CredentialsUtil.java:477)
	at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:340)
	at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:314)
	at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:169)
	at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:490)
	at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:695)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192)
	at org.apache.zookeeper.client.ZooKeeperSaslClient$1.run(ZooKeeperSaslClient.java:320)
	at org.apache.zookeeper.client.ZooKeeperSaslClient$1.run(ZooKeeperSaslClient.java:317)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslToken(ZooKeeperSaslClient.java:317)
	at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslToken(ZooKeeperSaslClient.java:303)
	at org.apache.zookeeper.client.ZooKeeperSaslClient.sendSaslPacket(ZooKeeperSaslClient.java:366)
	at org.apache.zookeeper.client.ZooKeeperSaslClient.initialize(ZooKeeperSaslClient.java:403)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1220)
Caused by: KrbException: Identifier doesn't match expected value (906)
	at sun.security.krb5.internal.KDCRep.init(KDCRep.java:140)
	at sun.security.krb5.internal.TGSRep.init(TGSRep.java:65)
	at sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60)
	at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55)
	... 20 more
```

Closes #37377 from HeartSaVioR/SPARK-39949.

Authored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['connector/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala'],"KafkaDelegationTokenSuite is failing on AWS EC2 due to server not found in Kerberos database error, potentially caused by misuse of host part of principal."
28a2f62a1de55dca14071986194a0a57a3c01bb0,1644291397,"[SPARK-38133][SQL] UnsafeRow should treat TIMESTAMP_NTZ as mutable and fixed width

### What changes were proposed in this pull request?

Add TimestampNTZType to UnsafeRow's list of mutable fields.

### Why are the changes needed?

Assume this data:
```
create or replace temp view v1 as
select * from values
  (1, timestamp_ntz'2012-01-01 00:00:00', 10000),
  (2, timestamp_ntz'2012-01-01 00:00:00', 20000),
  (1, timestamp_ntz'2012-01-01 00:00:00', 5000),
  (1, timestamp_ntz'2013-01-01 00:00:00', 48000),
  (2, timestamp_ntz'2013-01-01 00:00:00', 30000)
  as data(a, b, c);
```
The following query produces incorrect results:
```
select *
from v1
pivot (
  sum(c)
  for a in (1, 2)
);
```
The timestamp_ntz values are corrupted:
```
2012-01-01 19:05:19.476736	15000	20000
2013-01-01 19:05:19.476736	48000	30000
```
Because `UnsafeRow.isFixedLength` returns `false` for data type `TimestampNTZType`, `GenerateUnsafeRowJoiner` generates code for the `TIMESTAMP_NTZ` field as though it was a variable length field (it adds an offset to the Long value, thus corrupting the timestamp value).

By adding `TimestampNTZType` to `UnsafeRow`'s list of mutable fields, `UnsafeRow.isFixedLength` returns `true`.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New unit test.

Closes #35430 from bersprockets/isfixedlength_issue.

Authored-by: Bruce Robbins <bersprockets@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java', 'sql/core/src/test/scala/org/apache/spark/sql/DataFramePivotSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala']",UnsafeRow in Spark SQL is incorrectly treating TIMESTAMP_NTZ data type as a variable width field causing corruption in timestamp_ntz values when aggregated in a pivot operation.
959694271e30879c944d7fd5de2740571012460a,1647238258,"[SPARK-38523][SQL] Fix referring to the corrupt record column from CSV

### What changes were proposed in this pull request?
In the case when an user specifies the corrupt record column via the CSV option `columnNameOfCorruptRecord`:
1. Disable the column pruning feature in the CSV parser.
2. Don't push filters to `UnivocityParser` that refer to the ""virtual"" column `columnNameOfCorruptRecord`. Since the column cannot present in the input CSV, user's queries fail while compiling predicates. After the changes, the skipped filters are applied later on the upper layer.

### Why are the changes needed?
The changes allow to refer to the corrupt record column from user's queries:

```Scala
spark.read.format(""csv"")
  .option(""header"", ""true"")
  .option(""columnNameOfCorruptRecord"", ""corrRec"")
  .schema(schema)
  .load(""csv_corrupt_record.csv"")
  .filter($""corrRec"".isNotNull)
  .show()
```
for the input file ""csv_corrupt_record.csv"":
```
0,2013-111_11 12:13:14
1,1983-08-04
```
the query returns:
```
+---+----+----------------------+
|a  |b   |corrRec               |
+---+----+----------------------+
|0  |null|0,2013-111_11 12:13:14|
+---+----+----------------------+
```

### Does this PR introduce _any_ user-facing change?
Yes. Before the changes, the query above fails with the exception:
```Java
java.lang.IllegalArgumentException: _corrupt_record does not exist. Available: a, b
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:310) ~[classes/:?]
```

### How was this patch tested?
By running new CSV test:
```
$ build/sbt ""sql/testOnly *.CSVv1Suite""
$ build/sbt ""sql/testOnly *.CSVv2Suite""
$ build/sbt ""sql/testOnly *.CSVLegacyTimeParserSuite""
```

Closes #35817 from MaxGekk/csv-ref-_corupt_record.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVPartitionReaderFactory.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala']","When specifying the corrupt record column via CSV option `columnNameOfCorruptRecord`, user queries fail while compiling predicates and there are issues with column pruning in the CSV parser."
0ab9bd79b33857fbbfaa2233bc81462192b47291,1620878241,"[SPARK-35384][SQL] Improve performance for InvokeLike.invoke

### What changes were proposed in this pull request?

Change `map` in `InvokeLike.invoke` to a while loop to improve performance, following Spark [style guide](https://github.com/databricks/scala-style-guide#traversal-and-zipwithindex).

### Why are the changes needed?

`InvokeLike.invoke`, which is used in non-codegen path for `Invoke` and `StaticInvoke`, currently uses `map` to evaluate arguments:
```scala
val args = arguments.map(e => e.eval(input).asInstanceOf[Object])
if (needNullCheck && args.exists(_ == null)) {
  // return null if one of arguments is null
  null
} else {
  ...
```
which is pretty expensive if the method itself is trivial. We can change it to a plain while loop.

<img width=""871"" alt=""Screen Shot 2021-05-12 at 12 19 59 AM"" src=""https://user-images.githubusercontent.com/506679/118055719-7f985a00-b33d-11eb-943b-cf85eab35f44.png"">

Benchmark results show this can improve as much as 3x from `V2FunctionBenchmark`:

Before
```
 OpenJDK 64-Bit Server VM 1.8.0_292-b10 on Linux 5.4.0-1046-azure
 Intel(R) Xeon(R) CPU E5-2673 v3  2.40GHz
 scalar function (long + long) -> long, result_nullable = false codegen = false:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
 --------------------------------------------------------------------------------------------------------------------------------------------------------------
 native_long_add                                                                         36506          36656         251         13.7          73.0       1.0X
 java_long_add_default                                                                   47151          47540         370         10.6          94.3       0.8X
 java_long_add_magic                                                                    178691         182457        1327          2.8         357.4       0.2X
 java_long_add_static_magic                                                             177151         178258        1151          2.8         354.3       0.2X
```

After
```
 OpenJDK 64-Bit Server VM 1.8.0_292-b10 on Linux 5.4.0-1046-azure
 Intel(R) Xeon(R) CPU E5-2673 v3  2.40GHz
 scalar function (long + long) -> long, result_nullable = false codegen = false:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
 --------------------------------------------------------------------------------------------------------------------------------------------------------------
 native_long_add                                                                         29897          30342         568         16.7          59.8       1.0X
 java_long_add_default                                                                   40628          41075         664         12.3          81.3       0.7X
 java_long_add_magic                                                                     54553          54755         182          9.2         109.1       0.5X
 java_long_add_static_magic                                                              55410          55532         127          9.0         110.8       0.5X
```

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Existing tests.

Closes #32527 from sunchao/SPARK-35384.

Authored-by: Chao Sun <sunchao@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala'],"The `InvokeLike.invoke` method in non-codegen path for `Invoke` and `StaticInvoke` is showing degraded performance when evaluating arguments due to its use of `map`, especially noticeable when the method is trivial."
cc9a158712d7b46382885479dbacb46a253ca800,1640837550,"[SPARK-37726][SQL] Add spill size metrics for sort merge join

### What changes were proposed in this pull request?

Sort merge join allows buffered side to spill if the size is too large to hold in memory. It would be good to add a ""spill size"" SQL metrics in sort merge join, to track how often the spill happens, and how much of spill size would be in case when it spills.

### Why are the changes needed?

This helps to get more insights from query when the spill happens. Also help us decide whether to use Spark code-gen engine vs customized columnar engine, in case customized engine not support spill for now.

### Does this PR introduce _any_ user-facing change?

Yes the SQL metrics ""spill size"" itself on Spark UI.

### How was this patch tested?

* Modified existing unit test in `SQLMetricsSuite.scala`.
* Tested simple query with `spark-shell` locally:
<img width=""808"" alt=""Screen Shot 2021-12-23 at 5 21 20 PM"" src=""https://user-images.githubusercontent.com/4629931/147304391-48890d7c-9701-4112-8eda-da41fc7cacee.png"">

Closes #34999 from c21/smj-spill.

Authored-by: Cheng Su <chengsu@fb.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala']","There's a lack of spill size SQL metrics in sort merge join to track spill frequency and size, making it difficult to decide whether to use Spark code-gen engine or customized columnar engine."
0130a3813abee63c25618a38779c246595a521ba,1611037842,"[SPARK-33730][PYTHON][FOLLOW-UP] Consider the case when the current frame is None

### What changes were proposed in this pull request?

This PR proposes to consider the case when [`inspect.currentframe()`](https://docs.python.org/3/library/inspect.html#inspect.currentframe) returns `None` because the underlyining Python implementation does not support frame.

### Why are the changes needed?

To be safer and potentially for the official support of other Python implementations in the future.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Manually tested via:

When frame is available:

```
vi tmp.py
```

```python
from inspect import *
lineno = getframeinfo(currentframe()).lineno + 1 if currentframe() is not None else 0
print(warnings.formatwarning(
    ""Failed to set memory limit: {0}"".format(Exception(""argh!"")),
    ResourceWarning,
    __file__,
    lineno),
    file=sys.stderr)
```

```
python tmp.py
```

```
/.../tmp.py:3: ResourceWarning: Failed to set memory limit: argh!
  print(warnings.formatwarning(
```

When frame is not available:

```
vi tmp.py
```

```python
from inspect import *
lineno = getframeinfo(currentframe()).lineno + 1 if None is not None else 0
print(warnings.formatwarning(
    ""Failed to set memory limit: {0}"".format(Exception(""argh!"")),
    ResourceWarning,
    __file__,
    lineno),
    file=sys.stderr)
```

```
python tmp.py
```

```
/.../tmp.py:0: ResourceWarning: Failed to set memory limit: argh!
```

Closes #31239 from HyukjinKwon/SPARK-33730-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
",['python/pyspark/worker.py'],"The Python `inspect.currentframe()` function returns `None` when the underlying Python implementation does not support frames, which is not currently accounted for and may lead to potential issues."
15462e1a8fa8da54ac51f4d21f567f3c288e6701,1560575960,"[SPARK-28004][UI] Update jquery to 3.4.1

## What changes were proposed in this pull request?

We're using an old-ish jQuery, 1.12.4, and should probably update for Spark 3 to keep up in general, but also to keep up with CVEs. In fact, we know of at least one resolved in only 3.4.0+ (https://nvd.nist.gov/vuln/detail/CVE-2019-11358). They may not affect Spark, but, if the update isn't painful, maybe worthwhile in order to make future 3.x updates easier.

jQuery 1 -> 2 doesn't sound like a breaking change, as 2.0 is supposed to maintain compatibility with 1.9+ (https://blog.jquery.com/2013/04/18/jquery-2-0-released/)

2 -> 3 has breaking changes: https://jquery.com/upgrade-guide/3.0/. It's hard to evaluate each one, but the most likely area for problems is in ajax(). However, our usage of jQuery (and plugins) is pretty simple.

Update jquery to 3.4.1; update jquery blockUI and mustache to latest

## How was this patch tested?

Manual testing of docs build (except R docs), worker/master UI, spark application UI.
Note: this really doesn't guarantee it works, as our tests can't test javascript, and this is merely anecdotal testing, although I clicked about every link I could find. There's a risk this breaks a minor part of the UI; it does seem to work fine in the main.

Closes #24843 from srowen/SPARK-28004.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['core/src/main/resources/org/apache/spark/ui/static/historypage.js', 'core/src/main/resources/org/apache/spark/ui/static/jquery-1.12.4.min.js', 'core/src/main/resources/org/apache/spark/ui/static/jquery-3.4.1.min.js', 'core/src/main/resources/org/apache/spark/ui/static/jquery.blockUI.min.js', 'core/src/main/resources/org/apache/spark/ui/static/jquery.mustache.js', 'core/src/main/scala/org/apache/spark/ui/UIUtils.scala', 'core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala', 'docs/js/main.js', 'docs/js/vendor/jquery-1.12.4.min.js', 'docs/js/vendor/jquery-3.4.1.min.js']","The current version of jQuery (1.12.4) used in Spark might have security vulnerabilities and doesn't get updates anymore, potentially causing issues and making future updates more difficult."
ba0456223482b13540fbe62bb212bb9be50619be,1571068993,"[SPARK-29435][CORE] MapId in Shuffle Block is inconsistent at the writer and reader part when spark.shuffle.useOldFetchProtocol=true

### What changes were proposed in this pull request?
Shuffle Block Construction during Shuffle Write and Read is wrong

Shuffle Map Task  (Shuffle Write)
19/10/11 22:07:32| ERROR| [Executor task launch worker for task 3] org.apache.spark.shuffle.IndexShuffleBlockResolver: ####### For Debug ############ /tmp/hadoop-root1/nm-local-dir/usercache/root1/appcache/application_1570422377362_0008/blockmgr-6d03250d-6e7c-4bc2-bbb7-22b8e3981c35/0d/**shuffle_0_3_0.index**

Result Task (Shuffle Read)
19/10/11 22:07:32| ERROR| [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator: Error occurred while fetching local blocks
java.nio.file.NoSuchFileException: /tmp/hadoop-root1/nm-local-dir/usercache/root1/appcache/application_1570422377362_0008/blockmgr-6d03250d-6e7c-4bc2-bbb7-22b8e3981c35/30/**shuffle_0_0_0.index**
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)

As per [SPARK-25341](https://issues.apache.org/jira/browse/SPARK-25341) `mapId` of `SortShuffleManager.getWriter `changed to `context.taskAttemptId() ` from `partitionId`

[code]( https://github.com/apache/spark/pull/25620/files#diff-363c53ca5a72cfdc37dac4a723309638R54)

But `MapOutputTracker.convertMapStatuses` returns the wrong ShuffleBlock, if `spark.shuffle.useOldFetchProtocol `enabled, it returns `paritionId` as `mapID` which is wrong . [Code](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L912)

### Why are the changes needed?
Already MapStatus returned by the ShuffleWriter has the mapId for e.g.[ code here](https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java#L134). So it's nice to use `status.mapTaskId`

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing UT  and manually tested with `spark.shuffle.useOldFetchProtocol` as true and false

![image](https://user-images.githubusercontent.com/35216143/66716530-4f4caa80-edec-11e9-833d-7131a9fbd442.png)

Closes #26095 from sandeep-katta/shuffleIssue.

Lead-authored-by: sandeep katta <sandeep.katta2007@gmail.com>
Co-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['core/src/main/scala/org/apache/spark/MapOutputTracker.scala', 'core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala', 'core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala', 'core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala', 'core/src/main/scala/org/apache/spark/shuffle/ShuffleWriteProcessor.scala', 'core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala', 'core/src/test/scala/org/apache/spark/ShuffleOldFetchProtocolSuite.scala', 'core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala', 'core/src/test/scala/org/apache/spark/scheduler/MapStatusSuite.scala', 'core/src/test/scala/org/apache/spark/shuffle/BlockStoreShuffleReaderSuite.scala']","When spark.shuffle.useOldFetchProtocol is true, shuffle block construction during shuffle write and read is inconsistent. NoSuchFileExecption is thrown due to wrong MapId in MapOutputTracker.convertMapStatuses."
e0e06c18fd479ac86963ec26ab82b0dc5536bc83,1610255177,"[SPARK-34055][SQL] Refresh cache in `ALTER TABLE .. ADD PARTITION`

### What changes were proposed in this pull request?
Invoke `refreshTable()` from `CatalogImpl` which refreshes the cache in v1 `ALTER TABLE .. ADD PARTITION`.

### Why are the changes needed?
This fixes the issues portrayed by the example:
```sql
spark-sql> create table tbl (col int, part int) using parquet partitioned by (part);
spark-sql> insert into tbl partition (part=0) select 0;
spark-sql> cache table tbl;
spark-sql> select * from tbl;
0	0
spark-sql> show table extended like 'tbl' partition(part=0);
default	tbl	false	Partition Values: [part=0]
Location: file:/Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl/part=0
...
```
Create new partition by copying the existing one:
```
$ cp -r /Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl/part=0 /Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl/part=1
```
```sql
spark-sql> alter table tbl add partition (part=1) location '/Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl/part=1';
spark-sql> select * from tbl;
0	0
```

The last query must return `0	1` since it has been added by `ALTER TABLE .. ADD PARTITION`.

### Does this PR introduce _any_ user-facing change?
Yes. After the changes for the example above:
```sql
...
spark-sql> alter table tbl add partition (part=1) location '/Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl/part=1';
spark-sql> select * from tbl;
0	0
0	1
```

### How was this patch tested?
By running the affected test suite:
```
$ build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *AlterTableAddPartitionSuite""
```

Closes #31101 from MaxGekk/add-partition-refresh-cache-2.

Lead-authored-by: Max Gekk <max.gekk@gmail.com>
Co-authored-by: Hyukjin Kwon <gurwls223@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/AlterTableAddPartitionSuite.scala']","After adding a new partition using `ALTER TABLE..ADD PARTITION`, the refreshed cache does not reflect the newly added partition leading to incorrect query results."
dee294b453b550471028fdbd9e17952963504a3a,1644393578,"[SPARK-38056][WEB UI] Fix issue of Structured streaming not working in history server when using LevelDB

### What changes were proposed in this pull request?

Change type of `org.apache.spark.sql.streaming.ui.StreamingQueryData.runId` from `UUID` to `String`.

### Why are the changes needed?

In [SPARK-31953](https://github.com/apache/spark/commit/4f9667035886a67e6c9a4e8fad2efa390e87ca68), structured streaming support is added in history server. However this does not work when history server is using LevelDB instead of in-memory KV store.

- Level DB does not support `UUID` as key.
- If `spark.history.store.path` is set in history server to use Level DB, when writing info to the store during replaying events, error will occur.
- `StreamingQueryStatusListener` will throw exceptions when writing info, saying `java.lang.IllegalArgumentException: Type java.util.UUID not allowed as key.`.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added tests in `StreamingQueryStatusListenerSuite` to test whether `StreamingQueryData` can be successfully written to in-memory store,  LevelDB and RocksDB.

Closes #35356 from kuwii/hs-streaming-fix.

Authored-by: kuwii <kuwii.someone@gmail.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/streaming/ui/StreamingQueryStatisticsPage.scala', 'sql/core/src/main/scala/org/apache/spark/sql/streaming/ui/StreamingQueryStatusListener.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/ui/StreamingQueryPageSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/ui/StreamingQueryStatusListenerSuite.scala']",Structured streaming does not function on history server with LevelDB due to UUID not being supported as key in LevelDB causing a java.lang.IllegalArgumentException when trying to write info.
a2c1a55b1fed5d552f6bc355ba3c542dfeee5a91,1624395049,"[SPARK-35700][SQL][FOLLOWUP] Read schema from ORC files should strip CHAR/VARCHAR types

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/33001 , to provide a more direct fix.

The regression in 3.1 was caused by the fact that we changed the parser and allow the parser to return CHAR/VARCHAR type. We should have replaced CHAR/VARCHAR with STRING before the data type flows into the query engine, however, `OrcUtils` is missed.

When reading ORC files, at the task side we will read the real schema from ORC file metadata, then apply filter pushdown. For some reason, the implementation turns ORC schema to Spark schema before filter pushdown, and this step does not strip CHAR/VARCHAR. Note, for Parquet we use the Parquet schema directly in filter pushdown, and do not this have problem.

This PR proposes to replace the CHAR/VARCHAR with STRING when turning ORC schema to Spark schema.

### Why are the changes needed?

a more directly bug fix

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

existing tests

Closes #33030 from cloud-fan/help.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/HiveCharVarcharTestSuite.scala']","Reading schema from ORC files retains CHAR/VARCHAR types, inducing issues in filter pushdown due to absence of CHAR/VARCHAR stripping which is needed before the data type enters the query engine."
4ea54e8672757c0dbe3dd57c81763afdffcbcc1b,1632373009,"[SPARK-36782][CORE] Avoid blocking dispatcher-BlockManagerMaster during UpdateBlockInfo

### What changes were proposed in this pull request?
Delegate potentially blocking call to `mapOutputTracker.updateMapOutput` from within  `UpdateBlockInfo` from `dispatcher-BlockManagerMaster` to the threadpool to avoid blocking the endpoint. This code path is only accessed for `ShuffleIndexBlockId`, other blocks are still executed on the `dispatcher-BlockManagerMaster` itself.

Change `updateBlockInfo` to return `Future[Boolean]` instead of `Boolean`. Response will be sent to RPC caller upon successful completion of the future.

Introduce a unit test that forces `MapOutputTracker` to make a broadcast as part of `MapOutputTracker.serializeOutputStatuses` when running decommission tests.

### Why are the changes needed?
[SPARK-36782](https://issues.apache.org/jira/browse/SPARK-36782) describes a deadlock occurring if the `dispatcher-BlockManagerMaster` is allowed to block while waiting for write access to data structures.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Unit test as introduced in this PR.

---

Ping eejbyfeldt for notice.

Closes #34043 from f-thiele/SPARK-36782.

Lead-authored-by: Fabian A.J. Thiele <fabian.thiele@posteo.de>
Co-authored-by: Emil Ejbyfeldt <eejbyfeldt@liveintent.com>
Co-authored-by: Fabian A.J. Thiele <fthiele@liveintent.com>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala', 'core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala']","A deadlock occurs when `dispatcher-BlockManagerMaster` blocks while waiting for write access to data structures. This affects the `UpdateBlockInfo` functionality, specifically regarding `ShuffleIndexBlockId`."
f7e9b6efc70b6874587c440725d3af8efa3a316e,1616564050,"[SPARK-34763][SQL] col(), $""<name>"" and df(""name"") should handle quoted column names properly

### What changes were proposed in this pull request?

This PR fixes an issue that `col()`, `$""<name>""` and `df(""name"")` don't handle quoted column names  like ``` `a``b.c` ```properly.

For example, if we have a following DataFrame.
```
val df1 = spark.sql(""SELECT 'col1' AS `a``b.c`"")
```

For the DataFrame, this query is successfully executed.
```
scala> df1.selectExpr(""`a``b.c`"").show
+-----+
|a`b.c|
+-----+
| col1|
+-----+
```

But the following query will fail because ``` df1(""`a``b.c`"") ``` throws an exception.
```
scala> df1.select(df1(""`a``b.c`"")).show
org.apache.spark.sql.AnalysisException: syntax error in attribute name: `a``b.c`;
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute$.e$1(unresolved.scala:152)
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute$.parseAttributeName(unresolved.scala:162)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:121)
  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)
  at org.apache.spark.sql.Dataset.col(Dataset.scala:1274)
  at org.apache.spark.sql.Dataset.apply(Dataset.scala:1241)
  ... 49 elided
```
### Why are the changes needed?

It's a bug.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

New tests.

Closes #31854 from sarutak/fix-parseAttributeName.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala']","Quoted column names like ``` `a``b.c` ``` are not correctly processed by `col()`, `$""<name>""` and `df(""name"")` in Spark SQL, causing AnalysisException."
173fe450df203b262b58f7e71c6b52a79db95ee0,1528475531,"[SPARK-24477][SPARK-24454][ML][PYTHON] Imports submodule in ml/__init__.py and add ImageSchema into __all__

## What changes were proposed in this pull request?

This PR attaches submodules to ml's `__init__.py` module.

Also, adds `ImageSchema` into `image.py` explicitly.

## How was this patch tested?

Before:

```python
>>> from pyspark import ml
>>> ml.image
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'image'
>>> ml.image.ImageSchema
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'image'
```

```python
>>> ""image"" in globals()
False
>>> from pyspark.ml import *
>>> ""image"" in globals()
False
>>> image
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'image' is not defined
```

After:

```python
>>> from pyspark import ml
>>> ml.image
<module 'pyspark.ml.image' from '/.../spark/python/pyspark/ml/image.pyc'>
>>> ml.image.ImageSchema
<pyspark.ml.image._ImageSchema object at 0x10d973b10>
```

```python
>>> ""image"" in globals()
False
>>> from pyspark.ml import *
>>> ""image"" in globals()
True
>>> image
<module 'pyspark.ml.image' from  #'/.../spark/python/pyspark/ml/image.pyc'>
```

Author: hyukjinkwon <gurwls223@apache.org>

Closes #21483 from HyukjinKwon/SPARK-24454.
","['python/pyspark/ml/__init__.py', 'python/pyspark/ml/image.py']","The 'ml' module does not have an 'image' attribute and ImageSchema is not being imported explicitly, leading to AttributeErrors and inability to import these attributes."
469423f33887a966aaa33eb75f5e7974a0a97beb,1565176465,"[SPARK-28595][SQL] explain should not trigger partition listing

## What changes were proposed in this pull request?

Sometimes when you explain a query, you will get stuck for a while. What's worse, you will get stuck again if you explain again.

This is caused by `FileSourceScanExec`:
1. In its `toString`, it needs to report the number of partitions it reads. This needs to query the hive metastore.
2. In its `outputOrdering`, it needs to get all the files. This needs to query the hive metastore.

This PR fixes by:
1. `toString` do not need to report the number of partitions it reads. We should report it via SQL metrics.
2. The `outputOrdering` is not very useful. We can only apply it if a) all the bucket columns are read. b) there is only one file in each bucket. This condition is really hard to meet, and even if we meet, sorting an already sorted file is pretty fast and avoiding the sort is not that useful. I think it's worth to give up this optimization so that explain don't need to get stuck.

## How was this patch tested?

existing tests

Closes #25328 from cloud-fan/ui.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveExplainSuite.scala']",Running an EXPLAIN query for `FileSourceScanExec` triggers unnecessary and lengthy partition listing due to its reporting of the number of partitions and retrieving all the files in `outputOrdering`.
c755b0d910d68e7921807f2f2ac1e3fac7a8f357,1510215753,"[SPARK-22463][YARN][SQL][HIVE] add hadoop/hive/hbase/etc configuration files in SPARK_CONF_DIR to distribute archive

## What changes were proposed in this pull request?
When I ran self contained sql apps, such as
```scala
import org.apache.spark.sql.SparkSession

object ShowHiveTables {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName(""Show Hive Tables"")
      .enableHiveSupport()
      .getOrCreate()
    spark.sql(""show tables"").show()
    spark.stop()
  }
}
```
with **yarn cluster** mode and `hive-site.xml` correctly within `$SPARK_HOME/conf`,they failed to connect the right hive metestore for not seeing hive-site.xml in AM/Driver's classpath.

Although submitting them with `--files/--jars local/path/to/hive-site.xml` or puting it to `$HADOOP_CONF_DIR/YARN_CONF_DIR` can make these apps works well in cluster mode as client mode, according to the official doc, see  http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables
> Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.

We may respect these configuration files too or modify the doc for hive-tables in cluster mode.
## How was this patch tested?

cc cloud-fan gatorsmile

Author: Kent Yao <yaooqinn@hotmail.com>

Closes #19663 from yaooqinn/SPARK-21888.
",['resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala'],Self-contained SQL apps fail to connect to the appropriate Hive metastore in yarn cluster mode due to missing hive-site.xml in AM/Driver's classpath even when present in $SPARK_HOME/conf.
c1c710e7da75b989f4d14e84e85f336bc10920e0,1694751977,"[SPARK-45143][PYTHON][CONNECT] Make PySpark compatible with PyArrow 13.0.0

### What changes were proposed in this pull request?
1, in PyArrow 13.0.0, the behavior of [Table#to_pandas](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.to_pandas) and [ChunkedArray#to_pandas](https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html#pyarrow.ChunkedArray.to_pandas) changed, set the `coerce_temporal_nanoseconds=True`

2, there is another undocumented breaking change in data type conversion [`TimestampType#to_pandas_dtype`](https://arrow.apache.org/docs/python/generated/pyarrow.TimestampType.html#pyarrow.TimestampType.to_pandas_dtype):

12.0.1:
```
In [1]: import pyarrow as pa

In [2]: pa.timestamp(""us"", tz=None).to_pandas_dtype()
Out[2]: dtype('<M8[ns]')

In [3]: pa.timestamp(""ns"", tz=None).to_pandas_dtype()
Out[3]: dtype('<M8[ns]')

In [4]: pa.timestamp(""us"", tz=""UTC"").to_pandas_dtype()
Out[4]: datetime64[ns, UTC]

In [5]: pa.timestamp(""ns"", tz=""UTC"").to_pandas_dtype()
Out[5]: datetime64[ns, UTC]
```

13.0.0:
```
In [1]: import pyarrow as pa

In [2]: pa.timestamp(""us"", tz=None).to_pandas_dtype()
Out[2]: dtype('<M8[us]')

In [3]: pa.timestamp(""ns"", tz=None).to_pandas_dtype()
Out[3]: dtype('<M8[ns]')

In [4]: pa.timestamp(""us"", tz=""UTC"").to_pandas_dtype()
Out[4]: datetime64[us, UTC]

In [5]: pa.timestamp(""ns"", tz=""UTC"").to_pandas_dtype()
Out[5]: datetime64[ns, UTC]
```

### Why are the changes needed?
Make PySpark compatible with PyArrow 13.0.0

### Does this PR introduce _any_ user-facing change?
NO

### How was this patch tested?
CI

### Was this patch authored or co-authored using generative AI tooling?
NO

Closes #42920 from zhengruifeng/py_pyarrow_13.

Authored-by: Ruifeng Zheng <ruifengz@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['python/pyspark/pandas/typedef/typehints.py', 'python/pyspark/sql/connect/client/core.py', 'python/pyspark/sql/pandas/conversion.py', 'python/pyspark/sql/pandas/serializers.py']","PySpark is not compatible with PyArrow 13.0.0 due to changes in the behavior of `Table#to_pandas`, `ChunkedArray#to_pandas`, and an undocumented breaking change in `TimestampType#to_pandas_dtype`."
d782a1c4565c401a02531db3b8aa3cb6fc698fb1,1586151299,"[SPARK-31224][SQL] Add view support to SHOW CREATE TABLE

### What changes were proposed in this pull request?

For now `SHOW CREATE TABLE` command doesn't support views, but `SHOW CREATE TABLE AS SERDE` supports it. Since the views syntax are the same between Hive DDL and Spark DDL, we should be able to support views in both two commands.

This is Hive syntax for creating views:

```
CREATE VIEW [IF NOT EXISTS] [db_name.]view_name [(column_name [COMMENT column_comment], ...) ]
  [COMMENT view_comment]
  [TBLPROPERTIES (property_name = property_value, ...)]
  AS SELECT ...;
```

https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateView

This is Spark syntax for creating views:

```
CREATE [OR REPLACE] [[GLOBAL] TEMPORARY] VIEW [IF NOT EXISTS [db_name.]view_name
    create_view_clauses
    AS query;
```
https://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-ddl-create-view.html

Looks like it is the same. We could support views in both commands.

This patch proposes to add views support to `SHOW CREATE TABLE`.

### Why are the changes needed?

To extend the view support of `SHOW CREATE TABLE`, so users can use `SHOW CREATE TABLE` to show Spark DDL for views.

### Does this PR introduce any user-facing change?

Yes. `SHOW CREATE TABLE` can be used to show Spark DDL for views.

### How was this patch tested?

Unit tests.

Closes #27984 from viirya/spark-view.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala', 'sql/core/src/test/resources/sql-tests/inputs/show-create-table.sql', 'sql/core/src/test/scala/org/apache/spark/sql/ShowCreateTableSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveShowCreateTableSuite.scala']","The current `SHOW CREATE TABLE` command in SparkSQL does not support views, limiting the ability of users to display the DDL syntax for views."
ffdbbae1d465fe2c710d020de62ca1a6b0b924d9,1594965874,"[SPARK-32215] Expose a (protected) /workers/kill endpoint on the MasterWebUI

### What changes were proposed in this pull request?

This PR allows an external agent to inform the Master that certain hosts
are being decommissioned.

### Why are the changes needed?

The current decommissioning is triggered by the Worker getting getting a SIGPWR
(out of band possibly by some cleanup hook), which then informs the Master
about it. This approach may not be feasible in some environments that cannot
trigger a clean up hook on the Worker. In addition, when a large number of
worker nodes are being decommissioned then the master will get a flood of
messages.

So we add a new post endpoint `/workers/kill` on the MasterWebUI that allows an
external agent to inform the master about all the nodes being decommissioned in
bulk. The list of nodes is specified by providing a list of hostnames. All workers on those
hosts will be decommissioned.

This API is merely a new entry point into the existing decommissioning
logic. It does not change how the decommissioning request is handled in
its core.

### Does this PR introduce _any_ user-facing change?

Yes, a new endpoint `/workers/kill` is added to the MasterWebUI. By default only
requests originating from an IP address local to the MasterWebUI are allowed.

### How was this patch tested?

Added unit tests

Closes #29015 from agrawaldevesh/master_decom_endpoint.

Authored-by: Devesh Agrawal <devesh.agrawal@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala', 'core/src/main/scala/org/apache/spark/deploy/master/Master.scala', 'core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala', 'core/src/main/scala/org/apache/spark/internal/config/UI.scala', 'core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala', 'core/src/test/scala/org/apache/spark/deploy/master/ui/MasterWebUISuite.scala']",Current decommissioning process through Worker cleanup hook is unfeasible in certain environments and a large number of decommissioning workers flood the Master with messages.
57fcc49306296b474c5b8b685ad13082f9b50a49,1584533990,"[SPARK-31176][SQL] Remove support for 'e'/'c' as datetime pattern charactar

### What changes were proposed in this pull request?

The meaning of 'u' was day number of the week in SimpleDateFormat, it was changed to year in DateTimeFormatter. Now we keep the old meaning of 'u' by substituting 'u' to 'e' internally and use DateTimeFormatter to parse the pattern string. In DateTimeFormatter, the 'e' and 'c' also represents day-of-week. e.g.

```sql
select date_format(timestamp '2019-10-06', 'yyyy-MM-dd uuuu');
select date_format(timestamp '2019-10-06', 'yyyy-MM-dd uuee');
select date_format(timestamp '2019-10-06', 'yyyy-MM-dd eeee');
```
Because of the substitution, they all goes to `.... eeee` silently. The users may congitive problems of their meanings, so we should mark them as illegal pattern characters to stay the same as before.

This pr move the method `convertIncompatiblePattern` from `DatetimeUtils` to `DateTimeFormatterHelper` object, since it is quite specific for `DateTimeFormatterHelper` class.
And 'e' and 'c' char checking in this method.

Besides,`convertIncompatiblePattern` has a bug that will lose the last `'` if it ends with it, this pr fixes this too. e.g.

```sql
spark-sql> select date_format(timestamp ""2019-10-06"", ""yyyy-MM-dd'S'"");
20/03/18 11:19:45 ERROR SparkSQLDriver: Failed in [select date_format(timestamp ""2019-10-06"", ""yyyy-MM-dd'S'"")]
java.lang.IllegalArgumentException: Pattern ends with an incomplete string literal: uuuu-MM-dd'S

spark-sql> select to_timestamp(""2019-10-06S"", ""yyyy-MM-dd'S'"");
NULL
```
### Why are the changes needed?

avoid vagueness
bug fix

### Does this PR introduce any user-facing change?

no, these are not  exposed yet

### How was this patch tested?

add ut

Closes #27939 from yaooqinn/SPARK-31176.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeFormatterHelper.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeFormatterHelperSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeUtilsSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/datetime.sql']","Use of 'e'/'c' as datetime pattern characters in DateTimeFormatter leading to silent substitutions, causing confusing meanings and incomplete string literal issues when pattern ends with it."
3dd3a623f293bc7fd4937c95f06b967fa187b0f1,1575421830,"[SPARK-27990][SPARK-29903][PYTHON] Add recursiveFileLookup option to Python DataFrameReader

### What changes were proposed in this pull request?

As a follow-up to #24830, this PR adds the `recursiveFileLookup` option to the Python DataFrameReader API.

### Why are the changes needed?

This PR maintains Python feature parity with Scala.

### Does this PR introduce any user-facing change?

Yes.

Before this PR, you'd only be able to use this option as follows:

```python
spark.read.option(""recursiveFileLookup"", True).text(""test-data"").show()
```

With this PR, you can reference the option from within the format-specific method:

```python
spark.read.text(""test-data"", recursiveFileLookup=True).show()
```

This option now also shows up in the Python API docs.

### How was this patch tested?

I tested this manually by creating the following directories with dummy data:

```
test-data
├── 1.txt
└── nested
   └── 2.txt
test-parquet
├── nested
│  ├── _SUCCESS
│  ├── part-00000-...-.parquet
├── _SUCCESS
├── part-00000-...-.parquet
```

I then ran the following tests and confirmed the output looked good:

```python
spark.read.parquet(""test-parquet"", recursiveFileLookup=True).show()
spark.read.text(""test-data"", recursiveFileLookup=True).show()
spark.read.csv(""test-data"", recursiveFileLookup=True).show()
```

`python/pyspark/sql/tests/test_readwriter.py` seems pretty sparse. I'm happy to add my tests there, though it seems we have been deferring testing like this to the Scala side of things.

Closes #26718 from nchammas/SPARK-27990-recursiveFileLookup-python.

Authored-by: Nicholas Chammas <nicholas.chammas@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['python/pyspark/sql/readwriter.py', 'python/pyspark/sql/streaming.py']","The `recursiveFileLookup` option is not available directly in the Python DataFrameReader API, forcing users to use this option specifically from the format-specific method."
28a2a2238fbaf4fad3c98cfef2b3049c1f4616c8,1628076366,"[SPARK-36354][CORE] EventLogFileReader should skip rolling event log directories with no logs

### What changes were proposed in this pull request?

This PR aims to skip rolling event log directories which has only `appstatus` file.

### Why are the changes needed?

Currently, Spark History server shows `IllegalArgumentException` warning, but the event log might arrive later. The situation also can happen when the job is killed before uploading its first log to the remote storages like S3.
```
21/07/30 07:38:26 WARN FsHistoryProvider:
Error while reading new log s3a://.../eventlog_v2_spark-95b5c736c8e44037afcf152534d08771
java.lang.IllegalArgumentException: requirement failed:
Log directory must contain at least one event log file!
...
at org.apache.spark.deploy.history.RollingEventLogFilesFileReader.files$lzycompute(EventLogFileReaders.scala:216)
```

### Does this PR introduce _any_ user-facing change?

Yes. Users will not see `IllegalArgumentException` warnings.

### How was this patch tested?

Pass the CIs with the newly added test case.

Closes #33586 from dongjoon-hyun/SPARK-36354.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['core/src/main/scala/org/apache/spark/deploy/history/EventLogFileReaders.scala', 'core/src/test/scala/org/apache/spark/deploy/history/EventLogFileReadersSuite.scala', 'core/src/test/scala/org/apache/spark/deploy/history/FsHistoryProviderSuite.scala']","Spark History server raises IllegalArgumentException warnings when encountering rolling event log directories that only contain the 'appstatus' file, which occurs when a job doesn't upload its first log on time."
8bd27025b7cf0b44726b6f4020d294ef14dbbb7e,1525290199,"[SPARK-24133][SQL] Check for integer overflows when resizing WritableColumnVectors

## What changes were proposed in this pull request?

`ColumnVector`s store string data in one big byte array. Since the array size is capped at just under Integer.MAX_VALUE, a single `ColumnVector` cannot store more than 2GB of string data.
But since the Parquet files commonly contain large blobs stored as strings, and `ColumnVector`s by default carry 4096 values, it's entirely possible to go past that limit. In such cases a negative capacity is requested from `WritableColumnVector.reserve()`. The call succeeds (requested capacity is smaller than already allocated capacity), and consequently `java.lang.ArrayIndexOutOfBoundsException` is thrown when the reader actually attempts to put the data into the array.

This change introduces a simple check for integer overflow to `WritableColumnVector.reserve()` which should help catch the error earlier and provide more informative exception. Additionally, the error message in `WritableColumnVector.throwUnsupportedException()` was corrected, as it previously encouraged users to increase rather than reduce the batch size.

## How was this patch tested?

New units tests were added.

Author: Ala Luszczak <ala@databricks.com>

Closes #21206 from ala/overflow-reserve.
","['sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java', 'sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala']","WritableColumnVector's reserve() function allows for an integer overflow resulting in the request of a negative capacity, which in turn throws an ArrayIndexOutOfBoundsException when attempting to put data into the array."
ffc97530371433bc0221e06d8c1d11af8d92bd94,1574151022,"[SPARK-29918][SQL] RecordBinaryComparator should check endianness when compared by long

### What changes were proposed in this pull request?
This PR try to make sure the comparison results of  `compared by 8 bytes at a time` and `compared by bytes wise` in RecordBinaryComparator is *consistent*, by reverse long bytes if it is little-endian and using Long.compareUnsigned.

### Why are the changes needed?
If the architecture supports unaligned or the offset is 8 bytes aligned, `RecordBinaryComparator` compare 8 bytes at a time by reading 8 bytes as a long.  Related code is
```
    if (Platform.unaligned() || (((leftOff + i) % 8 == 0) && ((rightOff + i) % 8 == 0))) {
      while (i <= leftLen - 8) {
        final long v1 = Platform.getLong(leftObj, leftOff + i);
        final long v2 = Platform.getLong(rightObj, rightOff + i);
        if (v1 != v2) {
          return v1 > v2 ? 1 : -1;
        }
        i += 8;
      }
    }
```

Otherwise, it will compare bytes by bytes.  Related code is
```
    while (i < leftLen) {
      final int v1 = Platform.getByte(leftObj, leftOff + i) & 0xff;
      final int v2 = Platform.getByte(rightObj, rightOff + i) & 0xff;
      if (v1 != v2) {
        return v1 > v2 ? 1 : -1;
      }
      i += 1;
    }
```

However, on little-endian machine,  the result of *compared by a long value* and *compared bytes by bytes* maybe different.

For two same records, its offsets may vary in the first run and second run, which will lead to compare them using long comparison or byte-by-byte comparison, the result maybe different.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Add new test cases in RecordBinaryComparatorSuite

Closes #26548 from WangGuangxin/binary_comparator.

Authored-by: wangguangxin.cn <wangguangxin.cn@bytedance.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/java/org/apache/spark/sql/execution/RecordBinaryComparator.java', 'sql/core/src/test/java/test/org/apache/spark/sql/execution/sort/RecordBinaryComparatorSuite.java']","RecordBinaryComparator provides inconsistent results when comparing records byte-by-byte and as a long on little-endian machines, leading to potential discrepancies between multiple runs due to offset variances."
028992399b2be19b0e97225ef3e4def4e10b7570,1661411472,"[SPARK-40209][SQL] Don't change the interval value of Decimal in `changePrecision()` on errors

### What changes were proposed in this pull request?
In the PR, I propose to modify the internal value of Decimal `longVal` in the method `changePrecision` only in the success cases, and don't touch it on any errors to keep the original value.

### Why are the changes needed?
To don't confuse users by error messages. The fix might improve user experience with Spark SQL.

### Does this PR introduce _any_ user-facing change?
Yes. The PR changes user-facing errors.

Before:
```sql
spark-sql> select cast(interval '10.123' second as decimal(1, 0));
[NUMERIC_VALUE_OUT_OF_RANGE] 0.000010 cannot be represented ...
```

After:
```sql
spark-sql> select cast(interval '10.123' second as decimal(1, 0));
[NUMERIC_VALUE_OUT_OF_RANGE] 10.123000 cannot be represented ...
```

### How was this patch tested?
By running the affected test suites:
```
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z cast.sql""
```

Closes #37649 from MaxGekk/longVal-changePrecision.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
",['sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala'],"Casting intervals as decimals in Spark SQL changes the Decimal value in `changePrecision()` even when errors occur, leading to confusing error messages."
21232377ba654babd0dc929b5278835beefcc6a1,1618215832,"[SPARK-33229][SQL] Support partial grouping analytics and concatenated grouping analytics

### What changes were proposed in this pull request?
Support GROUP BY use Separate columns and CUBE/ROLLUP

In postgres sql, it support
```
select a, b, c, count(1) from t group by a, b, cube (a, b, c);
select a, b, c, count(1) from t group by a, b, rollup(a, b, c);
select a, b, c, count(1) from t group by cube(a, b), rollup (a, b, c);
select a, b, c, count(1) from t group by a, b, grouping sets((a, b), (a), ());
```
In this pr, we have done two things as below:

1. Support partial grouping analytics such as `group by a, cube(a, b)`
2. Support mixed grouping analytics such as `group by cube(a, b), rollup(b,c)`

*Partial Groupings*

    Partial Groupings means there are both `group_expression` and `CUBE|ROLLUP|GROUPING SETS`
    in GROUP BY clause. For example:
    `GROUP BY warehouse, CUBE(product, location)` is equivalent to
    `GROUP BY GROUPING SETS((warehouse, product, location), (warehouse, product), (warehouse, location), (warehouse))`.
    `GROUP BY warehouse, ROLLUP(product, location)` is equivalent to
    `GROUP BY GROUPING SETS((warehouse, product, location), (warehouse, product), (warehouse))`.
    `GROUP BY warehouse, GROUPING SETS((product, location), (producet), ())` is equivalent to
    `GROUP BY GROUPING SETS((warehouse, product, location), (warehouse, location), (warehouse))`.

*Concatenated Groupings*

    Concatenated groupings offer a concise way to generate useful combinations of groupings. Groupings specified
    with concatenated groupings yield the cross-product of groupings from each grouping set. The cross-product
    operation enables even a small number of concatenated groupings to generate a large number of final groups.
    The concatenated groupings are specified simply by listing multiple `GROUPING SETS`, `CUBES`, and `ROLLUP`,
    and separating them with commas. For example:
    `GROUP BY GROUPING SETS((warehouse), (producet)), GROUPING SETS((location), (size))` is equivalent to
    `GROUP BY GROUPING SETS((warehouse, location), (warehouse, size), (product, location), (product, size))`.
    `GROUP BY CUBE((warehouse), (producet)), ROLLUP((location), (size))` is equivalent to
    `GROUP BY GROUPING SETS((warehouse, product), (warehouse), (producet), ()), GROUPING SETS((location, size), (location), ())`
    `GROUP BY GROUPING SETS(
        (warehouse, product, location, size), (warehouse, product, location), (warehouse, product),
        (warehouse, location, size), (warehouse, location), (warehouse),
        (product, location, size), (product, location), (product),
        (location, size), (location), ())`.
    `GROUP BY order, CUBE((warehouse), (producet)), ROLLUP((location), (size))` is equivalent to
    `GROUP BY order, GROUPING SETS((warehouse, product), (warehouse), (producet), ()), GROUPING SETS((location, size), (location), ())`
    `GROUP BY GROUPING SETS(
        (order, warehouse, product, location, size), (order, warehouse, product, location), (order, warehouse, product),
        (order, warehouse, location, size), (order, warehouse, location), (order, warehouse),
        (order, product, location, size), (order, product, location), (order, product),
        (order, location, size), (order, location), (order))`.

### Why are the changes needed?
Support more flexible grouping analytics

### Does this PR introduce _any_ user-facing change?
User can use sql like
```
select a, b, c, agg_expr() from table group by a, cube(b, c)
```

### How was this patch tested?
Added UT

Closes #30144 from AngersZhuuuu/SPARK-33229.

Lead-authored-by: Angerszhuuuu <angers.zhu@gmail.com>
Co-authored-by: angerszhu <angers.zhu@gmail.com>
Co-authored-by: Wenchen Fan <cloud0fan@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/core/src/test/resources/sql-tests/inputs/group-analytics.sql']","Current SQL support lacks flexibility in grouping analytics. It doesn't support partial grouping analytics like `group by a, cube(a, b)` or mixed grouping analytics such as `group by cube(a, b), rollup(b,c)`."
3331d4ccb7df9aeb1972ed86472269a9dbd261ff,1657286427,"[SPARK-39620][WEB UI] Use same condition in history server page and API to filter applications

### What changes were proposed in this pull request?

Updated REST API `/api/v1/applications`, to use the same condition as history server page to filter completed/incomplete applications.

### Why are the changes needed?

When opening summary page, history server follows this logic:

- If there's completed/incomplete application, page will add script in response, using AJAX to call the REST API to get the filtered list.
- If there's no such application, page will only return a message telling nothing found.

Issue is that page and REST API are using different conditions to filter applications. In `HistoryPage`, an application is considered as completed as long as the last attempt is completed. But in `ApplicationListResource`, all attempts should be completed. This brings inconsistency and will cause issue in a corner case.

In driver, event queues have capacity to protect memory. When there's too many events, some of them will be dropped and the event log file will be incomplete. For an application with multiple attempts, there's possibility that the last attempt is completed, but the previous attempts is considered as incomplete due to loss of application end event.

For this type of application, page thinks it is completed, but the API thinks it is still running. When opening summary page:
- When checking completed applications, page will call script, but API returns nothing.
- When checking incomplete applications, page returns nothing.

So the user won't be able to see this app in history server.

### Does this PR introduce _any_ user-facing change?

Yes, there will be a change on `/api/v1/applications` API and history server summary page.

When calling API, for application mentioned above, previously it is considered as running. After the change it is considered as completed. So the result will be different using same filter. But this change should be OK. Because attempts are executed sequentially and incrementally. So if an attempt with bigger ID is completed, the previous attempts can be considered as completed.

For history server summary page, previously user is not able to see the application. Now it will appear in the completed applications.

### How was this patch tested?

Add a new unit test `HistoryServerPageSuite`, which will check whether `HistoryPage` behaves the same as `ApplicationListResource` when filtering applications. To implement the test, there's a minor change of `HistoryPage`, exposing a method called `shouldDisplayApplications` to tell whether the summary page will display applications.

The test verifies that:
- If no completed/incomplete application found, `HistoryPage` should not display applications, and API should return an empty list.
- Otherwise, `HistoryPage` should display applications, and API should return a non-empty list.

Currently 2 scenarios are included:
- Application with last attempt completed but previous attempt incomplete.
- Application with last attempt incomplete but previous attempt completed.

Closes #37008 from kuwii/kuwii/hs-fix.

Authored-by: kuwii <kuwii.someone@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala', 'core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala', 'core/src/test/scala/org/apache/spark/deploy/history/HistoryServerPageSuite.scala']",Inconsistent conditions used by the History Server page and REST API for filtering apps result in visibility issues on the summary page for apps with mixed completed and incomplete attempts.
e4df5d14b224f9445333f3457c7d05c5c7aeb461,1694193058,"[MINOR][DOCS] Change ""filter"" to ""transform"" in `transform` function docstring

### What changes were proposed in this pull request?
This PR proposes a simple change in the documentation for the `transform` function in `sql`. I believe where it currently reads ""filter"" it should read ""transform"".

### Why are the changes needed?
I believe this change might not be needed per se, but it would be a slight improvement to the current version to avoid the misnomer.

### Does this PR introduce _any_ user-facing change?
Yes, it shows the word ""transform' instead of ""filter"" in the documentation for the `transform` `sql`. function.

### How was this patch tested?
This patch was not tested because it only changes documentation.

### Was this patch authored or co-authored using generative AI tooling?
No

Closes #42858 from gdahia/patch-1.

Lead-authored-by: Gabriel Dahia <gdahia@protonmail.com>
Co-authored-by: Gabriel Dahia <gdahia@users.noreply.github.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['sql/core/src/main/scala/org/apache/spark/sql/functions.scala'],"The `transform` function in SQL documentation erroneously uses the term ""filter"", leading to potential misunderstanding for users."
026b0b926dfd40038f2cee932f38b917eb25b77e,1595127623,"[SPARK-32253][INFRA] Show errors only for the sbt tests of github actions

### What changes were proposed in this pull request?

Make the test result log of github action more readable by showing errors from SBT only.
1. Add ""--error"" flag to sbt in github action to set the log level as ""ERROR""
2. Show only failed test cases in stderr output of github action. According to https://www.scalatest.org/user_guide/using_the_runner, with SBT option `-eNCXEHLOPQMDF ` we can drop all the following events:
```
N - drop TestStarting events
C - drop TestSucceeded events
X - drop TestIgnored events
E - drop TestPending events
H - drop SuiteStarting events
L - drop SuiteCompleted events
O - drop InfoProvided events
P - drop ScopeOpened events
Q - drop ScopeClosed events
R - drop ScopePending events
M - drop MarkupProvided events
```
and enable the following two mode:
```
D - show all durations
F - show full stack traces
```

### Why are the changes needed?

Currently, the output of github action is very long and we have to scroll down to find the failed test cases. Even more, the log may be truncated. In such a case, we will have to wait until all the jobs are completed and then download all the raw logs.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Before changes, all the warnings in compiling are shown:
![image](https://user-images.githubusercontent.com/1097932/87846810-98ec8900-c887-11ea-913b-164b84df62cd.png)

as well as all the passed and ignored test cases:
![image](https://user-images.githubusercontent.com/1097932/87846834-ca655480-c887-11ea-9c29-977f802e4c82.png)

After changes, sbt test only shows the summary for a successful job:
![image](https://user-images.githubusercontent.com/1097932/87846961-e74e5780-c888-11ea-82d5-cf1da1740181.png)

![image](https://user-images.githubusercontent.com/1097932/87745273-5735e280-c7a2-11ea-8ac9-b4b0e3cb458d.png)

If there is a test failure, a full stack track is shown as well as a test failure summary at the end of test log:

![image](https://user-images.githubusercontent.com/1097932/87751143-3aa1a680-c7b2-11ea-9d09-52637a322270.png)

![image](https://user-images.githubusercontent.com/1097932/87752404-1f846600-c7b5-11ea-8106-8ddaf3cc3f7e.png)

Closes #29133 from gengliangwang/shortLog.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['dev/run-tests.py', 'project/SparkBuild.scala']","The output log of GitHub actions for sbt tests is unreadable due to verbosity, forcing download of raw logs to determine failed test cases."
d7df7a805fcbdf2435df1e78abd9899d3ca10dd2,1626628515,"[SPARK-36195][BUILD] Set MaxMetaspaceSize JVM option to 2g

### What changes were proposed in this pull request?

This PR aims to set `MaxMetaspaceSize` to `2g` because it's increasing the native memory consumption unlimitedly by default. The unlimited increasing memory causes GitHub Action flakiness. The value I observed during `hive` module test was over 1.8G and growing.

- https://docs.oracle.com/javase/10/gctuning/other-considerations.htm#JSGCT-GUID-BFB89453-60C0-42AC-81CA-87D59B0ACE2E
> Starting with JDK 8, the permanent generation was removed and the class metadata is allocated in native memory. The amount of native memory that can be used for class metadata is by default unlimited. Use the option -XX:MaxMetaspaceSize to put an upper limit on the amount of native memory used for class metadata.

In addition, I increased the following memory limit to 4g consistently from two places.
```xml
- <jvmArg>-Xms2048m</jvmArg>
- <jvmArg>-Xmx2048m</jvmArg>
+ <jvmArg>-Xms4g</jvmArg>
+ <jvmArg>-Xmx4g</jvmArg>
```

```scala
- javaOptions += ""-Xmx3g"",
+ javaOptions ++= ""-Xmx4g -XX:MaxMetaspaceSize=2g"".split("" "").toSeq,
```

### Why are the changes needed?

This will reduce the flakiness in CI environment by limiting the memory usage explicitly.

When we limit it with `1g`, Hive module fails with `OOM` like the following.
```
java.lang.OutOfMemoryError: Metaspace
Error: Exception in thread ""dispatcher-event-loop-110"" java.lang.OutOfMemoryError: Metaspace
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs.

Closes #33405 from dongjoon-hyun/SPARK-36195.

Lead-authored-by: Dongjoon Hyun <dongjoon@apache.org>
Co-authored-by: Kyle Bendickson <kbendickson@apple.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['project/SparkBuild.scala'],Unlimited native memory consumption is causing 'OutOfMemoryError: Metaspace' exceptions and overall GitHub Action instability during the Hive module test.
a2dbfeefa98e561734c10fc25cd44017882313d8,1667979667,"[SPARK-41056][R] Fix new R_LIBS_SITE behavior introduced in R 4.2

### What changes were proposed in this pull request?

This PR proposes to keep the `R_LIBS_SITE` as was. It has been changed from R 4.2.

### Why are the changes needed?

To keep the behaviour same as the previous. This especially affects the external libraries installed in SparkR worker sides. Especially this can break the user-installed libraries. See the paths below:

**R 4.2**

```r
# R
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/local/lib/R/site-library/:/usr/lib/R/site-library:/usr/lib/R/library'""
# R --vanilla
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/lib/R/site-library""
```

**R 4.1**

```r
# R
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library""
# R --vanilla
> Sys.getenv(""R_LIBS_SITE"")
[1] ""/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library""
```

### Does this PR introduce _any_ user-facing change?

Yes. With R 4.2, user-installed libraries won't be found in SparkR workers.

### How was this patch tested?

Manually tested, unittest added. It's difficult to add an e2e tests.

I also manually tested `getROptions` in Scala shall.

Closes #38570 from HyukjinKwon/SPARK-41056.

Authored-by: Hyukjin Kwon <gurwls223@apache.org>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['core/src/main/scala/org/apache/spark/api/r/BaseRRunner.scala', 'core/src/test/scala/org/apache/spark/api/r/BaseRRunnerSuite.scala']","Changes in R 4.2 result in `R_LIBS_SITE` behavior disrupting SparkR worker functionality, causing user-installed libraries to not be found as expected."
9b647e80b9e6b9a2b7afc9045cb8c4aa9b690aae,1673861588,"[SPARK-42087][SQL][TESTS] Use `--no-same-owner` when `HiveExternalCatalogVersionsSuite` untars

### What changes were proposed in this pull request?

This PR aims to use a new parameter `--no-same-owner` when `HiveExternalCatalogVersionsSuite` untars

### Why are the changes needed?

**BEFORE**
```
rootedeccfee6d53:/spark# tar xfz spark-3.3.1-bin-hadoop3.tgz
rootedeccfee6d53:/spark# ls -al spark-3.3.1-bin-hadoop3
total 96
drwxr-xr-x  17 110302528 1000   544 Oct 15 10:32 .
drwxr-xr-x   4 root      root   128 Jan 16 07:30 ..
-rw-r--r--   1 root      root 22940 Oct 15 10:32 LICENSE
-rw-r--r--   1 root      root 57842 Oct 15 10:32 NOTICE
drwxr-xr-x   3 110302528 1000    96 Oct 15 10:32 R
-rw-r--r--   1 root      root  4461 Oct 15 10:32 README.md
-rw-r--r--   1 root      root   165 Oct 15 10:32 RELEASE
drwxr-xr-x  29 110302528 1000   928 Oct 15 10:32 bin
drwxr-xr-x   8 110302528 1000   256 Oct 15 10:32 conf
drwxr-xr-x   5 110302528 1000   160 Oct 15 10:32 data
drwxr-xr-x   4 110302528 1000   128 Oct 15 10:32 examples
drwxr-xr-x 250 110302528 1000  8000 Oct 15 10:32 jars
drwxr-xr-x   4 110302528 1000   128 Oct 15 10:32 kubernetes
drwxr-xr-x  60 110302528 1000  1920 Oct 15 10:32 licenses
drwxr-xr-x  19 110302528 1000   608 Oct 15 10:32 python
drwxr-xr-x  29 110302528 1000   928 Oct 15 10:32 sbin
drwxr-xr-x   3 110302528 1000    96 Oct 15 10:32 yarn
```

**AFTER**
```
rootedeccfee6d53:/spark# tar xfz spark-3.3.1-bin-hadoop3.tgz --no-same-owner
rootedeccfee6d53:/spark# ls -al spark-3.3.1-bin-hadoop3
total 96
drwxr-xr-x  17 root root   544 Oct 15 10:32 .
drwxr-xr-x   4 root root   128 Jan 16 07:34 ..
-rw-r--r--   1 root root 22940 Oct 15 10:32 LICENSE
-rw-r--r--   1 root root 57842 Oct 15 10:32 NOTICE
drwxr-xr-x   3 root root    96 Oct 15 10:32 R
-rw-r--r--   1 root root  4461 Oct 15 10:32 README.md
-rw-r--r--   1 root root   165 Oct 15 10:32 RELEASE
drwxr-xr-x  29 root root   928 Oct 15 10:32 bin
drwxr-xr-x   8 root root   256 Oct 15 10:32 conf
drwxr-xr-x   5 root root   160 Oct 15 10:32 data
drwxr-xr-x   4 root root   128 Oct 15 10:32 examples
drwxr-xr-x 250 root root  8000 Oct 15 10:32 jars
drwxr-xr-x   4 root root   128 Oct 15 10:32 kubernetes
drwxr-xr-x  60 root root  1920 Oct 15 10:32 licenses
drwxr-xr-x  19 root root   608 Oct 15 10:32 python
drwxr-xr-x  29 root root   928 Oct 15 10:32 sbin
drwxr-xr-x   3 root root    96 Oct 15 10:32 yarn
```

### Does this PR introduce _any_ user-facing change?

No. This is a test-only improvement.

### How was this patch tested?

Pass the CIs.

Closes #39601 from dongjoon-hyun/SPARK-42087.

Authored-by: Dongjoon Hyun <dongjoon@apache.org>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
",['sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveExternalCatalogVersionsSuite.scala'],`HiveExternalCatalogVersionsSuite` untar operation results in incorrect file ownership causing tests to fail or behave unexpectedly.
969ffd631746125eb2b83722baf6f6e7ddd2092c,1507170322,"[SPARK-22187][SS] Update unsaferow format for saved state such that we can set timeouts when state is null

## What changes were proposed in this pull request?

Currently, the group state of user-defined-type is encoded as top-level columns in the UnsafeRows stores in the state store. The timeout timestamp is also saved as (when needed) as the last top-level column. Since the group state is serialized to top-level columns, you cannot save ""null"" as a value of state (setting null in all the top-level columns is not equivalent). So we don't let the user set the timeout without initializing the state for a key. Based on user experience, this leads to confusion.

This PR is to change the row format such that the state is saved as nested columns. This would allow the state to be set to null, and avoid these confusing corner cases.

## How was this patch tested?
Refactored tests.

Author: Tathagata Das <tathagata.das1565@gmail.com>

Closes #19416 from tdas/SPARK-22187.
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/FlatMapGroupsWithState_StateManager.scala', 'sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala']","The group state saved in the state store does not allow ""null"" values due to its encoding as top-level columns, leading to confusion and inability to set timeouts when the state is null."
5c9175c9da719536123477d7fcc784a4086fbe25,1659974779,"[SPARK-39912][SPARK-39828][SQL] Refine CatalogImpl

### What changes were proposed in this pull request?

`CatalogImpl` has been updated quite a bit recently, to support v2 catalogs. This PR revisits the recent changes and refines the code a little bit:
1. fix the naming ""3 layer namespace"". The spark catalog plugin supports n-part namespace. This PR changes it to `qualified name with catalog`.
2. always use the v2 code path. Today the v2 code path can already cover all the functionalities of `CatalogImpl` and it's unnecessary to keep the v1 code path in `CatalogImpl`. It also makes sure the behavior is consistent between `db.table` and `spark_catalog.db.table`. Previously it was not consistent in some cases, see the updated tests for functions.
3. simplify `try {v1 code path} catch {... v2 code path}` to `val name = if (table exists in HMS) {name qualified with spark_catalog} else {parsed name}; v2 code path`

### Why are the changes needed?

code cleanup.

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

existing tests

Closes #37287 from cloud-fan/catalog.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['python/pyspark/sql/catalog.py', 'python/pyspark/sql/tests/test_catalog.py', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala', 'sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/GlobalTempViewSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/internal/CatalogSuite.scala']","Inconsistencies are present between `db.table` and `spark_catalog.db.table` behaviors, along with redundant v1 code path present in `CatalogImpl`. Naming convention, ""3 layer namespace"", does not align with the n-part namespace of the spark catalog plugin."
21b74797978e998504d795551dcc6b6a0e5801ac,1600840185,"[SPARK-32959][SQL][TEST] Fix an invalid test in DataSourceV2SQLSuite

### What changes were proposed in this pull request?

This PR addresses two issues related to the `Relation: view text` test in `DataSourceV2SQLSuite`.

1. The test has the following block:
```scala
withView(""view1"") { v1: String =>
  sql(...)
}
```
Since `withView`'s signature is `withView(v: String*)(f: => Unit): Unit`, the `f` that will be executed is ` v1: String => sql(..)`, which is just defining the anonymous function, and _not_ executing it.

2. Once the test is fixed to run, it actually fails. The reason is that the v2 session catalog implementation used in tests does not correctly handle `V1Table` for views in `loadTable`. And this results in views resolved to `ResolvedTable` instead of `ResolvedView`, causing the test failure: https://github.com/apache/spark/blob/f1dc479d39a6f05df7155008d8ec26dff42bb06c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L1007-L1011

### Why are the changes needed?

Fixing a bug in test.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing test.

Closes #29811 from imback82/fix_minor_test.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/TestV2SessionCatalogBase.scala']","The 'Relation: view text' test in DataSourceV2SQLSuite is invalid due to the improper use of 'withView' block and failure in handling `V1Table` for views in `loadTable`, resulting in incorrect resolution of views."
e5728e2fb706b0b611b371bdb7216acbdfe5c49b,1693323006,"[SPARK-44999][CORE] Refactor `ExternalSorter` to reduce checks on `shouldPartition` when calling `getPartition`

### What changes were proposed in this pull request?
The `getPartition` method checks `shouldPartition` every time it is called. However, `shouldPartition` should not be changeable after the `ExternalSorter` is instantiated, so this PR makes the following changes to `getPartition` to avoid always checking `shouldPartition`:

1. Added `val actualPartitioner`: when `shouldPartition` is true, it uses `partitioner.get`, otherwise it returns `ConstantPartitioner`, where `ConstantPartitioner` is defined as follows:

https://github.com/apache/spark/blob/df63adf734370f5c2d71a348f9d36658718b302c/core/src/main/scala/org/apache/spark/Partitioner.scala#L156-L162

2. After step 1, the private method `getPartition` can directly call `actualPartitioner.getPartition`. In order to shorten the call stack, this PR replaces the call to `getPartition` in `ExternalSorter` with a call to `actualPartitioner.getPartition`.

3. Checked `numPartitions > 1` directly when initializing `val actualPartitioner` and removed `val shouldPartition`, because it is no longer used elsewhere.

### Why are the changes needed?
To reduce checks on `shouldPartition` when calling `getPartition`

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Pass GitHub Actions

### Was this patch authored or co-authored using generative AI tooling?
No

Closes #42713 from LuciferYang/ExternalSorter-partitioner.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala'],"The method `getPartition` in `ExternalSorter` unnecessarily checks `shouldPartition` each time it's called, despite `shouldPartition` being unchangeable post-instantiation."
f1495c5bc0351fa16bd4ab2a1f1540dc9a24ad92,1590084000,"[SPARK-31688][WEBUI] Refactor Pagination framework

### What changes were proposed in this pull request?
Currently while implementing pagination using the existing pagination framework, a lot of code is being copied as pointed out [here](https://github.com/apache/spark/pull/28485#pullrequestreview-408881656).

I introduced some changes in `PagedTable` which is the main trait for implementing the pagination.
* Added function for getting table parameters.
* Added a function for table header row. This will help in maintaining consistency across the tables. All the header rows across tables will be consistent now.

### Why are the changes needed?

* A lot of code is copied every time pagination is implemented for any table.
* Code readability is not great as lot of HTML is embedded.
* Paginating other tables will be a lot easier now.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Manually. This is mainly refactoring work, no new functionality introduced. Existing test cases should pass.

Closes #28512 from iRakson/refactorPaginationFramework.

Authored-by: iRakson <raksonrakesh@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>
","['core/src/main/scala/org/apache/spark/ui/PagedTable.scala', 'core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala', 'core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala', 'core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/ui/ThriftServerPage.scala', 'sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/ui/ThriftServerSessionPage.scala']",Implementing pagination with the current framework requires excessive code duplication and compromises code readability with embedded HTML. It also lacks consistency across table header rows.
8d9495a8f1e64dbc42c3741f9bcbd4893ce3f0e9,1535714649,"[SPARK-25207][SQL] Case-insensitve field resolution for filter pushdown when reading Parquet

## What changes were proposed in this pull request?

Currently, filter pushdown will not work if Parquet schema and Hive metastore schema are in different letter cases even spark.sql.caseSensitive is false.

Like the below case:
```scala
spark.sparkContext.hadoopConfiguration.setInt(""parquet.block.size"", 8 * 1024 * 1024)
spark.range(1, 40 * 1024 * 1024, 1, 1).sortWithinPartitions(""id"").write.parquet(""/tmp/t"")
sql(""CREATE TABLE t (ID LONG) USING parquet LOCATION '/tmp/t'"")
sql(""select * from t where id < 100L"").write.csv(""/tmp/id"")
```

Although filter ""ID < 100L"" is generated by Spark, it fails to pushdown into parquet actually, Spark still does the full table scan when reading.
This PR provides a case-insensitive field resolution to make it work.

Before - ""ID < 100L"" fail to pushedown:
<img width=""273"" alt=""screen shot 2018-08-23 at 10 08 26 pm"" src=""https://user-images.githubusercontent.com/2989575/44530558-40ef8b00-a721-11e8-8abc-7f97671590d3.png"">
After - ""ID < 100L"" pushedown sucessfully:
<img width=""267"" alt=""screen shot 2018-08-23 at 10 08 40 pm"" src=""https://user-images.githubusercontent.com/2989575/44530567-44831200-a721-11e8-8634-e9f664b33d39.png"">

## How was this patch tested?

Added UTs.

Closes #22197 from yucai/SPARK-25207.

Authored-by: yucai <yyu1@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala']","Filter pushdown is not functioning for case-insensitive Parquet and Hive metastore schemas, leading to unnecessary full-table scans even when spark.sql.caseSensitive is set to false."
0e33195d1ba1925c21abf8f2045e05ce517fef8f,1658908975,"[SPARK-39834][SQL][SS] Include the origin stats and constraints for LogicalRDD if it comes from DataFrame

Credit to juliuszsompolski for figuring out issues and proposing the alternative.

### What changes were proposed in this pull request?

This PR proposes to effectively revert SPARK-39748 but include the origin stats and constraints instead in LogicalRDD if it comes from DataFrame, to help optimizer figuring out better plan.

### Why are the changes needed?

We figured out several issues from [SPARK-39748](https://issues.apache.org/jira/browse/SPARK-39748):

1. One of major use case for DataFrame.checkpoint is ML, especially ""iterative algorithm"", and the purpose on calling checkpoint is to ""prune"" the logical plan. That is against the purpose of including origin logical plan and we have a risk to have nested LogicalRDDs which grows the size of logical plan infinitely.

2. We leverage logical plan to carry over stats, but the correct stats information is in optimized plan.

3. (Not an issue but missing spot) constraints is also something we can carry over.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing and new UTs.

Closes #37248 from HeartSaVioR/SPARK-39834.

Authored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ForeachBatchSink.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/ForeachBatchSinkSuite.scala']","The DataFrame.checkpoint use case in ML, particularly for ""iterative algorithms"", is being hindered due to nested LogicalRDDs that exponentially increase the size of the logical plan. Furthermore, stats information isn't accurately represented within logical plans, but rather in optimized plans."
4fbfa357f8f27d8ae263470975785fdaa26591ba,1681394072,"[SPARK-41674][SQL] Runtime filter should supports multi level shuffle join side as filter creation side

### What changes were proposed in this pull request?
Currently, Spark already supported the runtime filter. The runtime filter is a very good feature to reduce shuffle size.
This feature is implemented through the following steps:

1. Finds joins with equality conditions that can be evaluated using equi-join. e.g. ==.

2. Check some prerequisites for this join.

     2.1. If there is already a DPP filter on the key, refuse to inject runtime filter.
     2.2 If there is already a runtime filter (Bloom filter or IN subquery) on the key, refuse to inject runtime filter.
     2.3 If the keys of two side of join are not simple cheap expressions, refuse to inject runtime filter.

3. Check the benefit if inject runtime filter.

     3.1 The join expression of filter application side can be pushed down through joins, aggregates and windows. This decide to reduce shuffle size.
     3.2 The filter creation side has a selective predicate. The better selective, the better to reduce data size.
           **Note**: This PR will change the the behavior here, from **has one** selective predicate to **exists one** selective predicate. Before, we find the selective predicate from Filter node and check the expressions on Project are cheap. Now, not only do this and will find the selective predicate from Join node's child node on the same way.
     3.3 The current join is a shuffle join or a broadcast join that has a shuffle below it.
     3.4 The max scan size of filter application side is greater than a configurable threshold.

According to 3.3, Spark runtime filter supports two scenes.
1. When the Join itself is a Shuffle Join;
![image](https://user-images.githubusercontent.com/8486025/211998351-b616d2ca-cba6-4fd8-9768-8ff1d11ad8d6.png)

5. When the join itself is a broadcast join and there is a Shuffle Join under one end of the join.
![image](https://user-images.githubusercontent.com/8486025/211998413-e28f053f-ee15-4e8b-9281-1115f46e5892.png)

In user scene, the join relation is very complicated, such as, multi level join. To facilitate understanding, the below SQL is taken as an example.
```
SELECT *
FROM T1
    JOIN T2
    JOIN T3
    ON T1.c1 = T2.c2
        AND T3.c3 = T2.c2
WHERE T2.a2 = 5
```
Suppose T1 and T3 is big table or subquery and T2 is the smallest one. The current optimization only injects runtime filter(subquery on T2) in T1 since the SQL match the first scene.
![image](https://user-images.githubusercontent.com/8486025/212867612-238a70e9-39e9-43a2-beac-f6b994830e71.png)

In fact, we could also inject the runtime filter(subquery on T2) in T3 too.
![image](https://user-images.githubusercontent.com/8486025/212868009-f4278e2b-86c8-4b03-92b7-65cabde52370.png)

This PR will improve q24a, q24b, q37 and q82 in TPC-DS and q17 in TPC-H.

### Why are the changes needed?
1. Improve the supported scene for runtime filter
2. Reduct the data size for shuffle and improve the performance.

### Does this PR introduce _any_ user-facing change?
'No'.
Just update the inner implementation.

### How was this patch tested?
New tests.
Micro benchmark for q24a, q24b, q37 and q82 in TPC-DS.

**2TB TPC-DS**

TPC-DS Query   | Before(Seconds)  | After(Seconds)  | Speedup(Percent)
----  | ----  | ----  | ----
 q24a | 196.791 | 190.808 | 3.14%
 q24b | 178.022 | 178.953 | -0.52%
 q37 | 36.815 | 12.114 | 203.90%
 q82 | 63.822 | 19.543 | 226.57%

Micro benchmark for q17 in TPC-H.
**TPC-H 100**
Query | Master(ms) | PR(ms) | Difference(ms) | Percent
-- | -- | -- | -- | --
q17 | 29462.66667 | 7450 | -22012.66667 | 295.47%

Closes #39170 from beliefer/SPARK-41674.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala', 'sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala']","Runtime filter only supports two conditions: first, when the Join itself is a Shuffle Join; Second, when the join is a broadcast join and there is a Shuffle Join under one end of the join. This restricts optimization in complicated multi-level join scenarios.
"
52a1f8c0003d1713d661d197b39737f545723309,1622117559,"[SPARK-33428][SQL] Match the behavior of conv function to MySQL's

### What changes were proposed in this pull request?
Spark conv function is from MySQL and it's better to follow the MySQL behavior. MySQL returns the max unsigned long if the input string is too big, and Spark should follow it.

However, seems Spark has different behavior in two cases:

MySQL allows leading spaces but Spark does not.
If the input string is way too long, Spark fails with ArrayIndexOutOfBoundException

This patch now help conv follow behavior in those two cases
conv allows leading spaces
conv will return the max unsigned long when the input string is way too long

### Why are the changes needed?
fixing it to match the behavior of conv function to the (almost) only one reference of another DBMS, MySQL

### Does this PR introduce _any_ user-facing change?
Yes, as pointed out above

### How was this patch tested?
Add test

Closes #32684 from dgd-contributor/SPARK-33428.

Authored-by: dgd-contributor <dgd_contributor@viettel.com.vn>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberConverter.scala', 'sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala']",The conv function doesn't match MySQL's behavior: it fails with an ArrayIndexOutOfBoundException for exceedingly long input strings and disallows leading spaces.
fc12165f48b2e1dfe04116ddaa6ff6e8650a18fb,1583458400,"[SPARK-31036][SQL] Use stringArgs in Expression.toString to respect hidden parameters

### What changes were proposed in this pull request?

This PR proposes to respect hidden parameters by using `stringArgs`  in `Expression.toString `. By this, we can show the strings properly in some cases such as `NonSQLExpression`.

### Why are the changes needed?

To respect ""hidden"" arguments in the string representation.

### Does this PR introduce any user-facing change?

Yes, for example, on the top of https://github.com/apache/spark/pull/27657,

```scala
val identify = udf((input: Seq[Int]) => input)
spark.range(10).select(identify(array(""id""))).show()
```

shows hidden parameter `useStringTypeWhenEmpty`.

```
+---------------------+
|UDF(array(id, false))|
+---------------------+
|                  [0]|
|                  [1]|
...
```

whereas:

```scala
spark.range(10).select(array(""id"")).show()
```

```
+---------+
|array(id)|
+---------+
|      [0]|
|      [1]|
...
```

### How was this patch tested?

Manually tested as below:

```scala
val identify = udf((input: Boolean) => input)
spark.range(10).select(identify(exists(array(col(""id"")), _ % 2 === 0))).show()
```

Before:

```
+-------------------------------------------------------------------------------------+
|UDF(exists(array(id), lambdafunction(((lambda 'x % 2) = 0), lambda 'x, false), true))|
+-------------------------------------------------------------------------------------+
|                                                                                 true|
|                                                                                false|
|                                                                                 true|
...
```

After:

```
+-------------------------------------------------------------------------------+
|UDF(exists(array(id), lambdafunction(((lambda 'x % 2) = 0), lambda 'x, false)))|
+-------------------------------------------------------------------------------+
|                                                                           true|
|                                                                          false|
|                                                                           true|
...
```

Closes #27788 from HyukjinKwon/arguments-str-repr.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala']","Hidden parameters in `Expression.toString` are not being respected in `NonSQLExpression`, showing unusual behaviors in string presentations."
e3989ca35d2644e6ed34689ec67134d7e8ed0e1a,1667997929,"[SPARK-41071][BUILD] Remove `MaxMetaspaceSize` option from `make-distribution.sh` to make it run successfully

### What changes were proposed in this pull request?
Run

```
dev/make-distribution.sh --tgz -Phadoop-3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive
```

on the master branch fails as follows:

```
[ERROR] ## Exception when compiling 19 sources to /spark-source/connector/avro/target/scala-2.12/classes
java.lang.OutOfMemoryError: Metaspace
java.lang.ClassLoader.defineClass1(Native Method)
java.lang.ClassLoader.defineClass(ClassLoader.java:757)
java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
java.net.URLClassLoader.defineClass(URLClassLoader.java:473)
java.net.URLClassLoader.access$100(URLClassLoader.java:74)
java.net.URLClassLoader$1.run(URLClassLoader.java:369)
java.net.URLClassLoader$1.run(URLClassLoader.java:363)
java.security.AccessController.doPrivileged(Native Method)
java.net.URLClassLoader.findClass(URLClassLoader.java:362)
java.lang.ClassLoader.loadClass(ClassLoader.java:419)
scala_maven.ScalaCompilerLoader.loadClass(ScalaCompilerLoader.java:44)
java.lang.ClassLoader.loadClass(ClassLoader.java:352)
scala.collection.immutable.Set$Set2.$plus(Set.scala:170)
scala.collection.immutable.Set$Set2.$plus(Set.scala:164)
scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:28)
scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:24)
scala.collection.TraversableLike$grouper$1$.apply(TraversableLike.scala:467)
scala.collection.TraversableLike$grouper$1$.apply(TraversableLike.scala:455)
scala.collection.mutable.HashMap$$anon$1.$anonfun$foreach$2(HashMap.scala:153)
scala.collection.mutable.HashMap$$anon$1$$Lambda$68804/294594059.apply(Unknown Source)
scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:153)
scala.collection.TraversableLike.groupBy(TraversableLike.scala:524)
scala.collection.TraversableLike.groupBy$(TraversableLike.scala:454)
scala.collection.AbstractTraversable.groupBy(Traversable.scala:108)
scala.tools.nsc.settings.Warnings.$init$(Warnings.scala:91)
scala.tools.nsc.settings.MutableSettings.<init>(MutableSettings.scala:28)
scala.tools.nsc.Settings.<init>(Settings.scala:19)
scala.tools.nsc.Settings.<init>(Settings.scala:20)
xsbt.CachedCompiler0.<init>(CompilerBridge.scala:79)
```

So this pr remove `MaxMetaspaceSize` option  from `make-distribution.sh` to make `MaxMetaspaceSize` can use more memory resources and make `make-distribution.sh` run successfully.

### Why are the changes needed?
Make `make-distribution.sh`  run successfully.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Manual test:

```
dev/make-distribution.sh --tgz -Phadoop-3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive
```

- Before: java.lang.OutOfMemoryError: Metaspace
- After: successfully

Closes #38577 from LuciferYang/SPARK-41071.

Authored-by: yangjie01 <yangjie01@baidu.com>
Signed-off-by: Yuming Wang <yumwang@ebay.com>
",['dev/make-distribution.sh'],Running `make-distribution.sh` with multiple project flags causes `java.lang.OutOfMemoryError: Metaspace` failure due to restricted memory resource usage.
a848d552ef6b5d0d3bb3b2da903478437a8b10aa,1499135708,"[SPARK-21264][PYTHON] Call cross join path in join without 'on' and with 'how'

## What changes were proposed in this pull request?

Currently, it throws a NPE when missing columns but join type is speicified in join at PySpark as below:

```python
spark.conf.set(""spark.sql.crossJoin.enabled"", ""false"")
spark.range(1).join(spark.range(1), how=""inner"").show()
```

```
Traceback (most recent call last):
...
py4j.protocol.Py4JJavaError: An error occurred while calling o66.join.
: java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.join(Dataset.scala:931)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...
```

```python
spark.conf.set(""spark.sql.crossJoin.enabled"", ""true"")
spark.range(1).join(spark.range(1), how=""inner"").show()
```

```
...
py4j.protocol.Py4JJavaError: An error occurred while calling o84.join.
: java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.join(Dataset.scala:931)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...
```

This PR suggests to follow Scala's one as below:

```scala
scala> spark.conf.set(""spark.sql.crossJoin.enabled"", ""false"")

scala> spark.range(1).join(spark.range(1), Seq.empty[String], ""inner"").show()
```

```
org.apache.spark.sql.AnalysisException: Detected cartesian product for INNER join between logical plans
Range (0, 1, step=1, splits=Some(8))
and
Range (0, 1, step=1, splits=Some(8))
Join condition is missing or trivial.
Use the CROSS JOIN syntax to allow cartesian products between these relations.;
...
```

```scala
scala> spark.conf.set(""spark.sql.crossJoin.enabled"", ""true"")

scala> spark.range(1).join(spark.range(1), Seq.empty[String], ""inner"").show()
```
```
+---+---+
| id| id|
+---+---+
|  0|  0|
+---+---+
```

**After**

```python
spark.conf.set(""spark.sql.crossJoin.enabled"", ""false"")
spark.range(1).join(spark.range(1), how=""inner"").show()
```

```
Traceback (most recent call last):
...
pyspark.sql.utils.AnalysisException: u'Detected cartesian product for INNER join between logical plans\nRange (0, 1, step=1, splits=Some(8))\nand\nRange (0, 1, step=1, splits=Some(8))\nJoin condition is missing or trivial.\nUse the CROSS JOIN syntax to allow cartesian products between these relations.;'
```

```python
spark.conf.set(""spark.sql.crossJoin.enabled"", ""true"")
spark.range(1).join(spark.range(1), how=""inner"").show()
```
```
+---+---+
| id| id|
+---+---+
|  0|  0|
+---+---+
```

## How was this patch tested?

Added tests in `python/pyspark/sql/tests.py`.

Author: hyukjinkwon <gurwls223@gmail.com>

Closes #18484 from HyukjinKwon/SPARK-21264.
","['python/pyspark/sql/dataframe.py', 'python/pyspark/sql/tests.py']","When performing an inner join without an 'on' clause in PySpark, a NullPointerException error occurs, regardless of the ""spark.sql.crossJoin.enabled"" setting."
c65f9b2bc35c2926bb3658f65fe4f8a0b8e9fe4a,1553091172,"[SPARK-26839][SQL] Work around classloader changes in Java 9 for Hive isolation

Note, this doesn't really resolve the JIRA, but makes the changes we can make so far that would be required to solve it.

## What changes were proposed in this pull request?

Java 9+ changed how ClassLoaders work. The two most salient points:
- The boot classloader no longer 'sees' the platform classes. A new 'platform classloader' does and should be the parent of new ClassLoaders
- The system classloader is no longer a URLClassLoader, so we can't get the URLs of JARs in its classpath

## How was this patch tested?

We'll see whether Java 8 tests still pass here. Java 11 tests do not fully pass at this point; more notes below. This does make progress on the failures though.

(NB: to test with Java 11, you need to build with Java 8 first, setting JAVA_HOME and java's executable correctly, then switch both to Java 11 for testing.)

Closes #24057 from srowen/SPARK-26839.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>
","['sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala']","Issues with Hive isolation due to classloader changes in Java 9 and above such as the boot classloader no longer seeing platform classes, and the system classloader no longer being a URLClassLoader."
5d870ef0bc70527fd1bc99a4ad17e4941c923351,1577951085,"[SPARK-26560][SQL] Spark should be able to run Hive UDF using jar regardless of current thread context classloader

### What changes were proposed in this pull request?

This patch is based on #23921 but revised to be simpler, as well as adds UT to test the behavior.
(This patch contains the commit from #23921 to retain credit.)

Spark loads new JARs for `ADD JAR` and `CREATE FUNCTION ... USING JAR` into jar classloader in shared state, and changes current thread's context classloader to jar classloader as many parts of remaining codes rely on current thread's context classloader.

This would work if the further queries will run in same thread and there's no change on context classloader for the thread, but once the context classloader of current thread is switched back by various reason, Spark fails to create instance of class for the function.

This bug mostly affects spark-shell, as spark-shell will roll back current thread's context classloader at every prompt. But it may also affects the case of job-server, where the queries may be running in multiple threads.

This patch fixes the issue via switching the context classloader to the classloader which loads the class. Hopefully FunctionBuilder created by `makeFunctionBuilder` has the information of Class as a part of closure, hence the Class itself can be provided regardless of current thread's context classloader.

### Why are the changes needed?

Without this patch, end users cannot execute Hive UDF using JAR twice in spark-shell.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

New UT.

Closes #27025 from HeartSaVioR/SPARK-26560-revised.

Lead-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Co-authored-by: nivo091 <nivedeeta.singh@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala']","The current thread's context classloader inconveniences the functioning of Spark, in particular its inability to perform Hive UDF using JAR more than once in spark-shell. The issue also extends to cases running on job-server with multiple threads."
3b140885410362fced9d98fca61d6a357de604af,1564531824,"[SPARK-26175][PYTHON] Redirect the standard input of the forked child to devnull in daemon

## What changes were proposed in this pull request?

PySpark worker daemon reads from stdin the worker PIDs to kill. https://github.com/apache/spark/blob/1bb60ab8392adf8b896cc04fb1d060620cf09d8a/python/pyspark/daemon.py#L127

However, the worker process is a forked process from the worker daemon process and we didn't close stdin on the child after fork. This means the child and user program can read stdin as well, which blocks daemon from receiving the PID to kill. This can cause issues because the task reaper might detect the task was not terminated and eventually kill the JVM.

This PR fix this by redirecting the standard input of the forked child to devnull.

## How was this patch tested?

Manually test.

In `pyspark`, run:
```
import subprocess
def task(_):
  subprocess.check_output([""cat""])

sc.parallelize(range(1), 1).mapPartitions(task).count()
```

Before:
The job will get stuck and press Ctrl+C to exit the job but the python worker process do not exit.
After:
The job finish correctly. The ""cat"" print nothing (because the dummay stdin is ""/dev/null"").
The python worker process exit normally.

Please review https://spark.apache.org/contributing.html before opening a pull request.

Closes #25138 from WeichenXu123/SPARK-26175.

Authored-by: WeichenXu <weichen.xu@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['python/pyspark/daemon.py', 'python/pyspark/sql/tests/test_udf.py']","The PySpark worker daemon's inability to close standard input after forking causes blocking issues, preventing reception of PIDs to kill and leading to potential JVM termination by task reaper."
1295e8876cf722583039d7ff9c81c78735558f3a,1624525478,"[SPARK-35786][SQL] Add a new operator to distingush if AQE can optimize safely

### What changes were proposed in this pull request?

* Add a new repartition operator `RebalanceRepartition`.
* Support a new hint `REBALANCE`

After this patch, user can run this query:
```sql
SELECT /*+ REBALANCE(c) */ * FROM t
```

### Why are the changes needed?

Add a new hint to distingush if we can optimize it safely.

This new hint can let AQE optimize with `CustomShuffleReaderExec` safely. Currently, AQE can only coalesce shuffle partitions but can not expand shuffle partitions due to the semantics of output partitioning.
Let's say we have a query:
```sql
SELECT /*+ REPARTITION(col) */ * FROM t
```
AQE can not expand the shuffle partitions even if `col` is skewed because expanding shuffle partitions will break the hashed output paritioning of `RepartitionByExpression`. But if the query is use`REPARTITION_BY_AQE`, AQE can optimize it without considering the semantics of output partitioning.

### Does this PR introduce _any_ user-facing change?

Yes, a new hint.

### How was this patch tested?

Add test.

Closes #32932 from ulysses-you/SPARK-35786.

Authored-by: ulysses-you <ulyssesyou18@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveHints.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveHintsSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala']","Adaptive Query Execution (AQE) cannot optimize safely when expanding shuffle partitions, leading to skewed column. This creates unsafe optimization issues with CustomShuffleReaderExec."
4e9c1b8ba0b7ef44d62475835f424ec705e6f21e,1626944224,"[SPARK-34806][SQL] Add Observation helper for Dataset.observe

### What changes were proposed in this pull request?
This pull request introduces a helper class that simplifies usage of `Dataset.observe()` for batch datasets:

    val observation = Observation(""name"")
    val observed = ds.observe(observation, max($""id"").as(""max_id""))
    observed.count()
    val metrics = observation.get

### Why are the changes needed?
Currently, users are required to implement the `QueryExecutionListener` interface to retrieve the metrics, as well as apply some knowledge on threading and locking to pull the metrics over to the main thread. With the helper class, metrics can be retrieved from batch dataset processing with three lines of code (the action on the observed dataset does not count as a line of code here).

### Does this PR introduce _any_ user-facing change?
Yes, one new class and one `Dataset`` method.

### How was this patch tested?
Adds a unit test to `DataFrameSuite`, similar to `""get observable metrics by callback""` in `DataFrameCallbackSuite`.

Closes #33422 from EnricoMi/branch-observation.

Authored-by: Enrico Minack <github@enrico.minack.dev>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala', 'sql/core/src/main/scala/org/apache/spark/sql/Observation.scala', 'sql/core/src/test/java/test/org/apache/spark/sql/JavaDataFrameSuite.java', 'sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala']","Retrieving metrics from batch dataset processing using Dataset.observe() is overly complicated, requiring users to implement the QueryExecutionListener interface and manually handle threading and locking details."
a88934227523334550e451e437ce013772001079,1685556284,"[SPARK-43867][SQL] Improve suggested candidates for unresolved attribute

### What changes were proposed in this pull request?
In the PR, I propose to change the approach of stripping the common part of candidate qualifiers in `StringUtils.orderSuggestedIdentifiersBySimilarity`:
1. If all candidates have the same qualifier including namespace and table name, drop it. It should be dropped if the base string (unresolved attribute) doesn't include a namespace and table name. For example:
    - `[ns1.table1.col1, ns1.table1.col2] -> [col1, col2]` for unresolved attribute `col0`
    - `[ns1.table1.col1, ns1.table1.col2] -> [table1.col1, table1.col2]` for unresolved attribute `table1.col0`
2. If all candidates belong to the same namespace, just drop it. It should be dropped for any non-fully qualified unresolved attribute. For example:
    - `[ns1.table1.col1, ns1.table2.col2] -> [table1.col1, table2.col2]` for unresolved attribute `col0` or `table0.col0`
    - `[ns1.table1.col1, ns1.table1.col2] -> [ns1.table1.col1, ns1.table1.col2]` for unresolved attribute `ns0.table0.col0`
4. Otherwise take the suggested candidates AS IS.
5. Sort the candidate list using the levenshtein distance.

### Why are the changes needed?
This should improve user experience with Spark SQL by simplifying the error message about an unresolved attribute.

### Does this PR introduce _any_ user-facing change?
Yes, it changes the error message.

### How was this patch tested?
By running the existing test suites:
```
$ build/sbt ""test:testOnly *AnalysisErrorSuite""
$ build/sbt ""test:testOnly *QueryCompilationErrorsSuite""
$ PYSPARK_PYTHON=python3 build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite""
$ build/sbt ""test:testOnly *DatasetUnpivotSuite""
$ build/sbt ""test:testOnly *DatasetSuite""

```

Closes #41368 from MaxGekk/fix-suggested-column-list.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/StringUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/StringUtilsSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/DatasetUnpivotSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala']","Unresolved attribute error messages are complex and not clear, failing to simplify the cause of the issue and not effectively guiding users towards potential solutions."
8e7fc04637bbb8d2fdc2c758746e0eaf496c4d92,1594213125,"[SPARK-32024][WEBUI] Update ApplicationStoreInfo.size during HistoryServerDiskManager initializing

### What changes were proposed in this pull request?

Update ApplicationStoreInfo.size to real size during HistoryServerDiskManager initializing.

### Why are the changes needed?

This PR is for fixing bug [32024](https://issues.apache.org/jira/browse/SPARK-32024). We found after history server restart, below error would randomly happen: ""java.lang.IllegalStateException: Disk usage tracker went negative (now = -***, delta = -***)"" from `HistoryServerDiskManager`.
![Capture](https://user-images.githubusercontent.com/10524738/85034468-fda4ae80-b136-11ea-9011-f0c3e6508002.JPG)

**Cause**: Reading data from level db would trigger table file compaction, which may also trigger size of level db directory changes.  This size change may not be recorded in LevelDB (`ApplicationStoreInfo` in `listing`). When service restarts, `currentUsage` is calculated from real directory size, but `ApplicationStoreInfo` are loaded from leveldb, then `currentUsage` may be less then sum of `ApplicationStoreInfo.size`. In `makeRoom()` function, `ApplicationStoreInfo.size` is used to update usage. Then `currentUsage` becomes negative after several round of `release()` and `lease()` (`makeRoom()`).
**Reproduce**: we can reproduce this issue in dev environment by reducing config value of ""spark.history.retainedApplications"" and ""spark.history.store.maxDiskUsage"" to some small values. Here are steps: 1. start history server, load some applications and access some pages (maybe ""stages"" page to trigger leveldb compaction). 2. restart HS, and refresh pages.
I also added an UT to simulate this case in `HistoryServerDiskManagerSuite`.
**Benefit**: this change would help improve history server reliability.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Add unit test and manually tested it.

Closes #28859 from zhli1142015/update-ApplicationStoreInfo.size-during-disk-manager-initialize.

Authored-by: Zhen Li <zhli@microsoft.com>
Signed-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
","['core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala', 'core/src/test/scala/org/apache/spark/deploy/history/HistoryServerDiskManagerSuite.scala']","After the history server restarts, the HistoryServerDiskManager sometimes throws a ""java.lang.IllegalStateException: Disk usage tracker went negative"" error due to discrepancies between actual directory size and loaded ApplicationStoreInfo sizes."
8c26c014f11b1b9d7d6c3b315fbb633c2bb2ca73,1689914761,"[SPARK-44504][SS] Unload provider thereby forcing DB instance close and releasing resources on maintenance task error

### What changes were proposed in this pull request?
Unload provider thereby forcing DB instance close and releasing resources on maintenance task error

### Why are the changes needed?
If we don't do the close, the DB instance and corresponding resources (memory, file descriptors etc) are always left open and the pointer to these objects is lost since loadedProviders is cleared.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing unit tests

```
), ForkJoinPool.commonPool-worker-5 (daemon=true), ForkJoinPool.commonPool-worker-17 (daemon=true), shuffle-boss-6-1 (daemon=true), ForkJoinPool.commonPool-worker-3 (daemon=true), ForkJoinPool.commonPool-worker-31 (daemon=true), ForkJoinPool.commonPool-worker-23 (daemon=true), state-store-maintenance-task (daemon=true), ForkJoinPool.commonPool-worker-9 (daemon=true) =====
[info] Run completed in 2 minutes, 49 seconds.
[info] Total number of tests run: 32
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 32, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
```

Closes #42098 from anishshri-db/task/SPARK-44504.

Authored-by: Anish Shrigondekar <anish.shrigondekar@databricks.com>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala']","Failure in maintenance tasks does not trigger closing of DB instance, leading to resource leaks as memory and file descriptors are left open."
6a79539f501ae1bef45feef6615ff60f8947bc54,1645761448,"[SPARK-38298][SQL][TESTS] Fix DataExpressionSuite, NullExpressionsSuite, StringExpressionsSuite, complexTypesSuite, CastSuite under ANSI mode

### What changes were proposed in this pull request?
The PR fixes the following tests under ANSI mode:
* DataExpressionSuite,
* NullExpressionsSuite,
* StringExpressionsSuite,
* ComplexTypesSuite,
* CastSuite

Most of them should only work with ANSI off. The fix wrap the corresponding code with
```scala
withSQLConf(SQLConf.ANSI_ENABLED.key -> ""false"") {
  // code only works under ANSI off
}
```

The `CastSuite` intentionally tests the case of ANSI off. The fix explicitly set ANSI off in `beforeAll` and reset it in `afterAll`.

### Why are the changes needed?
To set up a new GA job to run tests with ANSI mode before 3.3.0 release.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Test locally with both ANSI on and off, both passed.

Closes #35618 from anchovYu/ansi-tests-multiple.

Authored-by: Xinyi Yu <xinyi.yu@databricks.com>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/NullExpressionsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala']","Several test suites including DataExpressionSuite, NullExpressionsSuite, StringExpressionsSuite, ComplexTypesSuite, and CastSuite are failing under ANSI mode."
b17a0e6931cac98cc839c047b1b5d4ea6d052009,1619070856,"[SPARK-34674][CORE][K8S] Close SparkContext after the Main method has finished

### What changes were proposed in this pull request?
Close SparkContext after the Main method has finished, to allow SparkApplication on K8S to complete.
This is fixed version of [merged and reverted PR](https://github.com/apache/spark/pull/32081).

### Why are the changes needed?
if I don't call the method sparkContext.stop() explicitly, then a Spark driver process doesn't terminate even after its Main method has been completed. This behaviour is different from spark on yarn, where the manual sparkContext stopping is not required. It looks like, the problem is in using non-daemon threads, which prevent the driver jvm process from terminating.
So I have inserted code that closes sparkContext automatically.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Manually on the production AWS EKS environment in my company.

Closes #32283 from kotlovs/close-spark-context-on-exit-2.

Authored-by: skotlov <skotlov@joom.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
",['core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala'],"Spark driver process does not terminate on K8S after Main method completion due to non-daemon threads, different from behavior on yarn where manual sparkContext stopping isn't required."
ce53b7199d15c8ed26aac2e7cecd2bff321a4caa,1624376754,"[SPARK-35854][SQL] Improve the error message of to_timestamp_ntz with invalid format pattern

### What changes were proposed in this pull request?

When SQL function `to_timestamp_ntz` has invalid format pattern input, throw a runtime exception with hints for the valid patterns, instead of throwing an upgrade exception with suggestions to use legacy formatters.

### Why are the changes needed?

As discussed in https://github.com/apache/spark/pull/32995/files#r655148980, there is an error message saying
""You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'yyyy-MM-dd GGGGG' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0""

This is not true for function to_timestamp_ntz, which only uses the Iso8601TimestampFormatter and added since Spark 3.2. We should improve it.

### Does this PR introduce _any_ user-facing change?

No, the new SQL function is not released yet.

### How was this patch tested?

Unit test

Closes #33019 from gengliangwang/improveError.

Authored-by: Gengliang Wang <gengliang@apache.org>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeFormatterHelper.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala']","The SQL function `to_timestamp_ntz` throws an upgrade exception with inaccurate suggestions when an invalid format pattern input is provided, causing confusion in diagnosing the problem."
42f118ad1e7ef837c18ce73b22adaf486d238994,1646163124,"[SPARK-33206][CORE] Fix shuffle index cache weight calculation for small index files

### What changes were proposed in this pull request?

Increasing the shuffle index weight with a constant number to avoid underestimating retained memory size caused by the bookkeeping objects: the `java.io.File` (depending on the path ~ 960 bytes) object and the `ShuffleIndexInformation` object (~180 bytes).

### Why are the changes needed?

Underestimating cache entry size easily can cause OOM in the Yarn NodeManager.
In the following analyses of a prod issue (HPROF file) we can see the leak suspect Guava's `LocalCache$Segment` objects:

<img width=""943"" alt=""Screenshot 2022-02-17 at 18 55 40"" src=""https://user-images.githubusercontent.com/2017933/154541995-44014212-2046-41d6-ba7f-99369ca7d739.png"">

Going further we can see a `ShuffleIndexInformation` for a small index file (16 bytes) but the retained heap memory is 1192 bytes:

<img width=""1351"" alt=""image"" src=""https://user-images.githubusercontent.com/2017933/154645212-e0318d0f-cefa-4ae3-8a3b-97d2b506757d.png"">

Finally we can see this is very common within this heap dump (using MAT's Object Query Language):

<img width=""1418"" alt=""image"" src=""https://user-images.githubusercontent.com/2017933/154547678-44c8af34-1765-4e14-b71a-dc03d1a304aa.png"">

I have even exported the data to a CSV and done some calculations with `awk`:

```
$ tail -n+2 export.csv | awk -F, 'BEGIN { numUnderEstimated=0; } { sumOldSize += $1; corrected=$1 + 1176; sumCorrectedSize += corrected; sumRetainedMem += $2; if (corrected < $2) numUnderEstimated+=1; } END { print ""sum old size: "" sumOldSize / 1024 / 1024   "" MB, sum corrected size: "" sumCorrectedSize / 1024 / 1024 "" MB, sum retained memory:"" sumRetainedMem / 1024 / 1024  "" MB, num under estimated: "" numUnderEstimated }'
```

It gives the followings:
```
sum old size: 76.8785 MB, sum corrected size: 1066.93 MB, sum retained memory:1064.47 MB, num under estimated: 0
```

So using the old calculation we were at 7.6.8 MB way under the default cache limit (100 MB).
Using the correction (applying 1176 as increment to the size) we are at 1066.93 MB (~1GB) which is close to the real retained sum heap: 1064.47 MB (~1GB) and there is no entry which was underestimated.

But we can go further and get rid of `java.io.File` completely and store the `ShuffleIndexInformation` for the file path.
This way not only the cache size estimate is improved but the its size is decreased as well.
Here the path size is not counted into the cache size as that string is interned.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

With the calculations above.

Closes #35559 from attilapiros/SPARK-33206.

Authored-by: attilapiros <piros.attila.zsolt@gmail.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExecutorDiskUtils.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolver.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java', 'common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java', 'common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/RemoteBlockPushResolverSuite.java', 'common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ShuffleIndexInformationSuite.java', 'common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/TestShuffleDataContext.java', 'core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala', 'core/src/main/scala/org/apache/spark/storage/BlockManager.scala', 'core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala', 'core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala']",Underestimation of cache entry size in shuffle index due to small index files causing `OutOfMemory` errors in the Yarn NodeManager.
c75a82794fc6a0f35697f8e1258562d43e860f68,1597995431,"[SPARK-32667][SQL] Script transform 'default-serde' mode should pad null value to filling column

### What changes were proposed in this pull request?
Hive no serde mode when  column less then output specified column, it will pad null value to it, spark should do this also.
```
hive> SELECT TRANSFORM(a, b)
    >   ROW FORMAT DELIMITED
    >   FIELDS TERMINATED BY '|'
    >   LINES TERMINATED BY '\n'
    >   NULL DEFINED AS 'NULL'
    > USING 'cat' as (a string, b string, c string, d string)
    >   ROW FORMAT DELIMITED
    >   FIELDS TERMINATED BY '|'
    >   LINES TERMINATED BY '\n'
    >   NULL DEFINED AS 'NULL'
    > FROM (
    > select 1 as a, 2 as b
    > ) tmp ;
OK
1	2	NULL	NULL
Time taken: 24.626 seconds, Fetched: 1 row(s)
```

### Why are the changes needed?
Keep save behavior with hive data.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added UT

Closes #29500 from AngersZhuuuu/SPARK-32667.

Authored-by: angerszhu <angers.zhu@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/BaseScriptTransformationSuite.scala']","In 'default-serde' mode of Script Transform, when the number of columns is less than the specified output columns, Spark does not pad null values to match the column count, unlike Hive."
aa3de4077302fe7e0b23b01a338c7feab0e5974e,1623301731,"[SPARK-35679][SQL] instantToMicros overflow

### Why are the changes needed?
With Long.minValue cast to an instant, secs will be floored in function microsToInstant and cause overflow when multiply with Micros_per_second

```
def microsToInstant(micros: Long): Instant = {
  val secs = Math.floorDiv(micros, MICROS_PER_SECOND)
  // Unfolded Math.floorMod(us, MICROS_PER_SECOND) to reuse the result of
  // the above calculation of `secs` via `floorDiv`.
  val mos = micros - secs * MICROS_PER_SECOND  <- it will overflow here
  Instant.ofEpochSecond(secs, mos * NANOS_PER_MICROS)
}
```

But the overflow is acceptable because it won't produce any change to the result

However, when convert the instant back to micro value, it will raise Overflow Error

```
def instantToMicros(instant: Instant): Long = {
  val us = Math.multiplyExact(instant.getEpochSecond, MICROS_PER_SECOND) <- It overflow here
  val result = Math.addExact(us, NANOSECONDS.toMicros(instant.getNano))
  result
}
```

Code to reproduce this error
```
instantToMicros(microToInstant(Long.MinValue))
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Test added

Closes #32839 from dgd-contributor/SPARK-35679_instantToMicro.

Authored-by: dgd-contributor <dgd_contributor@viettel.com.vn>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeUtilsSuite.scala']",Converting Long.minValue to an instant and back to micro value causes an overflow error in the instantToMicros method.
cc927edf770daa237993e076dc29b4793f4e2a84,1653400160,"[SPARK-39248][SQL] Improve divide performance for decimal type

### What changes were proposed in this pull request?

Switch decimal type divide from
```
toJavaBigDecimal.divide(that.toJavaBigDecimal, MATH_CONTEXT)
```
to
```
toJavaBigDecimal.divide(that.toJavaBigDecimal, DecimalType.MAX_SCALE, MATH_CONTEXT.getRoundingMode)
```

The difference is that [`preferredScale != scale`](https://github.com/openjdk/jdk8u-dev/blob/jdk8u342-b01/jdk/src/share/classes/java/math/BigDecimal.java#L4288) is false if using the new API.

This is the stack trace if using the old API:
```
java.math.MutableBigInteger.divideKnuth(MutableBigInteger.java:1203)
java.math.MutableBigInteger.divideKnuth(MutableBigInteger.java:1163)
java.math.BigInteger.divideAndRemainderKnuth(BigInteger.java:2235)
java.math.BigInteger.divideAndRemainder(BigInteger.java:2223)
java.math.BigDecimal.createAndStripZerosToMatchScale(BigDecimal.java:4404)
java.math.BigDecimal.divideAndRound(BigDecimal.java:4294)
java.math.BigDecimal.divide(BigDecimal.java:4660)
java.math.BigDecimal.divide(BigDecimal.java:1753)
...
```

### Why are the changes needed?

Improve divide performance for decimal type.

Benchmark code:
```scala
import org.apache.spark.benchmark.Benchmark

val valuesPerIteration = 2880404L
val dir = ""/tmp/spark/benchmark""
spark.range(valuesPerIteration).selectExpr(""CAST(id AS DECIMAL(9, 2)) AS d"").write.mode(""Overwrite"").parquet(dir)

val benchmark = new Benchmark(""Benchmark decimal"", valuesPerIteration, minNumIters = 5)
benchmark.addCase(""d * 2 > 0"") { _ =>
  spark.read.parquet(dir).where(""d * 2 > 0"").write.format(""noop"").mode(""Overwrite"").save()
}

benchmark.addCase(""d / 2 > 0"") { _ =>
  spark.read.parquet(dir).where(""d / 2 > 0"").write.format(""noop"").mode(""Overwrite"").save()
}
benchmark.run()
```

Before this PR:
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU  2.40GHz
Benchmark decimal:                        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
d * 2 > 0                                           480            585         141          6.0         166.7       1.0X
d / 2 > 0                                          4689           4920         243          0.6        1627.9       0.1X
```

After this PR:
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU  2.40GHz
Benchmark decimal:                        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
d * 2 > 0                                           529            580          35          5.4         183.6       1.0X
d / 2 > 0                                           811            916          80          3.6         281.4       0.7X
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #36628 from wangyum/SPARK-39248.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala', 'sql/core/src/test/resources/sql-tests/inputs/ansi/decimalArithmeticOperations.sql', 'sql/core/src/test/resources/sql-tests/inputs/decimalArithmeticOperations.sql']",Decimal type division operations in Spark SQL exhibit poor performance due to unnecessary scale matching and zero stripping operations.
90e56564cc3948a729aefe46da1d7cb66836586d,1654224701,"[SPARK-29260][SQL] Support `ALTER DATABASE SET LOCATION` if HMS supports

### What changes were proposed in this pull request?

Currently for `ALTER DATABASE SET LOCATION` command, Spark will throw exception when Hive version (e.g., specified via `spark.sql.hive.metastore.version`) is not 3.0/3.1. This PR removes the check so that the command works as long as the Hive version used by the Hive metastore (which could be different from the version used by Spark) supports the alter database location feature added via [HIVE-8472](https://issues.apache.org/jira/browse/HIVE-8472). If it does not support it, the same exception will still be thrown from Spark side.

### Why are the changes needed?

For the command `ALTER DATABASE SET LOCATION` command, Spark currently throws exception like the following:
```
AnalysisException: Hive 2.3.9 does not support altering database location
```

This is not accurate since it only considers the client version, while the feature support is on the Hive metastore server side. Therefore, the command should succeed if Spark is using Hive 2.3 while the remote Hive megastore is using Hive 3.1. On the other hand, the command will not succeed if Spark is using Hive 3.1 (thus no exception) but the remote Hive metastore is using 2.3.

### Does this PR introduce _any_ user-facing change?

Yes, previously Spark users using Hive client with version other than 3.0/3.1 won't be able to run `ALTER DATABASE SET LOCATION` command against Hive metastore 3.x. After this PR it should work.

### How was this patch tested?

Modified the existing test case.

Closes #36750 from sunchao/SPARK-29260.

Lead-authored-by: Chao Sun <sunchao@apple.com>
Co-authored-by: Chao Sun <sunchao@apache.org>
Signed-off-by: Yuming Wang <yumwang@ebay.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala', 'sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala']","`ALTER DATABASE SET LOCATION` command in Spark is inaccurately dependent on Hive client version, instead of metastore version resulting in incorrect exceptions thrown for incorrect Hive versions."
55ba63c257b6617ec3d2aca5bc1d0989d4f29de8,1686885518,"[SPARK-44040][SQL] Fix compute stats when AggregateExec node above QueryStageExec

### What changes were proposed in this pull request?

This PR fixes compute stats when `BaseAggregateExec` nodes above `QueryStageExec`.

For aggregation, when the number of shuffle output rows is 0, the final result may be 1. For example:
```sql
SELECT count(*) FROM tbl WHERE false;
```

The number of shuffle output rows is 0, and the final result is 1. Please see the [UI](https://github.com/apache/spark/assets/5399861/9d9ad999-b3a9-433e-9caf-c0b931423891).

### Why are the changes needed?

Fix data issue. `OptimizeOneRowPlan` will use stats to remove `Aggregate`:
```
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.OptimizeOneRowPlan ===
!Aggregate [id#5L], [id#5L]                                                                                   Project [id#5L]
 +- Union false, false                                                                                        +- Union false, false
    :- LogicalQueryStage Aggregate [sum(id#0L) AS id#5L], HashAggregate(keys=[], functions=[sum(id#0L)])         :- LogicalQueryStage Aggregate [sum(id#0L) AS id#5L], HashAggregate(keys=[], functions=[sum(id#0L)])
    +- LogicalQueryStage Aggregate [sum(id#18L) AS id#12L], HashAggregate(keys=[], functions=[sum(id#18L)])      +- LogicalQueryStage Aggregate [sum(id#18L) AS id#12L], HashAggregate(keys=[], functions=[sum(id#18L)])
```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #41576 from wangyum/SPARK-44040.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Yuming Wang <yumwang@ebay.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala', 'sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala']","When `BaseAggregateExec` node is above `QueryStageExec`, compute stats fails. This data issue leads to potential removal of Aggregate by `OptimizeOneRowPlan`, especially when the count of shuffle output rows is zero."
1d562904e4e75aec3ea8d4999ede0183fda326c7,1692061842,"[SPARK-44795][CONNECT] CodeGenerator Cache should be classloader specific

### What changes were proposed in this pull request?
When you currently use a REPL generated class in a UDF you can get an error saying that that class is not equal to that class. This error is thrown in a code generated class. The problem is that the classes have been loaded by different classloaders. We cache generated code and use the textual code as the string. The problem with this is that in Spark Connect users are free in supplying user classes that can have arbitrary names, a name can point to an entirely different class, or it can point to the same class (in bytes) but loaded by a different classloader. We need to add the classloader to the key in the code gen cache to make this safe.

There are roughly two ways how this problem can arise:
1. Two sessions use the same class names. This is particularly easy when you use the REPL because this  always generates the same names.
2. You run in single process mode. In this case wholestage codegen will test compile the class using a different classloader then the 'executor', while sharing the same code generator cache.

### Why are the changes needed?
We want to be able to use REPL (and other)

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
I added a test to the `ReplE2ESuite`.

Closes #42478 from hvanhovell/SPARK-44795.

Authored-by: Herman van Hovell <herman@databricks.com>
Signed-off-by: Herman van Hovell <herman@databricks.com>
","['connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/application/ReplE2ESuite.scala', 'sql/api/src/main/scala/org/apache/spark/sql/catalyst/encoders/OuterScopes.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala']","CodeGenerator Cache not classloader specific, causing class mismatch errors in UDFs when using REPL generated classes or classes with arbitrary names. Issue exacerbated during single process mode due to different classloader use."
556c74578eb2379fc6e0ec8d147674d0b10e5a2c,1649837247,"[SPARK-38833][PYTHON][SQL] Allow applyInPandas to return empty DataFrame without columns

### What changes were proposed in this pull request?
Methods `wrap_cogrouped_map_pandas_udf` and `wrap_grouped_map_pandas_udf` in `python/pyspark/worker.py` do not need to reject `pd.DataFrame`s with no columns return by udf when that DataFrame is empty (zero rows). This allows to return empty DataFrames without the need to define columns. The DataFrame is empty after all!

**The proposed behaviour is consistent with the current behaviour of `DataFrame.mapInPandas`.**

### Why are the changes needed?
Returning an empty DataFrame from the lambda given to `applyInPandas` should be as easy as this:

```python
return pd.DataFrame([])
```

However, PySpark requires that empty DataFrame to have the right _number_ of columns. This seems redundant as the schema is already defined in the `applyInPandas` call. Returning a non-empty DataFrame does not require defining columns.

Behaviour of `applyInPandas` should be consistent with `mapInPandas`.

Here is an example to reproduce:
```python
import pandas as pd

from pyspark.sql.functions import pandas_udf, ceil

df = spark.createDataFrame(
    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],
    (""id"", ""v""))

def mean_func(key, pdf):
    if key == (1,):
        return pd.DataFrame([])
    else:
        return pd.DataFrame([key + (pdf.v.mean(),)])

df.groupby(""id"").applyInPandas(mean_func, schema=""id long, v double"").show()
```

### Does this PR introduce _any_ user-facing change?
It changes the behaviour of the following calls to allow returning empty `pd.DataFrame` without defining columns. The PySpark DataFrame returned by `applyInPandas` is unchanged:

- `df.groupby(…).applyInPandas(…)`
- `df.cogroup(…).applyInPandas(…)`

### How was this patch tested?
Tests are added that test `applyInPandas` and `mapInPandas` when returning

- empty DataFrame with no columns
- empty DataFrame with the wrong number of columns
- non-empty DataFrame with wrong number of columns
- something other than `pd.DataFrame`

NOTE: It is not an error for `mapInPandas` to return DataFrames with more columns than specified in the `mapInPandas` schema.

Closes #36120 from EnricoMi/branch-empty-pd-dataframes.

Authored-by: Enrico Minack <github@enrico.minack.dev>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/sql/tests/test_pandas_cogrouped_map.py', 'python/pyspark/sql/tests/test_pandas_grouped_map.py', 'python/pyspark/sql/tests/test_pandas_map.py', 'python/pyspark/worker.py']","PySpark's applyInPandas function does not allow lambda functions to return an empty DataFrame without defined columns, differing from the behaviour of the mapInPandas function. This complicates the process of generating an empty DataFrame."
b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556,1573635987,"[SPARK-29851][SQL] V2 catalog: Change default behavior of dropping namespace to cascade

### What changes were proposed in this pull request?

Currently, `SupportsNamespaces.dropNamespace` drops a namespace only if it is empty. Thus, to implement a cascading drop, one needs to iterate all objects (tables, view, etc.) within the namespace (including its sub-namespaces recursively) and drop them one by one. This can have a negative impact on the performance when there are large number of objects.

Instead, this PR proposes to change the default behavior of dropping a namespace to cascading such that implementing cascading/non-cascading drop is simpler without performance penalties.

### Why are the changes needed?

The new behavior makes implementing cascading/non-cascading drop simple without performance penalties.

### Does this PR introduce any user-facing change?

Yes. The default behavior of `SupportsNamespaces.dropNamespace` is now cascading.

### How was this patch tested?

Added new unit tests.

Closes #26476 from imback82/drop_ns_cascade.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
","['sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsNamespaces.java', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryTableCatalog.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/TableCatalogSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DropNamespaceExec.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala']",Default behavior of 'SupportsNamespaces.dropNamespace' only allows dropping an empty namespace. Performing cascading drop with large number of objects leads to negative performance impact.
4482ff23ad984335b0d477100ac0815d5db8d532,1504142780,"[SPARK-17321][YARN] Avoid writing shuffle metadata to disk if NM recovery is disabled

In the current code, if NM recovery is not enabled then `YarnShuffleService` will write shuffle metadata to NM local dir-1, if this local dir-1 is on bad disk, then `YarnShuffleService` will be failed to start. So to solve this issue, in Spark side if NM recovery is not enabled, then Spark will not persist data into leveldb, in that case yarn shuffle service can still be served but lose the ability for recovery, (it is fine because the failure of NM will kill the containers as well as applications).

Tested in the local cluster with NM recovery off and on to see if folder is created or not. MiniCluster UT isn't added because in MiniCluster NM will always set port to 0, but NM recovery requires non-ephemeral port.

Author: jerryshao <sshao@hortonworks.com>

Closes #19032 from jerryshao/SPARK-17321.

Change-Id: I8f2fe73d175e2ad2c4e380caede3873e0192d027
","['common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java', 'resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnShuffleIntegrationSuite.scala', 'resource-managers/yarn/src/test/scala/org/apache/spark/network/yarn/YarnShuffleServiceSuite.scala']","The YarnShuffleService writes shuffle metadata to disk even when Node Manager (NM) recovery is disabled, causing service start failure if the target disk is bad."
11ea255283509a9f016f378df4865235a25b1851,1619758987,"[SPARK-35111][SQL] Support Cast string to year-month interval

### What changes were proposed in this pull request?
Support Cast string to year-month interval
Supported format as below
```
ANSI_STYLE, like
INTERVAL -'-10-1' YEAR TO MONTH
HIVE_STYLE like
10-1 or -10-1

Rules from the SQL standard about ANSI_STYLE:

<interval literal> ::=
  INTERVAL [ <sign> ] <interval string> <interval qualifier>
<interval string> ::=
  <quote> <unquoted interval string> <quote>
<unquoted interval string> ::=
  [ <sign> ] { <year-month literal> | <day-time literal> }
<year-month literal> ::=
  <years value> [ <minus sign> <months value> ]
  | <months value>
<years value> ::=
  <datetime value>
<months value> ::=
  <datetime value>
<datetime value> ::=
  <unsigned integer>
<unsigned integer> ::= <digit>...
```
### Why are the changes needed?
Support Cast string to year-month interval

### Does this PR introduce _any_ user-facing change?
User can cast year month interval string to YearMonthIntervalType

### How was this patch tested?
Added UT

Closes #32266 from AngersZhuuuu/SPARK-SPARK-35111.

Authored-by: Angerszhuuuu <angers.zhu@gmail.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala']","Casting a string to a year-month interval is unsupported, affecting the users' ability to convert interval strings to the YearMonthIntervalType."
873ad5596b5da9c157398a88ab58abf099624336,1654237749,"[SPARK-37623][SQL] Support ANSI Aggregate Function: regr_intercept

### What changes were proposed in this pull request?
`REGR_INTERCEPT` is an ANSI aggregate functions

**Syntax**: REGR_INTERCEPT(y, x)
**Arguments**:
- **y**:The dependent variable. This must be an expression that can be evaluated to a numeric type.
- **x**:The independent variable. This must be an expression that can be evaluated to a numeric type.

**Examples**:
`select k, regr_intercept(v, v2) from aggr group by k;`

|  k |    regr_intercept(v, v2) |
|---|--------------------|
| 1  |          [NULL]            |
| 2 |     1.154734411       |

The algorithm refers https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance

The mainstream database supports `regr_intercept` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/MpkZBV~MSTZ~I84I~ezxNg
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_intercept.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**DB2**
https://www.ibm.com/docs/en/db2/11.5?topic=af-regression-functions-regr-avgx-regr-avgy-regr-count
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_intercept
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-intercept-function.html
**Presto**
https://prestodb.io/docs/current/functions/aggregate.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`REGR_INTERCEPT` is very useful.

### Does this PR introduce _any_ user-facing change?
'No'. New feature.

### How was this patch tested?
New tests.

Closes #36708 from beliefer/SPARK-37623_new.

Authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Max Gekk <max.gekk@gmail.com>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Covariance.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpressionSuite.scala', 'sql/core/src/test/resources/sql-tests/inputs/linear-regression.sql', 'sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part1.sql', 'sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part1.sql']","The current version of Spark SQL does not support the ANSI aggregate function REGR_INTERCEPT, which is widely used and supported by multiple mainstream databases."
98f7182122e151cbf7ea83303e39c44d9acb1a72,1672796440,"[SPARK-41719][CORE] Skip SSLOptions sub-settings if `ssl` is disabled

### What changes were proposed in this pull request?
In SSLOptions rest of the settings should be set only when ssl is enabled.

### Why are the changes needed?
If spark.ssl.enabled is false, there is no use of setting rest of spark.ssl.* settings in SSLOptions as this requires unnecessary operations to be performed to set these properties.
Additional implication of trying to set the rest of settings is if any error occurs in setting these properties it will cause job failure which otherwise should have worked since ssl is disabled. For example, if the user doesn't have access to the keystore path which is set in hadoop.security.credential.provider.path of hive-site.xml, it can result in failure while launching spark shell since SSLOptions won't be initialized due to error in accessing the keystore.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added new test.

Closes #39221 from shrprasa/ssl_options_fix.

Authored-by: Shrikant Prasad <shrprasa@visa.com>
Signed-off-by: Dongjoon Hyun <dongjoon@apache.org>
","['core/src/main/scala/org/apache/spark/SSLOptions.scala', 'core/src/test/scala/org/apache/spark/SSLOptionsSuite.scala']","When 'spark.ssl.enabled' is set to false, attempting to set the additional 'spark.ssl.*' settings in SSLOptions can lead to unnecessary job failures, such as issues resulting from inaccessible keystore paths."
2fb85f6b684843f337b6e73ba57ee9e57a53496d,1588552790,"[SPARK-31527][SQL][TESTS][FOLLOWUP] Fix the number of rows in `DateTimeBenchmark`

### What changes were proposed in this pull request?
- Changed to the number of rows in benchmark cases from 3 to the actual number `N`.
- Regenerated benchmark results in the environment:

| Item | Description |
| ---- | ----|
| Region | us-west-2 (Oregon) |
| Instance | r3.xlarge |
| AMI | ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-20190722.1 (ami-06f2f779464715dc5) |
| Java | OpenJDK 64-Bit Server VM 1.8.0_242 and OpenJDK 64-Bit Server VM 11.0.6+10 |

### Why are the changes needed?
The changes are needed to have:
- Correct benchmark results
- Base line for other perf improvements that can be checked in the same environment.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
By running the benchmark and checking its output.

Closes #28440 from MaxGekk/SPARK-31527-DateTimeBenchmark-followup.

Authored-by: Max Gekk <max.gekk@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>
",['sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DateTimeBenchmark.scala'],"The number of rows in `DateTimeBenchmark` is inaccurately represented as 3 instead of the actual value 'N', causing invalid benchmark results."
bda5b51576e525724315d4892e34c8fa7e27f0c7,1565232445,"[SPARK-28454][PYTHON] Validate LongType in `createDataFrame(verifySchema=True)`

## What changes were proposed in this pull request?

Add missing validation for `LongType` in `pyspark.sql.types._make_type_verifier`.

## How was this patch tested?

Doctests / unittests / manual tests.

Unpatched version:
```
In [23]: s.createDataFrame([{'x': 1 << 64}], StructType([StructField('x', LongType())])).collect()
Out[23]: [Row(x=None)]
```

Patched:
```
In [5]: s.createDataFrame([{'x': 1 << 64}], StructType([StructField('x', LongType())])).collect()
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-c1740fcadbf9> in <module>
----> 1 s.createDataFrame([{'x': 1 << 64}], StructType([StructField('x', LongType())])).collect()

/usr/local/lib/python3.5/site-packages/pyspark/sql/session.py in createDataFrame(self, data, schema, samplingRatio, verifySchema)
    689             rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)
    690         else:
--> 691             rdd, schema = self._createFromLocal(map(prepare, data), schema)
    692         jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())
    693         jdf = self._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), schema.json())

/usr/local/lib/python3.5/site-packages/pyspark/sql/session.py in _createFromLocal(self, data, schema)
    405         # make sure data could consumed multiple times
    406         if not isinstance(data, list):
--> 407             data = list(data)
    408
    409         if schema is None or isinstance(schema, (list, tuple)):

/usr/local/lib/python3.5/site-packages/pyspark/sql/session.py in prepare(obj)
    671
    672             def prepare(obj):
--> 673                 verify_func(obj)
    674                 return obj
    675         elif isinstance(schema, DataType):

/usr/local/lib/python3.5/site-packages/pyspark/sql/types.py in verify(obj)
   1427     def verify(obj):
   1428         if not verify_nullability(obj):
-> 1429             verify_value(obj)
   1430
   1431     return verify

/usr/local/lib/python3.5/site-packages/pyspark/sql/types.py in verify_struct(obj)
   1397             if isinstance(obj, dict):
   1398                 for f, verifier in verifiers:
-> 1399                     verifier(obj.get(f))
   1400             elif isinstance(obj, Row) and getattr(obj, ""__from_dict__"", False):
   1401                 # the order in obj could be different than dataType.fields

/usr/local/lib/python3.5/site-packages/pyspark/sql/types.py in verify(obj)
   1427     def verify(obj):
   1428         if not verify_nullability(obj):
-> 1429             verify_value(obj)
   1430
   1431     return verify

/usr/local/lib/python3.5/site-packages/pyspark/sql/types.py in verify_long(obj)
   1356             if obj < -9223372036854775808 or obj > 9223372036854775807:
   1357                 raise ValueError(
-> 1358                     new_msg(""object of LongType out of range, got: %s"" % obj))
   1359
   1360         verify_value = verify_long

ValueError: field x: object of LongType out of range, got: 18446744073709551616
```

Closes #25117 from simplylizz/master.

Authored-by: Anton Yanchenko <simplylizz@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
","['python/pyspark/sql/tests/test_types.py', 'python/pyspark/sql/types.py']","Creating a DataFrame for LongType fields without validation allows entry of out-of-range values, leading to uninitialized field values."
a7a051afa8140f13575ad00e952b92bef989d7a9,1542126357,"[SPARK-25947][SQL] Reduce memory usage in ShuffleExchangeExec by selecting only the sort columns

## What changes were proposed in this pull request?

When sorting rows, ShuffleExchangeExec uses the entire row instead of just the columns references in SortOrder to create the RangePartitioner. This causes the RangePartitioner to sample entire rows to create rangeBounds and can cause OOM issues on the driver when rows contain large fields.

This change creates a projection and only use columns involved in the SortOrder for the RangePartitioner

## How was this patch tested?

Existing tests in spark-sql.

Plus

Started a local spark-shell with a small spark.driver.maxResultSize:

```
spark-shell --master 'local[16]' --conf spark.driver.maxResultSize=128M --driver-memory 4g
```

and ran the following script:

```
import com.google.common.io.Files
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession

import scala.util.Random

transient val sc = SparkContext.getOrCreate()
transient val spark = SparkSession.builder().getOrCreate()

import spark.implicits._

val path = Files.createTempDir().toString

// this creates a dataset with 1024 entries, each 1MB in size, across 16 partitions
sc.parallelize(0 until (1 << 10), sc.defaultParallelism).
  map(_ => Array.fill(1 << 18)(Random.nextInt)).
  toDS.
  write.mode(""overwrite"").parquet(path)

spark.read.parquet(path).
  orderBy('value (0)).
  write.mode(""overwrite"").parquet(s""$path-sorted"")

spark.read.parquet(s""$path-sorted"").show
```
execution would fail when initializing RangePartitioner without this change.
execution succeeds and generates a correctly sorted dataset with this change.

Please review http://spark.apache.org/contributing.html before opening a pull request.

Closes #22961 from mu5358271/sort-improvement.

Authored-by: mu5358271 <shuheng.dai@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>
",['sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala'],ShuffleExchangeExec uses the entire row in SortOrder which leads to Out of Memory (OOM) issues on the driver when rows contain large fields.
d67e22826cda41d732e010d73687e74fab60f4b6,1669877413,"[SPARK-41339][SQL] Close and recreate RocksDB write batch instead of just clearing

### What changes were proposed in this pull request?

Instead of just calling `writeBatch.clear`, close the write batch and recreate it.

### Why are the changes needed?

A RocksDB `WriteBatch` (and by extension `WriteBatchWithIndex`) stores its underlying data in a `std::string`. Why? I'm not sure. But after a partition is finished, `writeBatch.clear()` is called (somewhat indirectly through a call to `store.abort`), presumably clearing the data in the `WriteBatch`. This calls `std::string::clear` followed by `std::string::resize` underneath the hood. However, neither of these two things actually reclaims native memory. All the memory allocated for expanding the string when adding data to the `WriteBatch` will be there until the `std::string` is deallocated, which in this case means deleting the `WriteBatch`. This leads to native memory accumulation on an executor and it executes several partitions consecutively, which would happen when your total executor cores is less than your shuffle partitions for your stateful stream. So instead of just calling `writeBatch.clear()`, close the `WriteBatch` and create a new one to free up the native memory.

### Does this PR introduce _any_ user-facing change?

Fix for excess native memory usage.

### How was this patch tested?

Existing UTs, not sure how to test for memory usage.

Closes #38853 from Kimahriman/rocksdb-write-batch-close.

Lead-authored-by: Adam Binford <adamq43@gmail.com>
Co-authored-by: centos <centos@adam-dev.novalocal>
Signed-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>
",['sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala'],"RocksDB `WriteBatch` accumulates native memory on an executor when executing several consecutive partitions, leading to excess native memory usage due to its underlying storage in `std::string` not being properly deallocated."
ae08787f5c50e485ef4432a0c2da8b3b7290d725,1663051458,"[SPARK-40399][PS] Make `pearson` correlation in `DataFrame.corr` support missing values and `min_periods `

### What changes were proposed in this pull request?
refactor `pearson` correlation in `DataFrame.corr` to:

1. support missing values;
2. add parameter  `min_periods`;
3. enable arrow execution since no longer depend on `VectorUDT`;
4. support lazy evaluation;

before
```
In [1]: import pyspark.pandas as ps

In [2]: df = ps.DataFrame([[1,2], [3,None]])

In [3]: df

   0    1
0  1  2.0
1  3  NaN

In [4]: df.corr()
22/09/09 16:53:18 ERROR Executor: Exception in task 9.0 in stage 5.0 (TID 24)
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$2660/0x0000000801215840: (struct<0_double_VectorAssembler_0915f96ec689:double,1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
```

after
```
In [1]: import pyspark.pandas as ps

In [2]: df = ps.DataFrame([[1,2], [3,None]])

In [3]: df.corr()

     0   1
0  1.0 NaN
1  NaN NaN

In [4]: df.to_pandas().corr()
/Users/ruifeng.zheng/Dev/spark/python/pyspark/pandas/utils.py:976: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.
  warnings.warn(message, PandasAPIOnSparkAdviceWarning)
Out[4]:
     0   1
0  1.0 NaN
1  NaN NaN
```

### Why are the changes needed?
for API coverage and support common cases containing missing values

### Does this PR introduce _any_ user-facing change?
yes, API change, new parameter supported

### How was this patch tested?
added UT

Closes #37845 from zhengruifeng/ps_df_corr_missing_value.

Authored-by: Ruifeng Zheng <ruifengz@apache.org>
Signed-off-by: Ruifeng Zheng <ruifengz@apache.org>
","['python/pyspark/pandas/frame.py', 'python/pyspark/pandas/tests/test_stats.py']","The `pearson` correlation in `DataFrame.corr` doesn't support missing values or `min_periods` parameter, resulting in failed UDF execution."
0627850b7e1845147060bc298afb23d5939d4a87,1553185235,"[SPARK-25196][SQL] Extends the analyze column command for cached tables

## What changes were proposed in this pull request?
This pr extended `ANALYZE` commands to analyze column stats for cached table.

In common use cases, users read catalog table data, join/aggregate them, and then cache the result for following reuse. Since we are only allowed to analyze column statistics in catalog tables via ANALYZE commands, the current optimization depends on non-existing or inaccurate column statistics of cached data. So, it would be great if we could analyze cached data as follows;

```scala
scala> def printColumnStats(tableName: String) = {
     |   spark.table(tableName).queryExecution.optimizedPlan.stats.attributeStats.foreach {
     |     case (k, v) => println(s""[$k]: $v"")
     |   }
     | }

scala> sql(""SET spark.sql.cbo.enabled=true"")
scala> sql(""SET spark.sql.statistics.histogram.enabled=true"")

scala> spark.range(1000).selectExpr(""id % 33 AS c0"", ""rand() AS c1"", ""0 AS c2"").write.saveAsTable(""t"")
scala> sql(""ANALYZE TABLE t COMPUTE STATISTICS FOR COLUMNS c0, c1, c2"")
scala> spark.table(""t"").groupBy(""c0"").agg(count(""c1"").as(""v1""), sum(""c2"").as(""v2"")).createTempView(""temp"")

// Prints column statistics in catalog table `t`
scala> printColumnStats(""t"")
[c0#7073L]: ColumnStat(Some(33),Some(0),Some(32),Some(0),Some(8),Some(8),Some(Histogram(3.937007874015748,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;9f7c1c)),2)
[c1#7074]: ColumnStat(Some(944),Some(3.2108484832404915E-4),Some(0.997584797423909),Some(0),Some(8),Some(8),Some(Histogram(3.937007874015748,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;60a386b1)),2)
[c2#7075]: ColumnStat(Some(1),Some(0),Some(0),Some(0),Some(4),Some(4),Some(Histogram(3.937007874015748,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;5ffd29e8)),2)

// Prints column statistics on cached table `temp`
scala> sql(""CACHE TABLE temp"")
scala> printColumnStats(""temp"")
<No Column Statistics>

// Analyzes columns `v1` and `v2` on cached table `temp`
scala> sql(""ANALYZE TABLE temp COMPUTE STATISTICS FOR COLUMNS v1, v2"")

// Then, prints again
scala> printColumnStats(""temp"")
[v1#7084L]: ColumnStat(Some(2),Some(30),Some(31),Some(0),Some(8),Some(8),Some(Histogram(0.12992125984251968,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;49f7bb6f)),2)
[v2#7086L]: ColumnStat(Some(1),Some(0),Some(0),Some(0),Some(8),Some(8),Some(Histogram(0.12992125984251968,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;12701677)),2)

// Analyzes one left column and prints again
scala> sql(""ANALYZE TABLE temp COMPUTE STATISTICS FOR COLUMNS c0"")
scala> printColumnStats(""temp"")
[v1#7084L]: ColumnStat(Some(2),Some(30),Some(31),Some(0),Some(8),Some(8),Some(Histogram(0.12992125984251968,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;49f7bb6f)),2)
[v2#7086L]: ColumnStat(Some(1),Some(0),Some(0),Some(0),Some(8),Some(8),Some(Histogram(0.12992125984251968,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;12701677)),2)
[c0#7073L]: ColumnStat(Some(33),Some(0),Some(32),Some(0),Some(8),Some(8),Some(Histogram(0.12992125984251968,[Lorg.apache.spark.sql.catalyst.plans.logical.HistogramBin;1f5c1b81)),2)
```

## How was this patch tested?
Added tests in `CachedTableSuite` and `StatisticsCollectionSuite`.

Closes #24047 from maropu/SPARK-25196-4.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>
","['sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala', 'sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala', 'sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala']","The ANALYZE command does not collect column statistics for cached tables, leading to optimizations that rely on non-existing or inaccurate column stats."
4e95738fdfc334c25f44689ff8c2db5aa7c726f2,1648267796,"[SPARK-38336][SQL] Support DEFAULT column values in CREATE/REPLACE TABLE statements

### What changes were proposed in this pull request?

Extend CREATE TABLE and REPLACE TABLE statements to support columns with DEFAULT values. This information will be stored in the column metadata.

### Why are the changes needed?

This builds the foundation for future work (not included in this PR) to support INSERT INTO statements, which may then omit the default values or refer to them explicitly with the DEFAULT keyword, in which case the Spark analyzer will automatically insert the appropriate corresponding values in the right places.

Example:
```
CREATE TABLE T(a INT DEFAULT 4, b INT NOT NULL DEFAULT 5);
INSERT INTO T VALUES (1);
INSERT INTO T VALUES (1, DEFAULT);
INSERT INTO T VALUES (DEFAULT, 6);
SELECT * FROM T;
(1, 5)
(1, 5)
(4, 6)
```

### How was this patch tested?

This change is covered by new and existing unit test coverage as well as new INSERT INTO query test cases covering a variety of positive and negative scenarios.

Closes #35855 from dtenedor/default-cols-create-table.

Authored-by: Daniel Tenedorio <daniel.tenedorio@databricks.com>
Signed-off-by: Gengliang Wang <gengliang@apache.org>
","['sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ResolveDefaultColumns.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala', 'sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala', 'sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala', 'sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala', 'sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala', 'sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertSuite.scala']",CREATE TABLE and REPLACE TABLE statements currently lack support for defining DEFAULT values for columns.
c014fa2e18713f67deb072a4336286f4a3f4d3f4,1649345974,"[SPARK-38760][PYTHON][SQL][SS] Add DataFrame.observe(str, ...) for Structured Streaming in PySpark

### What changes were proposed in this pull request?

This PR adds the support of Structured Streaming at `DataFrame.observe` in PySpark. This is the same support with the Scala side.

### Why are the changes needed?

For SS users in PySpark to easily monitor their streaming queries.

### Does this PR introduce _any_ user-facing change?

Yes. After this PR, PySpark users can do:

```python
from pyspark.sql.functions import count, col, sum, lit
from pyspark.sql.streaming import StreamingQueryListener

class MyListener(StreamingQueryListener):
    def onQueryStarted(self, event):
        pass
    def onQueryProgress(self, event):
        row = event.progress.observedMetrics.get(""metric"")
        # Do something with the metric, e.g.) row.cnt and row.sum
        print(""Count: %s"" % row.cnt)
    def onQueryTerminated(self, event):
        pass

spark.streams.addListener(MyListener())

df = spark.readStream.format(""rate"").option(""rowsPerSecond"", 10).load()
observed_df = df.observe(""metric"", count(lit(1)).alias(""cnt""), sum(col(""value"")).alias(""sum""))
observed_df.writeStream.format(""noop"").queryName(""test"").start()
```

### How was this patch tested?

Manually tested with the example above, and unit test was added.

Closes #36082 from HyukjinKwon/SPARK-38760.

Lead-authored-by: Hyukjin Kwon <gurwls223@apache.org>
Co-authored-by: Hyukjin Kwon <gurwls223@gmail.com>
Signed-off-by: Hyukjin Kwon <gurwls223@apache.org>
","['python/pyspark/sql/dataframe.py', 'python/pyspark/sql/observation.py', 'python/pyspark/sql/streaming/listener.py', 'python/pyspark/sql/tests/test_dataframe.py', 'sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala']","The `DataFrame.observe` function in PySpark does not support Structured Streaming, limiting users' ability to monitor their streaming queries effectively."
