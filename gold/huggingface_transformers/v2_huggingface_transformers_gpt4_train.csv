commit_id,commit_date,commit_message,actual_files_modified,transformed_message_gpt4
bad83008377bf01a34ac2e08c74e7da89eaf4e07,1676403572,"Error (also in original) model, scaling only q matrix not qk.T dot product (qk.T/sqrt(dim_per_head)) (#21627)

* Error in model, scaling only q matrix not qK.T dot product (qk.T/sqrt(dim_per_head))

As per Vaswani et al, 2017 p.4
Is torch.matmul(q, k.transpose(2, 3)) / math.sqrt(dim_per_head) not q / math.sqrt(dim_per_head)
https://arxiv.org/pdf/1912.05372.pdf

Error was in original FlauBERT repo and effectively scales queries but not values
cf. https://github.com/getalp/Flaubert/pull/45/commits/6d176880ca3a1a8dfa2b76c97030bb51c5e917b8

* Update modeling_flaubert.py

Update to https://github.com/huggingface/transformers/pull/21627
make fixup
make repo_consistency

* Update modeling_xlm.py

* Update modeling_flaubert.py

* Update modeling_xlm.py","['src/transformers/models/flaubert/modeling_flaubert.py', 'src/transformers/models/xlm/modeling_xlm.py']","Incorrect scaling only on q matrix and not on qk.T dot product in FlauBERT model as per Vaswani's method (p.4, 2017), causing inaccurate results."
392740452e86ee6ca523b92bc4ef8527ed4e7a16,1688635596,"Add dropouts to GPT-NeoX (#24680)

* add attention dropout, post attention dropout, post mlp dropout to gpt-neox

* fix typo

* add documentation

* fix too long line

* ran Checking/fixing src/transformers/models/gpt_neox/configuration_gpt_neox.py src/transformers/models/gpt_neox/modeling_gpt_neox.py
python utils/custom_init_isort.py
python utils/sort_auto_mappings.py
doc-builder style src/transformers docs/source --max_len 119 --path_to_docs docs/source
python utils/check_doc_toc.py --fix_and_overwrite
running deps_table_update
updating src/transformers/dependency_versions_table.py
python utils/check_copies.py
python utils/check_table.py
python utils/check_dummies.py
python utils/check_repo.py
Checking all models are included.
Checking all models are public.
Checking all models are properly tested.
Checking all objects are properly documented.
Checking all models are in at least one auto class.
Checking all names in auto name mappings are defined.
Checking all keys in auto name mappings are defined in `CONFIG_MAPPING_NAMES`.
Checking all auto mappings could be imported.
Checking all objects are equally (across frameworks) in the main __init__.
python utils/check_inits.py
python utils/check_config_docstrings.py
python utils/check_config_attributes.py
python utils/check_doctest_list.py
python utils/update_metadata.py --check-only
python utils/check_task_guides.py","['src/transformers/models/gpt_neox/configuration_gpt_neox.py', 'src/transformers/models/gpt_neox/modeling_gpt_neox.py']","GPT-NeoX lacks attention, post attention, and post MLP dropout settings resulting in less optimal model performance."
14b04b4b9c483d94fadd2b5479ed9430bae8ac84,1697473665,"Conversation pipeline fixes (#26795)

* Adjust length limits and allow naked conversation list inputs

* Adjust length limits and allow naked conversation list inputs

* Maybe use a slightly more reasonable limit than 1024

* Skip tests for old models that never supported this anyway

* Cleanup input docstrings

* More docstring cleanup + skip failing TF test

* Make fixup","['src/transformers/pipelines/conversational.py', 'tests/models/bart/test_modeling_bart.py', 'tests/models/bart/test_modeling_tf_bart.py', 'tests/models/t5/test_modeling_t5.py', 'tests/models/t5/test_modeling_tf_t5.py']","Issues in conversation pipeline related to length limits, unsupported models, and inaccurate docstrings."
38043d8453b82a9c712f8d5c98323150fbee7503,1652452080,"Update self-push workflow (#17177)

* update push ci

* install git-python

* update comment

* update deepspeed jobs

* fix report

* skip 2 more tests that require fairscale

* Fix changes in test_fetcher.py (to deal with `setup.py` is changed)

* set RUN_PT_TF_CROSS_TESTS=1 and final clean-up

* remove SIGOPT_API_TOKEN

* remove echo ""$matrix_folders""

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['tests/extended/test_trainer_ext.py', 'utils/notification_service.py', 'utils/tests_fetcher.py']","CI workflow is not updated with latest changes, causing errors in report generation and failure for tests requiring fairscale. Additionally, certain environment variables and parameters are unnecessarily included in the current setup leading to potential data leaks."
587d84b1784cce30c59a12faee2a672bac49bbdd,1664898733,"Add `BloomForQuestionAnswering` (#19310)

* add bloom for question answering

- attempt to add Bloom for question answering
- adapted from `GPTJForQuestionAnswering`
- Fixed `num_labels` to `2` for common tests
- Added a bit of docstring
- All common tests pass

* Update src/transformers/models/bloom/modeling_bloom.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* revert changes related to `num_labels`

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/bloom/__init__.py', 'src/transformers/models/bloom/modeling_bloom.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/bloom/test_modeling_bloom.py']","Bloom model lacks an implementation for question answering, which is present in other models like `GPTJForQuestionAnswering`."
6a3d1a98e031a09e7783134ae76dc6f8a358d568,1675363913,"Fixes bug in the creation of ExponentialDecayLengthPenalty (#21423)

input_ids_seq_length doesn't exist in the GenerationConfig, it exists as local variable in the function.

Setting exponential_decay_length_penalty therefore results in an error:
`AttributeError: 'GenerationConfig' object has no attribute 'input_ids_seq_length'`

This simple change fixes this issue, and the exponential_decay_length_penalty works as expected.",['src/transformers/generation/utils.py'],'GenerationConfig' object throws an AttributeError when setting the 'exponential_decay_length_penalty' due to non-existing attribute 'input_ids_seq_length'.
fbf382c84da4506484a23e85bd8540da5192ff4e,1661437894,"Determine framework automatically before ONNX export (#18615)

* Automatic detection for framework to use when exporting to ONNX

* Log message change

* Incorporating PR comments, adding unit test

* Adding tf for pip install for run_tests_onnxruntime CI

* Restoring past changes to circleci yaml and test_onnx_v2.py, tests moved to tests/onnx/test_features.py

* Fixup

* Adding test to fetcher

* Updating circleci config to log more

* Changing test class name

* Comment typo fix in tests/onnx/test_features.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Moving torch_str/tf_str to self.framework_pt/tf

* Remove -rA flag in circleci config

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>","['src/transformers/onnx/__main__.py', 'src/transformers/onnx/features.py', 'tests/onnx/test_features.py', 'utils/tests_fetcher.py']","Framework for exporting to ONNX is not being detected automatically, causing potential incompatibilities or errors during the process."
1baeed5bdf3c58b723a6125632567f97bdf322c6,1690980234,"Fix return_dict_in_generate bug in InstructBlip generate function (#25246)

Fix bug in InstructBlip generate function

Previously, the postprocessing conducted on generated sequences in InstructBlip's generate function assumed these sequences were tensors (i.e. that `return_dict_in_generate == False`).

This commit checks whether the result of the call to the wrapped language model `generate()` is a tensor, and if not attempts to postprocess the sequence attribute of the returned results object.",['src/transformers/models/instructblip/modeling_instructblip.py'],"InstructBlip's generate function incorrectly assumes postprocessing sequences to be tensors, leading to potential bugs when `return_dict_in_generate == True`."
d25e25ee2b63ebfcd099deb689a5a7272574a10f,1643392523,"Add XGLM models (#14876)

* add xglm

* update vocab size

* fix model name

* style and tokenizer

* typo

* no mask token

* fix pos embed compute

* fix args

* fix tokenizer

* fix positions

* fix tokenization

* style and dic fixes

* fix imports

* add fast tokenizer

* update names

* add pt tests

* fix tokenizer

* fix typo

* fix tokenizer import

* fix fast tokenizer

* fix tokenizer

* fix converter

* add tokenizer test

* update checkpoint names

* fix tokenizer tests

* fix slow tests

* add copied from comments

* rst -> mdx

* flax model

* update flax tests

* quality

* style

* doc

* update index and readme

* fix copies

* fix doc

* update toctrr

* fix indent

* minor fixes

* fix config doc

* don't save embed_pos weights

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* address Sylvains commnets, few doc fixes

* fix check_repo

* align order of arguments

* fix copies

* fix labels

* remove unnecessary mapping

* fix saving tokenizer

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/convert_slow_tokenizer.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/xglm/__init__.py', 'src/transformers/models/xglm/configuration_xglm.py', 'src/transformers/models/xglm/modeling_flax_xglm.py', 'src/transformers/models/xglm/modeling_xglm.py', 'src/transformers/models/xglm/tokenization_xglm.py', 'src/transformers/models/xglm/tokenization_xglm_fast.py', 'src/transformers/utils/dummy_flax_objects.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_sentencepiece_objects.py', 'src/transformers/utils/dummy_tokenizers_objects.py', 'tests/test_modeling_flax_xglm.py', 'tests/test_modeling_xglm.py', 'tests/test_tokenization_xglm.py', 'utils/check_repo.py']","Implements XGLM models exhibiting multiple errors and inconsistencies; issues with tokenizers, style, ordering and label handling, failing tests, incorrect imports, difficulties in argument alignment, documentation inaccuracies, and problems saving tokenizers."
dcca71be6151c04ec89116d3275f7173b7936601,1666955141,"Create dummy models (#19901)

* create dummy models

* quality

* update

* update

* Make Wav2Vec2Conformer work

* style

* deal with models with text_config and vision_config

* apply suggestions

* Composite models

* style

* style

* fix shape issue

* fix shape issue

* For VisionTextDualEncoderModel

* show_progress=False when converting tokenizers

* Fix for OwlViT

* Fix for VisualBert

* Update

* final

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['utils/check_config_docstrings.py', 'utils/create_dummy_models.py']","Issues in various models (Wav2Vec2Conformer, OwlViT, VisualBert, and VisionTextDualEncoderModel) including incorrect shapes, failure in converting tokenizers, problems with text_config and vision_config, and lack of functioning with dummy models."
ff066119ca361dfe19601d91067ec03561470435,1639757219,"Implement head_mask for Flax BERT and other models copied from BERT (#14620)

* Implement head_mask for Flax BERT and other models copied from BERT

* Remove `from jax._src.nn.functions import sigmoid`

Remove `from jax._src.nn.functions import sigmoid` unintentionally added by IDE

* Remove no more valid copy statement

* Apply patil-suraj's suggestions from code review

* Apply suggestions from the code review

* Update Flax template

* Fix a typo

* Also update template for CausalLM modules","['src/transformers/models/bert/modeling_flax_bert.py', 'src/transformers/models/big_bird/modeling_flax_big_bird.py', 'src/transformers/models/electra/modeling_flax_electra.py', 'src/transformers/models/roberta/modeling_flax_roberta.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py', 'tests/test_modeling_flax_bert.py', 'tests/test_modeling_flax_common.py', 'tests/test_modeling_flax_electra.py', 'tests/test_modeling_flax_roberta.py']","Flax BERT and other models copied from BERT lack a head_mask implementation, causing unintended function additions and invalid copy statements."
56f50590d5a9ac881db9ee1753f4642cf3d33d28,1653330741,"Use Accelerate in `from_pretrained` for big model inference (#17341)

* Initial work

* More or less finished with first draft

* Update src/transformers/modeling_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Fix randomly initialized weights

* Update src/transformers/modeling_utils.py

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

* Address review comments

* Rename DeepSpeed folder to temporarily fix the test issue?

* Revert to try if Accelerate fix works

* Use latest Accelerate release

* Quality and fixes

* Style

* Quality

* Add doc

* Test + fix

* More blocks

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>","['setup.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/modeling_utils.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/models/gpt_neo/modeling_gpt_neo.py', 'src/transformers/models/gptj/modeling_gptj.py', 'src/transformers/models/t5/modeling_t5.py', 'tests/test_modeling_common.py']","Inference on large models using `from_pretrained` is inefficient, possibly due to an issue with weight initialization."
7cf52a49dee661f6adb7847991c6a84925999f5d,1656002182,"Nezha Pytorch implementation (#17776)

* wip

* rebase

* all tests pass

* rebase

* ready for PR

* address comments

* fix styles

* add require_torch to pipeline test

* remove remote image to improve CI consistency

* address comments; fix tf/flax tests

* address comments; fix tf/flax tests

* fix tests; add alias

* repo consistency tests

* Update src/transformers/pipelines/visual_question_answering.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* address comments

* Update src/transformers/pipelines/visual_question_answering.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* merge

* wip

* wip

* wip

* most basic tests passes

* all tests pass now

* relative embedding

* wip

* running make fixup

* remove bert changes

* fix doc

* fix doc

* fix issues

* fix doc

* address comments

* fix CI

* remove redundant copied from

* address comments

* fix broken test

Co-authored-by: Sijun He <sijunhe@Sijuns-MacBook-Pro.local>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/nezha/__init__.py', 'src/transformers/models/nezha/configuration_nezha.py', 'src/transformers/models/nezha/modeling_nezha.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/fx.py', 'tests/models/nezha/test_modeling_nezha.py']","Nezha Pytorch implementation is causing consistency issues with the pipeline tests, visual question answering, and repository. Some docs are also leading to confusions and there are broken tests that must be addressed."
07d8d6e2f7a920d399e5e86a82d78179cdfa6746,1668445761,"docs: translated index page to korean (#20180)

docs: i18n: first draft of index page
docs: fix: first revision of index page
docs: i18n: missed section - supported frameworks
docs: fix: second revision of index page
review by @ArthurZucker

refactor: remove untranslated files from korean
docs: fix: remove untranslated references from toctree.yml
feat: enable korean docs in gh actions
docs: feat: add in_translation page as placeholder
docs: bug: testing if internal toc need alphabet chars
docs: fix: custom english anchor for non-alphanumeric headings
review by @sgugger

docs: i18n: translate comments on install methods in _config.py
docs: refactor: more concise wording for translations",['docs/source/ko/_config.py'],"The index page is not available in Korean and there are untranslated files and references within the Korean documentation. Additionally, there are comments in _config.py that haven't been translated and improvements are needed in wording for translations."
e20fab0bbe8ca5b23738b670d1bd95aafbbbc53c,1692708612,"Fix bloom add prefix space (#25652)

* properly support Sequence of pretokenizers

* actual fix

* make sure the fix works. Tests are not working for sure!

* hacky way

* add TODO

* update

* add a todo

* nits

* rename test

* nits

* rename test","['src/transformers/models/bloom/tokenization_bloom_fast.py', 'tests/models/bloom/test_tokenization_bloom.py']","The 'bloom add' function lacks support for sequence of pretokenizer, leading to prefix space issues and failing tests."
d1f5ca1afd03e38f45062b2a06f1846a7c290da4,1642590291,"[FLAX] glue training example refactor (#13815)

* refactor run_flax_glue.py

* updated readme

* rm unused import and args typo fix

* refactor

* make consistent arg name across task

* has_tensorboard check

* argparse -> argument dataclasses

* refactor according to review

* fix","['examples/flax/test_examples.py', 'examples/flax/text-classification/run_flax_glue.py']","The current implementation of the Flax GLUE training example shows inconsistent argument names across tasks, usage of argparse over argument dataclasses, and presence of unused imports and typos, causing confusion and potential errors."
0a570dbd2e60a8b887dcc9fe213753cc71d2272b,1682427065,"Neptune fix bug init run (#22836)

* [neptune] fix checkpoint bug with relative out_dir

* update imports

* reformat with black

* check neptune without imports

* fix typing-related issue

* run black on code

* use os.path.sep instead of raw \

* simplify imports and remove type annotation

* make ruff happy

* apply review suggestions

* replace run with with_id kwarg to run

* update imports to avoid deprecation warnings for the latest client

---------

Co-authored-by: kshitij12345 <kshitijkalambarkar@gmail.com>",['src/transformers/integrations.py'],Relative out_dir results in a checkpoint bug in Neptune. There are also deprecation warnings due to outdated client imports.
4766e009b034ec698f09526a2225b6b5fc34a75e,1630501540,"Improve T5 docs  (#13240)

* Remove disclaimer

* First draft

* Fix rebase

* Improve docs some more

* Add inference section

* Improve example scripts section

* Improve code examples of modeling files

* Add docs regarding task prefix

* Address @craffel's comments

* Apply suggestions from @patrickvonplaten's review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Add suggestions from code review

* Apply @sgugger's suggestions

* Fix Flax code examples

* Fix index.rst

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/models/t5/modeling_flax_t5.py', 'src/transformers/models/t5/modeling_t5.py', 'src/transformers/models/t5/modeling_tf_t5.py']","The T5 documentation lacks clarity and completeness, resulting in several sections including inference, script examples, and modeling files being underexplained. Task prefixes concepts are also not properly elaborated."
d4e4efce68a5d18ca3175475b59051dec336fa6b,1633436387,"Initial support for symbolic tracing with torch.fx allowing dynamic axes (#13579)

* Symbolic trace dynamic axes support for BERT like models (albert, bert, distilbert, mobilebert, electra, megatron-bert)
* Sanity checks before tracing that make sure the model to trace is supported
* Adapted to PyTorch 1.9

Co-authored-by: Michael Benayoun <michael@huggingface.co>","['src/transformers/file_utils.py', 'src/transformers/models/distilbert/modeling_distilbert.py', 'src/transformers/utils/fx.py', 'src/transformers/utils/fx_transformations.py', 'tests/test_modeling_albert.py', 'tests/test_modeling_bert.py', 'tests/test_modeling_common.py', 'tests/test_modeling_distilbert.py', 'tests/test_modeling_electra.py', 'tests/test_modeling_megatron_bert.py', 'tests/test_modeling_mobilebert.py']","Lack of symbolic tracing support for dynamic axes in models like BERT, causing tracing failures and unsupported model errors."
6ef16f2b67bdf2797297d65f72efc68256d11e3f,1665159590,"Remove Dependency between Bart and LED (slow/fast) (#19408)

* removed dependency from bart(slow)

* removed dependency from bart(slow)

* adding copying comments (copied from bart to led)

* updated led docstring

* updated led docstring

* removed dependency from Bart (fast)

* replaced bart with LED in docstrings

* complying flake8

* added more copy comments

* fixing copying comments

* added comments back

* fix copy comments

* fixing copied from comments

* fixing copied from comments","['src/transformers/models/led/tokenization_led.py', 'src/transformers/models/led/tokenization_led_fast.py']","Bart (slow/fast) is dependent on LED, causing redundancy and potential synchronization issues. The docstrings also need to contain accurate references."
f62cb8313c2d7051e38f845823c1f4a7307aac3e,1660160851,"Adds CLIP to models exportable with ONNX (#18515)

* onnx config for clip

* default opset as 14

* changes from the original repo

* input values order fix

* outputs fix

* remove unused import

* ran make fix-copies

* black format

* review comments: forward ref, import fix, model change revert, .to cleanup

* make style

* formatting fixes

* revert groupvit

* comment for cast to int32

* comment fix

* make .T as .t() for onnx conversion

* ran make fix-copies

* remove unneeded comment

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix copies

* remove comment

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/clip/__init__.py', 'src/transformers/models/clip/configuration_clip.py', 'src/transformers/models/clip/modeling_clip.py', 'src/transformers/models/groupvit/modeling_groupvit.py', 'src/transformers/models/owlvit/modeling_owlvit.py', 'src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py', 'src/transformers/onnx/features.py', 'tests/onnx/test_onnx_v2.py']","CLIP model is not exportable with the ONNX framework, resulting in compatibility issues for non-PyTorch deployments."
5d3f03743301acf865be0dda93182c0abaadc49e,1660225947,"german docs translation (#18544)

* Create _config.py

* Create _toctree.yml

* Create index.mdx

not sure about ""du / ihr"" oder ""sie""

* Create quicktour.mdx

* Update _toctree.yml

* Update build_documentation.yml

* Update build_pr_documentation.yml

* fix build

* Update index.mdx

* Update quicktour.mdx

* Create installation.mdx

* Update _toctree.yml",['docs/source/de/_config.py'],"Absence of German translation for the documentation, making it difficult for German-speaking audience to comprehend."
7223844df9738719ee335428a326cd712f506806,1629722118,"Change how ""additional_special_tokens"" argument in the "".from_pretrained"" method of the tokenizer is taken into account (#13056)

* add test

* add change in PretrainedTokenizerBase

* change Luke

* deactivate

* add the possibility to add additional special tokens for M2M100

* format

* add special test for canine

* proposed changes for mbart

* proposed changes for mbart50

* proposed changes for byt5

* proposed changes for canine

* proposed changes for t5

* test fast and slow

* remove comment

* remove comment

* add fast version for all tests

* replace break by continue

* add more comments

* add check to avoid duplicates

* remove comment

* format

* proposed change for wave2vec2

* reverse changes mbart

* uncomment

* format","['src/transformers/models/luke/tokenization_luke.py', 'src/transformers/models/m2m_100/tokenization_m2m_100.py', 'src/transformers/models/mbart50/tokenization_mbart50.py', 'src/transformers/models/mbart50/tokenization_mbart50_fast.py', 'src/transformers/tokenization_utils_base.py', 'tests/test_processor_wav2vec2.py', 'tests/test_tokenization_byt5.py', 'tests/test_tokenization_canine.py', 'tests/test_tokenization_common.py', 'tests/test_tokenization_t5.py']","Additional special tokens in the "".from_pretrained"" method of the tokenizer are not properly handled, causing unexpected behavior and potentially leading to duplicate tokens."
6bc517ccd4a3bcda4d0621d54a37c3e047df223a,1693933280,"deepspeed resume from ckpt fixes and adding support for deepspeed optimizer and HF scheduler (#25863)

* Add support for deepspeed optimizer and HF scheduler

* fix bug

* fix the import

* fix issue with deepspeed scheduler saving for hf optim + hf scheduler scenario

* fix loading of hf scheduler when loading deepspeed checkpoint

* fix import of `DeepSpeedSchedulerWrapper`

* add tests

* add the comment and skip the failing tests

* address comment","['src/transformers/integrations/deepspeed.py', 'src/transformers/trainer.py', 'tests/deepspeed/test_deepspeed.py']","Deepspeed integration with hf optimizer and HF scheduler has issues with checkpoint resume, scheduler saving, and loading deepspeed check points."
ed858f535474d822615f846917254d586d2a5a31,1666027530,"Removed XLMModel inheritance from FlaubertModel(torch+tf) (#19432)

* FlaubertModel inheritance from XLMModel removed

* Fix style and add FlaubertPreTrainedModel to __init__

* Fix formatting issue

* Fix Typo and repo-consistency

* Fix style

* add FlaubertPreTrainedModel to TYPE_HINT

* fix repo consistency

* Update src/transformers/models/flaubert/modeling_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/flaubert/modeling_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/flaubert/modeling_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/flaubert/modeling_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/flaubert/modeling_tf_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/flaubert/modeling_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/flaubert/modeling_tf_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/flaubert/modeling_flaubert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* removed redundant Copied from comments

* added missing copied from comments

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/flaubert/__init__.py', 'src/transformers/models/flaubert/modeling_flaubert.py', 'src/transformers/models/flaubert/modeling_tf_flaubert.py', 'src/transformers/utils/dummy_pt_objects.py']",FlaubertModel is incorrectly inheriting from XLMModel causing inconsistencies and formatting issues in the modelling functionalities of FlaubertModel.
e20faa6f0317657c3c03c61c7550d0b805911ddb,1635792945,"Add BeitForSemanticSegmentation (#14096)

* Add first draft

* Make forward pass work

* Improve conversion script

* Add notebook that checks if it works

* Add BeitForSemanticSegmentation to the tests

* More improvements

* Make BeitForSemanticSegmentation consistent with Segformer

* Small bug fix

* Add BeitForSemanticSegmentation to docs

* Make sure model doesn't output hidden states when the user doesn't want to

* Make it possible to convert the large model

* Fix issue

* Fix conversion script for large model

* Add auxiliary_head option to semantic segmentation model

* Apply suggestions from @sgugger's review

* Apply suggestions from code review

* Fix failing test

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>","['src/transformers/__init__.py', 'src/transformers/models/beit/__init__.py', 'src/transformers/models/beit/configuration_beit.py', 'src/transformers/models/beit/convert_beit_unilm_to_pytorch.py', 'src/transformers/models/beit/modeling_beit.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_beit.py', 'tests/test_modeling_common.py', 'utils/check_repo.py']","Semantic segmentation model lacks BeitForSemanticSegmentation implementation, causing inadequate forward pass execution and inconsistencies with Segformer. Also, the conversion script for large model is failing."
83bfdbdd7511360f944fc5ac5609d7f894a69041,1629964243,"Migrating conversational pipeline tests to new testing format (#13114)

* New test format for conversational.

* Putting back old mixin.

* Re-enabling auto tests with LazyLoading.

* Feature extraction tests.

* Remove feature-extraction.

* Feature extraction with feature_extractor (No pun intended).

* Update check_model_type for fill-mask.","['src/transformers/models/auto/auto_factory.py', 'src/transformers/pipelines/base.py', 'src/transformers/pipelines/conversational.py', 'src/transformers/pipelines/feature_extraction.py', 'src/transformers/pipelines/fill_mask.py', 'tests/test_pipelines_common.py', 'tests/test_pipelines_conversational.py', 'tests/test_pipelines_feature_extraction.py', 'tests/test_pipelines_fill_mask.py', 'tests/test_pipelines_text_classification.py']","The conversational pipeline tests are still in the old format and need to be transitioned to the new testing structure to accommodate changes like lazy loading, feature extraction, etc."
522a9ece4baeb5abfec8953ef76469a530e987d5,1655489643,"Save huggingface checkpoint as artifact in mlflow callback (#17686)

* Fix eval to compute rouge correctly for rouge_score

* styling

* moving sentence tokenization to utils from run_eval

* saving ckpt in mlflow

* use existing format of args

* fix documentation

Co-authored-by: Swetha Mandava <smandava@nvidia.com>",['src/transformers/integrations.py'],"Rouge score calculation in eval is incorrect, and checkpoints are not being saved in mlflow during callbacks."
c595b6e6a90c29242c9dbfeecc5b132fe2e3d1f4,1648065409,"Make Transformers use cache files when hf.co is down (#16362)

* Make Transformers use cache files when hf.co is down

* Fix tests

* Was there a random circleCI failure?

* Isolate patches

* Style

* Comment out the failure since it doesn't fail anymore

* Better comment","['src/transformers/configuration_utils.py', 'src/transformers/feature_extraction_utils.py', 'src/transformers/modeling_flax_utils.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/modeling_utils.py', 'src/transformers/tokenization_utils_base.py', 'src/transformers/utils/hub.py', 'tests/test_configuration_common.py', 'tests/test_feature_extraction_common.py', 'tests/test_modeling_common.py', 'tests/test_modeling_tf_common.py', 'tests/test_tokenization_common.py', 'tests/utils/test_offline.py']","When hf.co is down, Transformers are not using cache files, for a fallback making the application reliant on hf.co's uptime."
32295b15a131c07884b409e485c3b823bcb5a3dc,1645550476,"Gelu10 (#15676)

* Add GeLU10 (clipped version of GeLU) to transformers to improve quantization performances.

* Add unittests.

* Import tensorflow after `is_tf_available` check.

* Fix tensorflow wrong function `tf.tensor` to `tf.constant`

* style.

* use `tf.math.max`

* Fix tf tests.

* style.

* style style style style style style

* style style style style style style

* Address @sgugger comments.

* Fix wrong operator for raising ValueError for ClippedGELUActivation.","['src/transformers/activations.py', 'src/transformers/activations_tf.py', 'tests/test_activations.py', 'tests/test_activations_tf.py']","The transformers currently lack a clipped version of GeLU (known as GeLU10), affecting quantization performances. Also, there is a use of incorrect function in tensorflow (`tf.tensor` instead of `tf.constant`)."
6a08162ad4ef2e08de45301cc9197f2bee01cbef,1664565219,"Fix cached lookup filepath on windows for hub (#19178)

* Update hub.py commit_hash extraction

Add safety mechanism for windows systems to unify logic (replace double backslashes with /)

* Fix string quotetype

* Aaaa circleci is messing with me.

* Switch to using as_posix() method from pathlib

* Update src/transformers/utils/hub.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/utils/hub.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/utils/hub.py'],Filepath lookup in the caching mechanism is inconsistent on Windows systems due to improper handling of backslashes in paths.
68a894a5875bfd958b8254afd3bbb23db9c2e813,1659432850,"Fix uninitialized parameter in conformer relative attention. (#18368)

`torch.Tensor` creates an unitialized tensor (as via `torch.empty`), this leads to undeterministic behavior, poor initialization, and nans if you have unlucky init. The paper does not specify the initialization for bias terms, so I guess zero seems like a good choice - no bias initially. `torch.Tensor` is usually populated with zeros, so this fix will be close to the intended behavior:

```
>>> torch.Tensor(100, 100).sum()
tensor(0.)
>>> torch.Tensor(100, 100).sum()
tensor(nan)
>>> torch.Tensor(100, 100).sum()
tensor(0.)
```",['src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py'],"Uninitialized parameter in conformer relative attention leads to undeterministic behavior, poor initialization, and occasionally NaN values."
8632a60d33456a486d5218710622531379750dea,1632413727,"Add cpu distributed fine-tuning support for transformers Trainer API (#13574)

* update trainer with cpu distributed fine-tuning support.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Style.

* refinement on cpu dist training check.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* style.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Test over private field not public one.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Morgan Funtowicz <funtowiczmo@gmail.com>
Co-authored-by: Funtowicz Morgan <mfuntowicz@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/trainer.py', 'src/transformers/trainer_pt_utils.py', 'src/transformers/training_args.py']","Transformers Trainer API doesn't support distributed fine-tuning on CPU, limiting training options."
2eaaf17a0b0ab4c13cb1b1e87accd2d5dee47be4,1684930403,"Export to ONNX doc refocused on using optimum, added tflite (#23434)

* doc refocused on using optimum, tflite

* minor updates to fix checks

* Apply suggestions from code review

Co-authored-by: regisss <15324346+regisss@users.noreply.github.com>

* TFLite to separate page, added links

* Removed the onnx list builder

* make style

* Update docs/source/en/serialization.mdx

Co-authored-by: regisss <15324346+regisss@users.noreply.github.com>

---------

Co-authored-by: regisss <15324346+regisss@users.noreply.github.com>",['utils/check_table.py'],"The documentation on exporting to ONNX is not focused on using Optimum and lacks information on TFLite. Also, the ONNX list builder seems unnecessary."
50dd314d939a86f3a81e19af01459f449fbaeeca,1646843819,"Add ONNX export for ViT (#15658)

* Add ONNX support for ViT

* Refactor to use generic preprocessor

* Add vision dep to tests

* Extend ONNX slow tests to ViT

* Add dummy image generator

* Use model_type to determine modality

* Add deprecation warnings for tokenizer argument

* Add warning when overwriting the preprocessor

* Add optional args to docstrings

* Add minimum PyTorch version to OnnxConfig

* Refactor OnnxConfig class variables from CONSTANT_NAME to snake_case

* Add reasonable value for default atol

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/bart/configuration_bart.py', 'src/transformers/models/marian/configuration_marian.py', 'src/transformers/models/mbart/configuration_mbart.py', 'src/transformers/models/vit/__init__.py', 'src/transformers/models/vit/configuration_vit.py', 'src/transformers/onnx/__main__.py', 'src/transformers/onnx/config.py', 'src/transformers/onnx/convert.py', 'src/transformers/onnx/features.py', 'tests/onnx/test_onnx_v2.py']",Vision Transformer (ViT) lacks ONNX export support causing difficulties in model interoperability and deployment.
a3034c7004db43d1082babb7da8606f3676d38b9,1675800050,"Add inverse sqrt learning rate scheduler (#21495)

* added inverse sqrt lr scheduler

* Updated get_scheduler in src/transformers/optimization.py

* Updated src/transformers/__init__.py

* Added inverse sqrt lr scheduler test

* Updated docs/source/en/main_classes/optimizer_schedules.mdx

* Ran style and quality scripts

* Fix get_inverse_sqrt_schedule docstring

* Comment implementation URL","['src/transformers/__init__.py', 'src/transformers/optimization.py', 'src/transformers/trainer_utils.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/optimization/test_optimization.py']",Lack of an inverse square root learning rate scheduler in the optimization module negatively impacts the training of transformer models.
1417978cd49181fd08837e7722c34dd5c8c113e3,1632233413,"[SequenceFeatureExtractor] Rewrite padding logic from pure python to numpy (#13650)

* Test np padding

* Pass feature extraction tests

* Update type hints

* Fix flaky integration tests

* Try a more stable waveform

* Add to_numpy jax support

* int32 attention masks

* Refactor normalization tests","['src/transformers/feature_extraction_sequence_utils.py', 'src/transformers/file_utils.py', 'src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py', 'tests/test_feature_extraction_speech_to_text.py', 'tests/test_feature_extraction_wav2vec2.py', 'tests/test_modeling_speech_to_text.py', 'tests/test_pipelines_automatic_speech_recognition.py', 'tests/test_sequence_feature_extraction_common.py']",The padding logic in SequenceFeatureExtractor is written in pure Python which might be causing performance issues and integration test failures.
13a9c9a354d07f7c21e968f2ac4c7c948b7307a0,1628509070,"[Flax] Refactor gpt2 & bert example docs (#13024)

* fix_torch_device_generate_test

* remove @

* improve docs for clm

* speed-ups

* correct t5 example as well

* push final touches

* Update examples/flax/language-modeling/README.md

* correct docs for mlm

* Update examples/flax/language-modeling/README.md

Co-authored-by: Patrick von Platen <patrick@huggingface.co>","['examples/flax/language-modeling/run_clm_flax.py', 'examples/flax/language-modeling/run_mlm_flax.py']","Documentation for GPT2 and BERT examples in Flax is unclear, requires improvement; speed issues reported for CLM and T5 examples also need addressing."
a04d4bf2d7512097aea3cfadd13b7ec30ab81bc9,1628844353,"Fix flax gpt2 hidden states (#13109)

* Fix inconsistency of the last element in hidden_states between PyTorch/Flax GPT2(Neo) (#13102)

* Fix missing elements in outputs tuple

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Fix local variable 'all_hidden_states' referenced before assignment

* Fix by returning tuple containing None values

* Fix quality

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>","['src/transformers/models/gpt2/modeling_flax_gpt2.py', 'src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py']",Inconsistency in the final element of hidden_states in PyTorch/Flax GPT2(Neo) and missing elements in outputs tuple. Local variable 'all_hidden_states' is referenced prior to assignment.
2ef774211733f0acf8d3415f9284c49ef219e991,1662572329,"Add DocumentQuestionAnswering pipeline (#18414)

* [WIP] Skeleton of VisualQuestionAnweringPipeline extended to support LayoutLM-like models

* Fixup

* Use the full encoding

* Basic refactoring to DocumentQuestionAnsweringPipeline

* Cleanup

* Improve args, docs, and implement preprocessing

* Integrate OCR

* Refactor question_answering pipeline

* Use refactored QA code in the document qa pipeline

* Fix tests

* Some small cleanups

* Use a string type annotation for Image.Image

* Update encoding with image features

* Wire through the basic docs

* Handle invalid response

* Handle empty word_boxes properly

* Docstring fix

* Integrate Donut model

* Fixup

* Incorporate comments

* Address comments

* Initial incorporation of tests

* Address Comments

* Change assert to ValueError

* Comments

* Wrap `score` in float to make it JSON serializable

* Incorporate AutoModeLForDocumentQuestionAnswering changes

* Fixup

* Rename postprocess function

* Fix auto import

* Applying comments

* Improve docs

* Remove extra assets and add copyright

* Address comments

Co-authored-by: Ankur Goyal <ankur@impira.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/base.py', 'src/transformers/pipelines/document_question_answering.py', 'src/transformers/pipelines/question_answering.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_tf_objects.py', 'src/transformers/utils/fx.py', 'tests/models/layoutlm/test_modeling_layoutlm.py', 'tests/models/layoutlm/test_modeling_tf_layoutlm.py', 'tests/pipelines/test_pipelines_document_question_answering.py', 'tests/test_modeling_common.py', 'tests/test_modeling_tf_common.py']","Pipeline for Document Question Answering doesn't support LayoutLM-like models or OCR, can't handle invalid responses or empty word_boxes properly, and has issues with `score` not being JSON serializable."
06910f5a764ff247c57ffd0d449eafb025e2bac1,1687874826,"[`T5`] Add T5ForQuestionAnswering and MT5ForQuestionAnswering (#24481)

* Adding T5ForQuestionAnswering

* Changed weight initialization that results in better initial loss when fine-tuning

* Update to class variables

* Running make fixup

* Running make fix-copies

* Remove model_parallel

* Adding MT5ForQuestionAnswering

* Adding docs

* Fix wrong doc

* Update src/transformers/models/mt5/modeling_mt5.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/models/t5/modeling_t5.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* File formatting

* Undoing change

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/mt5/__init__.py', 'src/transformers/models/mt5/modeling_mt5.py', 'src/transformers/models/t5/__init__.py', 'src/transformers/models/t5/modeling_t5.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/t5/test_modeling_t5.py']",'T5 transformer model cannot be used for question-answering tasks due to the lack of T5ForQuestionAnswering and MT5ForQuestionAnswering classes.'
255257f3ea0862cbb92ea9fa1113cbee1898aadd,1674648583,"[Whisper] Refactor whisper (#21252)

* update whisper logit processor

* add generate for whisper

* remove part of the whisper specific code from pipeline

* update logit processes

* major update

* enforce first timestamp

* update generate

* add more tests

* update new decoding strategy

* Apply suggestions from code review

* update docstring

* fixup

* default config will not have multilingual ar

* update expected tokenizer size, see pull on the hub for whisper-tiny","['src/transformers/generation/logits_process.py', 'src/transformers/models/whisper/modeling_whisper.py', 'src/transformers/models/whisper/tokenization_whisper.py', 'src/transformers/pipelines/automatic_speech_recognition.py', 'tests/models/whisper/test_tokenization_whisper.py', 'tests/pipelines/test_pipelines_automatic_speech_recognition.py']","The Whisper pipeline has Whisper-specific code intertwined, and the token size is larger than expected with default configuration, potentially leading to inefficiencies and blocks in pipeline processing."
463226e2ee372ae48f473cd9f93917839f0901ff,1665760341,"Improve error messaging for ASR pipeline. (#19570)

* Improve error messaging for ASR pipeline.

- Raise error early (in `_sanitize`) so users don't waste time trying to
  run queries with invalid params.

- Fix the error was after using `config.inputs_to_logits_ratio` so our
  check was masked by the failing property does not exist.

- Added some manual check on s2t for the error message.
  No non ctc model seems to be used by the default runner (they are all
  skipped).

* Removing pdb.

* Stop the early error it doesn't really work :(.","['src/transformers/pipelines/automatic_speech_recognition.py', 'tests/pipelines/test_pipelines_automatic_speech_recognition.py']","Invalid parameters in ASR pipeline queries aren't detected early, wasting user time. Additionally, error checks are masked due to use of non-existent `config.inputs_to_logits_ratio` property."
acfb714bdf6a7fdc52c2f171783c878bea6be4ea,1677609694,"Improve TF weight loading, especially PT crossloading (#21792)

* First commit for the improved PT-TF weight loading

* Remove workarounds from TFEncoderDecoder tests

* Allow a custom weight renaming function in from_pretrained and use that to clean up EncoderDecoder

* make fixup

* First attempt at visionencoderdecoder

* Disable tensorfloat32 in tests to get consistent outputs

* Quick fix to tf_vision_encoder_decoder tests

* make fixup

* Update Blenderbot tests

* Remove unused arg in modeling_tf_opt

* load_tf_sharded_weights had strict=True! This meant transfer learning was impossible, so I'm setting it to False.

* Support prefixes when loading sharded TF checkpoints

* make fixup

* Add test to load sharded models with a weight prefix

* Fix sharded weight loading test

* Add a test for transfer from a sharded checkpoint

* make fixup

* Add test to check that crossloading from PT with a prefix works

* Refactor from_pretrained in the encoderdecoder classes

* Refactor from_pretrained in the encoderdecoder classes

* missmatched -> mismatched

* Explicitly check for None

* No comments showing my very impressive and attractive knowledge of Py3.9+

* Disable TF32 across all TF tests","['src/transformers/modeling_tf_pytorch_utils.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py', 'src/transformers/models/opt/modeling_tf_opt.py', 'src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py', 'tests/models/vision_encoder_decoder/test_modeling_tf_vision_encoder_decoder.py', 'tests/test_modeling_tf_common.py']",PT-TF weight loading is not sufficient and lacks a custom weight renaming function. Inconsistencies in test outputs and inability for loading sharded models with a weight prefix when using transfer learning.
66336dc18374cdba550759cc923c36217159d4c9,1655120984,"Add Visual Question Answering (VQA) pipeline (#17286)

* wip

* rebase

* all tests pass

* rebase

* ready for PR

* address comments

* fix styles

* add require_torch to pipeline test

* remove remote image to improve CI consistency

* address comments; fix tf/flax tests

* address comments; fix tf/flax tests

* fix tests; add alias

* repo consistency tests

* Update src/transformers/pipelines/visual_question_answering.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* address comments

* Update src/transformers/pipelines/visual_question_answering.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* merge

* Update src/transformers/models/auto/modeling_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* merge

Co-authored-by: Sijun He <sijunhe@Sijuns-MacBook-Pro.local>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/visual_question_answering.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/pipelines/test_pipelines_visual_question_answering.py']","The pipeline for Visual Question Answering (VQA) is missing, impacting the functionality related to visual question answering tasks."
c74befc9e328472241c175351da8a3af5b058978,1659612558,"HFTracer.trace can now take callables and torch.nn.Module (#18457)

* Enable HFTracer to trace with custom dummy inputs instead of pre-computed ones

* Add HFTracer.trace docstring, and make it possible to handle callable and torch.nn.Module in general

* Remove pdb comment

* Apply suggestions",['src/transformers/utils/fx.py'],"HFTracer.trace only works with pre-computed inputs, and cannot handle callable inputs or general torch.nn.Module."
6b217c52e626729bd5de7142358dbaf67402bb40,1668696200,"Add AutoBackbone + ResNetBackbone (#20229)

* Add ResNetBackbone

* Define channels and strides as property

* Remove file

* Add test for backbone

* Update BackboneOutput class

* Remove strides property

* Fix docstring

* Add backbones to SHOULD_HAVE_THEIR_OWN_PAGE

* Fix auto mapping name

* Add sanity check for out_features

* Set stage names based on depths

* Update to tuple

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/modeling_outputs.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/resnet/__init__.py', 'src/transformers/models/resnet/configuration_resnet.py', 'src/transformers/models/resnet/modeling_resnet.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/resnet/test_modeling_resnet.py', 'utils/check_repo.py']","There is a need for the addition of the ResNetBackbone, and a method for automatic backbone (AutoBackbone). It also requires a check on 'out_features', and setting of stage names based on depths."
d43e308e7f288157f9bc7f3ce0738c41890d3cf1,1642763441,"Add Swin Transformer (#15085)

* Add all files

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Updates

* Apply suggestions from review

* Fix failing tests

* Update __init__.py

* Update configuration_swin.py

* Update auto_factory.py

* Fix pytests

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Fix tests and default checkpoint

* Fix Recursion error

* Code quality

* Remove copied from

* Update modeling_swin.py

* Code quality

* Update modeling_swin.py

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

* Fix feature extractor

* Fix code quality

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

* Update configuration_swin.py

* Update default checkpoint

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update docs/source/model_doc/swin.mdx

Co-authored-by: Mishig Davaadorj <mishig.davaadorj@coloradocollege.edu>

* Update conversion script

* Reformat conversion script

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Mishig Davaadorj <mishig.davaadorj@coloradocollege.edu>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/swin/__init__.py', 'src/transformers/models/swin/configuration_swin.py', 'src/transformers/models/swin/convert_swin_timm_to_pytorch.py', 'src/transformers/models/swin/modeling_swin.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_swin.py']",The current software lacks Swin Transformer implementation causing limitations in our model's capabilities.
149483b25297236fb3e82db1a108c9e4f1ee42b5,1669049588,"Update Special Language Tokens for PLBART (#19980)

* Update Special Language Tokens for PLBART

* fix format

* making mapping for language codes and updating tests:

* fix format

* fix consistency

* add assert to both tokenizer tests.

* fix format

* Update src/transformers/models/plbart/tokenization_plbart.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* improvin readability, setting self.tgt_lang

* fixing

* readability

Co-authored-by: jordiclive <jordiclive19@imperial.ac.uk>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>","['src/transformers/models/plbart/modeling_plbart.py', 'src/transformers/models/plbart/tokenization_plbart.py', 'tests/models/plbart/test_tokenization_plbart.py']","Special Language Tokens for PLBART are outdated, causing formatting, consistency and readability issues. Current language codes mapping isn't correct and tests are failing."
e34dd055e9178e363005bcd1f0ac28cc0c1efbc1,1641378848,"Fix doc example: mask_time_indices (numpy) has no attribute 'to' (#15033)

* fix doc example - AttributeError: 'numpy.ndarray' object has no attribute 'to'

* fix more

* Apply suggestions from code review

* Update src/transformers/models/unispeech/modeling_unispeech.py

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/models/unispeech/modeling_unispeech.py', 'src/transformers/models/unispeech_sat/modeling_unispeech_sat.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py']","The documentation example for 'mask_time_indices' method is causing an AttributeError, as 'numpy.ndarray' object seemingly has no attribute 'to'."
f0b490151e4a851c0821e1f215bb7a26565d24f7,1665047041,"🚨 🚨 🚨 Fix ViT parameter initialization (#19341)

This PR aims to rectify the discrepancy between the training performances of HF and Timm ViT implementations.

- Initializes torch and flax ViT dense layer weights with trunc_normal instead of normal (consistent with the TF implementation.
- Initializes cls_token and positional_embeddings with trunc_normal
- Updates DeiT copy to reflect the changes","['src/transformers/models/deit/modeling_deit.py', 'src/transformers/models/vit/modeling_flax_vit.py', 'src/transformers/models/vit/modeling_tf_vit.py', 'src/transformers/models/vit/modeling_vit.py', 'src/transformers/models/vit_mae/modeling_vit_mae.py']","Discrepancies detected in the training performances of HF and Timm ViT implementations, potentially due to incorrect parameter initializations in ViT dense layer weights, cls_token and positional_embeddings."
a4beb37b81c9fd86355edae2a0dd5571de84acc9,1669816681,"fix ipex+fp32 jit trace error in ipex 1.13 (#20504)

error show like: “Currently the auto_kernel_selection does not support the grad mode! Please add torch.no_grad() before the inference runtime..”
since jit mode only work in inference mode, it's safe to add such logic.",['src/transformers/trainer.py'],"The ipex+fp32 JIT tracing in ipex 1.13 is currently triggering an error stating that the auto_kernel_selection doesn't support the grad mode, indicating an issue in handling different execution modes."
6a3a197fcd1f3ac9472cbe05e373ea11839f5d5d,1632409311,"Add SigOpt HPO to transformers trainer api (#13572)

* add sigopt hpo to transformers.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* extend sigopt changes to test code and others..

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Style.

* fix style for sigopt integration.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Add necessary information to run unittests on SigOpt.

Co-authored-by: Morgan Funtowicz <funtowiczmo@gmail.com>","['setup.py', 'src/transformers/__init__.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/integrations.py', 'src/transformers/testing_utils.py', 'src/transformers/trainer.py', 'src/transformers/trainer_utils.py', 'tests/test_trainer.py']","Transformers trainer API lacks integration with SigOpt for hyperparameter optimisation, causing difficulties in running unittests on SigOpt."
972fdcc77878cf7afcc8aef8979d6b4241005bb6,1690880212,"[`Docs`/`quantization`] Clearer explanation on how things works under the hood. + remove outdated info (#25216)

* clearer explanation on how things works under the hood.

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* add `load_in_4bit` in `from_pretrained`

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",['src/transformers/modeling_utils.py'],Documentation for quantization contains outdated information and lacks clarity about the under-the-hood operations.
9d99489f2f79b81fa9131c9299c236006dff94fb,1654693398,"Add TFData2VecVision for semantic segmentation (#17271)

* feat: initial implementation of data2vec segmentation model in TF.

* chore: minor corrections to make the segmenter work.

* chore: removed unncessary files.

* chore: add tests and other modifications.

* fix: loss computation for segmentation.

* chore: remove unused variable.

* chore: formatting.

* added a dummy adaptive pooling layer.

* removed unnecessary file.

* potentially add identifiers to layer names.

* fix: layer naming.

* chore: removed unnecessary print.

* Skipping unneeded test

* chore: add logging to debug tolerance.

* fix: segmentation tests for tfdata2vecvision

* chore: make style.

* fix: layer names, assertion to be resolved.

* Bumping test tolerance a bit

* chore: bump the tol in PT test.

Co-authored-by: matt <rocketknight1@gmail.com>","['src/transformers/__init__.py', 'src/transformers/modeling_tf_outputs.py', 'src/transformers/modeling_tf_pytorch_utils.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/data2vec/__init__.py', 'src/transformers/models/data2vec/modeling_tf_data2vec_vision.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/models/data2vec/test_modeling_data2vec_vision.py', 'tests/models/data2vec/test_modeling_tf_data2vec_vision.py']","Semantic segmentation model (Data2Vec) in TensorFlow has issues with layer naming, loss computation, and testing tolerances that require debugging and fixes."
b2a41d2be478c12b6222c99b59adfc8663f2adc6,1677782782,"Faster zero shot image (#21897)

* Make ZeroShotImageClassificationPipeline faster

The pipeline makes separate calls to model for each candidate label.
This commit combines all labels into one call.
Original code takes more that 60 seconds to process one image and 1000
candidate labels. Updated code takes less than 2 seconds.

* implement batching

* code formatting

* Creating an even faster zero-shot-image-classifiction.

Unfortunately super tailored towards CLIP.

Co-Authored-By: Yessen Kanapin <yessen@deepinfra.com>

* Quality.

* Cleanup.

* Order different on the CI it seems.

* Cleanup.

* Quality.

---------

Co-authored-by: Yessen Kanapin <yessen@deepinfra.com>",['src/transformers/pipelines/zero_shot_image_classification.py'],"The ZeroShotImageClassificationPipeline is performing slowly, taking over 60 seconds to process an image with 1000 candidate labels, due to separate model calls for each label."
88399476c3892435395618ed37993176dbb0de73,1682617948,"Fix bigbird random attention (#21023)

* switch np.random.permutation to jax.random.permuation

* remove comments

* remove leftover comment

* skip similarity tests

* modify indices_prng_key usage, add deterministic behaviour

* update style

* remove unused import

* remove copy statement since classes are not identical

* remove numpy import

* revert removing copied from statements

* make style from copied

* remove copied from statement

* update copied from statement to include only np.ndarry

* add deterministic args, unittestskip equivalence tests","['src/transformers/models/big_bird/modeling_flax_big_bird.py', 'tests/models/big_bird/test_modeling_flax_big_bird.py', 'tests/test_modeling_flax_common.py']","The BigBird model exhibits non-deterministic behavior and suffers from unnecessary imports and copied statements, leading to possible inconsistencies or errors in attention permutation."
457d4a324586133cadb3598697d9a5d5e37de8d2,1655131446,"Add Ray's scope to training arguments (#17629)

* allow scope from trainer arg

* add ray_scope to training args

* escape double quotes

* make style && quality

* attempt to solve doc style issues

* splitting up URLs for style

* make fixup

* Update src/transformers/training_args.py

Co-authored-by: Antoni Baum <antoni.baum@protonmail.com>

* make style

Co-authored-by: Antoni Baum <antoni.baum@protonmail.com>","['src/transformers/integrations.py', 'src/transformers/training_args.py']","Ray's scope is not included in the training arguments, causing it to not be accessible from within the trainer."
62ceb4d661ce644ee9377ac8053cbb9afa737125,1661515915,"[Wav2vec2 + LM Test] Improve wav2vec2 with lm tests and make torch version dependent for now (#18749)

* add first generation tutorial

* remove generation

* make version dependent expected values

* Apply suggestions from code review

* Update tests/models/wav2vec2_with_lm/test_processor_wav2vec2_with_lm.py

* fix typo",['tests/models/wav2vec2_with_lm/test_processor_wav2vec2_with_lm.py'],"Tests and tutorials for wav2vec2 with language models are not version dependent, leading to inconsistencies and potential failure in certain versions of PyTorch."
d7754c43d0e4b9b70205eacef9d2a0ee35661616,1666012521,"Type hints MCTCT (#19618)

* add type hints to mctct

* run auto style corrections

* change torch.bool to bool#

* Update src/transformers/models/mctct/modeling_mctct.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* Remove optional tags for attention_mask and head_mask'

* fix optional tags'

* Update src/transformers/models/mctct/modeling_mctct.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>",['src/transformers/models/mctct/modeling_mctct.py'],Lack of type hints in MCTCT models and incorrect optional tag usage for attention_mask and head_mask can lead to inconsistencies and potential errors.
fe65657de112531a2d5303491f245f9e7534ae8d,1672298365,"Fix FP16 inference in TextGenerationPipeline (#20913)

* add torch_dtype attribute to Pipeline

* Use torch_dtype to cast input tensor type in AutomaticSpeechRecognitionPipeline

* Fix code quality

* Add TextGenerationPipeline fp16 test

* Fix code quality

* Remove useless require in tests

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>","['src/transformers/pipelines/automatic_speech_recognition.py', 'src/transformers/pipelines/base.py', 'tests/pipelines/test_pipelines_text_generation.py']","FP16 inference is not correctly working in TextGenerationPipeline, causing problems in the AutomaticSpeechRecognitionPipeline as well as failing tests."
a88a4dae19861aa3b386182930dfd66f38c03b39,1678978586,"Temporarily fix ONNX model exporting error (#21830)

* Temporarily fix https://github.com/microsoft/onnx-converters-private/issues/143

* Reduced column width

* Fix formatting.

* Revert ""Temporarily fix https://github.com/microsoft/onnx-converters-private/issues/143""

This reverts commit 6e95a108042118d204da447729f3834affa354fc.

* Fix export error.

* Revert ""Fix formatting.""

This reverts commit 8310f60da10358edbdf77a2a2f3c83ee55066cb8.

* Propagated changes made in SwinV2 to Swin2SR","['src/transformers/models/swin2sr/modeling_swin2sr.py', 'src/transformers/models/swinv2/modeling_swinv2.py']","Issue with ONNX model exporting, leading to errors when trying to export. Changes made in SwinV2 not propagating to Swin2SR as expected."
7418a48e34875092f3213753b890cfb2f5dcb32a,1665159844,"Removed `Bert` interdependency in `tokenization_electra.py` (#19356)

* Copied from BertTokenizer() in tokenization_bert

* Added BasicTokenizer and WordPieceTokenizer Class

* Update src/transformers/models/electra/tokenization_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Added copied from comments for basicTokenizer and WordPieceTokenizer

* Updated the comments for the tokenizerClasses

* Update src/transformers/models/electra/tokenization_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/electra/tokenization_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Formatted tokenization_electra with `make style`

* Fix repo inconsistencies

* Update src/transformers/models/electra/tokenization_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Set the logger

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/models/electra/tokenization_electra.py'],"`tokenization_electra.py` has an unnecessary dependency on `BertTokenizer()` from `tokenization_bert`, causing interdependency issues."
cbbeca3d1733aa7c9b443af5ff231a5affcd8a1e,1668162977,"[OWL-ViT] Make model consistent with CLIP (#20144)

* Apply fix

* Fix test

* Remove another argument which is not used

* Fix pipeline test

* Add argument back, add deprecation warning

* Add warning add other location

* Use warnings instead

* Add num_channels to config

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>","['src/transformers/models/owlvit/configuration_owlvit.py', 'src/transformers/models/owlvit/modeling_owlvit.py', 'tests/models/owlvit/test_modeling_owlvit.py']",Inconsistent model configuration with CLIP in OWL-ViT leading to unused arguments and failing tests.
7e348aac96dcc14cbe59496f0a1eb7df30a154d5,1665143942,"Making `ConvBert Tokenizer` independent from `bert Tokenizer` (#19347)

* ConvBert

* added comment

* Updated

* Final_updates

* Update tokenization_convbert.py

* Update tokenization_convbert_fast.py

* Update tokenization_convbert.py

* Update tokenization_convbert.py

* Update tokenization_convbert_fast.py

* Update tokenization_convbert.py

* Update tokenization_convbert_fast.py

* Updates

* Updates

* Updated

* Final Updates","['src/transformers/models/convbert/tokenization_convbert.py', 'src/transformers/models/convbert/tokenization_convbert_fast.py']","The ConvBert tokenizer is dependent on the BERT tokenizer, causing potential issues when extending or customizing this functionality."
457dd4392bb0caa8c30b33fc458e6d5e2d5443c5,1643055484,"[Examples] Correct run ner label2id for fine-tuned models (#15017)

* up

* up

* make style

* apply sylvains suggestions

* apply changes to accelerate as well

* more changes

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['examples/pytorch/token-classification/run_ner.py', 'examples/pytorch/token-classification/run_ner_no_trainer.py']","Fine-tuned models encounter an issue with incorrect label to ID mapping during runtime, affecting overall performance and reliability."
f0dde6012784c79155f7c1bd721864ea8ba40821,1625757471,"[model.from_pretrained] raise exception early on failed load (#12574)


* [model.from_pretrained] raise exception early on failed load

Currently if `load` pretrained weights fails in `from_pretrained`, we first print a whole bunch of successful messages and then fail - this PR puts the exception first to avoid all the misleading messages.

* style

Co-authored-by: Suraj Patil <surajp815@gmail.com>",['src/transformers/modeling_utils.py'],"Misleading success messages are printed when `load` fails to load pretrained weights in `from_pretrained`, which can cause confusion."
18643ff29a946c4d21b67d288e6da98bb0c1b169,1663703538,"Skip `test_export_to_onnx` for `LongT5` if `torch` < 1.11 (#19122)

* Skip if torch < 1.11

* fix quality

* fix import

* fix typo

* fix condition

* fix condition

* fix condition

* fix quality

* fix condition

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",['tests/models/longt5/test_modeling_longt5.py'],`test_export_to_onnx` for `LongT5` fails if the version of `torch` is less than 1.11.
91ff7efeeb3e6bb10d83702db24108bb6583e013,1674472547,"[DETR and friends] Use AutoBackbone as alternative to timm (#20833)

* First draft

* More improvements

* Add conversion script

* More improvements

* Add docs

* Address review

* Rename class to ConvEncoder

* Address review

* Apply suggestion

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update all DETR friends

* Add corresponding test

* Improve test

* Fix bug

* Add more tests

* Set out_features to last stage by default

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/conditional_detr/configuration_conditional_detr.py', 'src/transformers/models/conditional_detr/modeling_conditional_detr.py', 'src/transformers/models/deformable_detr/configuration_deformable_detr.py', 'src/transformers/models/deformable_detr/modeling_deformable_detr.py', 'src/transformers/models/detr/configuration_detr.py', 'src/transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py', 'src/transformers/models/detr/convert_detr_to_pytorch.py', 'src/transformers/models/detr/modeling_detr.py', 'src/transformers/models/table_transformer/configuration_table_transformer.py', 'src/transformers/models/table_transformer/modeling_table_transformer.py', 'tests/models/conditional_detr/test_modeling_conditional_detr.py', 'tests/models/deformable_detr/test_modeling_deformable_detr.py', 'tests/models/detr/test_modeling_detr.py', 'tests/models/table_transformer/test_modeling_table_transformer.py']","DETR model doesn't have an alternative to 'timm', limiting its customization and usage; testing of this feature also appears to be missing."
0c70f145d1ba79773f7fa532a5f05486e260200a,1638985196,"Added support for other features for already supported models (#14358)

* Added support for other features for already supported models

* Partial support for causal and seq2seq models

* Partial support for causal and seq2seq models

* OnnxSeq2SeqConfigWithPast to support seq2seq models

* Parameterized the onnx tests

* Restored run_mlm.py

* Restored run_mlm.py

* [WIP] BART update

* BART and MBART

* Added comments

* Another sequence length of the past_key_values","['src/transformers/models/albert/configuration_albert.py', 'src/transformers/models/bart/configuration_bart.py', 'src/transformers/models/bert/configuration_bert.py', 'src/transformers/models/distilbert/configuration_distilbert.py', 'src/transformers/models/gpt2/configuration_gpt2.py', 'src/transformers/models/gpt_neo/configuration_gpt_neo.py', 'src/transformers/models/mbart/configuration_mbart.py', 'src/transformers/models/roberta/configuration_roberta.py', 'src/transformers/models/t5/configuration_t5.py', 'src/transformers/models/xlm_roberta/configuration_xlm_roberta.py', 'src/transformers/onnx/__init__.py', 'src/transformers/onnx/__main__.py', 'src/transformers/onnx/config.py', 'src/transformers/onnx/convert.py', 'src/transformers/onnx/features.py', 'tests/test_onnx_v2.py']","The current models lack support for additional features, including causal and seq2seq."
20ac86c6f1caa821056a615dfd6c787c40f11d7a,1682512530,"Add TensorFlow Wav2Vec2 for sequence classification (#22073)

* Add initial changes for TF wav2vec2 for sequence classification

* Add suggested changes

* Add serving and serving output methods

* Add serving_output implementation and fix layer_weights

* Add fixes

* Fixed test cases

* Fixing test and adding suggested changes","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/wav2vec2/__init__.py', 'src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/models/wav2vec2/test_modeling_tf_wav2vec2.py']","Wav2Vec2 model for sequence classification in TensorFlow lacks implementation, causing issues with serving methods, layer weights and failing test cases."
e4d2588573f2c68eb792f2d11f092eb2c562bef5,1656599838,"[Pipelines] Add revision tag to all default pipelines (#17667)

* trigger test failure

* upload revision poc

* Update src/transformers/pipelines/base.py

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* up

* add test

* correct some stuff

* Update src/transformers/pipelines/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* correct require flag

Co-authored-by: Julien Chaumond <julien@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/base.py', 'tests/pipelines/test_pipelines_common.py']","Default pipelines are missing revision tags, leading to difficulty in tracking changes and potential issues with version control."
4a419d4995111c22d6842ee1bcd2d3f500150845,1652215198,"MobileBERT tokenizer tests (#16896)

* unhardcode pretrained model path, make it a class var

* add tests for mobilebert tokenizer

* allow tempfiles for vocab & merge similarity test to autodelete

* add explanatory comments

* remove unused imports, let make style do its.. thing

* remove inheritance and use BERT tok tests for MobileBERT

* Update tests/mobilebert/test_tokenization_mobilebert.py

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* amend class names, remove unused import, add fix for mobilebert's hub pathname

* unhardcode pretrained model path, make it a class var

* add tests for mobilebert tokenizer

* allow tempfiles for vocab & merge similarity test to autodelete

* add explanatory comments

* remove unused imports, let make style do its.. thing

* remove inheritance and use BERT tok tests for MobileBERT

* Update tests/mobilebert/test_tokenization_mobilebert.py

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* amend class names, remove unused import, add fix for mobilebert's hub pathname

* amend paths for model tests being in models/ subdir of /tests

* explicitly rm test from prev path

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>",['tests/models/mobilebert/test_tokenization_mobilebert.py'],"MobileBERT tokenizer lacks proper tests and its pretrained model paths are hardcoded, which inhibits adaptability across different setups."
4046e66e4064bff4b013a94bdb5c08c0e2448a2d,1630160549,"examples: only use keep_linebreaks when reading TXT files (#13320)

* examples: only use keep_linebreaks when reading TXT files for all CLM examples

* examples: only use keep_linebreaks when reading TXT files for all CLM examples

* examples: only use keep_linebreaks when reading TXT files for all CLM examples","['examples/flax/language-modeling/run_clm_flax.py', 'examples/pytorch/language-modeling/run_clm.py', 'examples/pytorch/language-modeling/run_clm_no_trainer.py', 'examples/tensorflow/language-modeling/run_clm.py']",The `keep_linebreaks` function is inaccurately applied when reading non-TXT files in all CLM examples.
43f953cc2eec804eba04e2a9ae164d1a33fd97a8,1638438095,"Add CodeParrot 🦜 codebase (#14536)

* add readme skeleton

* update readme

* add initialization script

* add deduplication script

* add codeparrot training script

* add code generation evaluation

* add validation loss script

* add requirements

* update readme

* tweak readme

* make style

* add highlights to readme

* add CLIs to scripts

* add tokenizer training script

* add docstring to constant length dataset

* fix defaults in arguments

* update readme with cli

* move image to hub

* tweaks of readme

* fix cli commands

* add author

* explain env variables

* fix formatting

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Apply suggestions from code review

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* replace generic with gpt2 tokenizer

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>","['examples/research_projects/codeparrot/scripts/arguments.py', 'examples/research_projects/codeparrot/scripts/bpe_training.py', 'examples/research_projects/codeparrot/scripts/codeparrot_training.py', 'examples/research_projects/codeparrot/scripts/human_eval.py', 'examples/research_projects/codeparrot/scripts/initialize_model.py', 'examples/research_projects/codeparrot/scripts/preprocessing.py', 'examples/research_projects/codeparrot/scripts/validation_loss.py']","The CodeParrot project lacks essential components such as initialization, deduplication and training scripts, documentation, CLI commands, and a detailed README. There are also issues with argument defaults and formatting."
fe5e7cea4ac9c7b0cca8c33b86a24827e8331311,1663591052,"Add type hints for TF MPNet models (#19089)

* Added type hints for TFMPNetModel

* Added type hints for TFMPNetForMaskedLM

* Added type hints for TFMPNetForSequenceClassification

* Added type hints for TFMPNetForMultipleChoice

* Added type hints for TFMPNetForTokenClassification

* Added Type hints for TFMPNetForQuestionAnswering",['src/transformers/models/mpnet/modeling_tf_mpnet.py'],Lack of type hints in various TF MPNet models could lead to potential misunderstandings and errors in interfacing or using these models.
0835119bf368b07490e413d216f49f07f8731c9d,1646908484,"Add Document Image Transformer (DiT) (#15984)

* Add conversion script

* Improve script

* Fix bug

* Add option to push to hub

* Add support for classification models

* Update model name

* Upload feature extractor files first

* Remove hash checking

* Fix config

* Add id2label

* Add import

* Fix id2label file name

* Fix expected shape

* Add model to README

* Improve docs

* Add integration test and fix CI

* Fix code style

* Add missing init

* Add model to SPECIAL_MODULE_TO_TEST_MAP

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/dit/convert_dit_unilm_to_pytorch.py', 'tests/dit/test_modeling_dit.py', 'utils/tests_fetcher.py']","The software currently lacks a Document Image Transformer, which restricts its capabilities to convert scripts, support classification models, and manage model names, feature extractor files, and id2label files effectively."
07e94bf1593424824cc9c6bf4a5e045d58f10707,1664972835,"Maskformer post-processing fixes and improvements (#19172)

- Improves MaskFormer docs, corrects minor typos
- Restructures MaskFormerFeatureExtractor.post_process_panoptic_segmentation for better readability, adds target_sizes argument for optional resizing
- Adds post_process_semantic_segmentation and post_process_instance_segmentation methods.
- Adds a deprecation warning to post_process_segmentation method in favour of post_process_instance_segmentation","['src/transformers/models/maskformer/feature_extraction_maskformer.py', 'src/transformers/models/maskformer/modeling_maskformer.py', 'tests/models/maskformer/test_feature_extraction_maskformer.py']","MaskFormer post-processing methods lack an option for optional resizing and has confusing structure, additionally, a deprecation warning is missing for outdated segmentation methods."
84c9bf74212728d337571e9b830ea4eabae34a0c,1670255777,"cross platform from_pretrained (#20538)

* add support for `from_pt`

* add tf_flax utility file

* Update src/transformers/modeling_tf_flax_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove flax related modifications

* add test

* remove FLAX related commits

* fixup

* remove safetensor todos

* revert deletion

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/modeling_tf_utils.py', 'tests/test_modeling_tf_common.py']","The current `from_pretrained` method lacks cross-platform support, necessitating platform-specific adaptations each time it's used."
5c4c869014f5839d04c1fd28133045df0c91fd84,1661933133,"Add LayoutLMForQuestionAnswering model (#18407)

* Add LayoutLMForQuestionAnswering model

* Fix output

* Remove TF TODOs

* Add test cases

* Add docs

* TF implementation

* Fix PT/TF equivalence

* Fix loss

* make fixup

* Fix up documentation code examples

* Fix up documentation examples + test them

* Remove LayoutLMForQuestionAnswering from the auto mapping

* Docstrings

* Add better docstrings

* Undo whitespace changes

* Update tokenizers in comments

* Fixup code and remove `from_pt=True`

* Fix tests

* Revert some unexpected docstring changes

* Fix tests by overriding _prepare_for_class

Co-authored-by: Ankur Goyal <ankur@impira.com>","['src/transformers/__init__.py', 'src/transformers/models/layoutlm/__init__.py', 'src/transformers/models/layoutlm/modeling_layoutlm.py', 'src/transformers/models/layoutlm/modeling_tf_layoutlm.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_tf_objects.py', 'src/transformers/utils/fx.py', 'tests/models/layoutlm/test_modeling_layoutlm.py', 'tests/models/layoutlm/test_modeling_tf_layoutlm.py', 'utils/check_repo.py']","Existing implementation lacks a model for LayoutLMForQuestionAnswering, causing difficulties in question answering tasks."
ab856f68dfdcd2a24afabad1a21a1906d08eac63,1665499523,"Decouples `XLMProphet` model from `Prophet`  (#19406)

* decouples xlm_prophet from prophet and adds copy patterns that pass the copy check

* adds copy patterns to copied docstrings too

* restores autodoc for XLMProphetNetModel

* removes all-casing in a bunch of places to ensure that the model is compatible with all checkpoints on the hub

* adds missing model to main init

* adds autodocs to make document checker happy

* adds missing pretrained model import

* adds missing pretrained model import to main init

* adds XLMProphetNetPreTrainedModel to the dummy pt objects

* removes examples from the source-doc file since docstrings contain them already

* adds a missing new line to make check_repo happy","['src/transformers/__init__.py', 'src/transformers/models/xlm_prophetnet/__init__.py', 'src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py', 'src/transformers/utils/dummy_pt_objects.py']","`XLMProphet` model is tightly coupled with `Prophet`, causing potential issues with model compatibility and misalignments with checkpoints on the model hub. There's also a lack of proper documentation and code structure for this model."
a54961c5f70ff01ca3d62a56ece083096b7c1a7d,1641819200,"Make OpenAIGPTTokenizer work with SpaCy 2.x and 3.x (#15019)

* Make OpenAIGPTTokenizer work with SpaCy 3.x

SpaCy 3.x introduced an API change to creating the tokenizer that
breaks OpenAIGPTTokenizer. The old API for creating the tokenizer in
SpaCy 2.x no longer works under SpaCy 3.x, but the new API for creating
the tokenizer in SpaCy 3.x DOES work under SpaCy 2.x. Switching to the
new API should allow OpenAIGPTTokenizer to work under both SpaCy 2.x and
SpaCy 3.x versions.

* Add is_spacy_available and is_ftfy_available methods to file utils

* Add spacy and ftfy unittest decorator to testing utils

* Add tests for OpenAIGPTTokenizer that require spacy and ftfy

* Modify CircleCI config to run tests that require spacy and ftfy

* Remove unneeded unittest decorators are reuse test code

* Run make fixup","['src/transformers/file_utils.py', 'src/transformers/models/openai/tokenization_openai.py', 'src/transformers/testing_utils.py', 'tests/test_tokenization_openai.py']","OpenAIGPTTokenizer is not compatible with SpaCy 3.x due to an API change in tokenizer creation, and fails to function properly."
48463ebb33c4a3f4035dbdaf55dc43778304f318,1639671734,"Add Speaker Diarization and Verification heads (#14723)

* Models

* Squashed commit of the following:

commit 72278e1e931a16d0879acc77f65762f3364833d0
Author: anton-l <aglozhkov@gmail.com>
Date:   Fri Dec 10 21:45:08 2021 +0300

* Add unispeech heads

* Add sd/sv automodels

* Docs cleanup

* Fix docstrings

* rename xvector classes

* examples

* Tests cleanup

* Style

* Better checkpoints for tests

* leftover docs

* apply review suggestions

* Style + init tests

* Update unispeech-sat tdnn downsampling","['src/transformers/__init__.py', 'src/transformers/file_utils.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/sew/modeling_sew.py', 'src/transformers/models/sew_d/modeling_sew_d.py', 'src/transformers/models/unispeech/modeling_unispeech.py', 'src/transformers/models/unispeech_sat/__init__.py', 'src/transformers/models/unispeech_sat/configuration_unispeech_sat.py', 'src/transformers/models/unispeech_sat/convert_unispeech_original_s3prl_checkpoint_to_pytorch.py', 'src/transformers/models/unispeech_sat/modeling_unispeech_sat.py', 'src/transformers/models/wav2vec2/__init__.py', 'src/transformers/models/wav2vec2/configuration_wav2vec2.py', 'src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_unispeech_sat.py', 'tests/test_modeling_wav2vec2.py', 'utils/update_metadata.py']",The software lacks Speaker Diarization and Verification functionalities in the current unispeech model which compromises the output quality.
c44d3675c285278406722b0fa9eb7afff2a3d434,1645554404,"Time stamps for CTC models (#15687)

* [Wav2Vec2 Time Stamps]

* Add first version

* add word time stamps

* Fix

* save intermediate space

* improve

* [Finish CTC Tokenizer]

* remove @

* remove @

* push

* continue with phonemes

* up

* finish PR

* up

* add example

* rename

* finish

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* correct split

* finalize

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/hubert/configuration_hubert.py', 'src/transformers/models/sew/configuration_sew.py', 'src/transformers/models/sew_d/configuration_sew_d.py', 'src/transformers/models/unispeech/configuration_unispeech.py', 'src/transformers/models/unispeech_sat/configuration_unispeech_sat.py', 'src/transformers/models/wav2vec2/configuration_wav2vec2.py', 'src/transformers/models/wav2vec2/tokenization_wav2vec2.py', 'src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py', 'src/transformers/models/wavlm/configuration_wavlm.py', 'tests/test_tokenization_wav2vec2.py', 'tests/test_tokenization_wav2vec2_phoneme.py']","There is a lack of time-stamp functionality in CTC models, specifically with Wav2Vec2 and CTC Tokenizer. There's no provision to save intermediate spaces or work with phonemes.
"
7b0e2cfdfb2427c7ab72bb9801842b59276154ac,1678897639,"Fix: unfinished_sequences with correct device  (#22184)

Fix: unfinished_sequences with correct device 

The original code was causing errors when running torch.jit.trace due to the tensor options being incorrect. I fixed this by using torch.ones to create a tensor with the correct device and dtype. This should resolve the issue with running torch.jit.trace.",['src/transformers/generation/utils.py'],"Errors occur when running torch.jit.trace due to incorrect tensor options, causing issues with unfinished_sequences."
16f0b7d72c6d4e122957392c342b074aa2c5c519,1641920765,"Update ONNX docs (#14904)

* Remove docs for deprecated ONNX export

* Tidy up the CLI help messages

* Revamp ONNX docs

* Update auto-config table

* Use DistilBERT as example for consistency

* Wrap up first pass at ONNX docs

* Fix table check

* Add tweaks and introduction

* Add cross-ref

* Fix missing import

* Fix style

* Add permalinks to ONNX configs

* Clarify role of OrderedDict

* Update docs/source/serialization.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add doctest syntax to code blocks

* Remove permalinks

* Revert ""Remove permalinks""

This reverts commit 099701daf0db27823457867938efdb2d4f22a7c1.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/onnx/__main__.py', 'utils/check_table.py']","ONNX documentation is outdated and contains references to deprecated ONNX export. Several inconsistencies in examples and CLI help messages, and missing imports are highlighted."
18447c206dc5c2e10e230dab4ebb84a34ddd9437,1631102615,"Enable automated model list copying for localized READMEs (#13465)

* Complete basic mechanism

* Save

* Complete everything

* Style & Quality

* Update READMEs

* Add testing

* Fix README.md format

* Apply suggestions

* Fix format

* Update utils/check_copies.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['tests/test_utils_check_copies.py', 'utils/check_copies.py']","Localized READMEs lack an automated mechanism to copy model lists, leading to inconsistencies and manual updates."
fc1d97f29d7b98e82ae17fc5ac49229e2859bcca,1638291108,"VisionTextDualEncoder (#13511)

* init vision_text_dual_encoder

* fix merge

* remove extra heads

* fix tests

* remove VISION_TEXT_DUAL_ENCODER_PRETRAINED_CONFIG_ARCHIVE_MAP

* remove archive map

* fix imports

* fix more imports

* fix init

* delete tokenizers

* fix imports

* clean

* support clip's vision model

* handle None config

* begin tests

* more test and few fixes

* warn about newly init weights

* more tests

* add loss to model

* remove extra classes from doc

* add processor

* doc and small fixes

* add start docstr

* update flax model

* flax tests

* more flax tests

* doc

* quality

* doc and quality

* fix doc

* doc

* remove comments

* update warning

* quality

* fix docs

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* replace asserts, fix imports

* update imports

* fix import

* address some review comments

* fix check

* reduce tolerance

* fix test

* add flax integration test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* address Sylvain's comments

* fix style

* add pt_flax_equivalence test in PT tests

* add pt integration test

* update test

* use pre-trained checkpoint in examples

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/vision_text_dual_encoder/__init__.py', 'src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py', 'src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py', 'src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py', 'src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py', 'src/transformers/utils/dummy_flax_objects.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_flax_vision_text_dual_encoder.py', 'tests/test_modeling_vision_text_dual_encoder.py', 'tests/test_processor_vision_text_dual_encoder.py', 'utils/check_repo.py']","There are issues related to imports, tests, and documentation in the initial version of the VisionTextDualEncoder. Furthermore, handling of the None config and the removal of unnecessary entities such as extra heads, tokenizers, extra classes, and unused comments is required."
d23cf5b1f13cfefd2b5746d418f6dd466626d79e,1658382980,"Add support for Sagemaker Model Parallel >= 1.10 new checkpoint API (#18221)

* Add support for Sagemaker Model Parallel >= 1.10 new checkpoint API

* Support loading checkpoints saved with SMP < 1.10 in SMP < 1.10 and SMP >= 1.10

* Support loading checkpoints saved with SMP >= 1.10 in SMP >= 1.10

* Fix bug and styling

* Update based on reviewer feedback","['src/transformers/modeling_utils.py', 'src/transformers/trainer.py']",Incompatibility issue exists while loading checkpoints saved with different versions of Sagemaker Model Parallel (SMP) and also there's lack of support for SMP >= 1.10 new checkpoint API.
8635407bc724c45142c1f91dbc9ef3ea681e1a56,1645805506,"Fix tf.concatenate + test past_key_values for TF models (#15774)

* fix wrong method name tf.concatenate

* add tests related to causal LM / decoder

* make style and quality

* clean-up

* Fix TFBertModel's extended_attention_mask when past_key_values is provided

* Fix tests

* fix copies

* More tf.int8 -> tf.int32 in TF test template

* clean-up

* Update TF test template

* revert the previous commit + update the TF test template

* Fix TF template extended_attention_mask when past_key_values is provided

* Fix some styles manually

* clean-up

* Fix ValueError: too many values to unpack in the test

* Fix more: too many values to unpack in the test

* Add a comment for extended_attention_mask when there is past_key_values

* Fix TFElectra extended_attention_mask when past_key_values is provided

* Add tests to other TF models

* Fix for TF Electra test: add prepare_config_and_inputs_for_decoder

* Fix not passing training arg to lm_head in TFRobertaForCausalLM

* Fix tests (with past) for TF Roberta

* add testing for pask_key_values for TFElectra model

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/bert/modeling_tf_bert.py', 'src/transformers/models/electra/modeling_tf_electra.py', 'src/transformers/models/layoutlm/modeling_tf_layoutlm.py', 'src/transformers/models/rembert/modeling_tf_rembert.py', 'src/transformers/models/roberta/modeling_tf_roberta.py', 'src/transformers/models/tapas/modeling_tf_tapas.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py', 'tests/bert/test_modeling_tf_bert.py', 'tests/electra/test_modeling_tf_electra.py', 'tests/rembert/test_modeling_tf_rembert.py', 'tests/roberta/test_modeling_tf_roberta.py']",Issues with tf.concatenate method in TensorFlow models causing test failures specifically when past_key_values is provided for models like TFBertModel and TFElectra.
5f0d07b36bcd34988fd064596a7784aeadea4e09,1648076854,"Make BigBird model compatiable to fp16 dtype. (#16034)

* Make BigBird model compatiable to fp16 dtype.

* Use tree_map instead of map

* Reformat the code

* Fix import order

* Convert masks to the correct dtype

* Fix format issue

* Address comments.",['src/transformers/models/big_bird/modeling_big_bird.py'],BigBird model is not compatible with fp16 dtype causing errors when trying to use this specific data type.
d6cec45801ceb57f25d14f180f196f2a13b1ff26,1656699074,"XLA train step fixes (#17973)

* Copy inputs to train and test step before modifying them, as this breaks things

* Add XLA tests, fix our loss functions to be XLA-compatible

* make fixup

* Update loss computation test to expect vector of per-sample losses

* Patch loss for TFLED

* Patch loss for TFAlbert

* Add a tf_legacy_loss config flag that enables old loss functions

* Stop using config.get() because it's not a dict

* Skip loss computation test for RAG because its loss is very strange and I'm afraid to rewrite it

* make fixup

* Add XLA-compatible RAG loss

* Fix dtype of loss mask for TFAlbert

* Fix test for XLNet too because it overrides the default one

* make fixup

* Fix config test

* No more depending on GPU NaN behaviour

* Add test, avoid potential zero division

* Fix test item assignment

* Fix loss computation masking test

* make fixup

* Fix dtype bugs","['src/transformers/configuration_utils.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/models/albert/modeling_tf_albert.py', 'src/transformers/models/bert/modeling_tf_bert.py', 'src/transformers/models/led/modeling_tf_led.py', 'src/transformers/models/rag/modeling_tf_rag.py', 'tests/models/xlnet/test_modeling_tf_xlnet.py', 'tests/test_configuration_common.py', 'tests/test_modeling_tf_common.py', 'tests/utils/test_modeling_tf_core.py']","Inputs to train and test step are being modified, breaking functionality. TFAlbert, TFLED, RAG, XLNet loss functions are incompatible with XLA. Zero division and dtype bugs exist in loss computation and tests."
7d2a5fa749d22f403fe6ceac7d62c003240aee45,1674758079,"Update Hebrew language code to he per IANA registry (#21310)

Here's my original PR into whisper that changes the same: 
https://github.com/openai/whisper/pull/401

Per [IANA registry](https://www.iana.org/assignments/language-subtag-registry/language-subtag-registry), `iw` was deprecated as the code for Hebrew in 1989 and the preferred code is `he`

The correct subtag: 
```
%%
Type: language
Subtag: he
Description: Hebrew
Added: 2005-10-16
Suppress-Script: Hebr
%%
``` 
And the deprecation
```
%%
Type: language
Subtag: iw
Description: Hebrew
Added: 2005-10-16
Deprecated: 1989-01-01
Preferred-Value: he
Suppress-Script: Hebr
%%
```",['src/transformers/models/whisper/tokenization_whisper.py'],Hebrew language is incorrectly coded as 'iw' instead of its new preferred code 'he' according to IANA registry.
babeff5524bf3d5d62cfa70e1297158a755b0810,1654256422,"Add support for Perceiver ONNX export (#17213)

* Start adding perceiver support for ONNX

* Fix pad token bug for fast tokenizers

* Fix formatting

* Make get_preprocesor more opinionated (processor priority, otherwise tokenizer/feature extractor)

* Clean docs format

* Minor cleanup following @sgugger's comments

* Fix typo in docs

* Fix another docs typo

* Fix one more typo in docs

* Update src/transformers/onnx/utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/onnx/utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/onnx/utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/perceiver/configuration_perceiver.py', 'src/transformers/models/perceiver/modeling_perceiver.py', 'src/transformers/onnx/__main__.py', 'src/transformers/onnx/features.py', 'src/transformers/onnx/utils.py', 'tests/onnx/test_onnx_v2.py']","Perceiver model lacks support for ONNX export, and pad token bug detected in implementation for fast tokenizers."
6ce6d62b6f20040129ec9831e7c4f6576402ea42,1687368251,"Explicit arguments in `from_pretrained` (#24306)

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/configuration_utils.py', 'src/transformers/feature_extraction_utils.py', 'src/transformers/generation/configuration_utils.py', 'src/transformers/image_processing_utils.py', 'src/transformers/modeling_flax_utils.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/modeling_utils.py', 'src/transformers/models/align/configuration_align.py', 'src/transformers/models/altclip/configuration_altclip.py', 'src/transformers/models/blip/configuration_blip.py', 'src/transformers/models/blip_2/configuration_blip_2.py', 'src/transformers/models/chinese_clip/configuration_chinese_clip.py', 'src/transformers/models/clap/configuration_clap.py', 'src/transformers/models/clip/configuration_clip.py', 'src/transformers/models/clipseg/configuration_clipseg.py', 'src/transformers/models/flava/configuration_flava.py', 'src/transformers/models/git/configuration_git.py', 'src/transformers/models/groupvit/configuration_groupvit.py', 'src/transformers/models/owlvit/configuration_owlvit.py', 'src/transformers/models/pix2struct/configuration_pix2struct.py', 'src/transformers/models/x_clip/configuration_x_clip.py', 'src/transformers/processing_utils.py', 'src/transformers/tokenization_utils_base.py']","Arguments in the `from_pretrained` method are not explicit, causing confusion or potentially incorrect usage."
a40386669f2b4c147439bb4370cecd7b764ace8a,1666259831,"`image-segmentation` pipeline: re-enable `small_model_pt` test. (#19716)

* Re-enable `small_model_pt`.

Re-enable `small_model_pt`.

Enabling the current test with the current values.

Debugging the values on the CI.

More logs ? Printing doesn't work ?

Using the CI values instead. Seems to be a Pillow sensitivity.

* Update src/transformers/pipelines/image_segmentation.py

Co-authored-by: Alara Dirik <8944735+alaradirik@users.noreply.github.com>

Co-authored-by: Alara Dirik <8944735+alaradirik@users.noreply.github.com>",['tests/pipelines/test_pipelines_image_segmentation.py'],"""small_model_pt"" test in the image-segmentation pipeline is currently disabled, causing lack of proper testing and potential undetected issues in the pipeline."
de4d71ea07b31c1bcef7ffccc3691f76658e291f,1665164724,"Removed Bert dependency from BertGeneration code base. (#19370)

* Copied all the code required from transformers.models.bert.modeling_bert to here

* Fixed styling issues

* Reformatted copied names with Model specific name.

* Reverted BertEncoder part as there is already a class called BertGenerationEncoder

* Added prefixes in missing places.

Co-authored-by: vishwaspai <vishwas.pai@emplay.net>",['src/transformers/models/bert_generation/modeling_bert_generation.py'],"BertGeneration code relied heavily on direct code from Bert, causing overlap and naming conflicts and creating unwanted dependencies."
bb6f6d53386bf2340eead6a8f9320ce61add3e96,1662641430,"Add X-CLIP (#18852)

* First draft

* Improve conversion script

* Make vision encoder work

* More improvements

* Improve conversion script

* Fix quality

* Add MultiframeIntegrationTransformer

* More improvements

* Make MiT output work

* Fix quality

* Add prompts generator

* Add tests

* Fix some tests

* Fix some more tests

* Fix more tests

* Improve conversion script

* Fix model outputs

* Fix more tests

* Add XClipProcessor

* Use processor in conversion script

* Fix integration test

* Update README, fix docs

* Fix all tests

* Add MIT output to XClipOutput

* Create better variable names

* Rename XClip to XCLIP

* Extend conversion script

* Add support for large models

* Add support for 16 frame models

* Add another model'

* Fix module issue

* Apply suggestions from code review

* Add figure to docs

* Fix CLIPProcessor issue

* Apply suggestions from code review

* Delete file

* Convert more checkpoints

* Convert last checkpoint

* Update nielsr to microsoft","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/clip/__init__.py', 'src/transformers/models/x_clip/__init__.py', 'src/transformers/models/x_clip/configuration_x_clip.py', 'src/transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py', 'src/transformers/models/x_clip/modeling_x_clip.py', 'src/transformers/models/x_clip/processing_x_clip.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/models/x_clip/test_modeling_x_clip.py', 'utils/check_config_docstrings.py', 'utils/check_repo.py']","Multiple issues with the initial draft of X-CLIP, including problems with conversion script, vision encoder, quality, tests, and model outputs. Additionally, there seem to be issues related to the integration of MultiframeIntegrationTransformer and XClipProcessor."
7829c890db958279ca49519cc009e4f2def3fccb,1666796806,"Change the import of kenlm from github to pypi (#19770)

* Change the import of kenlm from github to pypi

* Change the import of kenlm from github to pypi in circleci config

* Fix code quality issues

* Fix isort issue, add kenlm in extras for audio

* Add kenlm to deps

* Add kenlm to deps

* Commit 'make fixup' changes

* Remove version from kenlm deps

* commit make fixup changes

* Remove manual installation of kenlm

* Remove manual installation of kenlm

* Remove manual installation of kenlm","['.circleci/create_circleci_config.py', 'setup.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/import_utils.py']","The kenlm package is being imported from Github, causing issues with installation dependencies, and manual installation is required."
862e8e4f4a150addb03225eadcb11d2209c5a317,1676066080,"Added timesformer configuration (#21446)

* Added timesformer configuration

Co-authored-by: JuheonChu <chuj@dickinson.edu>

* Create documentation_tests.txt

* Update documentation_tests.txt

Co-authored-by: JuheonChu <chuj@dickinson.edu>

* Delete documentation_tests.txt

Updates, Deleting ""src/transformers/utils/documentation_tests.txt"" file.

Co-authored-by: JuheonChu <chuj@dickinson.edu>

* Create documentation_tests.txt

Co-authored-by: JuheonChu <chuj@dickinson.edu>

* Delete documentation_tests.txt


Co-authored-by: JuheonChu <chuj@dickinson.edu>

---------

Co-authored-by: JuheonChu <chuj@dickinson.edu>",['src/transformers/models/timesformer/configuration_timesformer.py'],Timesformer configuration is missing in the project.
c0f99b4d2ec73090595914dde4c16da207e21d73,1680527252,"Fix llama tokenizer (#22402)

* draft

* update tokenization limma and conversion script

* more udpates

* initial commit

* style

* default pad to None

* draft tokenization tests

* update test

* update tokenization tests

* nits

* update

* versioning test

* major fix

* fix more testst

* finish fixing special masks

* last nit

* more nits

* add encode decode tests

* add more

* fix token type ids

* style","['src/transformers/models/llama/convert_llama_weights_to_hf.py', 'src/transformers/models/llama/tokenization_llama.py', 'tests/models/llama/test_tokenization_llama.py', 'tests/test_tokenization_common.py']","The llama tokenizer is experiencing issues with tokenization, conversion scripts, padding, tests, and token type ids, causing errors and inconsistencies."
4212bb0d60b63f81cc2ef44efec02b2203c65eed,1665657216,"[Re-submit] Compute true loss Flax examples (#19504)

* Compute true loss

* fixup

* final

* final

* final

* Update examples/flax/language-modeling/run_bart_dlm_flax.py

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* jax.tree_map => jax.tree_util.tree_map

* Compute true loss

* final

* fixup

* final

* final

* Update examples/flax/language-modeling/run_bart_dlm_flax.py

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* jax.tree_map => jax.tree_util.tree_map

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>","['examples/flax/image-captioning/run_image_captioning_flax.py', 'examples/flax/language-modeling/run_bart_dlm_flax.py', 'examples/flax/language-modeling/run_mlm_flax.py', 'examples/flax/language-modeling/run_t5_mlm_flax.py', 'examples/flax/summarization/run_summarization_flax.py']","The Flax examples are not computing true loss correctly, causing inaccurate results."
021f2ea9870a1fad154d4309762320a5379f4246,1642001612,"Add ONNX configuration classes to docs (#15121)

* Add ONNX classes to main package

* Remove permalinks from ONNX guide

* Fix ToC entry

* Revert ""Add ONNX classes to main package""

This reverts commit eb794a5b00d66b0b4eab234987301676d8357630.

* Add ONNX classes to main doc

* Fix syntax highlighting in doc

* Fix text

* Add FeaturesManager to doc

* Use paths to reference ONNX classes

* Add FeaturesManager to init

* Add missing ONNX paths","['src/transformers/onnx/__init__.py', 'src/transformers/onnx/convert.py']","ONNX configuration classes are missing from the main documentation and syntax highlighting issues are present. Additionally, FeaturesManager is missing from the init file."
0e0b7cb72a25c14613c13b1e9741504649170482,1665757082,"Allow usage of TF Text BertTokenizer on TFBertTokenizer to make it servable on TF Serving (#19590)

* add suport for non fast tf bert tokenizer

* add tests for non fast tf bert tokenizer

* fix fast bert tf tokenizer flag

* double tokenizers list on tf tokenizers test to aovid breaking zip on test output equivalence

* reformat code with black to comply with code quality checks

* trigger ci","['src/transformers/models/bert/tokenization_bert_tf.py', 'tests/models/bert/test_tokenization_bert_tf.py']","Usage of BertTokenizer on TFBertTokenizer is not servable on TF Serving, also tests for non fast tf bert tokenizer are missing."
efa889d2e4ba0605baa77d3c364735d0caaa9463,1667919823,"Add RocBert (#20013)

* add roc_bert

* update roc_bert readme

* code style

* change name and delete unuse file

* udpate model file

* delete unuse log file

* delete tokenizer fast

* reformat code and change model file path

* add RocBertForPreTraining

* update docs

* delete wrong notes

* fix copies

* fix make repo-consistency error

* fix files are not present in the table of contents error

* change RocBert -> RoCBert

* add doc, add detail test

Co-authored-by: weiweishi <weiweishi@tencent.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/roc_bert/__init__.py', 'src/transformers/models/roc_bert/configuration_roc_bert.py', 'src/transformers/models/roc_bert/modeling_roc_bert.py', 'src/transformers/models/roc_bert/tokenization_roc_bert.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/roc_bert/test_modeling_roc_bert.py', 'tests/models/roc_bert/test_tokenization_roc_bert.py']","Existing RocBert module is missing RocBertForPreTraining class, there are unwanted files present, module name inconsistencies are found, and documentation needs updating."
bad358398a6c55aa7db0378bd4681ce5584266e3,1652823734,"Add support for pretraining recurring span selection to Splinter (#17247)

* Add SplinterForSpanSelection for pre-training recurring span selection.

* Formatting.

* Rename SplinterForSpanSelection to SplinterForPreTraining.

* Ensure repo consistency

* Fixup changes

* Address SplinterForPreTraining PR comments

* Incorporate feedback and derive multiple question tokens per example.

* Update src/transformers/models/splinter/modeling_splinter.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/splinter/modeling_splinter.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Jean Vancoppenole <jean.vancoppenolle@retresco.de>
Co-authored-by: Tobias Günther <tobias.guenther@retresco.de>
Co-authored-by: Tobias Günther <github@tobigue.de>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/splinter/__init__.py', 'src/transformers/models/splinter/modeling_splinter.py', 'tests/models/splinter/test_modeling_splinter.py']","Splinter model currently lacks support for pre-training with recurring span selection, necessitating an additional extension for this functionality."
5739726fccaddde4506242974c5c27468ff86d81,1689088078,"fix: Text splitting in the BasicTokenizer (#22280)

* fix: Apostraphe splitting in the BasicTokenizer for CLIPTokenizer

* account for apostrophe at start of new word

* remove _run_split_on_punc, use re.findall instead

* remove debugging, make style and quality

* use pattern and punc splitting, repo-consistency will fail

* remove commented out debugging

* adds bool args to BasicTokenizer, remove pattern

* do_split_on_punc default True

* clean stray comments and line breaks

* rebase, repo-consistency

* update to just do punctuation split

* add unicode normalizing back

* remove redundant line","['src/transformers/models/bert/tokenization_bert.py', 'src/transformers/models/bert_japanese/tokenization_bert_japanese.py', 'src/transformers/models/clip/tokenization_clip.py', 'src/transformers/models/convbert/tokenization_convbert.py', 'src/transformers/models/distilbert/tokenization_distilbert.py', 'src/transformers/models/electra/tokenization_electra.py', 'src/transformers/models/funnel/tokenization_funnel.py', 'src/transformers/models/herbert/tokenization_herbert.py', 'src/transformers/models/layoutlm/tokenization_layoutlm.py', 'src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py', 'src/transformers/models/lxmert/tokenization_lxmert.py', 'src/transformers/models/mobilebert/tokenization_mobilebert.py', 'src/transformers/models/mpnet/tokenization_mpnet.py', 'src/transformers/models/openai/tokenization_openai.py', 'src/transformers/models/prophetnet/tokenization_prophetnet.py', 'src/transformers/models/retribert/tokenization_retribert.py', 'src/transformers/models/roc_bert/tokenization_roc_bert.py', 'src/transformers/models/roformer/tokenization_roformer.py', 'src/transformers/models/squeezebert/tokenization_squeezebert.py', 'src/transformers/models/tapas/tokenization_tapas.py', 'tests/models/bert/test_tokenization_bert.py', 'tests/models/clip/test_tokenization_clip.py']","BasicTokenizer incorrectly splits text at apostrophes, fails to handle apostrophes at the start of new words, and possibly lacks unicode normalization."
c4d78f01de30152f0e7a6f39d0901b85b0fd422d,1630506776,"Fix tokenizer saving during training with `Trainer` (#12806)

* add test in trainer and test tokenizer saving wi
th trainer

* quality

* reverse trainer changes

* replace test in test_trainer by a test for all the tokenizers

* format

* add can_save_slow_tokenizer attribute to all tokenizers

* fix Herbert

* format

* Change comment in error

* add comments and a new assert

* Update src/transformers/models/albert/tokenization_albert_fast.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change ValueError barthez

* change ValueError BigBird

* change ValueError Camembert

* change ValueError Mbart50

* change ValueError Pegasus

* change ValueError ReFormer

* change ValueError T5

* change ValueError RoBERTa

* XLNET fast

* Update tests/test_tokenization_common.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change `assert` into `self.assertIn`

* format

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/albert/tokenization_albert_fast.py', 'src/transformers/models/barthez/tokenization_barthez_fast.py', 'src/transformers/models/big_bird/tokenization_big_bird_fast.py', 'src/transformers/models/camembert/tokenization_camembert_fast.py', 'src/transformers/models/herbert/tokenization_herbert_fast.py', 'src/transformers/models/mbart50/tokenization_mbart50_fast.py', 'src/transformers/models/pegasus/tokenization_pegasus_fast.py', 'src/transformers/models/reformer/tokenization_reformer_fast.py', 'src/transformers/models/t5/tokenization_t5_fast.py', 'src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py', 'src/transformers/models/xlnet/tokenization_xlnet_fast.py', 'src/transformers/tokenization_utils_fast.py', 'tests/test_tokenization_common.py']","Issues found when saving the tokenizer during training using `Trainer`, affecting a variety of models including Albert, BigBird, Camembert, and more."
216dff7549386f3f9ce0b79fd058343b6883ede9,1695893263,"Do not warn about unexpected decoder weights when loading T5EncoderModel and LongT5EncoderModel (#26211)

Ignore decoder weights when using T5EncoderModel and LongT5EncoderModel

Both T5EncoderModel and LongT5EncoderModel do not have any decoder layers, so
loading a pretrained model checkpoint such as t5-small will give warnings about
keys found in the model checkpoint that are not in the model itself.

To prevent this log warning, r""decoder"" has been added to _keys_to_ignore_on_load_unexpected for
both T5EncoderModel and LongT5EncoderModel","['src/transformers/models/longt5/modeling_longt5.py', 'src/transformers/models/t5/modeling_t5.py']",Loading a pretrained model checkpoint in T5EncoderModel and LongT5EncoderModel gives log warnings due to unexpected decoder weights present in the model checkpoint.
3ef7134553868446b1dea03498b50d9076c4995c,1697465763,"Llama tokenizer: remove space in template comment (#26788)

* Remove space in template comment

I think the space between the eos and bos tokens is not present in the actual template output. I'm using this documentation as a reference for everyone asking about prompting, so would like to clarify whether there's a space or not :)

* Update fast tokenizer too

* Apply to Code Llama

* Link to original code snippet.","['src/transformers/models/code_llama/tokenization_code_llama.py', 'src/transformers/models/code_llama/tokenization_code_llama_fast.py', 'src/transformers/models/llama/tokenization_llama.py', 'src/transformers/models/llama/tokenization_llama_fast.py']","Discrepancy in the Llama tokenizer documentation regarding the existence of space between the eos and bos tokens, causing confusion when prompting."
183f442ba8536435e8772e2d4812c7a7a6a22493,1687262228,"Fix resuming PeftModel checkpoints in Trainer  (#24274)

* Fix resuming checkpoints for PeftModels

Fix an error occurred when resuming a PeftModel from a training checkpoint. That was caused since PeftModel.pre_trained saves only adapter-related data while _load_from_checkpoint was expecting a torch sved model. This PR fix this issue and allows the adapter checkpoint to be loaded.

Resolves: #24252

* fix last comment

* fix nits

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>",['src/transformers/trainer.py'],Resuming a PeftModel from a training checkpoint is causing an error due to the mismatch in saved data: PeftModel.pre_trained only saves adapter-related data while the function _load_from_checkpoint expects a torch saved model.
279bc5849b6e497810cfb39bc1e991d2ff7e6679,1651675078,"Allow saved_model export of TFCLIPModel in save_pretrained (#16886)

* CLIP Serving

* Add type hints per code review

* Use black, flake8, and isort

* Update src/transformers/models/clip/modeling_tf_clip.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Rollback serving_output and add TODO

* Remove irrelevant portions of failing tests

* Revert ""Rollback serving_output and add TODO""

This reverts commit a4abfa6ba3b7875a13538dbc2ddc4eb17dfcca8d.

* Rollback to original test/serving_output

* Fix unused var

* Apply suggestions from code review

* Update formatting with black

* Fix style again from rebase

* Update tests/models/clip/test_modeling_tf_clip.py

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>
Co-authored-by: Sean Moriarity <sean.l.moriarity.mil@army.mil>
Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>","['src/transformers/models/clip/modeling_tf_clip.py', 'tests/models/clip/test_modeling_tf_clip.py']",Unable to export TFCLIPModel in save_pretrained in the saved_model format.
6ee1474b67b088829555364a14ebfb45e661fac4,1654007805,"Accumulate tokens into batches in `PreTrainedTokenizerBase.add_tokens()` (#17119)

* Accumulate tokens into batches in PreTrainedTokenizerBase.add_tokens()

For tokenizers with a small number of special tokens or special tokens
with consecutive token IDs, this reduces the time complexity of creating
the trie from quadratic to linear, see also #16936.

* Extend explanation of batching added tokens",['src/transformers/tokenization_utils_base.py'],"In `PreTrainedTokenizerBase.add_tokens()`, adding tokens with a small number or consecutive IDs leads to quadratic time complexity for trie creation."
f5f430e5c80b85b57bb910435e45d84746210133,1663165060,"Add support for Japanese GPT-NeoX-based model by ABEJA, Inc. (#18814)

* add gpt-neox-japanese model and tokenizer as new model

* Correction to PR's comment for GPT NeoX Japanese
- Fix to be able to use gpu
- Add comment # Copied... at the top of RotaryEmbedding
- Implement nn.Linear instead of original linear class
- Add generation test under @slow

* fix bias treatment for gpt-neox-japanese

* Modidy gpt-neox-japanese following PR
- add doc for bias_dropout_add
- style change following a PR comment

* add document for gpt-neox-japanese

* remove unused import from gpt-neox-japanese

* fix README for gpt-neox-japanese","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/gpt_neox_japanese/__init__.py', 'src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py', 'src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py', 'src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_tokenizers_objects.py', 'tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py', 'tests/models/gpt_neox_japanese/test_tokenization_gpt_neox_japanese.py']","The existing implementation doesn't support Japanese GPT-NeoX-based model, making it unable to handle Japanese language oriented tasks. Also, the current code base lacks necessary documentation and contains unnecessary imports."
4868a830db5f19f56712f540979d637368221d50,1649669072,"Jia multi gpu eval (#16428)

* add simple multi gpu complet

* add human_eval_multi_gpu

* use copy strategy to distribute across gpu, to avoid padding

* add doc string

* update code style

* use task id to arrange output

* truncate input to avoid zero pad

* Stop the copy mechanism

* update style

* restore copies to scale better in distributed mode

* update style

* replace human eval

* Apply suggestions from code review

1. Tokenize all input at the same time
2. use attention_mask to get the input length
3. other small fixes

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* correct typo and update docstring

* update code style

* remove num sample division constraint

* remove max len calculation

* use accelerator.gather once to speed up

* use accelerate set_seed; update accelerate version

* correct gather bug

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>",['examples/research_projects/codeparrot/scripts/human_eval.py'],"Multi-GPU evaluation is not properly distributed, leading to issues with input truncation, output arrangement and inefficiencies in gathering results. Constraints on number of samples and max length are restrictive and cause speed issues."
73f0a5d1f66b15080cc9976266eb87e7cf9ebe0d,1647510564,"Fixes Loss for TransfoXL when using Trainer API v2 (#16140)

* fix(transfo_xl): Fixes TransfoXL support when using Trainer.

* fix(tests): Uses losses_1 and losses_2 pattern with TransfoXL test.

* fix(transfo_xl): Adds requested changes to allow for backward compatibility.

fix(transfo_xl): Adds requested changes to allow for backward compatibility.

fix(transfo_xl): Fixes code styling.

* Backward compatibility

* Update src/transformers/models/transfo_xl/modeling_transfo_xl.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Gustavo de Rosa <gth.rosa@uol.com.br>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/transfo_xl/modeling_transfo_xl.py', 'tests/transfo_xl/test_modeling_transfo_xl.py']","Trainer API v2 is causing issues with TransfoXL, leading to incorrect value for the loss function and backward compatibility problems."
baf1daa58eb2960248fd9f7c3af0ed245b8ce4af,1691423242,"Migrate Trainer from `Repository` to `upload_folder` (#25095)

* First draft

* Deal with progress bars

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* Address review comments

* Forgot one

* Pin hf_hub

* Add argument for push all and fix tests

* Fix tests

* Address review comments

---------

Co-authored-by: Lucain <lucainp@gmail.com>","['setup.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/trainer.py', 'src/transformers/training_args.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/hub.py', 'tests/trainer/test_trainer.py']",Trainer's dependency on `Repository` causes progress bar and test failure issues; needs to switch dependency to `upload_folder` for smoother operation.
8eb38f638dbcc17caf5417081df8e7d92d6574cb,1681390874,"[Pix2struct] Simplify generation (#22527)

* Add model to doc tests

* Remove generate and replace by prepare_inputs_for_generation

* More fixes

* Remove print statements

* Update integration tests

* Fix generate

* Remove model from auto mapping

* Use auto processor

* Fix integration tests

* Fix test

* Add inference code snippet

* Remove is_encoder_decoder

* Update docs

* Remove notebook link","['src/transformers/generation/configuration_utils.py', 'src/transformers/models/pix2struct/configuration_pix2struct.py', 'src/transformers/models/pix2struct/modeling_pix2struct.py', 'tests/models/pix2struct/test_modeling_pix2struct.py']",Issues have occurred due to the usage of `generate` in the Pix2struct model. The `generate` method complicates input preparation and results in unnecessary prints and integration test failures.
1ac698744c4dbdf1495d303246d08ffacdf4f5b8,1651509055,"Add YOLOS (#16848)

* First draft

* Add YolosForObjectDetection

* Make forward pass work

* Add mid position embeddings

* Add interpolation of position encodings

* Add expected values

* Add YOLOS to tests

* Add integration test

* Support tiny model as well

* Support all models in conversion script

* Remove mid_pe_size attribute

* Make more tests pass

* Add model to README and fix config

* Add copied from statements

* Rename base_model_prefix to vit

* Add missing YOLOS_PRETRAINED_CONFIG_ARCHIVE_MAP

* Apply suggestions from code review

* Apply more suggestions from code review

* Convert remaining checkpoints

* Improve docstrings

* Add YolosFeatureExtractor

* Add feature extractor to docs

* Add corresponding tests

* Fix style

* Fix docs

* Apply suggestion from code review

* Fix bad rebase

* Fix some more bad rebase

* Fix missing character

* Improve docs and variable names

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/detr/modeling_detr.py', 'src/transformers/models/yolos/__init__.py', 'src/transformers/models/yolos/configuration_yolos.py', 'src/transformers/models/yolos/convert_yolos_to_pytorch.py', 'src/transformers/models/yolos/feature_extraction_yolos.py', 'src/transformers/models/yolos/modeling_yolos.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/yolos/test_feature_extraction_yolos.py', 'tests/yolos/test_modeling_yolos.py']","The repository currently lacks YOLOS (You Only Look Once) model for object detection, and the absence of an integration test for this model."
034bc5d26ad7c0e284265d92d3da39d786138545,1693844314,"Add proper Falcon docs and conversion script (#25954)

* Add proper Falcon docs and conversion script

* Autodetect the decoder architecture instead of using an arg

* Update docs now that we can autodetect

* Fix doc error

* Add doc to toctree

* Quick doc update",['src/transformers/models/falcon/convert_custom_code_checkpoint.py'],The Falcon documentation is incomplete and lacks a conversion script; Decoder architecture is not autodetected leading to potential usage errors.
bce36ee065f7749c997be4c30a3f9279df7d5dba,1660290490,"Load sharded pt to flax (#18419)

* initial commit

* add small test

* add cross pt tf flag to test

* fix quality

* style

* update test with new repo

* fix failing test

* update

* fix wrong param ordering

* style

* update based on review

* update related to recent new caching mechanism

* quality

* Update based on review

Co-authored-by: sgugger <sylvain.gugger@gmail.com>

* quality and style

* Update src/transformers/modeling_flax_utils.py
Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/modeling_flax_pytorch_utils.py', 'src/transformers/modeling_flax_utils.py', 'tests/test_modeling_flax_common.py']","Sharded parameter tensor loading to Flax is failing, and existing test cases are not comprehensive enough to detect all possible scenarios. Further, recent caching mechanisms may be causing additional issues."
2564f0c21d2b3e5ce73038a0510b4236c92044b2,1667487003,"fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc (#19891)

* fix jit trace error for classification usecase, update related doc

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* add implementation in torch 1.14.0

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* update_doc

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* update_doc

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>",['src/transformers/trainer.py'],"JIT trace error occurs when the model forward sequence does not align with the JIT trace tuple input sequence, especially in the classification use-case."
ba47efbfe4093e4b14e60b5ff7b90f9490c35a0f,1695888857,"docs: change assert to raise and some small docs (#26232)

* docs: change assert to raise and some small docs

* docs: add rule and some document

* fix: fix bug

* fix: fix bug

* chorse: revert logging

* chorse: revert","['examples/pytorch/language-modeling/run_clm_no_trainer.py', 'examples/pytorch/language-modeling/run_mlm_no_trainer.py']",Incorrect use of 'assert' causes potential runtime failures. Inaccurate documentation leads to misunderstanding of the system's functionality.
9763f829a5631de926db6fd267cb9ec3ab2da338,1670261016,"Fix whisper and speech to text doc  (#20595)

* Fix whisper and speech to text doc
# What does this PR do?
Previously the documentation was badly indented for both models and indicated that
> If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value of `inputs_embeds`.`
Which is on valid for the forward pass of the `ForConditionnalGeneration` not for the model alone.

* other fixes","['src/transformers/models/speech_to_text/modeling_speech_to_text.py', 'src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py', 'src/transformers/models/whisper/modeling_tf_whisper.py', 'src/transformers/models/whisper/modeling_whisper.py']",The documentation for both whisper and speech to text models contains incorrect and misleading indentation and states that `decoder_input_ids` and `decoder_inputs_embeds` are both unset for the forward pass of `ForConditionalGeneration` only.
ddd4d02f3013aa3f05c64e400c228171f59f4083,1632253177,"Layoutlm onnx support (Issue #13300) (#13562)

* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* cleanup

* Removed regression/ folder

* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* cleanup

* Fixed import error

* Remove unnecessary import statements

* Changed max_2d_positions from class variable to instance variable of the config class

* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* cleanup

* Add support for exporting PyTorch LayoutLM to ONNX

* cleanup

* Fixed import error

* Changed max_2d_positions from class variable to instance variable of the config class

* Use super class generate_dummy_inputs method

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>

* Add support for Masked LM, sequence classification and token classification

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>

* Removed uncessary import and method

* Fixed code styling

* Raise error if PyTorch is not installed

* Remove unnecessary import statement

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>","['src/transformers/models/layoutlm/__init__.py', 'src/transformers/models/layoutlm/configuration_layoutlm.py', 'src/transformers/onnx/features.py', 'tests/test_onnx_v2.py']","PyTorch's LayoutLM model is not exportable to ONNX, and relevant conversion tests are also missing. Some variable and method adjustments might be required for successful conversion.
"
7a1c68a8454c25c55f3f8978c182ea90e3412f5c,1666797777,"Add `flan-t5` documentation page (#19892)

* add `flan-t5` documentation page

* Update README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add more content

* revert `_toctree` modif

* revert `toctree` modif - 2

* Update README.md

* Revert ""Update README.md""

This reverts commit 56607144299c5fdf7b18abdb776efd0d03287727.

* Update README_es.md

* Update README_zh-hans.md

* Update README_zh-hant.md

* Update README_ko.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/models/auto/configuration_auto.py'],"The `flan-t5` module is missing a dedicated documentation page, leading to potential confusion among users."
68e85fc822097b3df8d685a4705804348245284d,1696002178,"[Flax Examples] Seq2Seq ASR Fine-Tuning Script (#21764)

* from seq2seq speech

* [Flax] Example script for speech seq2seq

* tests and fixes

* make style

* fix: label padding tokens

* fix: label padding tokens over list

* update ln names for Whisper

* try datasets iter loader

* create readme and append results

* style

* make style

* adjust lr

* use pt dataloader

* make fast

* pin gen max len

* finish

* add pt to requirements for test

* fix pt -> torch

* add accelerate","['examples/flax/speech-recognition/run_flax_speech_recognition_seq2seq.py', 'examples/flax/test_flax_examples.py']","Issues exist in the sequence-to-sequence speech model regarding padding token labels and slow learning rate. In addition, Whisper ln names need to be updated and Max length generation has to be pinned. The dataset loader performance also seems to be suboptimal."
e64d99fa6b2aa7855d7af3c7388bd690feba1ec8,1686677010,"Update urls in warnings for rich rendering (#24136)

* fixing typo in url in warnings

* fixing typo in url in warnings

* multi-line fix

* multi-line fix

* Update src/transformers/generation/utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/generation/flax_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/generation/tf_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/generation/flax_utils.py', 'src/transformers/generation/tf_utils.py', 'src/transformers/generation/utils.py']",URLs in warnings for rich rendering have typographical errors causing them to be broken/not reachable.
27597fea076712d9ab0b5335933380ac36ea69dd,1696600991,"remove SharedDDP as it is deprecated (#25702)

* remove SharedDDP as it was drepracated

* apply review suggestion

* make style

* Oops,forgot to remove the compute_loss context manager in Seq2SeqTrainer.

* remove the unnecessary conditional statement

* keep the logic of IPEX

* clean code

* mix precision setup & make fixup

---------

Co-authored-by: statelesshz <jihuazhong1@huawei.com>","['examples/legacy/seq2seq/seq2seq_trainer.py', 'setup.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/integrations/__init__.py', 'src/transformers/integrations/integration_utils.py', 'src/transformers/testing_utils.py', 'src/transformers/trainer.py', 'src/transformers/trainer_seq2seq.py', 'src/transformers/trainer_utils.py', 'src/transformers/training_args.py', 'tests/extended/test_trainer_ext.py']",The SharedDDP feature is still present in the codebase despite being deprecated and showing conflicts with other features like Seq2SeqTrainer and precision settings.
b4698b7ef23801dd02f6521e6c1ea7ab5c5fbec5,1684932463,"fix: use bool instead of uint8/byte in Deberta/DebertaV2/SEW-D to make it compatible with TensorRT (#23683)

* Use bool instead of uint8/byte in DebertaV2 to make it compatible with TensorRT

TensorRT cannot accept onnx graph with uint8/byte intermediate tensors. This PR uses bool tensors instead of unit8/byte tensors to make the exported onnx file can work with TensorRT.

* fix: use bool instead of uint8/byte in Deberta and SEW-D

---------

Co-authored-by: Yuxian Qiu <yuxianq@nvidia.com>","['src/transformers/models/deberta/modeling_deberta.py', 'src/transformers/models/deberta_v2/modeling_deberta_v2.py', 'src/transformers/models/sew_d/modeling_sew_d.py']","Onnx graphs with uint8/byte intermediate tensors in Deberta, DebertaV2 and SEW-D are not compatible with TensorRT."
607acd4fbd175feb458eda7317419cdee5d443fe,1654246597,"Add Gated-SiLU to T5 (#17420)

* Add gated-silu to t5 architecture to support UL2

* Fix error message

* formatting

* formatting again

* refactor

* fix classnames in _init_weights

* remove is_gated

* add test

* fix test

* Try without the test?

* Add back the test.

* Improve error message.

Co-authored-by: Daniel Hesslow <daniel@lighton.ai>","['src/transformers/models/t5/configuration_t5.py', 'src/transformers/models/t5/modeling_flax_t5.py', 'src/transformers/models/t5/modeling_t5.py', 'src/transformers/models/t5/modeling_tf_t5.py', 'tests/models/t5/test_modeling_t5.py']","T5 architecture lacks Gated-SiLU support, leading to difficulties in implementing UL2 model."
07cde58bdb455d7786c947772da6448f8daef413,1673372602,"feature: update wandb callback to upload checkpoints (#21035)

* docs: add wandb metrics and model checkpointing to callback docstrings

* docs: update reference to wandb documentation

* fix: change default of `""WANDB_WATCH""` from ``""gradients""` to ``""false""`

* feature: add `on_save` method and update `""WANDB_LOG_MODEL` behaviour

* fix: use default wandb run names instead of `output_dir`

- removes duplicated run names from wandb workspace
- models can be logged with corresponding run names

* fix: edit deprecation warning based on review suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix: change indentation of docstrings

* fix: change indentation of docstrings and run fixup

* fix: empty commit for circleci permissions issue

* fix: format deprecation doc strings review suggestion

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* docs: Highlight WANDB_DISABLED arg in documentaion

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* fix: run fixup after updating docstrings

Co-authored-by: Bharat Ramanathan <ramanathan.parameshwaran@gohuddl.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",['src/transformers/integrations.py'],"Wandb callback does not correctly upload model checkpoints, leading to replicated run names in the wandb workspace. Default wandb run names are not used causing inadequate run name-model logging correlation."
ed2ee373d07aa8fd3f97e5f9fac9649511cf46fd,1648236439,"Add TF implementation of GPT-J (#15623)

* Initial commit

* Add TFGPTJModel

* Fix a forward pass

* Add TFGPTJCausalLM

* Add TFGPTJForSequenceClassification

* Add TFGPTJForQuestionAnswering

* Fix docs

* Deal with TF dynamic shapes

* Add Loss parents to models

* Adjust split and merge heads to handle 4 and 5-dim tensors

* Update outputs for @tooslow tests","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/gptj/__init__.py', 'src/transformers/models/gptj/modeling_tf_gptj.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/gptj/test_modeling_tf_gptj.py']","Issues with existing GPT-J model not having TensorFlow implementation, limiting the usage and compatibility with TensorFlow's features and operations."
b47a16743bce02647e1b575a4b40b02618f3fa4d,1676066248,"Remove more unused attributes in config classes (#21543)

* Remove unused decoder_layerdrop

* Update SPECIAL_CASES_TO_ALLOW for MT5Config

* Remove unused position_embedding_init_scale

* Remove unused decoder_max_relative_position

* Use unused decoder_max_relative_position

* Remove unused init_std

* Remove unused forgotten attributes

* Remove unused patch_norm

* Remove unused max_seq_len

* Update SPECIAL_CASES_TO_ALLOW for OneFormerConfig

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/deformable_detr/configuration_deformable_detr.py', 'src/transformers/models/deta/configuration_deta.py', 'src/transformers/models/dinat/configuration_dinat.py', 'src/transformers/models/donut/configuration_donut_swin.py', 'src/transformers/models/jukebox/configuration_jukebox.py', 'src/transformers/models/maskformer/configuration_maskformer_swin.py', 'src/transformers/models/nat/configuration_nat.py', 'src/transformers/models/oneformer/configuration_oneformer.py', 'src/transformers/models/perceiver/configuration_perceiver.py', 'src/transformers/models/speecht5/configuration_speecht5.py', 'src/transformers/models/swin/configuration_swin.py', 'src/transformers/models/swin2sr/configuration_swin2sr.py', 'src/transformers/models/swinv2/configuration_swinv2.py', 'src/transformers/models/van/modeling_van.py', 'utils/check_config_attributes.py']","Unused attributes in various config classes are causing confusion and clutter, possibly affecting proper code utilization and modification."
5da33f872913255d64717efe745a053975bbc28e,1649985005,"[modeling utils] revamp `from_pretrained(..., low_cpu_mem_usage=True)` + tests (#16657)

* add low_cpu_mem_usage tests

* wip: revamping

* wip

* install /usr/bin/time

* wip

* cleanup

* cleanup

* cleanup

* cleanup

* cleanup

* fix assert

* put the wrapper back

* cleanup; switch to bert-base-cased

* Trigger CI

* Trigger CI","['src/transformers/modeling_utils.py', 'src/transformers/testing_utils.py', 'tests/test_modeling_common.py']","The `from_pretrained(..., low_cpu_mem_usage=True)` option in modeling utils causes high CPU memory usage, and there's a lack of tests covering its functionality."
c612628045822f909020f7eb6784c79700813eda,1680293923,"Test fetch v2 (#22367)

* Test fetcher v2

* Fix regexes

* Remove sanity check

* Fake modification to OPT

* Fixes some .sep issues

* Remove fake OPT change

* Fake modif for BERT

* Fake modif for init

* Exclude SageMaker tests

* Fix test and remove fake modif

* Fake setup modif

* Fake pipeline modif

* Remove all fake modifs

* Adds options to skip/force tests

* [test-all-models] Fake modif for BERT

* Try this way

* Does the command actually work?

* [test-all-models] Try again!

* [skip circleci] Remove fake modif

* Remove debug statements

* Add the list of important models

* Quality

* Update utils/tests_fetcher.py

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

* Address review comments

* Address review comments

* Fix and add test

* Apply suggestions from code review

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>

* Address review comments

---------

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>","['tests/repo_utils/test_tests_fetcher.py', 'utils/tests_fetcher.py']","Issues with fetcher v2 tests involving regex, sanity checks, .sep, SageMaker, BERT, init; unreliable debugging statements and problematic 'test-all-models' command; List of important models missing from utils/tests_fetcher.py."
1a2e966cfe916cdfd78ef7e1385911106bc33f50,1696328626,"Nit-added-tokens (#26538)

* fix stripping

* nits

* fix another test

* styling

* fix?

* update

* revert bad merge

* found the bug

* YES SIR

* is that change really required?

* make fast even faster

* re order functions","['src/transformers/tokenization_utils.py', 'src/transformers/tokenization_utils_base.py', 'src/transformers/tokenization_utils_fast.py']","There seems to be an issue with the stripping functionality leading to failed tests, unusual code styling, slow performance and potential merge conflicts."
c74f3d4c480a6971e302de7cef226e9a157ef0d0,1643828774,"Add W&B backend for hyperparameter sweep (#14582)

# Add support for W&B hyperparameter sweep
This PR:
* allows using wandb for running hyperparameter search.
* The runs are visualized on W&B sweeps dashboard
* This supports runnning sweeps on parallel devices, all reporting to the same central dashboard.

### Usage
**To run new a hyperparameter search:**
```
trainer.hyperparameter_search(
    backend=""wandb"", 
    project=""transformers_sweep"", # name of the project
    n_trials=5,
    metric=""eval/loss"", # metric to be optimized, default 'eval/loss'. A warning is raised if the passed metric is not found
)
```
This outputs a sweep id. Eg. `my_project/sweep_id`

**To run sweeps on parallel devices:**
Just pass sweep id which you want to run parallel
```
trainer.hyperparameter_search(
    backend=""wandb"", 
    sweep_id = ""my_project/sweep_id""
)
```
","['src/transformers/integrations.py', 'src/transformers/testing_utils.py', 'src/transformers/trainer.py', 'src/transformers/trainer_utils.py', 'tests/test_trainer.py']","There is no support for using W&B for running and visualizing hyperparameter search, especially across parallel devices reporting to a central dashboard."
8406fa6dd538c6e1b5a218b119e8efd771023112,1644337643,"Add TFSpeech2Text (#15113)

* Add wrapper classes

* convert inner layers to tf

* Add TF Encoder and Decoder layers

* TFSpeech2Text models

* Loadable model

* TF model with same outputs as PT model

* test skeleton

* correct tests and run the fixup

* correct attention expansion

* TFSpeech2Text pask_key_values with TF format","['src/transformers/__init__.py', 'src/transformers/generation_tf_utils.py', 'src/transformers/modeling_tf_pytorch_utils.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/bart/modeling_tf_bart.py', 'src/transformers/models/blenderbot/modeling_tf_blenderbot.py', 'src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py', 'src/transformers/models/hubert/modeling_tf_hubert.py', 'src/transformers/models/marian/modeling_tf_marian.py', 'src/transformers/models/mbart/modeling_tf_mbart.py', 'src/transformers/models/pegasus/modeling_tf_pegasus.py', 'src/transformers/models/speech_to_text/__init__.py', 'src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py', 'src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/test_modeling_common.py', 'tests/test_modeling_tf_common.py', 'tests/test_modeling_tf_speech_to_text.py']","The software lacks a TensorFlow-based speech-to-text conversion feature, preventing compatibility with TensorFlow models and hindering performance optimization."
9fc34235fa3329c918d5ba67ce09a0cc8f399c59,1654782650,"Use shape_list to safely get shapes for Swin (#17591)

* Use shape_list to safely get shapes

* Add relevant test

* Tidy and add metrics

* Resolve dynamic shaping issues and move test

* Tidy up and all samples in batch

* Formatting","['src/transformers/models/deberta/modeling_tf_deberta.py', 'src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py', 'src/transformers/models/swin/modeling_tf_swin.py', 'tests/test_modeling_tf_common.py']","Issues experienced with obtaining shapes safely in Swin, in addition to dynamic shaping problems and concerns regarding testing and metrics."
b9403e951661b53630afd95166874f75ede885c4,1674145225,"Add hallucination filter (#18675)

* Add hallucination penalty

* Make quality changes

* Inverse penalty

* Fix imports & quality

* Fix name spelling issue

* set encoder_repetition_penalty and fix quality

* Fix failing test

* Add to config_common_kwargs

* Fix modelling_rag error

* Update src/transformers/generation_logits_process.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Remove breakpoint

* Make style fixes

* Update encoder_repetition_penalty default value

* Merge latest main changes

* Make fixup changes

* Add EncoderRepetitionPenaltyLogitsProcessor to generation/__init__.py

* Fix repo-inconsistency

* Remove venv

* Remove tensorflow-macos & add tests

* Add documentation

* Fix quality issues

* move encoder_repetition_penalty to config

* Update src/transformers/configuration_utils.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update src/transformers/generation/configuration_utils.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Remove encoder_repetition_penalty from tests

* Fix type error

* Fix format error

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>","['src/transformers/generation/__init__.py', 'src/transformers/generation/configuration_utils.py', 'src/transformers/generation/logits_process.py', 'src/transformers/generation/utils.py', 'tests/generation/test_logits_process.py']","There's a lack of a hallucination penalty feature causing replication of encoder inputs, leading to quality and inconsistency issues across the repository. Also, there is missing documentation about this feature."
d9e4bc2895a818d7fb339254c07ce44b201d66d3,1695658092,"Update tiny model information and pipeline tests (#26285)

* Update tiny model summary file

* add to pipeline tests

* revert

* fix import

* fix import

* fix

* fix

* update

* update

* update

* fix

* remove BarkModelTest

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/auto/modeling_tf_auto.py', 'tests/models/bark/test_modeling_bark.py', 'tests/models/blip_2/test_modeling_blip_2.py', 'tests/models/bros/test_modeling_bros.py', 'tests/models/idefics/test_modeling_idefics.py', 'tests/models/pop2piano/test_modeling_pop2piano.py', 'tests/models/vits/test_modeling_vits.py', 'tests/pipelines/test_pipelines_text_to_audio.py']","The tiny model summary file is outdated, and pipeline tests are missing for this model. The BarkModelTest is possibly irrelevant or problematic."
678bb248d07c593c5ce94f49328835a90e0a8403,1632466335,"Make assertions only if actually chunking forward (#13598)

This moves the assertion on checking input dimensions into a block that will only be called if the function is actually going to do chunking forward. This is often not the case at inference time and PyTorch tracing a model with this assertion in it leads to a tracing warning.

TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors",['src/transformers/modeling_utils.py'],"PyTorch TracerWarning appears during model tracing due to an assertion block that checks input dimensions even when chunking forward isn't taking place, potentially leading to inaccuracies in the trace."
f9ac677eba999a1847314289e39ce14db3e8cece,1626272125,"Update TF examples README (#12703)

* Update Transformers README, rename token_classification example to token-classification to be consistent with the others

* Update examples/tensorflow/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add README for TF token classification

* Update examples/tensorflow/token-classification/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/tensorflow/token-classification/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['examples/tensorflow/token-classification/run_ner.py'],Inconsistencies in the naming schemes of token classification examples and lack of README for TensorFlow token classification.
7f9195090160d508c7afb2e444e34f181872dd10,1683637817,"audio_utils improvements (#21998)

* silly change to allow making a PR

* clean up doc comments

* simplify hertz_to_mel and mel_to_hertz

* fixup

* clean up power_to_db

* also add amplitude_to_db

* move functions

* clean up mel_filter_bank

* fixup

* credit librosa & torchaudio authors

* add unit tests

* tests for power_to_db and amplitude_to_db

* add mel_filter_bank tests

* rewrite STFT

* add convenience spectrogram function

* missing transpose

* fewer transposes

* add integration test to M-CTC-T

* frame length can be either window or FFT length

* rewrite stft API

* add preemphasis coefficient

* move argument

* add log option to spectrogram

* replace M-CTC-T feature extractor

* fix api thing

* replace whisper STFT

* replace whisper mel filters

* replace tvlt's stft

* allow alternate window names

* replace speecht5 stft

* fixup

* fix integration tests

* fix doc comments

* remove manual FFT length calculation

* fix docs

* go away, deprecation warnings

* combine everything into spectrogram function

* add deprecated functions back

* fixup","['src/transformers/audio_utils.py', 'src/transformers/models/clap/feature_extraction_clap.py', 'src/transformers/models/mctct/feature_extraction_mctct.py', 'src/transformers/models/speecht5/feature_extraction_speecht5.py', 'src/transformers/models/tvlt/feature_extraction_tvlt.py', 'src/transformers/models/whisper/feature_extraction_whisper.py', 'tests/models/audio_spectrogram_transformer/test_feature_extraction_audio_spectrogram_transformer.py', 'tests/models/mctct/test_feature_extraction_mctct.py', 'tests/models/speech_to_text/test_feature_extraction_speech_to_text.py', 'tests/models/speecht5/test_feature_extraction_speecht5.py', 'tests/models/tvlt/test_feature_extraction_tvlt.py', 'tests/models/whisper/test_feature_extraction_whisper.py', 'tests/utils/test_audio_utils.py']","Several functions in audio_utils exhibit redundancies and there are deprecated warnings. Also, some tests are missing for power_to_db, amplitude_to_db and mel_filter_bank."
6d02ca4bb9286404242a66f4744874e68f7713a2,1696237600,"Fix issue of canine forward requiring input_ids anyway (#26290)

* fix issue of canine forward requires input_ids anyway

The `forward` requires `input_ids` for deriving other variables in all cases. Change this to use the given one between `input_ids` and `inputs_embeds`

* fix canine forward

The current `forward` requires (the shape of) `input_ids` for deriving other variables whenever `input_ids` or `inputs_embeds` is provided. Change this to use the given one instead of `input_ids` all the time.

* fix format

* fix format",['src/transformers/models/canine/modeling_canine.py'],"The `forward` function always requires `input_ids`, even when `inputs_embeds` are provided, for deriving certain variables which conversely affects the functionality."
b4d554880013bf97718e1e1332715eeaba7dee17,1692284885,"🚨🚨🚨 [`SPM`] Finish fix spm models 🚨🚨🚨 (#25224)

* fix EVERYTHING

* more fixes

* ⚗️⚗️ Tokenizer magic ⚗️⚗️

* wrong value but test passes for the TODO

* update

* updat

* safe protobuf import?

* style

* non gated repo

* update

* fixup

* Update src/transformers/models/llama/tokenization_llama.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/models/llama/tokenization_llama.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update tests/models/t5/test_tokenization_t5.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* nits

* fix t5 too

* use assert equal

* fix llama decoding

* nits on t5

* fixup

* only remove the prefix space, not other spaces

* more deconding tests and more todos

* fix CI as well

* fixup

* skip failing test on CI (its tf its ok)

* skip test_subword_regularization_tokenizer that is also crashing on the CI for TF

* update llama

* revert good fixes

* fixup

* empty

* explain why we need to encode with an additional token

* better warning?

* nits

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/models/llama/tokenization_llama.py', 'src/transformers/models/t5/tokenization_t5.py', 'tests/models/llama/test_tokenization_llama.py', 'tests/models/t5/test_tokenization_t5.py']","Issues with SPM models: failing tests, problems with tokenization in T5 and Llama models, and issues with subword regularization tokenizing, and encoding issues affecting CI."
9b90810558460f971d5fa1c7abb814879f295ead,1625494390,"[Flax] Dataset streaming example (#12470)

* fix_torch_device_generate_test

* remove @

* upload

* finish dataset streaming

* adapt readme

* finish

* up

* up

* up

* up

* Apply suggestions from code review

* finish

* make style

* make style2

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>",['examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py'],Issues encountered with the dataset streaming example in Flax. Problems exist in torch device generate test and there are several modifications suggested during the code review.
78807d86eb8791bdbe149adff62af11f778a9267,1632229488,"[FLAX] Question Answering Example  (#13649)

* flax qa example

* Updated README:  Added Large model

* added utils_qa.py FULL_COPIES

* Updates:
1. Copyright Year updated
2. added dtype arg
3. passing seed and dtype to load model
4. Check eval flag before running eval

* updated README

* updated code comment","['examples/flax/question-answering/run_qa.py', 'examples/flax/question-answering/utils_qa.py', 'utils/check_copies.py']","The QA example in Flax is missing necessary features - there's no large model mentioned in README, lack of utility functions, outdated copyright year, no dtype arguments, and the evaluation flag is not checked before running eval."
023f51fe16e34e0ca2b5598791ae508874d5b443,1674037477,"`blip` support for training (#21021)

* `blip` support for training

* remove labels creation

* remove unneeded `decoder_input_ids` creation

* final changes

- add colab link to documentation
- reduction = mean for loss

* fix nits

* update link

* clearer error message","['src/transformers/models/blip/modeling_blip.py', 'src/transformers/models/blip/modeling_blip_text.py', 'tests/models/blip/test_modeling_blip.py']","Training lacks `blip` support, leading to unnecessary creation of labels and `decoder_input_ids`. Colab link to documentation is missing and error messages are not clear enough."
65bf05cd1897ef0a9b4ea6acceae5292e91adb5a,1626199705,"Adding TF translation example (#12667)

* Adding TF translation example

* Fixes and style pass for TF translation example

* Remove unused postprocess_text copied from run_summarization

* Adding README

* Review fixes

* Move changes to model.config to after we've initialized the model",['examples/tensorflow/translation/run_translation.py'],"The TensorFlow translation example is missing from the repository, causing potential confusion for users trying to understand translation operation."
226b0e46d50dc763dceeeab9682dc83ee70e8c6a,1664279645,"Add a use_parallel_residual argument to control the residual computing way (#18695)

* Add a gpt_j_residual argument to control the residual computing way

* Put duplicate code outside of the if block

* Rename parameter ""gpt_j_residual"" to ""use_parallel_residual"" and set the default value to True","['src/transformers/models/gpt_neox/configuration_gpt_neox.py', 'src/transformers/models/gpt_neox/modeling_gpt_neox.py']","The residual computing mode is currently fixed and lacks flexibility for optional parallel computation, resulting in potential performance bottlenecks."
6a5272b2054622e27b4843e0c9722cc716d5458d,1655826678,"Prepare transformers for v0.8.0 huggingface-hub release (#17716)

* Prepare CI for v0.8.0

* pin hfh (revert before merge)

* Revert ""pin hfh (revert before merge)""

This reverts commit a0103140e1c77b810ffcb735192968bc03be3e1f.

* Test rc3

* Test latest rc

* Unpin to the RC

Co-authored-by: Sylvain Gugger <Sylvain.gugger@gmail.com>","['src/transformers/commands/user.py', 'src/transformers/testing_utils.py', 'src/transformers/utils/hub.py', 'tests/models/auto/test_processor_auto.py', 'tests/test_configuration_common.py', 'tests/test_feature_extraction_common.py', 'tests/test_modeling_common.py', 'tests/test_modeling_flax_common.py', 'tests/test_modeling_tf_common.py', 'tests/test_tokenization_common.py', 'tests/trainer/test_trainer.py']","In preparation for the v0.8.0 release, the transformers are not compatible with the latest version of the huggingface-hub."
63d13d768b2cfbb79b3439b4cedc8dac479fe06e,1666103633,"Improving `image-segmentation` pipeline tests. (#19710)

This PR (https://github.com/huggingface/transformers/pull/19367) introduced a few breaking changes:

- Removed an argument `mask_threshold`.
- Broke the default behavior (instance vs panoptic in the function call)
  https://github.com/huggingface/transformers/pull/19367/files#diff-60f846b86fb6a21d4caf60f5b3d593a04accb8f248de3029cccae2ff898c5bc3R119-R120
- Broke the actual masks: https://github.com/huggingface/transformers/pull/1961

This PR is the start of a handful that will aim at bringing back the old
behavior(s).

- tests should not have to specify `task` by default, unless we want to
  modify the behavior and have a lower form of segmentation running)
- `test_small_model_pt` should be working.

This specific PR starts with adding more information to the masks hash
because missing the actual mask was actual easy to miss (the hashes do
change, but it was easy to miss that one code path wasn't properly
updated).

So we go from a simple `hash` to
```
{""hash"": #smaller hash, ""shape"": (h, w), ""white_pixels"": n}
```

The `shape` should help make sure the interpolation of the mask works
correctly, the `white_pixels` hopefully helps detect big regressions in
their amount when the hash gets modified.","['src/transformers/testing_utils.py', 'tests/pipelines/test_pipelines_image_segmentation.py']","The removal of `mask_threshold` argument, alteration of default behavior, and broken masks in `image-segmentation` pipeline has introduced a breaking change, affecting mask generation and segmentation tasks."
a73b1d59a34ca8e30098b3e32a977a7928663559,1685526382,"accelerate deepspeed and gradient accumulation integrate (#23236)

* mixed precision support via accelerate

* fix issues

* fix for the sharded ddp case

* fix flax and tf failing tests

* `refactor the place to create `Accelerator` object

* move ddp prep to accelerate

* fix 😅

* resolving comments

* move fsdp handling to accelerate

* fixex

* fix saving

* shift torch dynamo handling to accelerate

* shift deepspeed integration and save & load utils to accelerate

* fix accelerate launcher support

* oops

* fix 🐛

* save ckpt fix

* Trigger CI

* nasty 🐛 😅

* as deepspeed needs grad_acc fixes, transfer grad_acc to accelerate

* make tests happy

* quality ✨

* loss tracked needs to account for grad_acc

* fixing the deepspeed tests

* quality ✨

* 😅😅😅

* tests 😡

* quality ✨

* Trigger CI

* resolve comments and fix the issue with the previous merge from branch

* Trigger CI

* accelerate took over deepspeed integration

---------

Co-authored-by: Stas Bekman <stas@stason.org>","['src/transformers/deepspeed.py', 'src/transformers/testing_utils.py', 'src/transformers/trainer.py', 'src/transformers/trainer_pt_utils.py', 'src/transformers/training_args.py', 'tests/deepspeed/test_deepspeed.py']","Deepspeed integration issues with Accelerate library exist, including mixed precision support, incorrect checkpoint saving, and poor gradient accumulation handling. This problem also extends to tests failing on Flax and Tensorflow."
b45192ec47b7aed6ba3e3c1b00d33e547e468e9b,1678800016,"Fix big model inference for T5 models in float16 (#22095)

* Fix big model inference for T5 models in float16

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Style

* Trigger CI with latest release

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>",['src/transformers/modeling_utils.py'],Inference with large T5 models in float16 mode results in issues.
00b5887b948dc6e796b089ffa4057d06284b05d5,1680612786,"🚨🚨🚨 `[NLLB Tokenizer]` Fix the prefix tokens 🚨🚨🚨 (#22313)

* fix the prefix tokens

* update fast and test values

* add legacy behaviour

Co-authored-by: sgugger <sylvain.gugger@gmail.com>

* update disclaimer, linkissue PR and behaviral changes

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <hi@lysand.re>

* styling

* make a quote

* quote this time

---------

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Co-authored-by: Lysandre Debut <hi@lysand.re>","['src/transformers/models/nllb/tokenization_nllb.py', 'src/transformers/models/nllb/tokenization_nllb_fast.py', 'tests/models/nllb/test_tokenization_nllb.py']","Issue with NLLB tokenizer's prefix tokens, causing inconsistent token generation."
d2d8822604cf29f28b258a70a78af323d0ff4000,1684940342,"TF SAM memory reduction (#23732)

* Extremely small change to TF SAM dummies to reduce memory usage on build

* remove debug breakpoint

* Debug print statement to track array sizes

* More debug shape printing

* More debug shape printing

* Now remove the debug shape printing

* make fixup

* make fixup",['src/transformers/models/sam/modeling_tf_sam.py'],High memory usage observed while building TF SAM causing performance issues.
81643edda55dcf9e050612dec285f2fd8d8833e2,1647539497,"Support PEP 563 for HfArgumentParser (#15795)

* Support PEP 563 for HfArgumentParser

* Fix issues for Python 3.6

* Add test for string literal annotation for HfArgumentParser

* Remove wrong comment

* Fix typo

* Improve code readability

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Use `isinstance` to compare types to pass quality check

* Fix style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/hf_argparser.py', 'tests/utils/test_hf_argparser.py']","HfArgumentParser does not support PEP 563, creating compatibility issues with string literal annotations, particularly Python 3.6."
af1a7c8ca31f9637f810ae4606797cf856c5a32d,1668447946,"[Examples] Generalise Seq2Seq ASR to handle Whisper (#19519)

* merge conflicts

* bos and eos in datacollator

* (temp) hardcode removal of attention mask

* freeze encoder

* actually freeze encoder

* set max length / num beams according to gen kwargs

* (temp) fix tests

* don't pop attn mask

* override return attention mask config from Hub

* Hub configs updated 🤗

* final fixes

* update type annotations

* backward comp",['examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py'],"Seq2Seq ASR does not handle Whisper correctly leading to conflicts and issues in encoding and decoding processes. Features like freezing encoder, handling attention masks and setting max length is not configured properly."
5e68675755e3d45aade61950764e361ae82c7022,1649688320,"Fix t5 shard on TPU Pods (#16527)

* Fix t5 shard on TPU Pods

The current script doesn't work properly on a TPU pod because the global batch is not divided correctly per host.
This pull request fixes this issue by dividing the global batch to each host before it is shared on each host.

* fix style

Co-authored-by: ahmed-elnaggar <ahmed.elnaggar@allianz.com>",['examples/flax/language-modeling/run_t5_mlm_flax.py'],"The T5 shard script incorrectly divides global batches per host on TPU pods, leading to improper sharing of hosts."
462cd641d9b1fbe408964ffe60ee255bb94fd042,1665503283,"🚨🚨🚨  TF: Remove `TFWrappedEmbeddings` (breaking: TF embedding initialization updated for encoder-decoder models) (#19263)

* added test

* correct embedding init

* some changes in blenderbot (incomplete)

* update blenderbot (diff to be used as reference)

* update blenderbot_small

* update LED

* update marian

* update T5 and remove TFWrappedEmbeddings

* nullcontext() -> ContextManagers()

* fix embedding init","['src/transformers/modeling_tf_utils.py', 'src/transformers/models/bart/modeling_tf_bart.py', 'src/transformers/models/blenderbot/modeling_tf_blenderbot.py', 'src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py', 'src/transformers/models/led/modeling_tf_led.py', 'src/transformers/models/marian/modeling_tf_marian.py', 'src/transformers/models/mbart/modeling_tf_mbart.py', 'src/transformers/models/pegasus/modeling_tf_pegasus.py', 'src/transformers/models/t5/modeling_tf_t5.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py', 'tests/models/blenderbot/test_modeling_tf_blenderbot.py', 'tests/models/blenderbot_small/test_modeling_tf_blenderbot_small.py', 'tests/models/led/test_modeling_tf_led.py', 'tests/models/marian/test_modeling_tf_marian.py', 'tests/models/mbart/test_modeling_tf_mbart.py', 'tests/models/pegasus/test_modeling_tf_pegasus.py', 'tests/models/t5/test_modeling_tf_t5.py']","Inappropriate initialization of TF embeddings for encoder-decoder models in various modules including blenderbot, LED, marian, and T5."
871598be552c38537bc047a409b4a6840ba1c1e4,1680613504,"Implemented safetensors checkpoints save/load for Trainer (#22498)

* implemented safetensors save/load

* remove duplicated file

* added tests

* more tests

* style fix

* fix tf tests

* change to list comprehension

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* review fixes + safe load for sharded checkpoint

* style fix

* remove rogue import

* remove partial to avoid undefined exception

* use naming alias instead of safetensors.torch

* fix safe sharding in tests

* grammar

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* update docs

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* update docs

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* minor corrections

* style

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/modeling_utils.py', 'src/transformers/trainer.py', 'src/transformers/training_args.py', 'tests/trainer/test_trainer.py']","Trainer lacks the ability to save/load checkpoints using safetensors, causing potential data loss and inefficiency in training sessions."
edb6d950cbc2f38e723549ee366eab91c9fa6f12,1682342823,"Add an attribute to disable custom kernels in deformable detr in order to make the model ONNX exportable (#22918)

* add disable kernel option

* add comment

* fix copies

* add disable_custom_kernels to config

* Update src/transformers/models/deta/modeling_deta.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/models/deta/modeling_deta.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/models/deta/modeling_deta.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* style

* fix

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/models/deformable_detr/configuration_deformable_detr.py', 'src/transformers/models/deformable_detr/modeling_deformable_detr.py', 'src/transformers/models/deta/modeling_deta.py']",Deformable DETR with custom kernels impedes successful ONNX exporting of the model.
a1392883cedd0398a6ae39ccac7432ba72ad571b,1640854603,"[AutoProcessor] Correct AutoProcessor and automatically add processor… (#14881)

* [AutoProcessor] Correct AutoProcessor and automatically add processor class

* up

* up

* up

* up

* up

* up

* up

* up

* continue tomorrow

* up

* up

* up

* make processor class private

* fix loop","['src/transformers/feature_extraction_utils.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/clip/processing_clip.py', 'src/transformers/models/layoutlmv2/processing_layoutlmv2.py', 'src/transformers/models/layoutxlm/processing_layoutxlm.py', 'src/transformers/models/speech_to_text/processing_speech_to_text.py', 'src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py', 'src/transformers/models/wav2vec2/processing_wav2vec2.py', 'src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py', 'src/transformers/tokenization_utils_base.py', 'tests/test_processor_auto.py']","AutoProcessor fails to automatically add processor class and has loop issues. Also, processor class is not set to private."
4d10ffd50614ebcdfe7e99c0092740f0f7234923,1671703879,"[`FSMT`] Make it compatible with `xxxForConditionalGeneration` models (#20825)

* add `get_encoder` and `get_decoder`

* add additional kwargs support

* fix condition

* add better checks

* better checks

* fix embed positions

* better test to consider padding

* fix debug statement

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* add arguments on docstring

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",['src/transformers/models/fsmt/modeling_fsmt.py'],"The `FSMT` model is currently incompatible with `xxxForConditionalGeneration` models, lacking necessary methods like `get_encoder` and `get_decoder`, and doesn't support additional keyword arguments."
da2bd2ae968afa768837ee5e72f4f86a4651e59b,1655814216,"[CodeParrot] Near-deduplication with jaccard similarity (#17054)

* deduplication draft

* update style

* update style test

* dummy test main

* rename modules

* rename functions

* return extremes in deduplicate_clusters

* update style

* cast str for gzip

* update doc string

* time processing

* use dataset map to compute minhash

* fill value for short token

* remove da map method

* update style

* use share object to multiprocess

* update style

* use f-string and minor fix

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>
Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>

* update style

* use module parameters

* change ds_dedup to ds_filter

* save ds_dedup

* mv test to script tests

* make jaccard threshold a parameter of deduplicate_dataset

* update style

* add doc strings

* update style

* add doc string for DuplicationIndex

* save files into data dir

* update readme

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>

* make near deduplication optional

* move near deduplication in README

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* use f string

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>
Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>","['examples/research_projects/codeparrot/scripts/arguments.py', 'examples/research_projects/codeparrot/scripts/minhash_deduplication.py', 'examples/research_projects/codeparrot/scripts/preprocessing.py', 'examples/research_projects/codeparrot/scripts/tests/test_deduplicate.py']","CodeParrot dataset contains instances of near-duplicate data; deduplication is not optional, potentially affecting the efficiency and accuracy of data processing."
eb849f6604c7dcc0e96d68f4851e52e253b9f0e5,1687298867,"Migrate doc files to Markdown. (#24376)

* Rename index.mdx to index.md

* With saved modifs

* Address review comment

* Treat all files

* .mdx -> .md

* Remove special char

* Update utils/tests_fetcher.py

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

---------

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>","['.circleci/create_circleci_config.py', 'src/transformers/commands/add_new_model.py', 'src/transformers/commands/add_new_model_like.py', 'src/transformers/testing_utils.py', 'tests/utils/test_add_new_model_like.py', 'utils/check_copies.py', 'utils/check_repo.py', 'utils/check_table.py', 'utils/check_task_guides.py', 'utils/notification_service_doc_tests.py', 'utils/tests_fetcher.py']",Documentation files in .mdx format cause issues and make it difficult to address changes and review comments.
83e5a10603ca902c266e40fc98a01dd8a9b04ac4,1628094563,"Add BEiT (#12994)

* First pass

* Make conversion script work

* Improve conversion script

* Fix bug, conversion script working

* Improve conversion script, implement BEiTFeatureExtractor

* Make conversion script work based on URL

* Improve conversion script

* Add tests, add documentation

* Fix bug in conversion script

* Fix another bug

* Add support for converting masked image modeling model

* Add support for converting masked image modeling

* Fix bug

* Add print statement for debugging

* Fix another bug

* Make conversion script finally work for masked image modeling models

* Move id2label for datasets to JSON files on the hub

* Make sure id's are read in as integers

* Add integration tests

* Make style & quality

* Fix test, add BEiT to README

* Apply suggestions from @sgugger's review

* Apply suggestions from code review

* Make quality

* Replace nielsr by microsoft in tests, add docs

* Rename BEiT to Beit

* Minor fix

* Fix docs of BeitForMaskedImageModeling

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/image_utils.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/beit/__init__.py', 'src/transformers/models/beit/configuration_beit.py', 'src/transformers/models/beit/convert_beit_unilm_to_pytorch.py', 'src/transformers/models/beit/feature_extraction_beit.py', 'src/transformers/models/beit/modeling_beit.py', 'src/transformers/models/deit/convert_deit_timm_to_pytorch.py', 'src/transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py', 'src/transformers/models/vit/convert_vit_timm_to_pytorch.py', 'src/transformers/models/vit/feature_extraction_vit.py', 'src/transformers/utils/coco_classes.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'src/transformers/utils/imagenet_classes.py', 'src/transformers/utils/modeling_auto_mapping.py', 'tests/test_feature_extraction_beit.py', 'tests/test_modeling_beit.py', 'utils/check_repo.py']",Issues encountered with the BEiT conversion script; it's not working properly for masked image modeling models. IDs not being read as integers in datasets presents further problems.
6015f91a5a28548a597f8d24341d089fe04994e8,1696429898,"refactor: change default block_size (#26229)

* refactor: change default block_size

* fix: return tf to origin

* fix: change files to origin

* rebase

* rebase

* rebase

* rebase

* rebase

* rebase

* rebase

* rebase

* refactor: add min block_size to files

* reformat: add min block_size for run_clm tf","['examples/pytorch/language-modeling/run_clm.py', 'examples/pytorch/language-modeling/run_clm_no_trainer.py', 'examples/research_projects/jax-projects/model_parallel/run_clm_mp.py', 'examples/tensorflow/language-modeling/run_clm.py']","Default block size configuration isn't adjustable, affecting performance scalability in varying use-case environments."
74814574aeab5256ab3c6e428c247739aa0c869d,1650378746,"Add doc about `attention_mask` on gpt2  (#16829)

* Add doc about `attention_mask` on gpt2

Add a simple sentence describing how `attention_mask` needs to be constructed when ``past_key_values` is used.

* Add doc about attention_mask on gpt2_tf

* clean up style

* remove empty line white spaces

* remove whitespace in empty line","['src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/models/gpt2/modeling_tf_gpt2.py']","Documentation on gpt2 and gpt2_tf lacks information on how to construct `attention_mask` when `past_key_values` is used, causing confusion for users."
18c263c4b6b82726a3f2699e2dfee89383804391,1655992632,"BLOOM minor changes on tokenizer (#17823)

* few fixes:

- hardcode tokenizer padding side
- remove unused args

* few fixes:

- added new attribute on TokenizerTesterMixin
- added new slow test
- remove unused arg on tokenizer class

* make style

* Update src/transformers/models/bloom/tokenization_bloom_fast.py

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* make quality

* apply changes

- remove new attribute
- redefine test on the class

* add comments

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>","['src/transformers/models/bloom/tokenization_bloom_fast.py', 'tests/models/bloom/test_modeling_bloom.py', 'tests/models/bloom/test_tokenization_bloom.py']","Incorrect padding side is hard-coded and unnecessary arguments are present in the bloom tokenizer, affecting its performance and accuracy."
30b3c46ff5a7c9761a800a9ab4bcf8cdb206727e,1692357987,"[`split_special_tokens`] Add support for `split_special_tokens` argument to encode (#25081)

* draft changes

* update and add tests

* styling for no

* move test

* path to usable model

* update test

* small update

* update bertbased tokenizers

* don'tuse kwargs for _tokenize

* don'tuse kwargs for _tokenize

* fix copies

* update

* update test for special tokenizers

* fixup

* skip two tests

* remove pdb breakpiont()

* wowo

* rewrite custom tests

* nits

* revert chang in target keys

* fix markup lm

* update documentation of the argument","['src/transformers/models/bert/tokenization_bert.py', 'src/transformers/models/convbert/tokenization_convbert.py', 'src/transformers/models/deprecated/retribert/tokenization_retribert.py', 'src/transformers/models/distilbert/tokenization_distilbert.py', 'src/transformers/models/electra/tokenization_electra.py', 'src/transformers/models/funnel/tokenization_funnel.py', 'src/transformers/models/layoutlm/tokenization_layoutlm.py', 'src/transformers/models/lxmert/tokenization_lxmert.py', 'src/transformers/models/mobilebert/tokenization_mobilebert.py', 'src/transformers/models/roc_bert/tokenization_roc_bert.py', 'src/transformers/models/squeezebert/tokenization_squeezebert.py', 'src/transformers/tokenization_utils.py', 'src/transformers/tokenization_utils_base.py', 'tests/models/layoutlmv2/test_tokenization_layoutlmv2.py', 'tests/models/layoutlmv3/test_tokenization_layoutlmv3.py', 'tests/models/layoutxlm/test_tokenization_layoutxlm.py', 'tests/models/markuplm/test_tokenization_markuplm.py', 'tests/test_tokenization_common.py']",`split_special_tokens` argument is not supported in the encode function causing issues with special tokens handling.
6ef42587ae2f77301c8e227df85bc933244975cd,1670947619,"[NAT, DiNAT] Add backbone class (#20654)

* Add first draft

* Add out_features attribute to config

* Add corresponding test

* Add Dinat backbone

* Add BackboneMixin

* Add Backbone mixin, improve tests

* Fix embeddings

* Fix bug

* Improve backbones

* Fix Nat backbone tests

* Fix Dinat backbone tests

* Apply suggestions

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/dinat/__init__.py', 'src/transformers/models/dinat/configuration_dinat.py', 'src/transformers/models/dinat/modeling_dinat.py', 'src/transformers/models/nat/__init__.py', 'src/transformers/models/nat/configuration_nat.py', 'src/transformers/models/nat/modeling_nat.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/dinat/test_modeling_dinat.py', 'tests/models/nat/test_modeling_nat.py', 'utils/check_repo.py']",The DiNAT and NAT components lack a well-structured backbone class causing difficulties in performing tasks and running tests.
49cd736a288a315d741e5c337790effa4c9fa689,1656605468,"feat: add pipeline registry abstraction (#17905)

* feat: add pipeline registry abstraction

- added `PipelineRegistry` abstraction
- updates `add_new_pipeline.mdx` (english docs) to reflect the api addition
- migrate `check_task` and `get_supported_tasks` from
  transformers/pipelines/__init__.py to
  transformers/pipelines/base.py#PipelineRegistry.{check_task,get_supported_tasks}

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* fix: update with upstream/main

chore: Apply suggestions from sgugger's code review

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* chore: PR updates

- revert src/transformers/dependency_versions_table.py from upstream/main
- updates pipeline registry to use global variables

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* tests: add tests for pipeline registry

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* tests: add test for output warning.

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* chore: fmt and cleanup unused imports

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* fix: change imports to top of the file and address comments

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/base.py', 'tests/pipelines/test_pipelines_common.py']","Lack of an abstracted pipeline registry results in disorganized check tasks and supported tasks, causing difficulty in task management within the transformers/pipelines/__init__.py file."
cb7ed6e083365226819cf4d9370c757d5fda1256,1644488321,"Add Tensorflow handling of ONNX conversion (#13831)

* Add TensorFlow support for ONNX export

* Change documentation to mention conversion with Tensorflow

* Refactor export into export_pytorch and export_tensorflow

* Check model's type instead of framework installation to choose between TF and Pytorch

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Alberto Bégué <alberto.begue@della.ai>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>","['src/transformers/onnx/convert.py', 'src/transformers/onnx/features.py', 'tests/test_onnx_v2.py', 'utils/check_table.py']","ONNX export lacks support for TensorFlow models, limiting conversion options and requiring framework installation check for determination of export type."
6a062a3ed969c1d39ccb8fb8c4a750f4155d2bf9,1670578922,"Change transformers.onnx to use optimum.exporters.onnx (#20529)

* Change transformers.onnx to use optimum.exporters.onnx

* Update doc

* Remove print

* Fix transformers.onnx cli

* Update documentation

* Update documentation

* Small fixes

* Fix log message

* Apply suggestions

* Update src/transformers/onnx/__main__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions

* Add missing line break

* Ran make fix-copies

* Update src/transformers/onnx/__main__.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update src/transformers/onnx/__main__.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

Co-authored-by: Michael Benayoun <michael@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>","['src/transformers/onnx/__main__.py', 'src/transformers/utils/import_utils.py']","The transformers.onnx module is not using the optimum.exporters.onnx, resulting in sub-optimal functionality and incorrect logging."
41c559415ae72fc84afdf4150981e2db025b9c61,1630405144,"Add GPT2ForTokenClassification (#13290)

* Add GPT2ForTokenClassification

* Fix dropout exception for GPT2 NER

* Remove sequence label in test

* Change TokenClassifierOutput to TokenClassifierOutputWithPast

* Fix for black formatter

* Remove dummy

* Update docs for GPT2ForTokenClassification

* Fix check_inits ci fail

* Update dummy_pt_objects after make fix-copies

* Remove TokenClassifierOutputWithPast

* Fix tuple input issue

Co-authored-by: danielsejong55@gmail.com <danielsejong55@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/gpt2/__init__.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_gpt2.py']","GPT-2 model lacks token classification capabilities, causing it to throw exceptions on NER tasks and mishandle tuple inputs. It also does not appropriately update dummy_pt_objects after using make fix-copies."
8878eb1bd91aff414a00d65f27f72b3457df8043,1696580880,"Remove unnecessary `view`s of `position_ids` (#26059)

* Remove unnecessary `view` of `position_ids` in `modeling_llama`

When `position_ids` is `None`, its value is generated using
`torch.arange`, which creates a tensor of size `(seq_length +
past_key_values_length) - past_key_values_length = seq_length`. The
tensor is then unsqueezed, resulting in a tensor of shape `(1,
seq_length)`. This means that the last `view` to a tensor of shape
`(-1, seq_length)` is a no-op.

This commit removes the unnecessary view.

* Remove no-op `view` of `position_ids` in rest of transformer models","['src/transformers/models/codegen/modeling_codegen.py', 'src/transformers/models/ctrl/modeling_ctrl.py', 'src/transformers/models/decision_transformer/modeling_decision_transformer.py', 'src/transformers/models/deprecated/open_llama/modeling_open_llama.py', 'src/transformers/models/falcon/modeling_falcon.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py', 'src/transformers/models/gpt_neo/modeling_gpt_neo.py', 'src/transformers/models/gpt_neox/modeling_gpt_neox.py', 'src/transformers/models/gptj/modeling_gptj.py', 'src/transformers/models/idefics/modeling_idefics.py', 'src/transformers/models/imagegpt/modeling_imagegpt.py', 'src/transformers/models/llama/modeling_llama.py', 'src/transformers/models/persimmon/modeling_persimmon.py', 'src/transformers/models/xglm/modeling_xglm.py']",Unnecessary 'view' of 'position_ids' in transformer models when 'position_ids' is `None`. This results in tensors that do not change their shape.
b68d408f1b2f4371549cfceae3d7541a2e9e5e50,1656686682,"add ONNX support for BLOOM (#17961)

* add onnx support for BLOOM

* use TYPE_CHECKING for type annotations

* fix past_shape for bloom (different from gpt2)

* use logical_or instead of `+` for onnx support

* bigger `atol_for_validation` for larger bloom models

* copied -> taken because it's no longer an exact copy

* remove ""copied from"" comment

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/bloom/__init__.py', 'src/transformers/models/bloom/configuration_bloom.py', 'src/transformers/models/bloom/modeling_bloom.py', 'src/transformers/onnx/features.py', 'tests/onnx/test_onnx_v2.py']",The BLOOM model currently lacks ONNX support leading to compatibility issues and has incorrect 'past_shape' calculation.
2d70c912067ee28cbf35d333cda496120dae9fee,1624904615,"[Flax] Adapt flax examples to include `push_to_hub` (#12391)

* fix_torch_device_generate_test

* remove @

* finish

* correct summary writer

* correct push to hub

* fix indent

* finish

* finish

* finish

* finish

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>","['examples/flax/language-modeling/run_clm_flax.py', 'examples/flax/language-modeling/run_mlm_flax.py', 'examples/flax/summarization/run_summarization_flax.py', 'examples/flax/text-classification/run_flax_glue.py']","Flax examples do not include the `push_to_hub` functionality, hindering comprehensive testing."
786ced36390e9d1c4c5c32a81d7f00c36913aa2a,1626870276,"Add versioning system to fast tokenizer files (#12713)

* Add versioning system to fast tokenizer files

* Deal with offline mode

* Use staging env in tests

* Style

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Style

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>","['src/transformers/file_utils.py', 'src/transformers/tokenization_utils_base.py', 'src/transformers/training_args.py', 'tests/test_tokenization_fast.py']","Fast tokenizer files lack a versioning system, causing issues with offline mode and in tests using the staging environment."
78a2b19fc84ed55c65f4bf20a901edb7ceb73c5f,1688127579,"Show a warning for missing attention masks when pad_token_id is not None (#24510)

* Adding warning messages to BERT for missing attention masks

These warning messages when there are pad tokens within the input ids and
no attention masks are given. The warning message should only show up once.

* Adding warning messages to BERT for missing attention masks

These warning messages are shown when the pad_token_id is not None
and no attention masks are given. The warning message should only
show up once.

* Ran fix copies to copy over the changes to some of the other models

* Add logger.warning_once.cache_clear() to the test

* Shows warning when there are no attention masks and input_ids start/end with pad tokens

* Using warning_once() instead and fix indexing in input_ids check

---------

Co-authored-by: JB Lau <hckyn@voyager2.local>","['src/transformers/modeling_utils.py', 'src/transformers/models/altclip/modeling_altclip.py', 'src/transformers/models/bert/modeling_bert.py', 'src/transformers/models/bridgetower/modeling_bridgetower.py', 'src/transformers/models/camembert/modeling_camembert.py', 'src/transformers/models/clap/modeling_clap.py', 'src/transformers/models/data2vec/modeling_data2vec_text.py', 'src/transformers/models/roberta/modeling_roberta.py', 'src/transformers/models/xlm_roberta/modeling_xlm_roberta.py', 'src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py', 'tests/models/bert/test_modeling_bert.py', 'tests/test_modeling_utils.py']","BERT model fails to show warning messages when pad tokens are within the input ids and no attention masks are provided, especially when pad_token_id is not None."
9858195481e0d29e9b720705d359f98620680a06,1681315804,"add fast support and option (#22724)

* add fast support and option

* update based on review

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/llama/convert_llama_weights_to_hf.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* nit

* add print

* fixup

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/models/llama/convert_llama_weights_to_hf.py'],There is currently no fast support and option available for this tool which could enhance its performance and customize its utility better.
0eb02871ddee0c9471121c35eac3a7ea9827ed35,1631889873,"Removed console spam from misfiring warnings (#13625)

* Removed misfiring warnings

* Revert ""Removed misfiring warnings""

This reverts commit cea90de325056b9c1cbcda2bd2613a785c1639ce.

* Retain the warning, but only when the user actually overrides things

* Fix accidentally breaking just about every model on the hub simultaneously

* Style pass",['src/transformers/modeling_tf_utils.py'],"Misfiring warnings and console spam occur when user overrides things, causing accidental breakage of numerous models on the hub."
8581a798c0a48fca07b29ce2ca2ef55adcae8c7e,1657731848,"Add TF DeiT implementation (#17806)

* Initial TF DeiT implementation

* Fix copies naming issues

* Fix up + docs

* Properly same main layer

* Name layers properly

* Initial TF DeiT implementation

* Fix copies naming issues

* Fix up + docs

* Properly same main layer

* Name layers properly

* Fixup

* Fix import

* Fix import

* Fix import

* Fix weight loading for tests whilst not on hub

* Add doc tests and remove to_2tuple

* Add back to_2tuple
Removing to_2tuple results in many downstream changes needed because of the copies checks

* Incorporate updates in Improve vision models #17731 PR

* Don't hard code num_channels

* Copy PyTorch DeiT embeddings and remove pytorch operations with mask

* Fix patch embeddings & tidy up

* Update PixelShuffle to move logic into class layer

* Update doc strings - remove PT references

* Use NHWC format in internal layers

* Fix up

* Use linear activation layer

* Remove unused import

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Move dataclass to top of file

* Remove from_pt now weights on hub

* Fixup

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Amy Roberts <amyeroberts@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/modeling_tf_outputs.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/deit/__init__.py', 'src/transformers/models/deit/modeling_tf_deit.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/models/deit/test_modeling_tf_deit.py']","Issues found in the initial implementation of TF DeiT, including naming discrepancies, inappropriate hard coding, and downstream changes required due to to_2tuple removal, as well as PyTorch operations related to embeddings and masks."
bf174f916bf8f470f054259df8a62bf15578b8c5,1659699614,"Refactor `TFSwinLayer` to increase serving compatibility (#18352)

* Refactor `TFSwinLayer` to increase serving compatibility

Signed-off-by: Seunghwan Hong <seunghwan@scatterlab.co.kr>

* Fix missed parameters while refactoring

Signed-off-by: Seunghwan Hong <seunghwan@scatterlab.co.kr>

* Fix window_reverse to calculate batch size

Signed-off-by: Seunghwan Hong <harrydrippin@gmail.com>
Co-Authored-By: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",['src/transformers/models/swin/modeling_tf_swin.py'],"`TFSwinLayer` lacks compatibility with the server, potentially causing missed parameters and incorrect batch size calculations."
78b7debf56efb907c6af767882162050d4fbb294,1683143959,"GPTNeoForQuestionAnswering (#23057)

* first draft - gives index error in question_answering.py

* maturing

* no labels

* pipeline should know about QA

* fixing checks

* formatting

* fixed docstring

* initial commit

* formatting

* adding the class to many places

* towards less unhappy checks

* nearly there

* Update src/transformers/models/gpt_neo/modeling_gpt_neo.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* avoid error

* moving to device of star/end_logits

---------

Co-authored-by: Prof. Peter Schneider-Kamp <jps@ordbogen.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/gpt_neo/__init__.py', 'src/transformers/models/gpt_neo/modeling_gpt_neo.py', 'src/transformers/models/gptj/modeling_gptj.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/gpt_neo/test_modeling_gpt_neo.py']",GPTNeo for Question Answering is causing an index error in question_answering.py and several unhappy checks need to be addressed. It also requires additional integration into various parts of the codebase.
af14c61973effd8b8077ac61b3f24bdd4a632f25,1649361480,"RegNet (#16188)

* base model done

* make style

* done

* added files

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Trigger doc build

* resolved conversations

* resolved conversations

* seer models

* minor changes

* minor changes

* make fixup

* glob variables

* minor changes

* fix copies

* config when possibile

* resolved conflicts

* resolved conflicts

* resolved conflicts

* CI

* conversion script for 10b param

* fixed for 10b model

* minor updates in the doc + make style

* removed unused code

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* removed unused code

* removed unused code

* updated modeling_utils from main

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <Sylvain.gugger@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/regnet/__init__.py', 'src/transformers/models/regnet/configuration_regnet.py', 'src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py', 'src/transformers/models/regnet/convert_regnet_to_pytorch.py', 'src/transformers/models/regnet/modeling_regnet.py', 'src/transformers/models/resnet/modeling_resnet.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/regnet/test_modeling_regnet.py']","There are variables that are globally declared, and there is unused code in the program. Additionally, it seems that there are conflicts occurring in the code."
e99f0efedc30512e308e0684d3fe3afa4d374e34,1652207358,"Add MLFLOW_FLATTEN_PARAMS support in MLflowCallback (#17148)

* add support for MLFLOW_FLATTEN_PARAMS

* ensure key is str

* fix style and update warning msg

* Empty commit to trigger CI

* fix bug in check_inits.py

* add unittest for flatten_dict utils

* fix 'NoneType' object is not callable on __del__

* add generic flatten_dict unittest to SPECIAL_MODULE_TO_TEST_MAP

* fix style","['src/transformers/integrations.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/generic.py', 'tests/utils/test_generic.py', 'utils/check_inits.py', 'utils/tests_fetcher.py']",Lack of support for MLFLOW_FLATTEN_PARAMS in MLflowCallback may be causing issues. Unittest for flatten_dict utilities needed. Encountered 'NoneType' object is not callable error on __del__.
d53dffec6ef5f0cf28df3a1e7f70f1c5da5762ce,1660226083,"Deberta V2: Fix critical trace warnings to allow ONNX export (#18272)

* Fix critical trace warnings to allow ONNX export

* Force input to `sqrt` to be float type

* Cleanup code

* Remove unused import statement

* Update model sew

* Small refactor

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>

* Use broadcasting instead of repeat

* Implement suggestion

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>

* Match deberta v2 changes in sew_d

* Improve code quality

* Update code quality

* Consistency of small refactor

* Match changes in sew_d

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>","['src/transformers/models/deberta_v2/modeling_deberta_v2.py', 'src/transformers/models/sew_d/modeling_sew_d.py']","Critical trace warnings are preventing the successful export of the Deberta V2 model to ONNX.
"
623d8cb475804f2b0f85a47b04b8b2e522db06ef,1643811132,"Adding support for `microphone` streaming within pipeline. (#15046)

* Adding support for `microphone` streaming within pipeline.

- Uses `ffmpeg` to get microphone data.
- Makes sure alignment is made to `size_of_sample`.
- Works by sending `{""raw"": ..data.., ""stride"": (n, left, right),
""partial"": bool}`
directly to the pipeline enabling to stream partial results and still
get inference.
- Let's `partial` information flow through the pipeline to enable caller
  to get it back and choose to display text or not.

- The striding reconstitution is bound to have errors since CTC does not
keep previous state. Currently most of the errors are we don't know if
there's a space or not between two chunks.
Since we have some left striding info, we could use that during decoding
to choose what to do with those spaces and even extra letters maybe (if
the stride is long enough, it's bound to cover at least a few symbols)

Fixing tests.

Protecting with `require_torch`.

`raw_ctc` support for nicer demo.

Post rebase fixes.

Revamp to split raw_mic_data from it's live chunking.

- Requires a refactor to make everything a bit cleaner.

Automatic resampling.

Small fix.

Small fix.

* Post rebase fix (need to let super handle more logic, reorder args.)

* Update docstrings

* Docstring format.

* Remove print.

* Prevent flow of `input_values`.

* Fixing `stride` too.

* Fixing the PR by removing `raw_ctc`.

* Better docstrings.

* Fixing init.

* Update src/transformers/pipelines/audio_utils.py

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* Update tests/test_pipelines_automatic_speech_recognition.py

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* Quality.

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>","['src/transformers/pipelines/audio_utils.py', 'src/transformers/pipelines/automatic_speech_recognition.py', 'tests/test_pipelines_automatic_speech_recognition.py']","Pipeline lacks support for microphone data streaming, causing problems with handling partial results, striding reconstitution errors, and absence of automatic resampling functionality."
a8b6443e065a14af755f8f0ad1d3b5d70cc5f3ed,1648711953,"Refactor Modeling Outputs (#16341)

* first proposal

* replace model outputs in various models

* conflicts

* docstring

* update poolformer

* minor change in docstring

* CI

* removed poolformer specific outputs from doc

* removed convnext specific outputs from doc

* CI

* weird char in segformer

* conversations

* reverted docstring for BaseModelOutputWithPooling

* update outputs

* changed docstring in BaseModelOutput

* updated docstring in modeling outputs

* typos :)

* fixed typo after copy & paste it all around

* CI

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* segformer

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>","['src/transformers/modeling_outputs.py', 'src/transformers/models/beit/modeling_beit.py', 'src/transformers/models/convnext/modeling_convnext.py', 'src/transformers/models/deit/modeling_deit.py', 'src/transformers/models/poolformer/modeling_poolformer.py', 'src/transformers/models/resnet/modeling_resnet.py', 'src/transformers/models/segformer/modeling_segformer.py', 'src/transformers/models/van/modeling_van.py', 'src/transformers/models/vit/modeling_vit.py']","There's inconsistency and redundancy in the model outputs across different models, leading to confusion and less optimal code practices."
12d51db243a00726a548a43cc333390ebae731e3,1680785415,"Backbone add mixin tests (#22542)

* Add out_indices to backbones, deprecate out_features

* Update - can specify both out_features and out_indices but not both

* Add backbone mixin tests

* Test tidy up

* Add test_backbone for convnext

* Remove redefinition of method

* Update for Dinat and Nat backbones

* Update tests

* Smarter indexing

* Add checks on config creation for backbone

* PR comments","['src/transformers/models/maskformer/modeling_maskformer_swin.py', 'tests/models/bit/test_modeling_bit.py', 'tests/models/convnext/test_modeling_convnext.py', 'tests/models/convnextv2/test_modeling_convnextv2.py', 'tests/models/dinat/test_modeling_dinat.py', 'tests/models/maskformer/test_modeling_maskformer_swin.py', 'tests/models/nat/test_modeling_nat.py', 'tests/models/resnet/test_modeling_resnet.py', 'tests/models/swin/test_modeling_swin.py', 'tests/test_backbone_common.py']",Inability to specify both out_features and out_indices in backbones causing indexing issues; lack of tests for backbone mixins and inconsistencies in Dinat and Nat backbones configurations.
b5c6fdecf0cab6ffe22bee2ca5b8474afba0d813,1644416689,"PoC for a ProcessorMixin class (#15549)

* PoC for a ProcessorMixin class

* Documentation

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Roll out to other processors

* Add base feature extractor class in init

* Use args and kwargs

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/clip/processing_clip.py', 'src/transformers/models/layoutlmv2/processing_layoutlmv2.py', 'src/transformers/models/layoutxlm/processing_layoutxlm.py', 'src/transformers/models/speech_to_text/processing_speech_to_text.py', 'src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py', 'src/transformers/models/trocr/processing_trocr.py', 'src/transformers/models/vilt/processing_vilt.py', 'src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py', 'src/transformers/models/wav2vec2/processing_wav2vec2.py', 'src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py', 'src/transformers/processing_utils.py']","The lack of a ProcessorMixin class requires repeated code across different processors, potentially leading to inconsistencies and maintenance difficulties."
e94384e4d879505dd0a0617fd8e199282d309a08,1665579260,"Add depth estimation pipeline (#18618)

* Add initial files for depth estimation pipelines

* Add test file for depth estimation pipeline

* Update model mapping names

* Add updates for depth estimation output

* Add generic test

* Hopefully fixing the tests.

* Check if test passes

* Add make fixup and make fix-copies changes after rebase with main

* Rebase with main

* Fixing up depth pipeline.

* This is not used anymore.

* Fixing the test. `Image` is a module `Image.Image` is the type.

* Update docs/source/en/main_classes/pipelines.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/depth_estimation.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/pipelines/test_pipelines_depth_estimation.py', 'utils/update_metadata.py']","Existing pipeline lacks capability for depth estimation, causing various inconsistencies in output."
b77406bcb20ba3999f9e77fd01d37b1b6e63e847,1667829010,"Removing RobertaConfig inheritance from CamembertConfig (#20059)

* swap RobertaConfig with PretrainedConfig

* Add camembert specific attributes

* Add PretrainedConfig docstring

* Add arguments docstring

* Change CamembertConfig docstring definition

* Fix typo CamembertConfig -> CamembertModel

* Fix typo BertModel -> CamembertModel

* Fix style of CamembertConfig",['src/transformers/models/camembert/configuration_camembert.py'],"CamembertConfig is improperly inheriting from RobertaConfig, causing unexpected behavior in the CamembertModel due to incorrect attributes and docstring definitions."
501307b58bdc2db1b6a25271a3f60975130f1c6c,1640605072,"Add `ElectraForCausalLM` -> Enable Electra encoder-decoder model (#14729)

* Add ElectraForCausalLM and cover some basic tests & need to fix a few tests

* Fix bugs

* make style

* make fix-copies

* Update doc

* Change docstring to markdown format

* Remove redundant update_keys_to_ignore","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/electra/__init__.py', 'src/transformers/models/electra/modeling_electra.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_electra.py']","The Electra encoder-decoder model is unsupported and lacks basic tests, resulting in failed test cases."
d6bf08f7f6f8bf5d6d94e9b10b7d8203906353ad,1692284432,"[`resize_embedding`] Introduce `pad_to_multiple_of` and guidance (#25088)

* fix

* revert cahnges and update resizing of embedding layer

* use wraning

* fixup

* more styling nits

* fix all tests that overload the embedding tests

* 👀👀 remove breakpoint

* remove useless overload + overload correctly where needed

* resize lm head with new vocab size

* reverse not necessary changes

* style

* fix CIs!

* fix last CI tests, adapt bark and Marian

* fixup","['src/transformers/modeling_utils.py', 'src/transformers/models/bark/modeling_bark.py', 'src/transformers/models/bart/modeling_bart.py', 'src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py', 'src/transformers/models/blenderbot/modeling_blenderbot.py', 'src/transformers/models/blenderbot_small/modeling_blenderbot_small.py', 'src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py', 'src/transformers/models/led/modeling_led.py', 'src/transformers/models/m2m_100/modeling_m2m_100.py', 'src/transformers/models/marian/modeling_marian.py', 'src/transformers/models/mbart/modeling_mbart.py', 'src/transformers/models/mvp/modeling_mvp.py', 'src/transformers/models/nllb_moe/modeling_nllb_moe.py', 'src/transformers/models/pegasus/modeling_pegasus.py', 'src/transformers/models/pegasus_x/modeling_pegasus_x.py', 'src/transformers/models/plbart/modeling_plbart.py', 'src/transformers/models/speech_to_text/modeling_speech_to_text.py', 'src/transformers/models/speecht5/modeling_speecht5.py', 'src/transformers/models/whisper/modeling_whisper.py', 'tests/test_modeling_common.py']",Resizing of the embed layer seems incorrect and causing various test failures. Additional overload on embedding tests not providing desired functionality. Problems also observed with the Language Model head while using the new vocab size.
d7389cd20168052e5fc7abe0cf31cd1eb960fbc9,1686649361,"fix: TextIteratorStreamer cannot work with pipeline (#23641)

* fix: TextIteratorStreamer cannot work with pipeline

Deepcopying the TextIteratorStreamer object causes the exception.

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Update src/transformers/pipelines/text_generation.py

Got it. I will update the patch.

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update src/transformers/pipelines/text_generation.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update text_generation.py

---------

Signed-off-by: yuanwu <yuan.wu@intel.com>
Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",['src/transformers/pipelines/text_generation.py'],Deepcopying the TextIteratorStreamer object for pipelines results in an exception.
7487829a230de3823be61020d05949a12d1a1756,1666355530,"Added support for multivariate independent emission heads (#19453)

* Added support for multivariate independent emission heads

* fix typo

* rename distr_cls

* scale is a vector for multivariate

* set affine transform event_dim

* fix typo

* added variable

* added beta in the config

* set beta

* remove beta-nll option in nll","['src/transformers/models/time_series_transformer/configuration_time_series_transformer.py', 'src/transformers/models/time_series_transformer/modeling_time_series_transformer.py']",Lack of support for multivariate independent emission heads in the existing software framework.
5506d0496957cde19318eee3d34ee682b654abe8,1679928455,"Seq2seq trainer generation config arg (#22323)

* seq2seq trainer and training arguments accepting GenerationConfig arg

* seq2seq Trainer and training arguments docstring fixes

* Update training_args_seq2seq.py docstring

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Fixing trainer_seq2seq.py docstring

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* seq2seq trainer: legacy gen args back & GenerationConfig created at init

* Seq2seq trainer: fix in case gen_config.max_new_tokens is None

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* seq2seq trainer: adding legacy arg retrocompatibility

* seq2seq trainer and training arguments accepting GenerationConfig arg

* seq2seq Trainer and training arguments docstring fixes

* Update training_args_seq2seq.py docstring

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Fixing trainer_seq2seq.py docstring

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* seq2seq trainer: legacy gen args back & GenerationConfig created at init

* Seq2seq trainer: fix in case gen_config.max_new_tokens is None

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* seq2seq trainer: adding legacy arg retrocompatibility

* seq2seq trainer: evaluate and predict untouched

* Apply suggestions from code review

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* seq2seq trainer: adding init args, keeping IDEs hints

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/trainer_seq2seq.py', 'src/transformers/training_args_seq2seq.py']",Seq2seq trainer and training arguments currently lack support for GenerationConfig arguments. Issue arises when gen_config.max_new_tokens is None. Legacy arguments not maintained in new version.
45da7cec5aa1b1bf1031af9caa9085e95e262e11,1672777104,"Add custom stop token ids for generation (#20727)

* Add StopIdStoppingCriteria

* add a working test for stop id criteria

* add to global scope

* add stop_ids to generate

* add pipeline test

* use tokenizer encode in test

* add test to generation utils

* reformat

* fixup

* make-fix-copies

* rename to stop_token_id

* use stop_tokens instead

* add to text to text generation

* make fixup

* make repo-consistency

* Add support for list of ints for eos_token_id inside generation/utils.py

* Instead of having if elses, cast the eos_token_id into a List[int]

* Add List[int] support for logits_process.py

* add List[int] for beam_search.py

* add List[int] for forced_eos_token_id

* revert stop token id stopping criteria changes

* make fixup

* fix tests

* add eos_token_id to generation/utils.py and added tests test_utils.py

* add eos_token_id type hints and fix for pad tokens

* add comments

* remove some prints and remove forced false test

* fix

* put back test_stop_sequence_stopping_criteria

* remove unused import and make fixup

* add a none check

* update docstring

* add more docstring for list ints

* make fixup","['src/transformers/generation/beam_search.py', 'src/transformers/generation/configuration_utils.py', 'src/transformers/generation/logits_process.py', 'src/transformers/generation/utils.py', 'tests/generation/test_utils.py']","The text generation process does not support using a list of integer IDs for eos_token_id. Moreover, stop token IDs are not customizable for generation."
f9a0008d2d3082a665f711b24f5314e4a8205fab,1659628975,"Add VideoMAE (#17821)

* First draft

* Add VideoMAEForVideoClassification

* Improve conversion script

* Add VideoMAEForPreTraining

* Add VideoMAEFeatureExtractor

* Improve VideoMAEFeatureExtractor

* Improve docs

* Add first draft of model tests

* Improve VideoMAEForPreTraining

* Fix base_model_prefix

* Make model take pixel_values of shape (B, T, C, H, W)

* Add loss computation of VideoMAEForPreTraining

* Improve tests

* Improve model testsé

* Make all tests pass

* Add VideoMAE to main README

* Add tests for VideoMAEFeatureExtractor

* Add integration test

* Improve conversion script

* Rename patch embedding class

* Remove VideoMAELayer from init

* Update design of patch embeddings

* Improve comments

* Improve conversion script

* Improve conversion script

* Add conversion of pretrained model

* Add loss verification of pretrained model

* Add loss verification of unnormalized targets

* Add integration test for pretraining model

* Apply suggestions from code review

* Fix bug to make feature extractor resize only shorter edge

* Address more comments

* Improve normalization of videos

* Add doc examples

* Move constants to dedicated script

* Remove scripts

* Transfer checkpoints, fix docs

* Update script

* Update image mean and std

* Fix doc tests

* Set return_tensors to NumPy by default

* Revert the previous change

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/image_utils.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/videomae/__init__.py', 'src/transformers/models/videomae/configuration_videomae.py', 'src/transformers/models/videomae/convert_videomae_to_pytorch.py', 'src/transformers/models/videomae/feature_extraction_videomae.py', 'src/transformers/models/videomae/modeling_videomae.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/constants.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/models/videomae/test_feature_extraction_videomae.py', 'tests/models/videomae/test_modeling_videomae.py', 'tests/test_feature_extraction_common.py', 'tests/test_modeling_common.py']","The VideoMAE model does not handle pixel values in the required shape (B, T, C, H, W), and a bug in the feature extractor is causing inappropriate resizing of images. The pretrained model lacks the ability to transcode checkpoints, and the model tests need enhancement."
e46ad22cd6cb28f78f4d9b6314e7581d8fd97dc5,1628786754,"Improve type checker performance (#13094)

* conditional declare `TOKENIZER_MAPPING_NAMES` within a `if TYPE_CHECKING` block so that type checkers dont need to evaluate the RHS of the assignment.

this improves performance of the pylance/pyright type checkers

* Update src/transformers/models/auto/tokenization_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* adding missing import

* format

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/models/auto/tokenization_auto.py'],"Type checker performance is slow due to unnecessary evaluation of `TOKENIZER_MAPPING_NAMES` assignment, impacting pylance/pyright type checkers."
1ccc033c56f7686d21e2ab6ed2bd042c75316064,1638795711,"Update the example of exporting Bart + BeamSearch to ONNX module to resolve comments. (#14310)

* Update code to resolve comments left in previous PR.

* Add README.md file for this example.

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update README.md file to resolve comments.

* Add a section name.

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: Gary Miguel <garymm@garymm.org>

* Add more comments for _convert_past_list_to_tuple().

* Change the default file name to a consistent one.

* Fix a format issue.

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: Gary Miguel <garymm@garymm.org>

* Update examples/onnx/pytorch/translation/run_onnx_exporter.py

Co-authored-by: Gary Miguel <garymm@garymm.org>

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Change the folder to summarization and address some other coments.

* Update the torch version.

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Gary Miguel <garymm@garymm.org>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>","['examples/onnx/pytorch/summarization/bart_onnx/generation_onnx.py', 'examples/onnx/pytorch/summarization/bart_onnx/reduce_onnx_size.py', 'examples/onnx/pytorch/summarization/run_onnx_exporter.py']","Previous PR left various unresolved issues: inconsistent naming in example of exporting Bart + BeamSearch to ONNX module, incorrect _convert_past_list_to_tuple() comments, and unavailable README.md file."
dc9147ff362e4e69829f64d28178c77cab4bef6f,1658224955,"Custom pipeline (#18079)

* Initial work

* More work

* Add tests for custom pipelines on the Hub

* Protect import

* Make the test work for TF as well

* Last PyTorch specific bit

* Add documentation

* Style

* Title in toc

* Bad names!

* Update docs/source/en/add_new_pipeline.mdx

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

* Auto stash before merge of ""custom_pipeline"" and ""origin/custom_pipeline""

* Address review comments

* Address more review comments

* Update src/transformers/pipelines/__init__.py

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>","['src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/base.py', 'tests/pipelines/test_pipelines_common.py', 'utils/test_module/custom_pipeline.py']",Incomplete support for custom pipelines in the Hub and lack of necessary tests for TensorFlow and PyTorch integrations.
3b3024da70a7ada6599390c5b3e1a721c9a4aa4c,1666012576,"TF port of ESM (#19587)

* Partial TF port for ESM model

* Add ESM-TF tests

* Add the various imports for TF-ESM

* TF weight conversion almost ready

* Stop ignoring the decoder weights in PT

* Add tests and lots of fixes

* fix-copies

* Fix imports, add model docs

* Add get_vocab() to tokenizer

* Fix vocab links for pretrained files

* Allow multiple inputs with a sep

* Use EOS as SEP token because ESM vocab lacks SEP

* Correctly return special tokens mask from ESM tokenizer

* make fixup

* Stop testing unsupported embedding resizing

* Handle TF bias correctly

* Skip all models with slow tokenizers in the token classification test

* Fixing the batch/unbatcher of pipelines to accomodate the `None` being

passed around.

* Fixing pipeline bug caused by slow tokenizer  being different.

* Update src/transformers/models/esm/modeling_tf_esm.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update src/transformers/models/esm/modeling_tf_esm.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update src/transformers/models/esm/modeling_tf_esm.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update set_input_embeddings and the copyright notices

Co-authored-by: Your Name <you@example.com>
Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/esm/__init__.py', 'src/transformers/models/esm/modeling_esm.py', 'src/transformers/models/esm/modeling_tf_esm.py', 'src/transformers/models/esm/tokenization_esm.py', 'src/transformers/pipelines/fill_mask.py', 'src/transformers/pipelines/pt_utils.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/models/esm/test_modeling_esm.py', 'tests/models/esm/test_modeling_tf_esm.py', 'tests/pipelines/test_pipelines_token_classification.py']","Various issues with Tensorflow port for ESM model - SOAP token is missing, slow tokenizers cause batch/unbatch pipeline failures, unsupported embedding resizing being tested, and incorrect TF bias handling."
c2c99dc7ef5edab8f7674a1eb00cf6ac6996fd0f,1682694092,"add open-llama model with ckpt (#22795)

* update Open-Llama model

* update

* update format

* update doc

* update

* update stable embedding test

* update test case

* update format

* update readme

* fix typo

* update name

* remove tokenizer and update format

* remove convert_open_llama_weights_to_hf

* update warning and doc_string

---------

Co-authored-by: songliang.bayesian <songliang.bayesian@bytedance.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/open_llama/__init__.py', 'src/transformers/models/open_llama/configuration_open_llama.py', 'src/transformers/models/open_llama/modeling_open_llama.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/open_llama/test_modeling_open_llama.py']","The Open-Llama model is missing from the repository, requiring an update, including its documentation, test cases, and format adjustments. Furthermore, the tokenizer needs to be removed."
c8545d2a9c38aa998b874e982b379610829af867,1677233272,"[Whisper] Add SpecAugment (#21298)

* Return and rescale attention_mask

* Add SpecAugment to Whisper modeling

* Fix test

* Update docstring

* Add SpecAug related parameters to model config

* Add the _mask_input_features function to doc

* Fix quality

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Remove dev comments

* Add test

* Resolve conflict

* feat: mask {feature, time} prob fast tests

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: sanchit-gandhi <sanchit@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/whisper/configuration_whisper.py', 'src/transformers/models/whisper/feature_extraction_whisper.py', 'src/transformers/models/whisper/modeling_whisper.py', 'tests/models/whisper/test_modeling_whisper.py']","Whisper model lacks SpecAugment capabilities and associated necessary configurations, potentially affecting model performance and feature scalability."
c53c8e490c158f505a9271f7f5d8248473da3d24,1690376841,"fix ""UserWarning: Creating a tensor from a list of numpy.ndarrays is … (#24772)

fix ""UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.""

Co-authored-by: 刘长伟 <hzliuchw@corp.netease.com>",['src/transformers/tokenization_utils_base.py'],Creating a tensor from a list of numpy.ndarrays triggers a user warning about performance and suggests converting to a single numpy.ndarray.
347ba38cb45e30a4f98aa730aeb540bf538bf176,1666961078,"Support segformer fx (#19924)

* Support segformer fx

* Add fx_compatible attribute to test_modeling_segformer.py

* Update glpn model (fx support)

glpn model was copied from segformer.

* Update utils/fx.py | add semantic-segmentation

for SegformerForSemanticSegmentation model

* Fix minor import order(isort)

* Add random input generation for segformer fx

Co-authored-by: noelbird <lduldu00228@gmail.com>","['src/transformers/models/glpn/modeling_glpn.py', 'src/transformers/models/segformer/modeling_segformer.py', 'src/transformers/utils/fx.py', 'tests/models/segformer/test_modeling_segformer.py']","The Segformer model lacks support for fx causing inconsistencies in semantic-segmentation and glpn models, and the test_modeling_segformer.py lacks an fx_compatible attribute. Also, random input generation for segformer fx is missing."
16121bae5cfab419cc8d68e3721a83dba8344d27,1678910078,"Update BridgeTowerForContrastiveLearning (#22145)

* Use return_loss for BridgeTowerForContrastiveLearning, add example

* fix tests

* Update example in BridgeTowerForContrastiveLearning

* Update test_modeling_bridgetower.py

* update model output format

* minor update

* Update src/transformers/models/bridgetower/modeling_bridgetower.py

* make style

---------

Co-authored-by: Tiep Le <97980157+tileintel@users.noreply.github.com>
Co-authored-by: Tiep Le <tiep.le@intel.com>
Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/bridgetower/modeling_bridgetower.py', 'tests/models/bridgetower/test_modeling_bridgetower.py']","The BridgeTowerForContrastiveLearning model does not properly return its loss, causing issues with example output and alignment with output formatting and style in the model and associated tests."
5f9b2ce0ea89448d08e367b308e0a977ee427760,1672212294,"Avoid collisions in writing metrics via 2 APIs - azureml + mlflow (#20837)

* Avoid collisions in writing metrics via 2 APIs - azureml + mlflow

MLflow tracking API is enabled by default in AzureML and HF MLflow integration is more fully featured. I'd remove the AzureML integration but leaving the current behavior for backwards compatibility (though it should really be removed)

* Trigger CI",['src/transformers/integrations.py'],"Metrics written via AzureML and MLflow APIs are colliding, potentially resulting in inaccurate tracking of machine learning experiments."
2c887cf8e0cb1ac96d28361ff3235a77f83c36ee,1686166292,"Do not prepare lr scheduler as it as the right number of steps (#24088)

* Do not prepare lr scheduler as it as the right number of steps

* Trigger CI

* Trigger CI

* Trigger CI

* Add fake comment

* Remove fake comment

* Trigger CI please!",['src/transformers/trainer.py'],The learning rate (lr) scheduler is preparing unnecessarily even though it already has the correct number of steps.
0c1c42c120519eca74082f819a20bfe3f02fe027,1627302605,"add `classifier_dropout` to classification heads (#12794)

* add classifier_dropout to Electra

* no type annotations yet

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add classifier_dropout to Electra

* add classifier_dropout to Electra ForTokenClass.

* add classifier_dropout to bert

* add classifier_dropout to roberta

* add classifier_dropout to big_bird

* add classifier_dropout to mobilebert

* empty commit to trigger CI

* add classifier_dropout to reformer

* add classifier_dropout to ConvBERT

* add classifier_dropout to Albert

* add classifier_dropout to Albert

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/albert/modeling_albert.py', 'src/transformers/models/albert/modeling_tf_albert.py', 'src/transformers/models/bert/configuration_bert.py', 'src/transformers/models/bert/modeling_bert.py', 'src/transformers/models/bert/modeling_flax_bert.py', 'src/transformers/models/bert/modeling_tf_bert.py', 'src/transformers/models/big_bird/configuration_big_bird.py', 'src/transformers/models/big_bird/modeling_big_bird.py', 'src/transformers/models/big_bird/modeling_flax_big_bird.py', 'src/transformers/models/convbert/configuration_convbert.py', 'src/transformers/models/convbert/modeling_convbert.py', 'src/transformers/models/convbert/modeling_tf_convbert.py', 'src/transformers/models/electra/configuration_electra.py', 'src/transformers/models/electra/modeling_electra.py', 'src/transformers/models/electra/modeling_flax_electra.py', 'src/transformers/models/electra/modeling_tf_electra.py', 'src/transformers/models/mobilebert/configuration_mobilebert.py', 'src/transformers/models/mobilebert/modeling_mobilebert.py', 'src/transformers/models/mobilebert/modeling_tf_mobilebert.py', 'src/transformers/models/reformer/configuration_reformer.py', 'src/transformers/models/reformer/modeling_reformer.py', 'src/transformers/models/roberta/modeling_flax_roberta.py', 'src/transformers/models/roberta/modeling_roberta.py', 'src/transformers/models/roberta/modeling_tf_roberta.py']","Classification heads including Electra, Bert, Roberta, big_bird, mobilebert, reformer, ConvBERT, Albert do not currently include `classifier_dropout`, leading to potential overfitting issues."
1670be4bdec19d5a8893f943bf78a8d9b3dc8911,1680767583,"Adding Llama FastTokenizer support. (#22264)

* Adding Llama FastTokenizer support.

- Requires https://github.com/huggingface/tokenizers/pull/1183 version
- Only support byte_fallback for llama, raise otherwise (safety net).
- Lots of questions are special tokens

How to test:

```python

from transformers.convert_slow_tokenizer import convert_slow_tokenizer
from transformers import AutoTokenizer
from tokenizers import Tokenizer

tokenizer = AutoTokenizer.from_pretrained(""huggingface/llama-7b"")

if False:
    new_tokenizer = Tokenizer.from_file(""tok.json"")
else:
    new_tokenizer = convert_slow_tokenizer(tokenizer)
    new_tokenizer.save(""tok.json"")

strings = [
    ""This is a test"",
    ""生活的真谛是"",
    ""生活的真谛是[MASK]。"",
    # XXX: This one is problematic because of special tokens
    # ""<s> Something something"",
]

for string in strings:
    encoded = tokenizer(string)[""input_ids""]
    encoded2 = new_tokenizer.encode(string).ids

    assert encoded == encoded2, f""{encoded} != {encoded2}""

    decoded = tokenizer.decode(encoded)
    decoded2 = new_tokenizer.decode(encoded2)

    assert decoded.strip() == decoded2, f""{repr(decoded)} != {repr(decoded2)}""
```

The converter + some test script.

The test script.

Tmp save.

Adding Fast tokenizer + tests.

Adding the tokenization tests.

Correct combination.

Small fix.

Fixing tests.

Fixing with latest update.

Rebased.

fix copies + normalized added tokens  + copies.

Adding doc.

TMP.

Doc + split files.

Doc.

Versions + try import.

Fix Camembert + warnings -> Error.

Fix by ArthurZucker.

Not a decorator.

* Fixing comments.

* Adding more to docstring.

* Doc rewriting.","['setup.py', 'src/transformers/__init__.py', 'src/transformers/convert_slow_tokenizer.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/llama/__init__.py', 'src/transformers/models/llama/tokenization_llama_fast.py', 'src/transformers/utils/dummy_tokenizers_objects.py', 'tests/models/llama/test_tokenization_llama.py', 'tests/utils/test_convert_slow_tokenizer.py']","The current tokenizer does not support Llama FastTokenizer, creating potential compatibility and performance issues. Special token handling raises questions."
4e94c6c00847ddf809b18b962edcbb3dabeb202d,1679483694,"Fix position embeddings for GPT-J and CodeGen (#22069)

* Revert ""[GPT-J] add deprecation warning (#21869)""

This reverts commit fb76994c41d1eaf09e50020cbd849d3bb686b6a3.

* Fix position embeddings for GPT-J and CodeGen

* Address review comments from @gante

* Fix ""Copied from"" comment referencing wrong function

* Fix copy/paste mistake

* Fix training path

* Hopefully make torch.fx happy

* Move position_ids long cast

* Revert ""Hopefully make torch.fx happy""

This reverts commit e41a6f4cad3ff441124c7457b19cfb630d4ca025.

* Changes to help with torch.fx tracing

* Linter fix

* Correct position_ids tensor type hint

* Work-around torch.fx tracing issue

* Get the changes to work with torch.fx

* Address review comment from @michaelbenayoun

* Another small adjustment

* Add explanatory comment; small code tidyup","['src/transformers/models/codegen/modeling_codegen.py', 'src/transformers/models/gptj/modeling_gptj.py', 'src/transformers/utils/fx.py']","GPT-J and CodeGen have issues with position embeddings during model training and effects on torch.fx tracing, exhibiting unexpected behaviors."
70a98024b1b0007d2d8bdced854cd9b638dbb07b,1693924902,"Patch with accelerate xpu (#25714)

* patch with accelerate xpu

* patch with accelerate xpu

* formatting

* fix tests

* revert ruff unrelated fixes

* revert ruff unrelated fixes

* revert ruff unrelated fixes

* fix test

* review fixes

* review fixes

* black fixed

* review commits

* review commits

* style fix

* use pytorch_utils

* revert markuplm test","['src/transformers/__init__.py', 'src/transformers/testing_utils.py', 'src/transformers/trainer_utils.py', 'src/transformers/training_args.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/import_utils.py', 'tests/trainer/test_trainer_distributed.py']","Accelerating XPU patches seem to be non-functioning, which might be causing test failures and performance degradation."
66ea739168cdb53a8a3ade35febf3a27f4cd1387,1638517150,"Improve tokenizer tests (#13594)

* Use new method to acquire tokenizers

* Resolve TODOs.

* Style

* Fix

* Enable do_lower_case in test_tokenize_special_tokens

* Apply suggestion from code review

* Fix mask token handling

* Revert ""Fix mask token handling""

This reverts commit daaa3f5291b1f71e5bc3604ca281c000000c4648.

* Fix FNet mask token tokenization

* Complete everything

* Apply suggestions from code review",['tests/test_tokenization_common.py'],"Issues in tokenizer tests, including unresolved TODOs, improper handling of mask tokens, and issues related to tokenizing special tokens in FNet."
5483388631f864e1ab7000010a3478e249b9ab13,1655128767,"Update modeling_gpt_neox.py (#17575)

I'm guessing that the intention was to have the `_no_split_modules` class attribute for `GPTNeoXPreTrainedModel` to be set to `[""GPTNeoXLayer""]`, akin to how its set as `[""GPTJBlock""]` for `GPTJPreTrainedModel`.

If this is incorrect, please feel free to just close the PR.

Thanks!",['src/transformers/models/gpt_neox/modeling_gpt_neox.py'],`GPTNeoXPreTrainedModel` class attribute `_no_split_modules` is incorrectly configured and differs from similar structure in `GPTJPreTrainedModel`.
2e20c0f34ade1f0ec6fa1ed24fd1de0b8970f0da,1629734249,"Make Flax GPT2 working with cross attention (#13008)

* make flax gpt2 working with cross attention

* Remove encoder->decoder projection layer

* A draft (incomplete) for FlaxEncoderDecoderModel

* Add the method from_encoder_decoder_pretrained + the docstrings

* Fix the mistakes of using EncoderDecoderModel

* Fix style

* Add FlaxEncoderDecoderModel to the library

* Fix cyclic imports

* Add FlaxEncoderDecoderModel to modeling_flax_auto.py

* Remove question comments

* add tests for FlaxEncoderDecoderModel

* add flax_encoder_decoder to the lists of ignored entries in check_repo.py

* fix missing required positional arguments

* Remove **kwargs when creating FlaxEncoderDecoderModel in from_encoder_decoder_pretrained()

Also fix generation eos/pad tokens issue

* Fix: Use sequences from the generated_output

* Change a check from assert to raise ValueError

* Fix examples and token ids issues

* Fix missing all_cross_attentions when outputting tuple in modeling_gpt2

* Remove the changes in configuration docstrings.

* allow for bert 2 gpt2

* make fix-copies

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Change remaining examples to bert2gpt2

* Change the test to Bert2GPT2

* Fix examples

* Fix import

* Fix unpack bug

* Rename to FlaxEncoderDecoderModelTest and change the test to bert2gpt2

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Fix: NotImplentedError -> NotImplementedError

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* up

* finalize

Co-authored-by: ydshieh <ydshieh@user.noreply>
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/bert/modeling_flax_bert.py', 'src/transformers/models/encoder_decoder/__init__.py', 'src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py', 'src/transformers/models/gpt2/modeling_flax_gpt2.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/models/roberta/modeling_flax_roberta.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/test_modeling_encoder_decoder.py', 'tests/test_modeling_flax_encoder_decoder.py', 'tests/test_modeling_flax_gpt2.py', 'utils/check_repo.py']","Flax GPT2 implementation is unable to work with cross attention, leading to issues with encoder-decoder models and generation of eos/pad tokens."
1a66a6c67734f39e8dddc3c5635b8b845f748c39,1652330107,"Translate index.mdx (to ES) and add Spanish models to quicktour.mdx examples (#16685)

* Change nits in Spanish for quicktour.mdx

- Add tasks names in English too.
- Fix small nits in Spanish

* Translate index.mdx to Spanish

* Translate body of index.
* Translated the compatible models list (not the papers´ names). Since this should not be updated manually, I can come back to the original text.

* Add models and a  dataset for Spanish in the code exmaples

* Replaced the English models to Spanish versions.

* Add index to _toctree.yml and fix Spanish

* Fix double ““ error

* Change negative example in ASR example

* make style

* Debug style in quicktour.mdx",['docs/source/es/_config.py'],"The repository lacks Spanish translations for index.mdx and quicktour.mdx, and the examples in quicktour.mdx don't have Spanish models or datasets."
367fdf3330121a075c06d796bb95dfb1c69c65e4,1672745342,"`MinNewTokensLengthLogitsProcessor` for `.generate` method #20814 (#20892)

* feat: add min new length logit processor

* test: add min new length logit processor

* docs: add MinNewTokensLengthLogitsProcessor

* feat: import MinNewTokensLengthLogitsProcessor

* fix: update pytorch dummy objects

* refactor & fix: rename attributes and var and get rid of dynamic attribute

* tests: align test with new interface

* docs: fix typo

* docs: minor clarification

* Empty-Commit

* empty commit

* run automated quality edits

Co-authored-by: Joao Gante <joao@huggingface.co>","['src/transformers/__init__.py', 'src/transformers/generation/__init__.py', 'src/transformers/generation/logits_process.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/generation/test_logits_process.py']","The `.generate` method lacks a feature to manage minimal new token length, making it less effective in handling specific token processing requirements."
367558b90d506e5ea668064e7676698c71ce16df,1649166645,"Adding missing type hints for BigBird model   (#16555)

* added type hints for mbart tensorflow tf implementation

* Adding missing type hints for mBART model 

Tensorflow Implementation model added with missing type hints

* Missing Type hints - correction

For TF model

* Code fixup using make quality tests

* Hint types - typo error

* make fix-copies and make fixup

* type hints

* updated files

* type hints update

* making dependent modesls coherent

* Type hints for BigBird

* removing typos

Co-authored-by: matt <rocketknight1@gmail.com>",['src/transformers/models/big_bird/modeling_big_bird.py'],"BigBird and mBART model implementations in TensorFlow are missing proper type hints, leading to less coherent and harder to maintain code."
0759f2510ccdbf79d0c77c85a572516eaf7406a1,1629991520,"Add DINO conversion script (#13265)

* First commit

* Add interpolation of patch embeddings

* Comment out code

* Fix bug

* Fix another bug

* Fix bug

* Fix another bug

* Remove print statements

* Update conversion script

* Use the official vit implementation

* Add support for converting dino_vits8

* Add DINO to docs of ViT

* Remove assertion

* Add interpolation of position encodings

* Fix bug

* Add align_corners

* Add interpolate_pos_encoding option to forward pass of ViTModel

* Improve interpolate_pos_encoding method

* Add docstring","['src/transformers/models/deit/modeling_deit.py', 'src/transformers/models/vit/convert_dino_to_pytorch.py', 'src/transformers/models/vit/modeling_vit.py']","Issues with the current implementation of DINO in ViTModel; Position encodings and patch embeddings not interpolated, and lack of conversion script for dino_vits8."
7f74433814c916079dfaeb2de4163bc044b46717,1668531007,"[CLIP] allow loading projection layer in vision and text model (#18962)

* allow loading projection in text and vision model

* begin tests

* finish test for CLIPTextModelTest

* style

* add slow tests

* add new classes for projection heads

* remove with_projection

* add in init

* add in doc

* fix tests

* fix some more tests

* fix copies

* fix docs

* remove leftover from fix-copies

* add the head models in IGNORE_NON_AUTO_CONFIGURED

* fix docstr

* fix tests

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* add docstr for models

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/clip/__init__.py', 'src/transformers/models/clip/configuration_clip.py', 'src/transformers/models/clip/modeling_clip.py', 'src/transformers/models/clipseg/modeling_clipseg.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/fx.py', 'tests/models/clip/test_modeling_clip.py', 'utils/check_repo.py']","Unable to load projection layer in both text and vision models of CLIP, causing issues in model functionality and creating difficulties in testing."
3efcfeab672323b1336bbeb50d5a23a0c663e9e1,1630405967,"Deberta_v2 tf (#13120)

* Deberta_v2 tf

* added new line at the end of file, make style

* +V2, typo

* remove never executed branch of code

* rm cmnt and fixed typo in url filter

* cleanup according to review comments

* added #Copied from","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/deberta_v2/__init__.py', 'src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/test_modeling_tf_deberta_v2.py']","There is unreachable code in the Deberta_v2 tf component, resulting in unnecessary code clutter. Also noticed typos in V2 and the url filter that need fixing."
a5ca56ff158075351149220319c14dde555a86f5,1660313709,"Supporting seq2seq models for `bitsandbytes` integration (#18579)

* Supporting seq2seq models for `bitsandbytes` integration

- `bitsandbytes` integration supports now seq2seq models
- check if a model has tied weights as an additional check

* small modification

- tie the weights before looking at tied weights!","['src/transformers/utils/bitsandbytes.py', 'tests/mixed_int8/test_mixed_int8.py']",The `bitsandbytes` integration currently lacks support for seq2seq models and fails to check for tied weights in a model.
a3bd7637322b5928409da695586eefb482c0c9f0,1627309286,"Better heuristic for token-classification pipeline. (#12611)

* Better heuristic for token-classification pipeline.

Relooking at the problem makes thing actually much simpler,
when we look at ids from a tokenizer, we have no way in **general**
to recover if some substring is part of a word or not.

However, within the pipeline, with offsets we still have access to the
original string, so we can simply look if previous character (if it
exists) of a token, is actually a space. This will obviously be wrong
for tokenizers that contain spaces within tokens, tokenizers where
offsets include spaces too (Don't think there are a lot).

This heuristic hopefully is fully bc and still can handle non-word based
tokenizers.

* Updating test with real values.

* We still need the older ""correct"" heuristic to prevent fusing
punctuation.

* Adding a real warning when important.","['src/transformers/pipelines/token_classification.py', 'tests/test_pipelines_token_classification.py']","The current heuristic for the token-classification pipeline is unable to accurately determine if a substring is part of a word or not, causing issues with word-based tokenizers."
cc44e72d147f9d334367acf96045704194357903,1677494624,"[Pipeline] Add zero shot audio classificatoin pipeline (#21600)

* add pipeline

* update init

* add zero shot to init

* update inits and correct checkpoints

* update base to support input features

* add tests

* Update src/transformers/pipelines/zero_shot_audio_classification.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/pipelines/zero_shot_audio_classification.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* update pieline code

* use tiny checkpoint

* nits and expected value with tiny model

* style

* last nit on tests values

* fix styling

* fix collate fn that was casting t float

* update

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/clap/modeling_clap.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/base.py', 'src/transformers/pipelines/zero_shot_audio_classification.py', 'tests/pipelines/test_pipelines_zero_shot_audio_classification.py']","The current pipeline lacks support for zero-shot audio classification, which affects feature inputs and test capabilities."
2e7e4280aa6f380a4e3afad6524295a17901c56c,1653321040,"Traced models serialization and torchscripting fix (#17206)

* Fix torch.jit.script and pickling issues

* Fix get_attr issues

* Fix import in function

* Fix GPT-J and T5 tracing for torch=1.11

* Gate graph surgery on torch version

* Modeling minor changes to enable TorchScripting

* Model serialization / deserialization test

* Remove _assert_is_none users
","['src/transformers/models/decision_transformer/modeling_decision_transformer.py', 'src/transformers/models/distilbert/modeling_distilbert.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/models/gpt_neo/modeling_gpt_neo.py', 'src/transformers/models/gptj/modeling_gptj.py', 'src/transformers/models/mobilebert/modeling_mobilebert.py', 'src/transformers/utils/fx.py', 'src/transformers/utils/import_utils.py', 'tests/models/swin/test_modeling_swin.py', 'tests/test_modeling_common.py']","Torch.jit.script and pickling issues present in model serialization and deserialization along with get_attr problems. Furthermore, issues exist in GPT-J and T5 tracing for torch=1.11."
82aac00e0f8a14b1012fa5812a97bd15435cc57a,1677862644,"[Flan-UL2] Add-flan-ul2 (#21929)

* add doc and readme

* add model docs

* update toctree and fix copies

* update

* update doc file

* fix

* add FLAN-UL2 to configuration mapping

* fixup

* Apply suggestions from code review

* more clarification

---------

Co-authored-by: younesbelakda <younesbelkada@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>",['src/transformers/models/auto/configuration_auto.py'],"The FLAN-UL2 model is not available in the configuration mapping, causing issues with model functionality and documentation."
6ac77534bfe97c00e0127bb4fc846ae0faf1c9c5,1642795209,"Refine errors for pretrained objects (#15261)

* Refine errors for pretrained objects

* PoC to avoid using get_list_of_files

* Adapt tests to use new errors

* Quality + Fix PoC

* Revert ""PoC to avoid using get_list_of_files""

This reverts commit cb93b7cae8504ef837c2a7663cb7955e714f323e.

* Revert ""Quality + Fix PoC""

This reverts commit 3ba6d0d4ca546708b31d355baa9e68ba9736508f.

* Fix doc

* Revert PoC

* Add feature extractors

* More tests and PT model

* Adapt error message

* Feature extractor tests

* TF model

* Flax model and test

* Merge flax auto tests

* Add tokenization

* Fix test","['src/transformers/configuration_utils.py', 'src/transformers/feature_extraction_utils.py', 'src/transformers/file_utils.py', 'src/transformers/modeling_flax_utils.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/modeling_utils.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/tokenization_utils_base.py', 'tests/test_configuration_auto.py', 'tests/test_feature_extraction_auto.py', 'tests/test_file_utils.py', 'tests/test_modeling_auto.py', 'tests/test_modeling_flax_auto.py', 'tests/test_modeling_tf_auto.py', 'tests/test_tokenization_auto.py', 'utils/tests_fetcher.py']",Pretrained object errors are unclear and the use of get_list_of_files potentially leads to inconsistencies and issues in testing.
2f53ab5745cee94e7d8bb2c8a097f4f2edef15ed,1664984497,"Add sudachi and jumanpp tokenizers for bert_japanese (#19043)

* add sudachipy and jumanpp tokenizers for bert_japanese

* use ImportError instead of ModuleNotFoundError in SudachiTokenizer and JumanppTokenizer

* put test cases of test_tokenization_bert_japanese in one line

* add require_sudachi and require_jumanpp decorator for testing

* add sudachi and pyknp(jumanpp) to dependencies

* remove sudachi_dict_small and sudachi_dict_full from dependencies

* empty commit for ci","['setup.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/models/bert_japanese/tokenization_bert_japanese.py', 'src/transformers/testing_utils.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/import_utils.py', 'tests/models/bert_japanese/test_tokenization_bert_japanese.py']","The Bert Japanese model lacks SudachiPy and Juman++ tokenizers, leading to limitations in Japanese text tokenization capabilities."
956a483173e77ebf655ca9636a5f7b6ef010b307,1637705355,"[deepspeed] zero inference (#14253)

* [deepspeed] zero inference

* only z3 makes sense for inference

* fix and style

* docs

* rework

* fix test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* responding to suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['setup.py', 'src/transformers/deepspeed.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/trainer.py', 'tests/deepspeed/test_deepspeed.py']","Inference process fails or doesn't work as expected when zero optimization level is used, particularly in stage 3."
0235bc57abf0f829c1a511218df2443ef870fb1f,1649857471,"Fix and improve CTRL doctests (#16573)

* Improve CTRL doctests

* Fix `CTRLForSequenceClassification` flakiness with inconsistent losses

* Remove unused

* Fixup

* Add CTRL to documentation_tests.txt

* Fix control code not being first

* Add output assertions

* Change from sshleifer/tiny-ctrl -> ctrl

* Run `make fixup`

* apply `list` to output logits shape for clarity

* Reduce output loss precision to make assertion more robust

* Add assertion of control code being first

* Fix docstyle

* upper case sentence following control code

* Weird bug fixes

* Add a better generation example

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",['src/transformers/models/ctrl/modeling_ctrl.py'],"Inconsistent losses in `CTRLForSequenceClassification` cause test flakiness, while CTRL doctests need improvement and clarification. Also, assertions for output logits shape and control code order require adjustments."
a459f7f97d0ad4980aa715661c4de82fcb5ca785,1644248137,"Add ASR CTC streaming example  (#15309)

* Single-epoch run

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Infinite dataset

* Trainer fix + distributed benchmark

* Benchmark fix

* unused import

* interleaved splits

* interleaved splits

* has_length util

* Move to research projects

* Leftover Sized checks

* Bump min version

* Unused import

* Revert trainer changes

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['examples/pytorch/speech-recognition/run_speech_recognition_ctc.py', 'examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py', 'examples/research_projects/robust-speech-event/run_speech_recognition_ctc_streaming.py']","Lack of ASR CTC streaming example, and associated elements such as single-epoch run, trainer fix, benchmark, and interleaved splits. Issues with leftover checks, unused imports, and minimum version specification."
b67fd797bec56b59e1cd3ad54fa2783f7d7b7cbc,1641839414,"Add TFVisionEncoderDecoderModel (#14148)

* Start the work on TFVisionEncoderDecoderModel

* Expose TFVisionEncoderDecoderModel

* fix import

* Add modeling_tf_vision_encoder_decoder to _ignore_modules in get_model_modules()

* reorder

* Apply the fix for checkpoint loading as in #14016

* remove attention_mask + fix VISION_DUMMY_INPUTS

* A minimal change to make TF generate() work for vision models as encoder in encoder-decoder setting

* fix wrong condition: shape_list(input_ids) == 2

* add tests

* use personal TFViTModel checkpoint (for now)

* Add equivalence tests + projection layer

* style

* make sure projection layer can run

* Add examples

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Clean comments (need to work on TODOs for PyTorch models)

* Remove TF -> PT in check_pt_tf_equivalence for TFVisionEncoderDecoderModel

* fixes

* Revert changes in PT code.

* Update tests/test_modeling_tf_vision_encoder_decoder.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Add test_inference_coco_en for TF test

* fix quality

* fix name

* build doc

* add main_input_name

* Fix ckpt name in test

* fix diff between master and this PR

* fix doc

* fix style and quality

* fix missing doc

* fix labels handling

* Delete auto.rst

* Add the changes done in #14016

* fix prefix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/generation_tf_utils.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py', 'src/transformers/models/vision_encoder_decoder/__init__.py', 'src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/test_modeling_tf_encoder_decoder.py', 'tests/test_modeling_tf_vision_encoder_decoder.py', 'utils/check_repo.py']","Current implementation lacks a Tensorflow model for vision encoders and decoders, thus we can't use vision models in an encoder-decoder setting. Various issues are occurring related to import handling, checkpoint loading, and testing."
4c99e553c152ce9b709d7c138379b0b126ed2fa1,1639398276,"Improve documentation of some models (#14695)

* Migrate docs to mdx

* Update TAPAS docs

* Remove lines

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply some more suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add pt/tf switch to code examples

* More improvements

* Improve docstrings

* More improvements

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/perceiver/configuration_perceiver.py', 'src/transformers/models/perceiver/modeling_perceiver.py']","Existing documentation for certain models is unclear and lacking in detail, requiring a comprehensive update and enhancement."
cd3166a8ed612ace5c6ee2f70fb41943974ee49c,1634311586,"Add the SEW and SEW-D speech models (#13962)

* Working encoder

* SEW-D and tests

* Further conv fixes

* Automodels and conv inits

* Update integration tests, add docs

* Docs cleanup, resolve todos

* Conf fix

* Fix docs

* Fix tests, apply suggestions

* Update src/transformers/models/sew/modeling_sew.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Model conversion and updated no-mask tests

* Remove copy of feature_proj

* Style

* Update src/transformers/models/auto/feature_extraction_auto.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/auto/feature_extraction_auto.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Move orgs

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/sew/__init__.py', 'src/transformers/models/sew/configuration_sew.py', 'src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py', 'src/transformers/models/sew/modeling_sew.py', 'src/transformers/models/sew_d/__init__.py', 'src/transformers/models/sew_d/configuration_sew_d.py', 'src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py', 'src/transformers/models/sew_d/modeling_sew_d.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_sew.py', 'tests/test_modeling_sew_d.py', 'utils/check_repo.py']","The software lacks the implementation of SEW and SEW-D speech models, causing tests and documentation inconsistencies."
1762ded30a49649bdd5f8f5ee38b46dea051026a,1652897860,"Fix metric calculation in examples and setup tests to run on multi-gpu for no_trainer scripts (#17331)

* Fix length in no_trainer examples

* Add setup and teardown

* Use new accelerator config generator to automatically make tests able to run based on environment

","['examples/pytorch/image-classification/run_image_classification_no_trainer.py', 'examples/pytorch/multiple-choice/run_swag_no_trainer.py', 'examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py', 'examples/pytorch/summarization/run_summarization_no_trainer.py', 'examples/pytorch/test_accelerate_examples.py', 'examples/pytorch/text-classification/run_glue_no_trainer.py', 'examples/pytorch/token-classification/run_ner_no_trainer.py', 'examples/pytorch/translation/run_translation_no_trainer.py']",Metric calculation in no_trainer examples is incorrect and tests are unable to run on multi-gpu environments.
fb13a7df95f4d378cbd80e2f0014d67d26eb2778,1675443453,"do not scale gradient in bf16 mode (#21428)

* no dot scale gradient in bf16 mode

* fix since args.fp16 might be none

* fixed typo

* typo

* only do if grad scaling is true

* self.amp_dtype == torch.float16 is true

* put back prop when fsdp is not none",['src/transformers/trainer.py'],"In bf16 mode, gradient scaling is causing issues as it might not be properly detected when applicable or not."
eec0d84e6a98f597508fc177ec3a961f923d4e5d,1690973756,"[DOCS] Add example and modified docs of EtaLogitsWarper (#25125)

* added example and modified docs for EtaLogitsWarper

* make style

* fixed styling issue on 544

* removed error info and added set_seed

* Update src/transformers/generation/logits_process.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/generation/logits_process.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* updated the results

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",['src/transformers/generation/logits_process.py'],"Documentation for EtaLogitsWarper is unclear and lacks comprehensive examples, resulting in difficulties understanding its usage and functionality. There are also styling issues present in the code."
c1aaa439350051acdcd585946e91525502a6b063,1646827796,"[Doctests] Move doctests to new GPU & Fix bugs (#15969)

* test

* up

* up

* Empty test commit

* up

* update tests

* up

* fix some vision models

* correct

* correct docs

* Trigger notification

* finalize

* check

* correct quicktour

* Apply suggestions from code review

* improve doctests

* Trigger Build

* next try

* next try

* and again

* Output current clone information

* Output current clone information

* Correct path

* add tf round again

* revert to daily job

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>","['src/transformers/models/beit/modeling_beit.py', 'src/transformers/models/convnext/modeling_convnext.py', 'src/transformers/models/deit/modeling_deit.py', 'src/transformers/models/poolformer/modeling_poolformer.py', 'src/transformers/models/segformer/modeling_segformer.py', 'src/transformers/models/speech_to_text/modeling_speech_to_text.py', 'src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py', 'src/transformers/models/swin/modeling_swin.py', 'src/transformers/models/vit/modeling_vit.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py']","Doctests are failing and there are issues with some vision models in the current GPU setup. Also, the path for clone information and Tensorflow round operation seem to have problems."
3080bb4754e641b169ee5485441f4f79872f587e,1665408019,"Add onnx support for VisionEncoderDecoder (#19254)

* Add onnx support for VisionEncoderDecoder

* Add onnx support for VisionEncoderDecoder

* Removed unused import

* Rename encoder hidden state

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docstrings and removed redundant code

* Added test function for enc-dec models

* Update doc string text

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* fixed code style

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>","['src/transformers/models/vision_encoder_decoder/__init__.py', 'src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py', 'src/transformers/onnx/__main__.py', 'src/transformers/onnx/config.py', 'src/transformers/onnx/features.py', 'tests/onnx/test_onnx_v2.py']","'VisionEncoderDecoder lacks ONNX support, inhibiting its interoperability with Open Neural Network Exchange applications.'"
b18d8534ea62f144a4002b9e2afcb4588518e945,1639674235,"[Generate] Make generate multi-modal (#14784)

* finish refactor

* refactor

* add tests

* add more tests

* up

* finish tests

* finish

* up

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* improve docstring

* fix docs

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/generation_utils.py', 'tests/test_generation_utils.py']","The existing generate function does not support multi-modal generation, limiting its usability in diverse test cases."
4c8ec66a7433589436d13d95d48601f274c92b44,1660222299,"Fix LayoutLMv3 documentation (#17932)

* fix typos

* fix sequence_length docs of LayoutLMv3Model

* delete trailing white spaces

* fix layoutlmv3 docs more

* apply make fixup & quality

* change to two versions of input docstring

* apply make fixup & quality",['src/transformers/models/layoutlmv3/modeling_layoutlmv3.py'],"Typos and inaccuracies found in LayoutLMv3Model documentation, including confusion in sequence_length docs and excessive trailing white spaces. Also, uncertain about two versions of input docstring."
488a179ce10ab1da4eae4b5945e141fc9e0e9283,1673877867,"Fixing batching pipelines on single items for ChunkPipeline (#21132)

* Fixing #20783

* Update src/transformers/pipelines/base.py

* Fixing some tests.

* Fixup.

* Remove ffmpeg dep + a bit more relaxed for bigbird QA precision.

* Better dataset.

* Prevent failing on TF.

* Better condition. We can't use `can_use_iterator` since we cannot use it
directly.","['src/transformers/pipelines/base.py', 'tests/pipelines/test_pipelines_common.py', 'tests/pipelines/test_pipelines_question_answering.py']","Batching pipelines for ChunkPipeline fails when operating on single items, leading to issues with the correct application of transformers."
8aad4363d8963a6c6b32b8d2b3f0553ebd8d1b0a,1666011980,"Fix pipeline predict transform methods (#19657)

* Remove key word argument X from pipeline predict and transform methods

As __call__ of pipeline clasees require one positional argument, passing
the input as a keyword argument inside predict, transform methods, causing
__call__ to fail. Hence in this commit the keyword argument is modified
into positional argument.

* Implement basic tests for scikitcompat pipeline interface

* Seperate tests instead of running with parameterized based on framework as both frameworks will not be active at the same time","['src/transformers/pipelines/base.py', 'tests/pipelines/test_pipelines_common.py']","The input passed as a keyword argument inside predict, transform methods of pipeline classes causes fail due to __call__ requirement of one positional argument."
692e61e91a0b83f5b847902ed619b7c74c0a5dda,1656542942,"Flax t5 Encoder (#17784)

* first draft adding Flax-t5-encoder and Flax-mt5-encoder

* imports

* after make fixup

* flax t5 encoder test

* black on test

* make fix-copies

* clean

* all_model_classes -> tuple

* clean test

* is_encoder_decoder=False in t5-enc tester

* remove file docstring before FlaxT5Encoder

* black

* isort

* commit suggestions on src/transformers/models/t5/modeling_flax_t5.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* commit suggestions on src/transformers/models/t5/modeling_flax_t5.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* remove _get_encoder_module

* self.decoder_seq_length -> self.encoder_seq_length as t5-enc does not have decoder

* bugfix - self.module_class is class itself, not instance;

* docs for mt5 and t5

* call -> __call__ in t5 doc

* FlaxMT5EncoderModel to TYPE_HINT

* run doc-builder to allow change the files

Co-authored-by: Suraj Patil <surajp815@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/mt5/__init__.py', 'src/transformers/models/mt5/modeling_flax_mt5.py', 'src/transformers/models/t5/__init__.py', 'src/transformers/models/t5/modeling_flax_t5.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/models/t5/test_modeling_flax_t5.py']","There's no available Flax-based encoder for the T5 or MT5 models, leading to difficulties in utilizing them for certain transformer architectures."
2d4572b5c96661a301a44b149766e56e383104aa,1665758798,"GPTTokenizer dependency removed from deberta class (#19551)

* GPTTOkenizer dependency removed from deberta class

Fixup

made the Deberta Tokenizer fast independent of GPT-2 tokenizer

Copied annotation added

Done the dependency removal

* Added some missing copied statement

* Added some copied statements","['src/transformers/models/deberta/tokenization_deberta.py', 'src/transformers/models/deberta/tokenization_deberta_fast.py']",Deberta class has a dependency on GPTTokenizer which potentially restricts its usage in environments where GPT2 tokenizer is not available or desirable.
76c4d8bf26de3e4ab23b8afeed68479c2bbd9cbd,1630610982,"✨ Add PyTorch image classification example (#13134)

* :sparkles: add pytorch image classification example

* :fire: remove utils.py

* :lipstick: fix flake8 style issues

* :fire: remove unnecessary line

* :sparkles: limit dataset sizes

* :pushpin: update reqs

* :art: restructure - use datasets lib

* :art: import transforms directly

* :memo: add comments

* :lipstick: style

* :fire: remove flag

* :pushpin: update requirement warning

* :memo: add vision README.md

* :memo: update README.md

* :memo: update README.md

* :art: add image-classification tag to model card

* :truck: rename vision ➡️ image-classification

* :memo: update image-classification README.md","['examples/pytorch/image-classification/run_image_classification.py', 'examples/pytorch/test_examples.py']",There is a lack of PyTorch image classification example in the current project which could help developers understand how to implement similar solutions.
eaf5e98ec03d73c24367438100b05c02ce5ad10c,1693497603,"Add type hints for tf models batch 1 (#25853)

* Add type hints to `TFBlipTextModel`

* Add missing type hints to DPR family models

* Add type hints to `TFLEDModel`

* Add type hints to `TFLxmertForPreTraining`

* Add missing type hints to `TFMarianMTModel` and `TFMarianModel`

* Add missing type hints to `TFRagModel` & `TFRagTokenForGeneration`

* Make type hints annotations consistent","['src/transformers/models/blip/modeling_tf_blip_text.py', 'src/transformers/models/dpr/modeling_tf_dpr.py', 'src/transformers/models/led/modeling_tf_led.py', 'src/transformers/models/lxmert/modeling_tf_lxmert.py', 'src/transformers/models/marian/modeling_tf_marian.py', 'src/transformers/models/rag/modeling_tf_rag.py']","Certain TensorFlow models lack type hints, resulting in potential ambiguity and difficulty in maintaining or extending code."
8767958fc15a5549cc0f94db5ee721ccc426bb55,1687524637,"Allow dict input for audio classification pipeline (#23445)

* Allow dict input for audio classification pipeline

* make style

* Empty commit to trigger CI

* Empty commit to trigger CI

* check for torchaudio

* add pip instructions

Co-authored-by: Sylvain <sylvain.gugger@gmail.com>

* Update src/transformers/pipelines/audio_classification.py

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

* asr -> audio class

* asr -> audio class

---------

Co-authored-by: Sylvain <sylvain.gugger@gmail.com>
Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>","['src/transformers/pipelines/audio_classification.py', 'tests/pipelines/test_pipelines_audio_classification.py']","The audio classification pipeline doesn't accept dict input, limiting its usability."
eec76042f438226429d9f1b545a49a483ce33abb,1677596048,"Fix the issue of blip model returning loss even when the label is not provided.  (#21811)

* Fix the issue of blip model returning loss even when the label is not provoided

* Fix ruff failure

* Incorporate PR feedbacks

* Incorporate PR feedbacks

* Incorporate PR feedbacks

* Incorporate PR feedbacks","['src/transformers/models/blip/modeling_blip.py', 'tests/models/blip/test_modeling_blip.py']",The blip model is incorrectly returning a loss value even when no label is provided for computation.
854260ca44080a13bbf1937c3c6ce3a2d17aba07,1630411608,"TF/Numpy variants for all DataCollator classes (#13105)

* Adding a TF variant of the DataCollatorForTokenClassification to get feedback

* Added a Numpy variant and a post_init check to fail early if a missing import is found

* Fixed call to Numpy variant

* Added a couple more of the collators

* Update src/transformers/data/data_collator.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fixes, style pass, finished DataCollatorForSeqToSeq

* Added all the LanguageModeling DataCollators, except SOP and PermutationLanguageModeling

* Adding DataCollatorForPermutationLanguageModeling

* Style pass

* Add missing `__call__` for PLM

* Remove `post_init` checks for frameworks because the imports inside them were making us fail code quality checks

* Remove unused imports

* First attempt at some TF tests

* A second attempt to make any of those tests actually work

* TF tests, round three

* TF tests, round four

* TF tests, round five

* TF tests, all enabled!

* Style pass

* Merging tests into `test_data_collator.py`

* Merging tests into `test_data_collator.py`

* Fixing up test imports

* Fixing up test imports

* Trying shuffling the conditionals around

* Commenting out non-functional old tests

* Completed all tests for all three frameworks

* Style pass

* Fixed test typo

* Style pass

* Move standard `__call__` method to mixin

* Rearranged imports for `test_data_collator`

* Fix data collator typo ""torch"" -> ""pt""

* Fixed the most embarrassingly obvious bug

* Update src/transformers/data/data_collator.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Renaming mixin

* Updating docs

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Dalton Walker <dalton_walker@icloud.com>
Co-authored-by: Andrew Romans <andrew.romans@hotmail.com>","['src/transformers/__init__.py', 'src/transformers/data/data_collator.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_data_collator.py']","Missing TensorFlow and Numpy variants for DataCollator classes causing compatibility issues. Also, some class methods (like `__call__`) and important imports are missing."
0b774074a5f7f2137d0f1743bb9990cfb7e7a1d8,1685522446,"move fsdp handling to accelerate (#23158)

* mixed precision support via accelerate

* fix issues

* fix for the sharded ddp case

* fix flax and tf failing tests

* `refactor the place to create `Accelerator` object

* move ddp prep to accelerate

* fix 😅

* resolving comments

* move fsdp handling to accelerate

* fixex

* fix saving","['src/transformers/trainer.py', 'src/transformers/training_args.py']",The current handling of Fully Sharded Data Parallel (FSDP) and its alignment with mixed precision support and DDP are causing testing failures and need redoing.
b6204c9e9b0c03a5a1209842d77a05eadbeb007e,1666016102,"fix warnings in deberta (#19458)

* fix warnings in deberta

* fix copies

* Revert ""fix copies""

This reverts commit 324cb3fed11e04190ba7b4662644baa8143b60be.

* fix copies

* fix copies again

* revert changes to whitespace that make style did since it results in an infinite chain of fix-copies

* argh

Co-authored-by: Sander Land <sander@chatdesk.com>","['src/transformers/models/deberta/modeling_deberta.py', 'src/transformers/models/deberta_v2/modeling_deberta_v2.py', 'src/transformers/models/sew_d/modeling_sew_d.py']","Warnings are occurring in deberta, and the process of fixing copies is causing an infinite chain of problems."
253f9a3f9716d08a81fb305fe71f983122eb608b,1696493139,"[`GPTNeoX`] Faster rotary embedding for GPTNeoX (based on llama changes) (#25830)

* Faster rotary embedding for GPTNeoX

* there might be un-necessary moves from device

* fixup

* fix dtype issue

* add copied from statements

* fox copies

* oupsy

* add copied from Llama for scaled ones as well

* fixup

* fix

* fix copies","['src/transformers/models/deprecated/open_llama/modeling_open_llama.py', 'src/transformers/models/gpt_neox/modeling_gpt_neox.py', 'src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py', 'src/transformers/models/idefics/modeling_idefics.py']","There are performance issues with the rotary embedding functionality for GPTNeoX, possibly due to unnecessary device moves and dtype mismatches."
6deac5c824ad3ecd992cfb4c452f11b23b48cc62,1665746888,"Adding type hints for TFXLnet (#19344)

* Added type hints for TF: XLNet

* Added type hints for TF: XLNet

* Added type hints for TF: XLNet

* Added type hints for TF: XLNet

* Added type hints for TF: XLNet

* Added type hints for TF: XLNet

* Add type hints for XLnet (TF)
* Added type hints for XLnet (TF)

* Update src/transformers/models/xlnet/modeling_tf_xlnet.py",['src/transformers/models/xlnet/modeling_tf_xlnet.py'],"TF: XLNet code lacks type hints, potentially leading to incorrect usage and making maintenance more difficult."
102b5ff4a813eea848bb82ff2f451e0f6b17b30c,1678702291,"add new model of MGP-STR (#21418)

* add new model of MGP-STR

* fix the check failings

* remove torch and numpy from mgp_tokenization

* remove unused import from modeling_mgp_str

* add test_processing_mgp_str

* rm test_processing_mgp_str.py

* add test_processing_mgp_str

* add test_processing_mgp_str

* add test_processing_mgp_str

* rm test_processing_mgp_str and add softmax outs to model

* rm test_processing_mgp_str and add softmax outs to model

* rewrite the code of mgp-str according to PR suggestions

* rewrite the code of mgp-str according to PR suggestions

* add new model of MGP-STR

* fix the check failings

* remove torch and numpy from mgp_tokenization

* remove unused import from modeling_mgp_str

* add test_processing_mgp_str

* rm test_processing_mgp_str.py

* add test_processing_mgp_str

* add test_processing_mgp_str

* add test_processing_mgp_str

* rm test_processing_mgp_str and add softmax outs to model

* rewrite the code of mgp-str according to PR suggestions

* rewrite the code of mgp-str according to PR suggestions

* remove representation_size from MGPSTRConfig

* reformat configuration_mgp_str.py

* format test_processor_mgp_str.py

* add test for tokenizer and complete model/processer test and model file

* rm Unnecessary tupple in modeling_mgp_str

* reduce hidden_size/layers/label_size in test_model

* add integration tests and change MGPSTR to Mgpstr

* add test for logit values

* reformat test model file

---------

Co-authored-by: yue kun <yuekun.wp@alibaba-inc.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/image_processing_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/mgp_str/__init__.py', 'src/transformers/models/mgp_str/configuration_mgp_str.py', 'src/transformers/models/mgp_str/modeling_mgp_str.py', 'src/transformers/models/mgp_str/processing_mgp_str.py', 'src/transformers/models/mgp_str/tokenization_mgp_str.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/mgp_str/test_modeling_mgp_str.py', 'tests/models/mgp_str/test_processor_mgp_str.py', 'tests/models/mgp_str/test_tokenization_mgp_str.py', 'utils/check_repo.py']",The current MGP-STR model has issues which are causing check failings and unnecessary imports in the tokenization and modeling process. This is leading to incorrect processing and testing outcomes.
8d6acc6c2931e4b41baf099e1101da1635474614,1643055201,"[Beam Search] Correct returned beam scores (#14654)

* better

* save intermediate

* finish code

* up

* docs

* Apply suggestions from code review

* up

* add compute transition  beam scores function to model and make sure scores are correct with eos

* apply nicos comments

* Apply suggestions from code review

* another fix","['src/transformers/generation_utils.py', 'tests/test_generation_utils.py']","Beam search scores are incorrect when returned, specifically issues noticed with end of sentence (EOS) transitions."
5eeaef921f70acd68073d1066ccb09d7c6e6f475,1692716893,"Adds `TRANSFORMERS_TEST_BACKEND` (#25655)

* Adds `TRANSFORMERS_TEST_BACKEND`
Allows specifying arbitrary additional import following first `import torch`.
This is useful for some custom backends, that will require additional imports to trigger backend registration with upstream torch.
See https://github.com/pytorch/benchmark/pull/1805 for a similar change in `torchbench`.

* Update src/transformers/testing_utils.py

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>

* Adds real backend example to documentation

---------

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",['src/transformers/testing_utils.py'],"Custom backends requiring additional imports to trigger backend registration with upstream torch cannot be specified, limiting the ability to work with these backends."
7f99861218babf897c7d0d6051b43d65962671c0,1671196966,"Add Universal Segmentation class + mapping (#20766)

* Add mapping

* Add mapping to pipeline

* Apply suggestions

* Fix feature extractor tests

* Use ForInstance, add model to universal mapping

* More fixes

* Remove model from deprecated objectsé

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/pipelines/image_segmentation.py', 'src/transformers/utils/dummy_pt_objects.py', 'utils/check_repo.py']",The lack of a universal segmentation class and mapping is causing inconsistencies and inefficiencies in the pipeline.
06b4aac9ebab77a0065ec2cab40a8085ad71946f,1649855087,"Add Doc Test for GPT-J (#16507)

* Required the values GPTJ unfortunately cannot run the model =)

* Added the file to the doc tests

* Run Fixup and Style

* Fixed with the test versions of gptj. Ran Style and Fixup.

* Trigger ci

* A Minor Change to License

* Fixed spacing added to the benchmark_utils. Then refactored tests to const variables.

* Removed strings that were included as default parameters anyways.

Co-authored-by: ArEnSc <xx.mike.chung.xx@gmail.com>",['src/transformers/models/gptj/modeling_gptj.py'],"Lack of documentation tests for GPT-J model, resulting in confusion about model requirements and usage."
e118e085eabdc95f0c9b8c66058308dd1a367ee3,1642527888,"[Robust Speech Event] Add guides (#15155)

* up

* improve readme

* up

* up

* more info

* up

* up

* Apply suggestions from code review

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* add more stuff for eval

* update

* up

* Update README.md

* Update examples/research_projects/xls_r/README.md

Co-authored-by: Omar Sanseviero <osanseviero@users.noreply.github.com>

* apply omar's suggestions

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
Co-authored-by: Omar Sanseviero <osanseviero@users.noreply.github.com>","['examples/research_projects/robust-speech-event/eval.py', 'examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py']",Lack of sufficient guidance and details in the README for the Robust Speech Event research project causing difficulty for users to understand and evaluate.
6f8e367ae942f8892f7824860efff86ac64db6ae,1626781007,"Fix Padded Batch Error 12282 (#12487)

This fixes the padded batch [issue](https://github.com/huggingface/transformers/issues/12282). The error was generated due to the maximum sequence length of the attention mask not matching the padded sequence length of the hidden_states. `np.allclose` now passes with a 1e-2 absolute tolerance.

This change fixes",['src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py'],"The maximum sequence length of the attention mask is not matching the padded sequence length of the hidden_states, generating an error in padded batch."
70a9bc69a80bcfb3d97fb40e88c2036817d62736,1648149274,"Added type hints (#16389)

* Added type hints for PyTorch T5 model

* removed a type hint

* ran make style

* added type hints for ibert pytorch

* added type hints for lxmert pytorch

* removed kwargs type hint and fixed arguments order","['src/transformers/models/ibert/modeling_ibert.py', 'src/transformers/models/lxmert/modeling_lxmert.py']","PyTorch T5, iBert Pytorch, and Lxmert Pytorch models are missing type hints, resulting in potential runtime errors and lack of code clarity."
0d0aada56444ad554021947addaa035feb55948f,1660146918,"Use commit hash to look in cache instead of calling head (#18534)

* Use commit hash to look in cache instead of calling head

* Add tests

* Add attr for local configs too

* Stupid typos

* Fix tests

* Update src/transformers/utils/hub.py

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Address Julien's comments

Co-authored-by: Julien Chaumond <julien@huggingface.co>","['src/transformers/configuration_utils.py', 'src/transformers/modeling_flax_utils.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/modeling_utils.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/testing_utils.py', 'src/transformers/tokenization_utils_base.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/hub.py', 'tests/models/auto/test_modeling_auto.py', 'tests/models/auto/test_modeling_tf_auto.py', 'tests/models/auto/test_tokenization_auto.py', 'tests/pipelines/test_pipelines_common.py', 'tests/test_configuration_common.py']",Calls to 'head' instead of using the commit hash are causing inefficiencies when looking into cache. Involving local configs further complicates the issue.
e0921c6b53310a47b10f01633809b2b9f785a465,1681117041,"Add GPTBigCode model (Optimized GPT2 with MQA from Santacoder & BigCode) (#22575)

* Add model with cli tool

* Remove unwanted stuff

* Add new code

* Remove inference runner

* Style

* Fix checks

* Test updates

* make fixup

* fix docs

* fix doc

* fix test

* hopefully fix pipeline tests

* refactor

* fix CIs

* add comment

* rename to `GPTBigCodeForCausalLM`

* correct readme

* make fixup + docs

* make fixup

* fixes

* fixes

* Remove pruning

* Remove import

* Doc updates

* More pruning removal

* Combine copies

* Single MQA implementation, remove kv cache pre-allocation and padding

* Update doc

* Revert refactor to match gpt2 style

* Merge back key and value caches, fix some type hints

* Update doc

* Fix position ids pith padding (PR 21080)

* Add conversion script temporarily

* Update conversion script

* Remove checkpoint conversion

* New model

* Fix MQA test

* Fix copies

* try fix tests

* FIX TEST!!

* remove  `DoubleHeadsModel`

* add MQA tests

* add slow tests

* clean up

* add CPU checker

* final fixes

* fixes

- fix GPU issue
- fixed slow tests
- skip disk offload

* fix final issue

* Simplify and comment baddbmm fix

* Remove unnecessary code

* Transpose tweaks

* Use beta=1 on cpu, improve tests

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/gpt_bigcode/__init__.py', 'src/transformers/models/gpt_bigcode/configuration_gpt_bigcode.py', 'src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py']","Incorporation of new GPTBigCode model faces issues concerning tests, documentation updates, refactoring, slow test, GPU and CPU checker issues, disk offload skipping, as well as problems with model checkpoint conversion."
ea540a5977e602eb8072bfb3120c95f163c64a02,1664277176,"add wav2vec2_alignment (#16782)

* add wav2vec2_alignment

* Update alignment.py

* Update examples/research_projects/wav2vec2/alignment.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update examples/research_projects/wav2vec2/alignment.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update examples/research_projects/wav2vec2/alignment.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update examples/research_projects/wav2vec2/alignment.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update README.md

* fix style

* fix imports

* fix multithread

* fix bash script

* [@anton-l] Style fixes and docstrings

* [@anton-l] Style fixes and docstrings

* Update alignment.py

fix blank id in backtrack

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: anton-l <aglozhkov@gmail.com>","['examples/research_projects/wav2vec2/alignment.py', 'examples/research_projects/wav2vec2/run_alignment.sh']","The wav2vec2 project lacks alignment functionality, leading to blank IDs popping up during backtrack. Multi-threading and import issues were also observed."
56efbf430190f61177ec01e4a03de3f6db378bfe,1687262361,"TensorFlow CI fixes (#24360)

* Fix saved_model_creation_extended

* Skip the BLIP model creation test for now

* Fix TF SAM test

* Fix longformer tests

* Fix Wav2Vec2

* Add a skip for XLNet

* make fixup

* make fix-copies

* Add comments","['src/transformers/models/hubert/modeling_tf_hubert.py', 'src/transformers/models/sam/modeling_tf_sam.py', 'src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py', 'tests/models/blip/test_modeling_tf_blip.py', 'tests/models/longformer/test_modeling_tf_longformer.py', 'tests/models/xlnet/test_modeling_tf_xlnet.py', 'tests/utils/test_modeling_tf_core.py']","Multiple tests, including saved_model_creation_extended, TF SAM, longformer, Wav2Vec2 tests, and the BLIP model creation test, are failing in TensorFlow's continuous integration."
9a30753485653697c7db79e12b0cb2b8872c94c6,1695311567,"Porting the torchaudio kaldi fbank implementation to audio_utils (#26182)

* add kaldi fbank

* make style

* add herz_to_mel_kaldi tests

* add mel to hertz kaldi test

* integration tests

* correct test and remove comment

* make style

* Apply suggestions from code review

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* change parameter name

* Apply suggestions from Arthur review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update remove_dc_offset description

* fix bug  + make style

* fix error in using np.exp instead of np.power

* make style

---------

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>","['src/transformers/audio_utils.py', 'tests/utils/test_audio_utils.py']",The current torchaudio implementation lacks a filter bank (fbank) feature extractor function equivalent to the one provided by kaldi toolkit.
afe2a466bbaaa86ab751aa867a33996d41fed97c,1670257395,"ESM openfold_utils type hints (#20544)

* add type annotations for esm chunk_utils

use isinstance builtin instead of 'type(x) is y'; add assertions to aid in type inferencing; use bools instead of ints in _get_minimal_slice_set for improved type clarity; refactor to avoid re-assigning to the same variable with a different type

* add type annotations for esm data_transforms

refactor to avoid re-assigning to the same variable with a different type

* add type annotations for esm feats utils

refactor to avoid re-assigning to the same variable with a different type

* add type annotations for esm loss utils

* add/fix type annotations for esm rigit_utils

refactor to avoid re-assigning to the same variable with a different type; fix Callable, Tuple type hints; match conditional structure to other methods; fix return type on Rotation.cat and Rotation.unsqueeze

* add type annotations for esm tensor_utils

overload for tree_map; use insinstance builtin instead of 'type(x) is y'; export dict_multimap, flatten_final_dims, permute_final_dims in openfold_utils

* add type annotations for esm protein utils

add FIXME for attempted string mutation; add missing None check in get_pdb_headers; fix potentially unbound variable 'chain_tag' in to_pdb; modify get_pdb_headers return type

* add type annotations for esm residue constants

hints on collection constants; remove magic trailing comma to reduce number of lines; change list -> tuple for rigid_group_atom_positions for improved hinting

* code style fixup

Co-authored-by: Matt <rocketknight1@gmail.com>","['src/transformers/models/esm/openfold_utils/__init__.py', 'src/transformers/models/esm/openfold_utils/chunk_utils.py', 'src/transformers/models/esm/openfold_utils/data_transforms.py', 'src/transformers/models/esm/openfold_utils/feats.py', 'src/transformers/models/esm/openfold_utils/loss.py', 'src/transformers/models/esm/openfold_utils/protein.py', 'src/transformers/models/esm/openfold_utils/residue_constants.py', 'src/transformers/models/esm/openfold_utils/rigid_utils.py', 'src/transformers/models/esm/openfold_utils/tensor_utils.py']","Lack of type annotations in various ESM utilities, causing unclear variable types and potential unbound variables."
6c4d688ffa8095f6dbaa959a51b53a91073f2aeb,1637752923,"add cache_dir for tokenizer verification loading (#14508)

When loading a pretrained tokenizer, a verification is done to ensure
that the actual tokenizer class matches the class it was called from.
If the tokenizer is absent, its config file is loaded from the repo.

However, the cache_dir for downloading is not provided, which leads to
ignoring of the user-specified cache_dir, storing files in several
places and and may result in incorrect warnings when the default
cache_dir is unreachsble.

This commit fixes that.",['src/transformers/tokenization_utils_base.py'],"Absence of cache_dir in the pretrained tokenizer verification process disregards user-defined cache_dir, leading to improper file storage and potential misleading warnings when the default cache_dir is not accessible."
5fa0b17c3d10cdb6411a173a7dce42b0de56a8f2,1672162645,"[Past CI] 🔥 Leave Past CI failures in the past 🔥  (#20861)

* torch.jit._state

* Fix past CI

* Fix for perceiver

* Fix REALM

* Fix for Bloom

* Fix for SwinMode

* Fix for TrajectoryTransformerModel

* Fix for test_wav2vec2_with_lm

* make style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/mctct/modeling_mctct.py', 'src/transformers/models/perceiver/modeling_perceiver.py', 'src/transformers/models/realm/modeling_realm.py', 'src/transformers/pytorch_utils.py', 'tests/models/bloom/test_modeling_bloom.py', 'tests/models/mctct/test_modeling_mctct.py', 'tests/models/swin/test_modeling_swin.py', 'tests/models/trajectory_transformer/test_modeling_trajectory_transformer.py', 'tests/models/wav2vec2/test_modeling_wav2vec2.py', 'tests/test_modeling_common.py']","Multiple model implementations such as Perceiver, REALM, Bloom, SwinMode, and TrajectoryTransformerModel are causing failures in the CI tests."
04a5c859b0045238edf589700b45446ee491c8cb,1690376306,"Add descriptive docstring to TemperatureLogitsWarper (#24892)

* Add descriptive docstring to TemperatureLogitsWarper

It addresses https://github.com/huggingface/transformers/issues/24783

* Remove niche features

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Commit suggestion

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Refactor the examples to simpler ones

* Add a missing comma

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Make args description more compact

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Remove extra text after making description more compact

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Fix linter

---------

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",['src/transformers/generation/logits_process.py'],The TemperatureLogitsWarper lacks a descriptive docstring and some arguments' descriptions are unnecessarily verbose.
b6f332ecaf18054109294dd2efa1a5e6aa274a03,1630086771,"Add Wav2Vec2 & Hubert ForSequenceClassification (#13153)

* Add hubert classifier + tests

* Add hubert classifier + tests

* Dummies for all classification tests

* Wav2Vec2 classifier + ER test

* Fix hubert integration tests

* Add hubert IC

* Pass tests for all classification tasks on Hubert

* Pass all tests + copies

* Move models to the SUPERB org","['src/transformers/__init__.py', 'src/transformers/models/hubert/__init__.py', 'src/transformers/models/hubert/configuration_hubert.py', 'src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py', 'src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/wav2vec2/__init__.py', 'src/transformers/models/wav2vec2/configuration_wav2vec2.py', 'src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py', 'src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_hubert.py', 'tests/test_modeling_wav2vec2.py', 'utils/check_repo.py']",Classification tasks on Hubert and Wav2Vec2 are failing due to missing classifier implementations and the relevant tests.
a2dec768a27ab7520d4ae4f5f72643043f305fd3,1632224059,"beit-flax (#13515)

* beit-flax

* updated FLAX_BEIT_MLM_DOCSTRING

* removed bool_masked_pos from classification

* updated Copyright

* code refactoring: x -> embeddings

* updated test: rm from_pt

* Update docs/source/model_doc/beit.rst

* model code dtype updates and
other changes according to review

* relative_position_bias
revert back to pytorch design","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/beit/__init__.py', 'src/transformers/models/beit/modeling_flax_beit.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/test_modeling_flax_beit.py', 'utils/check_repo.py']",The BEiT classification implementation in Flax has an undesired Boolean masked position and the dtype used in the code does not align with the model requirements.
1a7ef3349fd9acfc8ee7bc718f9864ac4ea9d064,1655996393,"Fix broken test for models with batchnorm (#17841)

* Fix tests that broke when models used batchnorm

* Initializing the model twice does not actually...
...give you the same weights each time.
I am good at machine learning.

* Fix speed regression",['tests/test_modeling_tf_common.py'],"Tests fail for models using batch normalization due to inconsistent weight initialization. Also, speed regression problems have been detected."
12f043eaeaabfef6f6efea411d98e6f6d3c094b7,1694548411,"Fix `MarianTokenizer` to remove metaspace character in `decode` (#26091)

* add: check to remove metaspace from marian tokenizer

* fix: metaspace character being removed from everywhere

* fix: remove redundant check at top

* add: test for marian tokenizer decode fix

* fix: simplified the test","['src/transformers/models/marian/tokenization_marian.py', 'tests/models/marian/test_tokenization_marian.py']","`MarianTokenizer.decode` includes metaspace characters in its output, which is not intended. The issue also extends to redundant checks and lack of proper tests."
f3d2f7a6e08efe18debf59512325f02128394b43,1664519143,"Add MarkupLM (#19198)

* First draft

* Make basic test work

* Fix most tokenizer tests

* More improvements

* Make more tests pass

* Fix more tests

* Fix some code quality

* Improve truncation

* Implement feature extractor

* Improve feature extractor and add tests

* Improve feature extractor tests

* Fix pair_input test partly

* Add fast tokenizer

* Improve implementation

* Fix rebase

* Fix rebase

* Fix most of the tokenizer tests.

* propose solution for fast

* add: integration test for fasttokenizer, warning for decode, fix template in slow tokenizer

* add: modify markuplmconverter

* add: some modify on converter and tokenizerfast

* Fix style, copies

* Make fixup

* Update tokenization_markuplm.py

* Update test_tokenization_markuplm.py

* Update markuplm related

* Improve processor, add integration test

* Add processor test file

* Improve processor

* Improve processor tests

* Fix more processor tests

* Fix processor tests

* Update docstrings

* Add Copied from statements

* Add more Copied from statements

* Add code examples

* Improve code examples

* Add model to doc tests

* Adding dependency check

* Add dummy file

* Add requires_backends

* Add model to toctree

* Fix more things, disable dependency check for now

* Apply more suggestions

* Add soft dependency

* Add annotators to tests

* Fix style

* Remove from_slow=True

* Remove print statements

* Add sanity check

* Fix processor test

* Fix processor tests, add more docs

* Add doc tests for mdx file

* Add more tips

* Apply suggestions

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
Co-authored-by: lockon-n <45759388+lockon-n@users.noreply.github.com>
Co-authored-by: SaulLu <lucilesaul.com@gmail.com>
Co-authored-by: lockon-n <dd098309@126.com>","['src/transformers/__init__.py', 'src/transformers/convert_slow_tokenizer.py', 'src/transformers/file_utils.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/markuplm/__init__.py', 'src/transformers/models/markuplm/configuration_markuplm.py', 'src/transformers/models/markuplm/feature_extraction_markuplm.py', 'src/transformers/models/markuplm/modeling_markuplm.py', 'src/transformers/models/markuplm/processing_markuplm.py', 'src/transformers/models/markuplm/tokenization_markuplm.py', 'src/transformers/models/markuplm/tokenization_markuplm_fast.py', 'src/transformers/testing_utils.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_tokenizers_objects.py', 'src/transformers/utils/import_utils.py', 'tests/models/markuplm/test_feature_extraction_markuplm.py', 'tests/models/markuplm/test_modeling_markuplm.py', 'tests/models/markuplm/test_processor_markuplm.py', 'tests/models/markuplm/test_tokenization_markuplm.py']","Issues with MarkupLM implementation, including problems with tokenizer tests, feature extractor, truncation strategies, processor tests, docstrings, code examples and project dependencies. Integration tests and sanity checks are needed for validation."
61400e1ec7898892e77314dd819c1a1a52bd5268,1625640638,"[Flax] Add FlaxMBart (#12236)

* Copy BART to MBart and rename some stuff

* Add copy statements pointing to FlaxBart

* Update/add some common files

* Update shift_tokens_rigth + fix imports

* Fix shift_tokens_right method according to MBart implementation

* Update shift_tokens_right in tests accordingly

* Fix the import issue and update docs file
* make style quality

* Do some minor changes according to patil-suraj suggestions

* Change the order of normalization layer and attention

* Add some copu statementes

* Update generate method and add integration test for mBart

* Make a few updates after a review

Besides, add `lang_code_to_id` to MBartTokenizeFast

* fix-copies; make style quality

* Apply suggestions from code review

* Apply suggestions from code review

* Apply suggestions from code review

* fix output type, style

* add copied from

* resolve conflicts

Co-authored-by: Suraj Patil <surajp815@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/mbart/__init__.py', 'src/transformers/models/mbart/modeling_flax_mbart.py', 'src/transformers/models/mbart/tokenization_mbart_fast.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/test_modeling_flax_mbart.py']","The Flax implementation of MBart is missing, causing issues in token shifting and normalization layer order, and lacks integration tests and necessary updates in the generate method. The Fast MBartTokenizer also doesn't have the `lang_code_to_id` function."
ac6aa10f23967373142d7a23d84a45ffd494d64b,1644004327,"Standardize semantic segmentation models outputs (#15469)

* Standardize instance segmentation models outputs

* Rename output

* Update src/transformers/modeling_outputs.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Add legacy argument to the config and model forward

* Update src/transformers/models/beit/modeling_beit.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Copy fix in Segformer

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>","['src/transformers/__init__.py', 'src/transformers/modeling_outputs.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/beit/configuration_beit.py', 'src/transformers/models/beit/modeling_beit.py', 'src/transformers/models/segformer/configuration_segformer.py', 'src/transformers/models/segformer/modeling_segformer.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_beit.py', 'tests/test_modeling_segformer.py', 'utils/check_repo.py']",Inconsistency in output format across different instance segmentation models causing interoperability issues.
f5221c06e4a4172a7dd38b17c3f7f7b28987894b,1657628315,"Report value for a step instead of epoch. (#18095)

* Report value for a step instead of epoch.

Report an objective function value for a step instead of epoch to optuna.
I made this modification for the following reason:
If ""eval_steps"" is less than steps per epoch, there maybe warnings like this: ""optuna/trial/_trial.py:592: UserWarning: The reported value is ignored because this `step` 0 is already reported."". So ""step"" are more appropriate than ""epoch"" here.

* MOD: make style.

Co-authored-by: zhaowei01 <zhaowei01@yuanfudao.com>",['src/transformers/trainer.py'],"When ""eval_steps"" is less than steps per epoch, a warning is triggered due to reporting value for an epoch rather than a step causing repeated step numbers."
3baa407f92eb8d4a64601e390e8cb9293d0a0f17,1676298296,"Add: document question answering task guide (#21518)

* document question answering guide

* Added the list of supported models

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* switched to AutoProcessor

* feedback addressed

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>

* Update docs/source/en/tasks/document_question_answering.mdx

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>

* more feedback addressed

* addressed comments about evaluation loss

* added appropriate image link

* make style

* typo fix

* resolving toc conflict

* fixed the image link

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>",['utils/check_task_guides.py'],"There's a lack of documentation for the question answering task, including model support list, APIs, usage, and evaluation loss information."
8298e4ec0291ee0d8bfd4fc620d3ab824e8b7bb4,1675352363,"[`bnb`] Fine-tuning HF 8-bit models (#21290)

* force `memory_efficient_backward=True`

* enhancements

- trainer support
- add new flag

* some changes

- internal changes in `Trainer`
- small refactor

* make quality

* Fixes

- add new testing util
- add new test
- change test in Trainer

* fix CI test

* educate users on how to ft 8bit models

* more checks

* fix `logger` error

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* adapt from review

* fix

* add comment

* use return instead

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/modeling_utils.py', 'src/transformers/trainer.py', 'tests/mixed_int8/test_mixed_int8.py']","8-bit models are causing memory efficiency issues and errors with the logger during fine-tuning, with additional unexplained testing util errors cropping up."
a2586795e577b29c526569b115f0cb4b002db968,1659032667,"Migrate metric to Evaluate library for tensorflow examples  (#18327)

* Migrate metric to Evaluate library in tf examples

Currently tensorflow examples use `load_metric` function from Datasets
library , commit migrates function call to `load` function to
Evaluate library.

Fix for #18306

* Migrate metric to Evaluate library in tf examples

Currently tensorflow examples use `load_metric` function from Datasets
library , commit migrates function call to `load` function to
Evaluate library.

Fix for #18306

* Migrate `metric` to Evaluate for all tf examples

Currently tensorflow examples use `load_metric` function from Datasets
library , commit migrates function call to `load` function to
Evaluate library.","['examples/tensorflow/question-answering/run_qa.py', 'examples/tensorflow/summarization/run_summarization.py', 'examples/tensorflow/text-classification/run_glue.py', 'examples/tensorflow/token-classification/run_ner.py', 'examples/tensorflow/translation/run_translation.py']",Tensorflow examples are currently using the `load_metric` function from Datasets library which may not be ideal.
494e96d8d61277cd7509e5f90aa14e6ac604063a,1692963726,"Generate: logits processors are doctested and fix broken doctests (#25692)

* shorter example

* add logits processors to doctests

* remove file from conflict?

* tmp commit

* Fix broken tests; Shorter sampling tests

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",['src/transformers/generation/logits_process.py'],Doctests for logits processors are failing and lack complete coverage.
a4386d7e405712fb9e9ad1066828ded3174f6a61,1652948956,"[BC] Fixing usage of text pairs (#17324)

* [BC] Fixing usage of text pairs

The BC is actually preventing users from misusing the pipeline since
users could have been willing to send text pairs and the pipeline would
instead understand the thing as a batch returning bogus results.

The correct usage of text pairs is preserved in this PR even when that
makes the code clunky.

Adds support for {""text"":..,, ""text_pair"": ...} inputs for both dataset
iteration and more explicit usage to pairs.

* Updating the doc.

* Update src/transformers/pipelines/text_classification.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/text_classification.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/pipelines/test_pipelines_text_classification.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* quality.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>","['src/transformers/pipelines/text_classification.py', 'tests/pipelines/test_pipelines_text_classification.py']","Misuse of text pairs in pipeline causing it to interpret batches incorrectly, leading to incorrect results. The issue also affects dataset iteration and explicit usage of pairs."
9264fc915a3295c6fd0e05f54ee409917ac43f60,1692285594,"Inconsistency in PreTrainedModel.resize_token_embeddings When ZeRO3 Is Enabled (#25394)

* Inconsistency in PreTrainedModel.resize_token_embeddings

This PR addresses https://github.com/huggingface/transformers/issues/25241.

In previous implementation when ZeRO stage 3 was enbaled, resize_token_embeddings would create independent PyTorch weights on each device. Here we ensure that new embeddings are created with DeepSpeed init, and are properly partitioned accros devices.

* formatting with black

* adding the removed comments back in

---------

Co-authored-by: Sina Moeini <smoeini@amazon.com>",['src/transformers/modeling_utils.py'],"Enabling ZeRO stage 3 causes PreTrainedModel.resize_token_embeddings to create independent PyTorch weights on each device, leading to inconsistency across devices."
69e16abf98c94b8a6d2cf7d60ca36f13e4fbee58,1637615846,"Switch from using sum for flattening lists of lists in group_texts (#14472)

* remove sum for list flattening

* change to chain(*)

* make chain object a list

* delete empty lines

per sgugger's suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Nicholas Broad <nicholas@nmbroad.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['examples/flax/language-modeling/run_clm_flax.py', 'examples/flax/language-modeling/run_mlm_flax.py', 'examples/flax/language-modeling/run_t5_mlm_flax.py', 'examples/pytorch/language-modeling/run_clm.py', 'examples/pytorch/language-modeling/run_clm_no_trainer.py', 'examples/pytorch/language-modeling/run_mlm.py', 'examples/pytorch/language-modeling/run_mlm_no_trainer.py', 'examples/pytorch/language-modeling/run_plm.py', 'examples/pytorch/multiple-choice/run_swag.py', 'examples/pytorch/multiple-choice/run_swag_no_trainer.py', 'examples/research_projects/jax-projects/model_parallel/run_clm_mp.py', 'examples/tensorflow/language-modeling/run_clm.py', 'examples/tensorflow/language-modeling/run_mlm.py', 'examples/tensorflow/multiple-choice/run_swag.py', 'src/transformers/file_utils.py']",The usage of sum for flattening lists of lists in group_texts is inefficient leading to performance issues.
6e32959329822136051d2416481307ef81b27096,1682349875,"Reverting Deta cloning mecanism. (#22656)

* Fixed the revert by making sure that even the regexp can cover all
duplicates.

* Code simplification using hash.

* Fixing the `ident`.

* Fixing ignoring patterened duplicate names.

* Using `accelerate@find_tied_parameters` for from_pretrained

This is more correct there, since it handles meta device seemlessly
and we don't need to handle ""non-duplicate"" tensors (slices of each
other).

* Protecting accelerate.

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/modeling_utils.py'],"Deta cloning mechanism is causing issues by not covering all duplicates with regexp, allowing patterned duplicate names, incorrectly handling ""non-duplicate"" tensors and does not protect accelerate."
897a8dd89f40817201bc4aebe532a096405bdeb1,1653491769,"Support compilation via Torchdynamo, AOT Autograd, NVFuser (#17308)

* Support compilation via Torchdynamo, AOT Autograd, NVFuser

* Address comments

* Lint

* Stas comments - missing quality test

* Lintere

* Quality test

* Doc lint

* Reset CUDA peak mem

* Add CustomTrainer

* require a single gpu

Co-authored-by: Stas Bekman <stas@stason.org>","['src/transformers/testing_utils.py', 'src/transformers/trainer.py', 'src/transformers/trainer_seq2seq.py', 'src/transformers/training_args.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/import_utils.py', 'tests/trainer/test_trainer.py']","Lack of support for compilation through Torchdynamo, AOT Autograd, NVFuser impacting build process and limiting customization options."
eae7a96b7d5810ee6723e41f9b316cc51672fbb1,1631887672,"Optimize Token Classification models for TPU (#13096)

* Optimize Token Classification models for TPU

As per the XLA document XLA cannot handle masked indexing well. So token classification
models for BERT and others use an implementation based on `torch.where`. This implementation
works well on TPU. 

ALBERT token classification model uses the masked indexing which causes performance issues
on TPU. This PR fixes this issue by following the BERT implementation.

* Same fix for ELECTRA

* Same fix for LayoutLM","['src/transformers/models/albert/modeling_albert.py', 'src/transformers/models/electra/modeling_electra.py', 'src/transformers/models/layoutlm/modeling_layoutlm.py']",ALBERT and ELECTRA token classification models' use of masked indexing leads to performance degradation on TPUs.
39b5d1a63a07d60e496a6bd98c3a60d32e8b9e6d,1643840289,"fix set truncation attribute in `__init__` of `PreTrainedTokenizerBase` (#15456)

* change truncation_side in init of `PreTrainedTokenizerBase`

Co-authored-by: LSinev <LSinev@users.noreply.github.com>

* add test

* Revert ""replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`""

This reverts commit 7a98b87962d2635c7e4d4f00db3948b694624843.

* fix kwargs

* Revert ""fix kwargs""

This reverts commit 67b0a5270e8cf1dbf70e6b0232e94c0452b6946f.

* Update tests/test_tokenization_common.py

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

* delete truncation_side variable

* reorganize test

* format

* complete doc

* Revert ""Revert ""replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`""""

This reverts commit d5a10a7e2680539e5d9e98ae5d896c893d224b80.

* fix typo

* fix typos to render documentation

* Revert ""Revert ""Revert ""replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`""""""

This reverts commit 16cf58811943a08f43409a7c83eaa330686591d0.

* format

Co-authored-by: LSinev <LSinev@users.noreply.github.com>
Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>","['src/transformers/tokenization_utils_base.py', 'tests/test_tokenization_common.py']",The `PreTrainedTokenizerBase`'s `__init__` method does not correctly set the truncation attribute leading to potential issues with tokenization and padding.
f689743e7454b93f6cab4343026de03fa530bfb9,1629706687,"SageMaker: Fix sagemaker DDP & metric logs (#13181)

* Barrier -> barrier

* added logger for metrics

* removed stream handler in trainer

* moved handler

* removed streamhandler from trainer

* updated test image and instance type added datasets version to test

* Update tests/sagemaker/scripts/pytorch/requirements.txt

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>","['src/transformers/trainer.py', 'src/transformers/trainer_pt_utils.py', 'src/transformers/training_args.py', 'tests/sagemaker/conftest.py', 'tests/sagemaker/test_multi_node_data_parallel.py']","SageMaker DDP and metric logs not functioning correctly due to issues with barriers and stream handlers, also causing tests to fail."
71d18d08319db50a1a5bd022aed54d2613eec3b9,1652701227,"fixed bug in run_mlm_flax_stream.py (#17203)

* fixed bug run_mlm_flax_stream.py

Fixed bug caused by an update to tokenizer keys introduced in recent transformers versions (between `4.6.2` and `4.18.0`) where additional keys were introduced to the tokenizer output.

* Update run_mlm_flax_stream.py

* adding missing paranthesis

* formatted to black

* remove cols from dataset instead

* reformat to black

* moved rem. columns to map

* formatted to black

Co-authored-by: KennethEnevoldsen <kennethcenevolsen@gmail.com>",['examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py'],"Recent updates between transformers versions `4.6.2` and `4.18.0` have introduced additional keys to the tokenizer output, causing a bug in run_mlm_flax_stream.py."
3668ec17165dbb7823f3bc7e190e1733040c3af8,1676623441,"[`bnb`] Introducing `BitsAndBytesConfig` (#21579)

* v1 `BitsandbytesConfig`

- add v1
- add tests
- more user-friendly API
- add docs

* change to `BitsAndBytesConfig`

* replace logic

* changes

* make fixup

* quality

* make fixup

* fix doc

* fix test

* update toctree

* fix slow test

* add tips

* add warning

* change title

* oops

* Update docs/source/en/main_classes/quantization.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/utils/bitsandbytes.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove unused file

* adapt suggestion

- add also tests
- change logic

* update docs

* adapt suggestions

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/modeling_utils.py', 'src/transformers/utils/bitsandbytes.py', 'src/transformers/utils/quantization_config.py', 'tests/mixed_int8/test_mixed_int8.py']","There's a lack of a user-friendly API for managing settings in the `bitsandbytes` module, causing difficult usage and a deficiency in tests for `BitsAndBytesConfig`."
5041bc3511d098814598cf1cfc6c6bd20e72c144,1666192501,"Image transforms add center crop (#19718)

* Add center crop to transforms library

* Return PIL images if PIL image input by default

* Fixup and add docstring

* Trigger CI

* Update src/transformers/image_transforms.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/image_transforms.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* PR comments - move comments; unindent

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/image_transforms.py', 'tests/test_image_transforms.py']",The image transformation library lacks a center crop function and does not return PIL images by default for PIL image inputs.
a2ef9c5446b65c9d20c06ab6940f89fcbee89382,1632472283,"Use torch.unique_consecutive to check same element (#13637)

We use `torch.unique` here only to check whether every elements have
the same value.
Therefore, we can use `torch.unique_consecutive` here.

This function eliminates all but the first element from every consecutive
group of equivalent elements.
Like, if we apply this function to `[1, 2, 2, 1]`, it will result in
`[1, 2, 1]`.

As you could see, this is enough for checking whether every elements
have the same value.

Since `torch.unique_consecutive` do less thing, it is much more faster.
On my computer, it is 25x faster on GPU and 15x faster on CPU.","['src/transformers/models/bart/modeling_bart.py', 'src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py', 'src/transformers/models/led/modeling_led.py', 'src/transformers/models/mbart/modeling_mbart.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py']","`torch.unique` function is being used inefficiently to check if all elements in a sequence are identical, causing slower performance."
dfc38463b84c8e87f925059a41068baf5a2029ce,1654005463,"Setup for Italian translation and add quicktour.mdx translation (#17472)

* Setup for Italian translation and add first document

- Add 'it' folder for files translated into Italian
- Add _config.py and _toctree.yml files
- Add translation of quicktour.mdx

* Fix style issue of italian documentation files

* Add 'it' to the languages section in the .github/workflows

* Remove - installation from _toctree for Italian

* Translation for index file

- Add index to _toctree.yml
- Add translation of index.mdx

* Fix typo in docs/source/it/index.mdx

* Translate code comments in docs/source/it/_config.py

Co-authored-by: Martina Fumanelli <martinafumanelli@Martinas-MBP.homenet.telecomitalia.it>",['docs/source/it/_config.py'],"Italian translation setup is missing, including the initial document translation for 'quicktour.mdx' and configuration files, as well as a language section in the .github/workflows.
"
176ceff91f5e5ff15922715e5a4a4d9f66b92d14,1680709428,"Add DePlot + MatCha on `transformers` (#22528)

* add deplot + matcha on `transformers`

* more docs

* correct path

* Update docs/source/en/model_doc/deplot.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix

* use auto processor

* Update docs/source/en/model_doc/matcha.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make fixup

* Update docs/source/en/model_doc/deplot.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* add correct names

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>",['src/transformers/models/auto/configuration_auto.py'],"The 'transformers' library lacks DePlot and MatCha features, with additionally incomplete documentation and some naming inconsistencies."
94a7edd938221101a1d0ae5bed6557debdce1fdd,1674583482,"[GenerationConfig] add additional kwargs handling (#21269)

* add additional kwargs handling

* fix issue when serializing

* correct order of kwargs removal for serialization in from dict

* add `dict_torch_dtype_to_str` in case a dtype is needed for generation

* add condition when adding the kwargs : not from config

* Add comment based on review

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* add test function

* default None when poping arg

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>","['src/transformers/generation/configuration_utils.py', 'tests/generation/test_configuration_utils.py']",Issues with handling and serializing additional kwargs in GenerationConfig. Potential problems with from_dict method and dtype conversion in case of generation. Possible undetected conflicts when kwargs are added and not from the config.
da69de17e86501b95396086a5b6479f645e8f70e,1697032340,"[Assistant Generation] Improve Encoder Decoder (#26701)

* [Assistant Generation] Improve enc dec

* save more

* Fix logit processor checks

* Clean

* make style

* fix deprecation

* fix generation test

* Apply suggestions from code review

* fix biogpt

* make style","['src/transformers/generation/configuration_utils.py', 'src/transformers/generation/utils.py', 'src/transformers/models/biogpt/modeling_biogpt.py', 'tests/generation/test_utils.py']","The existing Encoder-Decoder for Assistant Generation has performance issues, and occasionally fails the logit processor checks and some generation tests."
2351729f7d1c606624dcd2d1ad5dc8e627e17320,1654792390,"Adding `top_k` argument to `text-classification` pipeline. (#17606)

* Adding `top_k` and `sort` arguments to `text-classification` pipeline.

- Deprecate `return_all_scores` as `top_k` is more uniform with other
  pipelines, and a superset of what `return_all_scores` can do.
  BC is maintained though.
  `return_all_scores=True` -> `top_k=None`
  `return_all_scores=False` -> `top_k=1`

- Using `top_k` will imply sorting the results, but using no argument
  will keep the results unsorted for backward compatibility.

* Remove `sort`.

* Fixing the test.

* Remove bad doc.","['src/transformers/pipelines/text_classification.py', 'tests/pipelines/test_pipelines_text_classification.py']","The `text-classification` pipeline lacks a `top_k` argument for returning top K highest scoring classes, making it inconsistent with other pipelines. Current option `return_all_scores` is unclear in its purpose."
269b05493917af2f7e86bafc735576a1a22caf4f,1677695011,"Add ALIGN to transformers (#21741)

Adds the ALIGN model to transformers. ALIGN is introduced in ""Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"" by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/align/__init__.py', 'src/transformers/models/align/configuration_align.py', 'src/transformers/models/align/convert_align_tf_to_hf.py', 'src/transformers/models/align/modeling_align.py', 'src/transformers/models/align/processing_align.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/image_processing_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/efficientnet/image_processing_efficientnet.py', 'src/transformers/models/efficientnet/modeling_efficientnet.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/align/test_modeling_align.py', 'tests/models/align/test_processor_align.py', 'utils/check_repo.py']","The transformers lack the ALIGN model, introduced in ""Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision."""
6326aa4bf0f20c1f7825d096e64143343986cef2,1636462193,"Correct order of overflowing tokens for LayoutLmV2 tokenizer (#13495)

* correct order of overflowing tokens for LayoutLmV2 tokenizer

* test to check order of overflowing_tokens for a seq of input_ids

* fix up quality

* added suggested changes

* check that tests the bbox sequence

* pair_input test added

* pass quality test

* check bbox sequence added

* unittest method

* comments added

* add overflowing bbox test

* improved ""seq_1""

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* improve code quality

Co-authored-by: SaulLu <lucilesaul.com@gmail.com>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>","['src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py', 'src/transformers/tokenization_utils_base.py', 'tests/test_tokenization_layoutlmv2.py']",The order of overflowing tokens generated by the LayoutLmV2 tokenizer is incorrect.
1d6623c6a25f9c1be3af36ffdcc3b0e0d3848999,1625663144,"MLM training fails with no validation file(same as #12406 for pytorch now) (#12517)

* Validation split percentage to be used for custom data files also

Issue same as https://github.com/huggingface/transformers/issues/12406 fixed for pytorch branch run_mlm.py

* Validation split added in the right place

* Update run_clm.py

* validation split added for custom files

* Validation split added for custom files

* Update run_plm.py

* fixed validation split for custom files as input for pytorch examples in lm

* Update run_clm_no_trainer.py

* args modified","['examples/pytorch/language-modeling/run_clm.py', 'examples/pytorch/language-modeling/run_clm_no_trainer.py', 'examples/pytorch/language-modeling/run_mlm.py', 'examples/pytorch/language-modeling/run_mlm_no_trainer.py', 'examples/pytorch/language-modeling/run_plm.py']",MLM training crashes when no validation file is provided for custom data files in pytorch examples in lm.
7f9b7b3f0e181e2393fc5b5c70ad2da2a190aa08,1667266378,"Add ESMFold (#19977)

* initial commit

* First draft that gets outputs without crashing!

* Add all the ported openfold dependencies

* testing

* Restructure config files for ESMFold

* Debugging to find output discrepancies

* Mainly style

* Make model runnable without extra deps

* Remove utils and merge them to the modeling file

* Use correct gelu and remove some debug prints

* More cleanup

* Update esm docs

* Update conversion script to support ESMFold properly

* Port some top-level changes from ESMFold repo

* Expand EsmFold docstrings

* Make attention_mask optional (default to all 1s)

* Add inference test for ESMFold

* Use config and not n kwargs

* Add modeling output class

* Remove einops

* Remove chunking in ESM FFN

* Update tests for ESMFold

* Quality

* REpo consistency

* Remove tree dependency from ESMFold

* make fixup

* Add an error in case my structure map function breaks later

* Remove needless code

* Stop auto-casting the LM to float16 so CPU tests pass

* Stop auto-casting the LM to float16 so CPU tests pass

* Final test updates

* Split test file

* Copyright and quality

* Unpin PyTorch to see built doc

* Fix config file to_dict() method

* Add some docstrings to the output

* Skip TF checkpoint tests for ESM until we reupload those

* make fixup

* More docstrings

* Unpin to get even with main

* Flag example to write

Co-authored-by: Sylvain Gugger <Sylvain.gugger@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/esm/__init__.py', 'src/transformers/models/esm/configuration_esm.py', 'src/transformers/models/esm/convert_esm.py', 'src/transformers/models/esm/modeling_esm.py', 'src/transformers/models/esm/modeling_esmfold.py', 'src/transformers/models/esm/openfold_utils/__init__.py', 'src/transformers/models/esm/openfold_utils/chunk_utils.py', 'src/transformers/models/esm/openfold_utils/data_transforms.py', 'src/transformers/models/esm/openfold_utils/feats.py', 'src/transformers/models/esm/openfold_utils/loss.py', 'src/transformers/models/esm/openfold_utils/protein.py', 'src/transformers/models/esm/openfold_utils/residue_constants.py', 'src/transformers/models/esm/openfold_utils/rigid_utils.py', 'src/transformers/models/esm/openfold_utils/tensor_utils.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/esm/test_modeling_esm.py', 'tests/models/esm/test_modeling_esmfold.py', 'tests/models/esm/test_modeling_tf_esm.py', 'utils/check_inits.py', 'utils/check_repo.py']","ESMFold integration with the main model is incomplete, causing output discrepancies, improper handling of dependencies, incorrect test suite implementation, and issues with the configuration files."
1cc3bc22fed6ffc5937cf66c799dd97840622e69,1693906674,"nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the minimum PyTorch version we currently support is 1.10.0 (#25974)

nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the
minimum PyTorch version we currently support is 1.10.0",['src/transformers/modeling_utils.py'],"The current system unnecessarily maintains compatibility with PyTorch versions below 1.1.0, despite our minimum supported version being 1.10.0."
86119c115496ca773b82f8a1c8cbf2d4e44149fa,1645456259,"add VisionTextDualEncoder and CLIP fine-tuning script (#15701)

* begin script

* update script

* fix features and data args

* main

* add requirements

* add column name args

* fix captions

* don't jit transforms

* fix caption

* fix labels, handle attention mask

* convert pixel values to numpy

* labels => input_ids

* transform images on the fly

* use AutoModel class, create the hybird model outside of the script

* fix version message

* add readme

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adderss review comments

* add more comments

* allow freezing vision and text models

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>",['examples/pytorch/contrastive-image-text/run_clip.py'],"Issues with VisionTextDualEncoder and CLIP fine-tuning script including label inputs, JIT transforms, and hybrid model creation. Image transformation and freezing of vision and text models not working properly."
ea55bd86b9a452c87c5383afc707ab7d710a3043,1675367135,"Add VQGAN-CLIP research project (#21329)

* Add VQGAN-CLIP research project

* fixed style issues

* Update examples/research_projects/vqgan-clip/README.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update examples/research_projects/vqgan-clip/VQGAN_CLIP.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update examples/research_projects/vqgan-clip/requirements.txt

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update examples/research_projects/vqgan-clip/README.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update examples/research_projects/vqgan-clip/VQGAN_CLIP.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update examples/research_projects/vqgan-clip/VQGAN_CLIP.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update examples/research_projects/vqgan-clip/VQGAN_CLIP.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update examples/research_projects/vqgan-clip/loaders.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* replace CLIPProcessor with tokenizer, change asserts to exceptions

* rm unused import

* remove large files (jupyter notebook linked in readme, imgs migrated to hf dataset)

* add tokenizers dependency

* Remove comment

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* rm model checkpoints

---------

Co-authored-by: Erwann Millon <erwann@Erwanns-MacBook-Air.local>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['examples/research_projects/vqgan-clip/VQGAN_CLIP.py', 'examples/research_projects/vqgan-clip/img_processing.py', 'examples/research_projects/vqgan-clip/loaders.py', 'examples/research_projects/vqgan-clip/utils.py']","The VQGAN-CLIP research project is missing from the examples/research_projects directory, including its necessary files and dependencies."
edb672ac5edcd92fadb15d3172a115eb5fe6f663,1655219412,"Add `BloomForSequenceClassification` and `BloomForTokenClassification` classes (#17639)

* add new bloom classes

* (feat) add bloom classification tests; make style

* style: change import in test

* add some typehints to bloom classes

* merge main into branch

* fix: input checking in bloom seq classification

* fix tests

* change model class tests

* fix few tests

- more tests should pass
- one test left

* make token classifier return hidden states

* style: make BLOOM typehints consistent

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

Co-authored-by: younesbelkada <younesbelkada@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/bloom/__init__.py', 'src/transformers/models/bloom/modeling_bloom.py', 'src/transformers/trainer.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/bloom/test_modeling_bloom.py']",The Bloom Classification model currently lacks comprehensive tests and the token classifier does not return hidden states. Some test failures and inconsistencies in typehints for Bloom are also observed.
a929f81e92416bbba6a75f461c6ace2da2ff44b4,1666115256,"Repo utils test (#19696)

* Create repo utils test job

* Last occurence

* Add tests for tests_fetcher

* Better filtering

* Let's learn more

* Should fix

* Should fix

* Remove debug

* Style

* WiP

WiP

WiP

WiP

WiP

WiP

WiP

WiP

WiP

* Quality

* address review comments

* Fix link","['.circleci/create_circleci_config.py', 'tests/repo_utils/test_tests_fetcher.py', 'utils/tests_fetcher.py']","The repo utils test job does not exist, and there's lack of tests for the tests_fetcher. The filtering system is also inadequate."
a10f61834dbae03975d06ccba8ce34f02b44a0cd,1652306208,"[feat] Add FLAVA model (#16654)

* [WIP] Add FLAVA model

This PR aims to add [FLAVA](ihttps://arxiv.org/abs/2112.04482) model to the transformers repo.

Following checklist delineates the list of things to be done for this PR
to be complete:

[x] Flava init
[x] Flava base models
[x] Flava layers
[x] Flava Configs
[x] Flava encoders
[x] Flava pretraining models
[ ] Flava classification/retrieval models (To be added in a separate PR)
[x] Documentation updates 
[x] Imports updates 
[x] Argstring updates
[x] Flava pretrained checkpoints 
[x] Flava tests
[x] Flava processors 
[x] Sanity check
[x] Lint
","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/flava/__init__.py', 'src/transformers/models/flava/configuration_flava.py', 'src/transformers/models/flava/convert_dalle_to_flava_codebook.py', 'src/transformers/models/flava/convert_flava_original_pytorch_to_hf.py', 'src/transformers/models/flava/feature_extraction_flava.py', 'src/transformers/models/flava/modeling_flava.py', 'src/transformers/models/flava/processing_flava.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/models/flava/test_feature_extraction_flava.py', 'tests/models/flava/test_modeling_flava.py', 'tests/models/flava/test_processor_flava.py', 'utils/check_repo.py']",The transformers repository lacks support for the FLAVA model.
ac957f69cc51497ed36a9583b6b8759044f377fa,1694689243,"[Whisper Tokenizer] Encode timestamps (#26054)

* [Whisper Tokenizer] Fix tests after adding timestamps

* fix s2t tokenizer tests

* fix vocab test

* backwards comp

* fix tests

* comment

* style

* fix last test

* fix fast

* make faster

* move logic to decode

* remove skip test

* fix decode with offsets

* fix special tokens

* empty commit to re-trigger ci

* use lru cache","['src/transformers/models/whisper/tokenization_whisper.py', 'src/transformers/models/whisper/tokenization_whisper_fast.py', 'tests/models/whisper/test_tokenization_whisper.py']","Issues with Whisper tokenizer failing to include timestamps in encoding, causing discrepancies in vocabulary tests and speed of tokenization."
03966cacf9f277161afdb127026bc9a8fb8b188b,1679927866,"Wav2Vec2ProcessorWithLM can return N best hypotheses now (#22235)

* Wav2Vec2ProcessorWithLM can return N best hypotheses now

Signed-off-by: Vladislav Sokolovskii <vladislav@parrothq.com>

* Wav2Vec2ProcessorWithLM n_best cannot be None

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Batch decoding can return  N best hypotheses now

batch_decode was extended with the same functionality as decode
function, N best hypotheses per sample can be returned

Signed-off-by: Vladislav Sokolovskii <vladislav@parrothq.com>

---------

Signed-off-by: Vladislav Sokolovskii <vladislav@parrothq.com>
Co-authored-by: Vladislav Sokolovskii <vladislav@parrothq.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",['src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py'],The Wav2Vec2ProcessorWithLM and batch_decode functions are currently unable to return the N best hypotheses.
707f7eb181e1191a64268be902407dacd5e605c7,1633109817,"Bart: check if decoder_inputs_embeds is set (#13800)

In BartForConditionalGeneration.forward, if labels are provided,
   decoder_input_ids are set to the labels shifted to the right.
   This is problematic: if decoder_inputs_embeds is also set,
   the call to self.model, which eventually gets to BartDecoder.forward,
   will raise an error.
   The fix is quite simple, similar to what is there already in
   BartModel.forward. Mainly, we should not
   compute decoder_input_ids if decoder_inputs_embeds is provided.

Co-authored-by: Silviu Vlad Oprea <silviuvo@amazon.co.uk>","['src/transformers/models/bart/modeling_bart.py', 'src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py']","In BartForConditionalGeneration.forward, providing labels alongside decoder_inputs_embeds results in an error when calling BartDecoder.forward due to incorrect handling of decoder_input_ids."
0bae286de94f7131b4a2db3f85754b0961c4aaf5,1669652424,"[AutoBackbone] Improve API (#20407)

* Add hidden states and attentions to backbone outputs

* Update ResNet

* Fix more tests

* Debug test

* Fix test_determinism

* Fix test_save_load

* Remove file

* Disable fx tests

* Test

* Add fx support for backbones

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/modeling_outputs.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/resnet/modeling_resnet.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/fx.py', 'tests/models/resnet/test_modeling_resnet.py', 'tests/test_modeling_common.py', 'utils/check_repo.py']","Backbone outputs lack hidden states and attentions, leading to problems in ResNet and test functionality. An absence of fx support for backbones poses compatibility issues."
0b92ae3489577ea5a1ced167f7b60ac243f333a1,1690468517,"Add offload support to Bark (#25037)

* initial Bark offload proposal

* use hooks instead of manually offloading

* add test of bark offload to cpu feature

* Apply nit suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docstrings of offload

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* remove unecessary set_seed in Bark tests

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>","['src/transformers/models/bark/modeling_bark.py', 'tests/models/bark/test_modeling_bark.py']",Bark currently lacks offload support which can potentially improve performance and efficiency.
3744126c87ad429ba60efc690d8ceb4630dff523,1694103450,"Add `tgs` speed metrics (#25858)

* Add tgs metrics

* bugfix and black formatting

* workaround for tokens counting

* formating and bugfix

* Fix

* Add opt-in for tgs metrics

* make style and fix error

* Fix doc

* fix docbuild

* hf-doc-build

* fix

* test

* Update src/transformers/training_args.py

renaming

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* Update src/transformers/training_args.py

renaming

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* Fix some symbol

* test

* Update src/transformers/trainer_utils.py

match nameing patterns

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/trainer.py

nice

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Fix reviews

* Fix

* Fix black

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/trainer.py', 'src/transformers/trainer_utils.py', 'src/transformers/training_args.py']","'tgs' speed metrics are missing, causing issues in performance monitoring and inconsistencies in code formatting and symbol naming."
dde718e7a62bf8caa6623b5635ba02d6cb758c75,1678220379,"[DETR and friends] Remove is_timm_available (#21814)

* First draft

* Fix to_dict

* Improve conversion script

* Update config

* Remove timm dependency

* Fix dummies

* Fix typo, add integration test

* Upload 101 model as well

* Remove timm dummies

* Fix style

---------

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/conditional_detr/__init__.py', 'src/transformers/models/conditional_detr/image_processing_conditional_detr.py', 'src/transformers/models/deformable_detr/__init__.py', 'src/transformers/models/deformable_detr/image_processing_deformable_detr.py', 'src/transformers/models/detr/__init__.py', 'src/transformers/models/detr/configuration_detr.py', 'src/transformers/models/detr/convert_detr_to_pytorch.py', 'src/transformers/models/detr/image_processing_detr.py', 'src/transformers/models/table_transformer/__init__.py', 'src/transformers/models/table_transformer/configuration_table_transformer.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_timm_and_vision_objects.py', 'tests/models/detr/test_modeling_detr.py']",The software has a dependency on 'timm' module which is causing issues.
971da2e6ec256b7b263be7035462fc58dd4d3d35,1664908108,"Clamping hidden state values to allow FP16 (#19229)

* Clamping hidden state values to allow FP16

* Reformating

* Adding missing if condition

* Update src/transformers/models/longt5/modeling_longt5.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/models/longt5/modeling_longt5.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/models/longt5/modeling_longt5.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Formating file

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>",['src/transformers/models/longt5/modeling_longt5.py'],"Hidden state values are not supported for FP16, requiring a process for value clamping in longt5 model."
c1e47bf4fe1d9de06d774cc2c24ec5a93461c5a5,1631632519,"[Flax] Addition of FlaxPegasus (#13420)

* added initial files

* fixes pipeline

* fixes style and quality

* fixes doc issue and positional encoding

* fixes layer norm and test

* fixes quality issue

* fixes code quality

* removed extra layer norm

* added layer norm back in encoder and decoder

* added more code copy quality checks

* update tests

* Apply suggestions from code review

* fix import

* fix test

Co-authored-by: patil-suraj <surajp815@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/pegasus/__init__.py', 'src/transformers/models/pegasus/modeling_flax_pegasus.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/test_modeling_flax_pegasus.py']","FlaxPegasus is missing from the codebase causing pipeline issues, doc errors, and misalignment in layer normalization and positional encoding tests."
81156d20cd76c1a43ed44fdbc785e237d60b6896,1643055910,"Add model like (#14992)

* Add new model like command

* Bad doc-styler

* black and doc-styler, stop fighting!

* black and doc-styler, stop fighting!

* At last

* Clean up

* Typo

* Bad doc-styler

* Bad doc-styler

* All good maybe?

* Use constants

* Add doc and type hints

* More cleaning

* Add doc

* Fix Copied from

* Doc template

* Use typing.Pattern instead

* Framework-specific files

* Fixes

* Select frameworks clean model init

* Deal with frameworks in main init

* fixes

* Last fix

* Prompt user for info

* Delete exemple config

* Last fixes

* Add test config

* Fix bug with model_type included in each other

* Fixes

* More fixes

* More fixes

* Adapt config

* Remove print statements

* Will fix tokenization later, leave it broken for now

* Add test

* Quality

* Try this way

* Debug

* Maybe by setting the path?

* Let's try another way

* It should go better when actually passing the arg...

* Remove debug statements and style

* Fix config

* Add tests

* Test require the three backends

* intermediate commit

* Revamp pattern replacements and start work on feature extractors

* Adapt model info

* Finalize code for processors

* Fix in main init additions

* Finish questionnaire for processing classes

* Fix file name

* Fix for real

* Fix patterns

* Style

* Remove needless warnings

* Copied from should work now.

* Include Copied form in blocks

* Add test

* More fixes and tests

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comment

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>","['src/transformers/commands/add_new_model_like.py', 'src/transformers/commands/transformers_cli.py', 'tests/test_add_new_model_like.py', 'utils/tests_fetcher.py']","The current software lacks a ""Model Like"" command functionality, preventing users from creating a clone or template of a particular model configuration. Also, there are some issues with tokenization and pattern replacements."
3a8a8013adc5802cc302e63e403f0e8925fab0ea,1632916055,"Keras callback to push to hub each epoch, or after N steps (#13773)

* Keras callback to push to hub each epoch, or after N steps

* Reworked the callback to use Repository

* Use an Enum for save_strategy

* Style pass

* Correct type for tokenizer

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Adding print message to the final upload

* Adding print message to the final upload

* Change how we wait for the last process to finish

* is_done is a property, not a method, derp

* Docstrings and documentation

* Style pass

* Style edit

* Docstring reformat

* Docstring rewrite

* Replacing print with internal logger

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/keras_callbacks.py'],"Keras callback isn't able to push updates to hub at each epoch or after a certain number of steps, causing inconsistent model version tracking and updates."
a69e185074fff529ed60d936c6afe05580aee8ac,1646854238,"add doctests for bart like seq2seq models (#15987)

* boom boom

* enable doctest for few seq2seq models

* add seq2seq models in documentation_tests.txt

* fix docstring blenderbot

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix seq classif doc sample

* don't check loss for seq classif examples

* +IGNORE_OUTPUT => +IGNORE_RESULT

* fix _SEQ_CLASS_EXPECTED_OUTPUT_SHAPE

* fix some docs

* more fixes

* last fix (hopefully)

* fix big bird gen example

* fix mbart gen example

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/file_utils.py', 'src/transformers/models/bart/modeling_bart.py', 'src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py', 'src/transformers/models/blenderbot/modeling_blenderbot.py', 'src/transformers/models/blenderbot_small/modeling_blenderbot_small.py', 'src/transformers/models/marian/modeling_marian.py', 'src/transformers/models/mbart/modeling_mbart.py', 'src/transformers/models/pegasus/modeling_pegasus.py', 'src/transformers/models/plbart/modeling_plbart.py']","Certain seq2seq models like BART are lacking in-line documentation tests, making it difficult to verify the accuracy of the code documentation and examples."
d51aa48a76b470d95fe665bbe7ee97a3d4c21fb9,1688162643,"Limit Pydantic to V1 in dependencies (#24596)

* Limit Pydantic to V1 in dependencies

Pydantic is about to release V2 release which will break a lot of things. This change prevents `transformers` to be used with Pydantic V2 to avoid breaking things.

* more

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['setup.py', 'src/transformers/dependency_versions_table.py']","Upcoming Pydantic V2 release anticipated to cause compatibility issues with current version of 'transformers' framework, potentially breaking functionality."
39f1dff5a0e4633e1859567d31108353a4bc13c7,1638896958,"Fix a Bug, trainer_seq2seq.py, in the else branch at Line 172, generation_inputs should be a dict (#14546)

* fix bug, trainer_seq2seq.py, Line 172, generation_inputs must be a dict before feeding into self.model.generation()

* fix bug, trainer_seq2seq.py, Line 172, generation_inputs must be a dict before feeding into self.model.generation()",['src/transformers/trainer_seq2seq.py'],"In the else branch at line 172 in trainer_seq2seq.py, generation_inputs is not correctly formatted as a dictionary before input into self.model.generation()."
aea761499f4b1193f2706f471442da6f9df65d65,1693914231,"Update training_args.py to remove the runtime error (#25920)

This cl iterates through a list of keys rather than dict items while updating the dict elements. Fixes the following error:
File ""..../transformers/training_args.py"", line 1544, in post_init
for k, v in self.fsdp_config.items():
RuntimeError: dictionary keys changed during iteration",['src/transformers/training_args.py'],Iterating through dictionary items in training_args.py causes RuntimeError due to change in dictionary keys during iteration.
d50db469c09d35bbde4f0feacf2542238c673bd8,1681989508,"Generation: only search for eos_token if set (#22875)

Generation: only check for eos_token if set

The check for unfinished_sequences.max(), which is to find sequences
that have ended early via eos_token_id, creates a synchronization point
even when there is no eos_token, which slows inference down.

This change moves the calculation to inside the condition checking for
eos_token, so that such slowdown may be removed by disabling this token.

Co-authored-by: John Doe <john.doe@example.com>",['src/transformers/generation/utils.py'],"Inference slows down due to unnecessary synchronization point created when checking for eos_token_id, even when no eos_token is set."
53e33e6f1be980353e8cd3f75c7cf1e58720f557,1662141242,"PEGASUS-X (#18551)

* PegasusX Initial commit

* rename

* pegasus X implementation

* pegx update

* pegx fix

* pegasus-x fixes

* pegx updates

* cleanup

* cleanup

* cleanup

* tests

* stylefixes

* Documentation update

* Model hub fix

* cleanup

* update

* update

* testfix

* Check fix

* tweaks for merging

* style

* style

* updates for pr

* style

* change pegasus-x repo","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/pegasus_x/__init__.py', 'src/transformers/models/pegasus_x/configuration_pegasus_x.py', 'src/transformers/models/pegasus_x/modeling_pegasus_x.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/pegasus_x/test_modeling_pegasus_x.py', 'utils/check_repo.py']","Initial implementation of PegasusX is buggy and lacks complete tests, thorough documentation, and proper coding style. Also, the model hub doesn't integrate seamlessly with the new introduction."
98d40fed3a4515077163adab9dfd8fb2fccf1267,1675169656,"Cleanup the usage of `layer_norm_eps` in some models (#21336)

* fix

* fix

* make style

* For CLIP

* For OwlViT

* For XCLIP

* For CLIPSeg

* For GroupViT

* fix docstrings

* fix docstrings

* For AltCLIP

* For ChineseCLIP

* For Blip

* For GiT

* make style

* update

* update

* update

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/altclip/configuration_altclip.py', 'src/transformers/models/altclip/modeling_altclip.py', 'src/transformers/models/blip/configuration_blip.py', 'src/transformers/models/blip/modeling_blip.py', 'src/transformers/models/chinese_clip/configuration_chinese_clip.py', 'src/transformers/models/chinese_clip/modeling_chinese_clip.py', 'src/transformers/models/clip/configuration_clip.py', 'src/transformers/models/clip/modeling_clip.py', 'src/transformers/models/clipseg/configuration_clipseg.py', 'src/transformers/models/clipseg/modeling_clipseg.py', 'src/transformers/models/git/configuration_git.py', 'src/transformers/models/git/modeling_git.py', 'src/transformers/models/groupvit/configuration_groupvit.py', 'src/transformers/models/groupvit/modeling_groupvit.py', 'src/transformers/models/oneformer/modeling_oneformer.py', 'src/transformers/models/owlvit/configuration_owlvit.py', 'src/transformers/models/owlvit/modeling_owlvit.py', 'src/transformers/models/x_clip/configuration_x_clip.py', 'src/transformers/models/x_clip/modeling_x_clip.py']","Inconsistent and improper usage of `layer_norm_eps` across multiple models including CLIP, OwlViT, XCLIP, etc., potentially causing unexpected model behavior."
954e18ab9713da83e1484f78a6f6e178b0d9fe2a,1662042915,"TensorFlow MobileViT (#18555)

* initial implementation.

* add: working model till image classification.

* add: initial implementation that passes intg tests.

Co-authored-by: Amy <aeroberts4444@gmail.com>

* chore: formatting.

* add: tests (still breaking because of config mismatch).

Coo-authored-by: Yih <2521628+ydshieh@users.noreply.github.com>

* add: corrected tests and remaning changes.

* fix code style and repo consistency.

* address PR comments.

* address Amy's comments.

* chore: remove from_pt argument.

* chore: add full-stop.

* fix: TFLite model conversion in the doc.

* Update src/transformers/models/mobilevit/modeling_tf_mobilevit.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/mobilevit/modeling_tf_mobilevit.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/mobilevit/modeling_tf_mobilevit.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/mobilevit/modeling_tf_mobilevit.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/mobilevit/modeling_tf_mobilevit.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply formatting.

* chore: remove comments from the example block.

* remove identation in the example.

Co-authored-by: Amy <aeroberts4444@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/modeling_tf_outputs.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/mobilevit/__init__.py', 'src/transformers/models/mobilevit/modeling_tf_mobilevit.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/models/mobilevit/test_modeling_tf_mobilevit.py']","Initial implementation of MobileViT in TensorFlow misconfigures tests, leading to test failures. Additionally, an issue occurs related to the model's TFLite conversion documentation."
040c4613c2fac59f16e333a630d9a69b6ff9ca5d,1693847770,"Add type hints for tf models final batch (#25883)

* Add missing type hints and consistency to `RegNet` models

* Add missing type hints and consistency to `TFSamModel`

* Add missing type hints to `TFSegformerDecodeHead`

* Add missing type hints and consistency to `TransfoXL` family models

* Add missing type hints and consistency to `TFWav2Vec2ForSequenceClassification`

* Add type hints to `TFXLMModel`

* Fix linter

* Revert the type hints for `RegNet` to python 3.8 compliant

* Remove the redundant np.ndarray type hint.","['src/transformers/models/regnet/modeling_tf_regnet.py', 'src/transformers/models/sam/modeling_tf_sam.py', 'src/transformers/models/segformer/modeling_tf_segformer.py', 'src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py', 'src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py', 'src/transformers/models/xlm/modeling_tf_xlm.py']","Missing type hints in several TensorFlow models (RegNet, TFSamModel, TFSegformerDecodeHead, TransfoXL, TFWav2Vec2ForSequenceClassification, TFXLMModel) leading to inconsistency and potential type errors."
01485ceec3d2e0a9a957ec86f0a10096cecb4a94,1646400988,"Add missing support for Flax XLM-RoBERTa (#15900)

* Adding Flax XLM-RoBERTa

* Add Flax to __init__

* Adding doc and dummy objects

* Add tests

* Add Flax XLM-R models autodoc

* Fix tests

* Add Flask XLM-RoBERTa to TEST_FILES_WITH_NO_COMMON_TESTS

* Update src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update tests/xlm_roberta/test_modeling_flax_xlm_roberta.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update tests/xlm_roberta/test_modeling_flax_xlm_roberta.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Remove test on large Flask XLM-RoBERTa

* Add tokenizer to the test

Co-authored-by: Suraj Patil <surajp815@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/xlm_roberta/__init__.py', 'src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/xlm_roberta/test_modeling_flax_xlm_roberta.py', 'utils/check_repo.py']","The Flax XLM-RoBERTa model is not supported, causing errors in testing and usage, and the tokenizer for the model was not included in the test."
d329b633692c6accbc6ea03f80a67a5d3e5814e0,1628758886,"Deberta tf (#12972)

* TFDeberta

moved weights to build and fixed name scope

added missing ,

bug fixes to enable graph mode execution

updated setup.py

fixing typo

fix imports

embedding mask fix

added layer names avoid autmatic incremental names

+XSoftmax

cleanup

added names to layer

disable keras_serializable
Distangled attention output shape hidden_size==None
using symbolic inputs

test for Deberta tf

make style

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

removed tensorflow-probability

removed blank line

* removed tf experimental api
+torch_gather tf implementation from @Rocketknight1

* layername DeBERTa --> deberta

* copyright fix

* added docs for TFDeberta & make style

* layer_name change to fix load from pt model

* layer_name change as pt model

* SequenceClassification layername change,
to same as pt model

* switched to keras built-in LayerNormalization

* added `TFDeberta` prefix most layer classes

* updated to tf.Tensor in the docstring","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/deberta/__init__.py', 'src/transformers/models/deberta/modeling_tf_deberta.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/test_modeling_tf_deberta.py']","Issues discovered in the TFDeberta model, including incorrect layer names, problems with weight building, import errors, typos, and lack of graph mode execution support. Incorrect output shape under 'None' hidden_size for the attention layer."
531336bbfd2a97cf800f610d971d6ec0a1578752,1642508933,"Fix deprecation warnings for int div (#15180)

* Fix deprecation warnings for int div

Co-authored-by: mgoldey <matthew.goldey@gmail.com>

* Fix import

* ensure that tensor output is python scalar

* make backward compatible

* make code more readable

* adapt test functions

Co-authored-by: mgoldey <matthew.goldey@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py', 'src/transformers/modeling_utils.py', 'src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/sew/modeling_sew.py', 'src/transformers/models/sew_d/modeling_sew_d.py', 'src/transformers/models/unispeech/modeling_unispeech.py', 'src/transformers/models/unispeech_sat/modeling_unispeech_sat.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'src/transformers/models/wavlm/modeling_wavlm.py', 'tests/test_modeling_wav2vec2.py']","Deprecation warnings are being shown for integer division operations, alongside issues with tensor outputs not being compatible with Python scalars."
bc9ecef94242f3247441ec60167cb026d871280e,1686072897,"Use new parametrization based weight norm if available (#24030)

* Use new parametrization based weight norm if available

See https://github.com/pytorch/pytorch/pull/103001

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

* handle copies

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

* black

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

---------

Signed-off-by: Edward Z. Yang <ezyang@meta.com>","['src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/speecht5/modeling_speecht5.py', 'src/transformers/models/unispeech/modeling_unispeech.py', 'src/transformers/models/unispeech_sat/modeling_unispeech_sat.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py', 'src/transformers/models/wavlm/modeling_wavlm.py']",The current weight normalization does not have the option to utilize the new parameterization if it's available.
aeae97829fd29b8cf5bed76bbb283d1ce01d66d7,1666795474,"Add missing information on token_type_ids for roberta model (#19766)

* Add missing information on token_type_ids for roberta model

* Fix code format issues

* Fix code format issues

* Add more explicit document for token_type_ids for roberta

* Fix flake8 issues

* Fix flake8 issues

* Fix flake8 issues

* Fix flake8 issues

* Fix flake8 issues",['src/transformers/models/roberta/modeling_roberta.py'],Roberta model documentation lacks information regarding token_type_ids; code formatting and flake8 issues present.
6ce11c2c0f216f4d9d7f386003a58c06c9e34783,1646909685,"[Docs] Improve PyTorch, Flax generate API (#15988)

* Move generate docs

* up

* Update docs/source/_toctree.yml

* correct

* correct some stuff

* correct tests

* more fixes

* finish generate

* add to doc stest

* finish

* finalize

* add warning to generate method","['src/transformers/generation_flax_utils.py', 'src/transformers/generation_utils.py']","Some parts of the PyTorch, Flax generate API documentation are incorrect or incomplete, causing confusion and potential misuse of these APIs. The generated tests related to these APIs also need corrections."
defdcd286280ec6bddd19c81a308d35de67c3efd,1674046179,"Remove Roberta Dependencies from XLM Roberta Flax and Tensorflow models (#21047)

* Added flax model code

* Added tf changes

* missed some

* Added copy comments

* Added style hints

* Fixed copy statements

* Added suggested fixes

* Made some fixes

* Style fixup

* Added necessary copy statements

* Fixing copy statements

* Added more copies

* Final copy fix

* Some bugfixes

* Adding imports to init

* Fixed up all make fixup errors

* Fixed doc errors

* Auto model changes","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/xlm_roberta/__init__.py', 'src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py', 'src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py', 'src/transformers/models/xlm_roberta/modeling_xlm_roberta.py', 'src/transformers/utils/dummy_flax_objects.py', 'src/transformers/utils/dummy_tf_objects.py']","XLM Roberta Flax and Tensorflow models have unnecessary dependencies on Roberta, leading to potential conflicts and inefficiencies."
9450bfcc6cce6ea189487266e80646a2fa19a944,1635763956,"Add more missing models to models/__init__.py (#14177)

* Add missing models to models/__init__.py

* Fix issues previously undetected

* Add UniSpeechSatForPreTraining to all_model_classes

* fix unispeech sat

* fix

* Add check_model_list() to check_repo.py

* Remove _ignore_models = [""bort""]

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>","['src/transformers/models/__init__.py', 'src/transformers/models/unispeech_sat/modeling_unispeech_sat.py', 'tests/test_modeling_unispeech_sat.py', 'utils/check_repo.py']","Certain models are absent in models/__init__.py file, leading to undetected issues and discrepancies in check_repo.py script."
919a964b8ffd2f53b60504072a4127632ba79d60,1635257339,"Include Keras tensor in the allowed types (#14155)

* Include KerasTensor in allowed types

- This allows propagating symbolic tensors through TFBert models and layers' call(),
  which allows converting the subclass models to functional models.

* Style pass

Co-authored-by: Sergio Valcarcel Macua <sergiov@graphcore.ai>
Co-authored-by: matt <rocketknight1@gmail.com>",['src/transformers/modeling_tf_utils.py'],"TFBert models are unable to convert subclass models to functional models due to the inability to propagate symbolic tensors, particularly of the type KerasTensor, through the models and layers' call()."
6de4ee61a03d3f5f8184195c92e58b7f5b8ba642,1650368344,"Wav2 vec2 phoneme ctc tokenizer optimisation (#16817)

* Solved href rendering issue in heading

Markdown references in headings such as '####' don't render well.
Replaced it with <h4>...<a></a></h> banners.

* PhonemeTokenizer optimization using phonemizer lib

The backend should only be initialized once, otherwise it is reloaded.
Added `init_backend` function, intializes a backend attribute.
Phonemize re-uses self.backend.
Should give ~10 times faster phonemization.

* formatted file with make style

* Documentation suggestion

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update /tokenization_wav2vec2_phoneme.py based on PR suggestion

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update CONTRIBUTING.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py'],"Repeated initializations of backend causing slower phonemization due to unnecessary reloading, and markdown references in the headings are not rendering correctly."
675d2a5a006d3efe81fd7f08ac62eec404f3be7e,1679059697,"fix AutoTP in deepspeed could not work for bloom (#22196)

* fix AutoTP in deepspeed could not work for bloom

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* add a method in BloomModel to build ailib

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>",['src/transformers/models/bloom/modeling_bloom.py'],The AutoTP feature in Deepspeed fails to function properly for the BloomModel.
9870093f7b31bf774fe6bdfeed5e08f0d4649b07,1628248350,"[WIP] Disentangle auto modules from other modeling files (#13023)

* Initial work

* All auto models

* All tf auto models

* All flax auto models

* Tokenizers

* Add feature extractors

* Fix typos

* Fix other typo

* Use the right config

* Remove old mapping names and update logic in AutoTokenizer

* Update check_table

* Fix copies and check_repo script

* Fix last test

* Add back name

* clean up

* Update template

* Update template

* Forgot a )

* Use alternative to fixup

* Fix TF model template

* Address review comments

* Address review comments

* Style","['src/transformers/__init__.py', 'src/transformers/modelcard.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/auto_factory.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/mbart/__init__.py', 'src/transformers/models/mbart50/__init__.py', 'src/transformers/models/mbart50/tokenization_mbart50.py', 'src/transformers/models/mbart50/tokenization_mbart50_fast.py', 'src/transformers/tokenization_utils_base.py', 'src/transformers/trainer.py', 'src/transformers/utils/dummy_tokenizers_objects.py', 'src/transformers/utils/modeling_auto_mapping.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py', 'tests/test_pipelines_translation.py', 'utils/check_repo.py', 'utils/check_table.py', 'utils/class_mapping_update.py']","Interdependencies between auto modules and other modeling files are creating conflicts, specifically with correct tokenizers, feature extractions, and config usage."
029b0d95ed7282697bd2b361e95e6f6144151ae2,1648067771,"add GPT-J ONNX config to Transformers (#16274)

* add GPT-J ONNX config to Transformers

* remove token-classification features mapping

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add question-answering features mapping

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add GPT2 config init to GPT2 config + copie shebang for fix-copies

Co-authored-by: ChainYo <t.chaigneau.tc@gmail.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>","['src/transformers/models/gptj/__init__.py', 'src/transformers/models/gptj/configuration_gptj.py', 'src/transformers/onnx/features.py']","Transformers library lacks GPT-J ONNX configuration, causing incompatibility issues. Feature mappings for token-classification and question-answering are missing as well."
50d909be28381a9f796a7754ec834815b321847f,1638435108,"[Flax] Add FlaxBlenderbotSmall (#14576)

* [WIP] Add FlaxBlenderbotSmall

* Revert some unintentionally changed files

Revert some unintentionally files changed by improperly filled cookiecutter instructions.

* Fix repo consistency

* Fix Flax-PT equivalence

* Apply suggestions from code review

* Update index.mdx

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/blenderbot_small/__init__.py', 'src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/test_modeling_flax_blenderbot_small.py']",The project lacks a FlaxBlenderbotSmall model resulting in missing functionalities and limited toolset.
05a90579a806b37be5860a8461fae6164c209b43,1652707936,"CodeParrot data pretokenization (#16932)

* add pretokenization arguments

* add pretokenization script

* add support for pretokenized data

* reformat code

* fix run command for training

* fix model call from config

* remove a package

* add comments on pretokenization in the readme

* remove explicit parallelization

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* update readme

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* update readme -remove username

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* update readme -remove username

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* keep data parallelization

* reformat code

* reformat code

* update readme

* reformat code

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>
Co-authored-by: Loubna ben allal <loubnabenallal@gmail.com>","['examples/research_projects/codeparrot/scripts/arguments.py', 'examples/research_projects/codeparrot/scripts/codeparrot_training.py', 'examples/research_projects/codeparrot/scripts/initialize_model.py', 'examples/research_projects/codeparrot/scripts/pretokenizing.py']",The CodeParrot dataset lacks a preprocessing step for pretokenization of data which might cause issues for specific model configurations during training.
48bf7e47a01f310ca8e76bd90be14e06dbd08329,1639384250,"Code parrot minor fixes/niceties (#14666)

* Add some nicety flags for better controlling evaluation.

* Fix dependency issue with outdated requirement

* Add additional flag to example to ensure eval is done

* Wrap code into main function for accelerate launcher to find

* Fix valid batch size flag in readme

* Add note to install git-lfs when initializing/training the model

* Update examples/research_projects/codeparrot/scripts/arguments.py

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Revert ""Wrap code into main function for accelerate launcher to find""

This reverts commit ff11df1c810d4df198d04b827538eb4572147ba3.

* Fix formatting issue

* Move git-lfs instructions to installation section

* Add a quick check before code generation for code evaluation

* Fix styling issue

* Update examples/research_projects/codeparrot/scripts/human_eval.py

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Make iterable dataset use passed in tokenizer rather than globally defined one

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>
Co-authored-by: ncoop57 <nac33@students.uwf.edu>","['examples/research_projects/codeparrot/scripts/arguments.py', 'examples/research_projects/codeparrot/scripts/codeparrot_training.py', 'examples/research_projects/codeparrot/scripts/human_eval.py']","Several issues present including lack of evaluation control, outdated dependencies, readme inaccuracies, and problems with Accelerate launcher locating main function. Global tokenizer incorrectly utilized in iterable dataset."
e9a49babeecbcb23db97debd88c42da351822878,1665151219,"[WIP] Add ZeroShotObjectDetectionPipeline (#18445) (#18930)

* Add ZeroShotObjectDetectionPipeline (#18445)

* Add AutoModelForZeroShotObjectDetection task

This commit also adds the following

- Add explicit _processor method for ZeroShotObjectDetectionPipeline.
  This is necessary as pipelines don't auto infer processors yet and
  `OwlVitProcessor` wraps tokenizer and feature_extractor together, to
  process multiple images at once

- Add auto tests and other tests for ZeroShotObjectDetectionPipeline

* Add AutoModelForZeroShotObjectDetection task

This commit also adds the following

- Add explicit _processor method for ZeroShotObjectDetectionPipeline.
  This is necessary as pipelines don't auto infer processors yet and
  `OwlVitProcessor` wraps tokenizer and feature_extractor together, to
  process multiple images at once

- Add auto tests and other tests for ZeroShotObjectDetectionPipeline

* Add batching for ZeroShotObjectDetectionPipeline

* Fix doc-string ZeroShotObjectDetectionPipeline

* Fix output format: ZeroShotObjectDetectionPipeline","['src/transformers/__init__.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/zero_shot_object_detection.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/pipelines/test_pipelines_zero_shot_object_detection.py', 'utils/update_metadata.py']","The pipeline for ZeroShotObjectDetection doesn't automatically infer processors and doesn't support batch processing, leading to improper handling of multiple images. Additionally, documentation and output format need corrections."
da36c557f763cb4e515f71ba7c49191fede8966f,1637249074,"Add ImageGPT (#14240)

* First draft

* More improvements

* Improve conversion script

* Fix init weights for layer norm

* Fix correct model for conversion script

* Don't tie input and output embeddings

* Add print statements for debugging

* Add print statements for debugging

* Fix vocab size of model

* Improve documentation, remove fast tokenizer

* Add ImageGPTForImageClassification, improve docs

* Fix docs issue

* Set verbosity level back to info

* Improve tests

* Fix tests and add figure

* Delete tokenizer file

* Remove ImageGPTTokenizer from init files

* Remove ImageGPTLayer from init files

* Remove ImageGPT tokenizer from docs

* First draft of ImageGPTFeatureExtractor

* Fix typo

* Fix bug

* More improvements

* Apply suggestions from code review, add tests for feature extractor

* Fix layernorm

* Update save_pretrained method

* Fix issue

* Make all tests of ImageGPTFeatureExtractor pass

* Update code examples

* Rename model inputs to pixel_values

* Improve code examples

* Update init_weights to post_init

* Fix post_init","['src/transformers/__init__.py', 'src/transformers/feature_extraction_utils.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/imagegpt/__init__.py', 'src/transformers/models/imagegpt/configuration_imagegpt.py', 'src/transformers/models/imagegpt/convert_imagegpt_original_tf2_to_pytorch.py', 'src/transformers/models/imagegpt/feature_extraction_imagegpt.py', 'src/transformers/models/imagegpt/modeling_imagegpt.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/test_feature_extraction_imagegpt.py', 'tests/test_modeling_imagegpt.py', 'utils/check_repo.py']","There are several issues with ImageGPT model integration. These include incorrect model weight initialisation and vocab sizing, erroneous ties between input and output embeddings, and lack of robust image classification component and feature extractor. Additionally, the documentation needs enhancement and the test coverage is lacking."
fb2b45e5627c99a8d65c6cb42cad70bfef7b87be,1669734160,"add in layer gpt2 tokenizer (#20421)

* add minimal working gpt2 tokenizer

* graph mode and output equivalence tests working

* not today tensorflow. serialization test passing!

* fix style, documentation, docstrings and all that jazz

* passing consistency checks

* move keras nlp to tf dependencies

* fix tf modeling utils and gpt2 attention to enable compiling

* fix (I hope) keras nlp dependencies

* rever changes on generation

* remove debug prints

* remove redundant tf dummy objects

* add from config, get config and max length settings to address review

* let flake ignore the error on distillation you are welcome

* test from config

* add padding test

* address sgugger review","['examples/research_projects/seq2seq-distillation/distillation.py', 'setup.py', 'src/transformers/__init__.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/models/gpt2/__init__.py', 'src/transformers/models/gpt2/tokenization_gpt2_tf.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/dummy_keras_nlp_objects.py', 'src/transformers/utils/import_utils.py', 'tests/models/gpt2/test_tokenization_gpt2_tf.py']",The current tokenizer does not support GPT-2 functionality. This limits the applicability of the project to GPT-2 related tasks.
d68d6665f94f86cbd9d5f512c9d2942528661cad,1685540550,"Support shared tensors (#23871)

* Suport shared storage

* Really be sure we have the same storage

* Make style

* - Refactor storage identifier mechanism
 - Group everything into a single for loop

* Make style

* PR

* make style

* Update src/transformers/pytorch_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/modeling_utils.py', 'src/transformers/pytorch_utils.py']",Lack of shared tensor storage support is causing inconsistencies and redundancy in storage mechanisms.
6add3b313defc35b5d8ae3d946131aeb625e0441,1692807928,"[DOCS] Added docstring example for EpsilonLogitsWarper #24783 (#25378)

* [DOCS] Added docstring example for EpsilonLogitsWarper #24783

* minor code changes based on review comments

* set seed for both generate calls, reduced the example length

* fixed line length under 120 chars",['src/transformers/generation/logits_process.py'],"The EpsilonLogitsWarper lacks a docstring example, causing confusion regarding its usage and implementation."
d6b6fb9963e094216daa30ebf61224ca1c46921e,1656083438,"Add CodeGen model (#17443)

* Add CodeGen model

* Add missing key and switch order of super()

* Fix torch.ones init with uint8 instead of bool

* Address comments: copy statements and doc

* update tests

* remove old model parallel

* fix batch gen tests

* fix batch gen test

* update test_gpt2_sample_max_time

* fix codgen test and revert gpt2 test change

* Fix incorrect tie_word_embedding value, typo, URL

* Fix model order in README and styling

* Reorder model list alphabetically

* Set tie_word_embedding to False by default

* Apply suggestions from code review

* Better attn mask name & remove attn masked_bias

* add tokenizer for codegen

* quality

* doc tokenizer

* fix-copies

* add CodeGenTokenizer in converter

* make truncation optional

* add test for truncation

* add copyright

* fix-copies

* fix fast tokenizer decode

* Update src/transformers/models/codegen/tokenization_codegen.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* increase vocab_size in tests

Co-authored-by: patil-suraj <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/convert_slow_tokenizer.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/codegen/__init__.py', 'src/transformers/models/codegen/configuration_codegen.py', 'src/transformers/models/codegen/modeling_codegen.py', 'src/transformers/models/codegen/tokenization_codegen.py', 'src/transformers/models/codegen/tokenization_codegen_fast.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_tokenizers_objects.py', 'tests/models/codegen/test_modeling_codegen.py', 'tests/models/codegen/test_tokenization_codegen.py']","The existing model had various problems including, incorrect initialization of torch.ones with uint8 instead of bool, missing key in the CodeGen model, disordered models in README, and more."
4354005291dacd5c8264f0936a33678df4f4bb71,1649165007,"Adding new train_step logic to make things less confusing for users (#15994)

* Adding new train_step logic to make things less confusing for users

* DO NOT ASK WHY WE NEED THAT SUBCLASS

* Metrics now working, at least for single-output models with type annotations!

* Updates and TODOs for the new train_step

* Make fixup

* Temporary test workaround until T5 has types

* Temporary test workaround until T5 has types

* I think this actually works! Needs a lot of tests though

* MAke style/quality

* Revert changes to T5 tests

* Deleting the aforementioned unmentionable subclass

* Deleting the aforementioned unmentionable subclass

* Adding a Keras API test

* Style fixes

* Removing unneeded TODO and comments

* Update test_step too

* Stop trying to compute metrics with the dummy_loss, patch up test

* Make style

* make fixup

* Docstring cleanup

* make fixup

* make fixup

* Stop expanding 1D input tensors when using dummy loss

* Adjust T5 test given the new compile()

* make fixup

* Skipping test for convnext

* Removing old T5-specific Keras test now that we have a common one

* make fixup

* make fixup

* Only skip convnext test on CPU

* Update src/transformers/modeling_tf_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Avoiding TF import issues

* make fixup

* Update compile() to support TF 2.3

* Skipping model.fit() on template classes for now

* Skipping model.fit() on template class tests for now

* Replace ad-hoc solution with find_labels

* make fixup

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/modeling_tf_utils.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py', 'tests/convnext/test_modeling_tf_convnext.py', 'tests/t5/test_modeling_tf_t5.py', 'tests/test_modeling_tf_common.py']",The existing `train_step` logic in the system is found confusing by some users. There are also issues with the computation of metrics for single-output models with type annotations and handling of 1D input tensors with dummy loss.
4cdb67caba9abee8b5b94b4f88f55e95a3d1013f,1638833252,"Use cross_attention_hidden_size in Encoder-Decoder models (#14378)

* add cross_attention_hidden_size to text-2-text encoder-decoder models (PT/Flax)

* for TFEncoderDecoderModel

* add equivalence test for TFEncoderDecoderModel

* fix

* fix failed equivalence tests

* remove unused import

* add detailed comment

* Fix check_equivalence_tf_to_pt by using encoder/decoder

* cleaning

* Use cross_attention_hidden_size in speech-to-text

* clean fast init logging msg in encoder decoder models

* increase tol from 1e-5 to 1e-3 for tf test

* style

* style

* make sure projection layer can run

* remove type conversion + add check

* fix conflict (config.output_hidden_size)

* Remove TF -> PT in check_pt_tf_equivalence for TFEncoderDecoderModel

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/encoder_decoder/modeling_encoder_decoder.py', 'src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py', 'src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py', 'src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py', 'src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py', 'src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py', 'tests/test_modeling_flax_encoder_decoder.py', 'tests/test_modeling_tf_encoder_decoder.py']","Inconsistencies observed when we use cross_attention_hidden_size in Encoder-Decoder models across varied tasks and frameworks (PT/FLax/TF). Additional discrepancies noted in the run of the projection layer, and type conversion-checks."
1d71ad8905fdf98a0e8c8dd136a995b9a618cb8d,1658915948,"Update CodeParrot readme to include training in Megatron (#17798)

* add info about megatron training

* upload models and datasets from CodeParrot organization

* upload models and datasets from CodeParrot organization

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* fix typo and add comment about codeparrot vs megatron

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>",['examples/research_projects/codeparrot/scripts/arguments.py'],"""CodeParrot's README lacks information about Megatron training and differences between CodeParrot and Megatron, and has some typographical errors."""
f84e0dbd2afa580a1a1c3d8643144b43b2ec654a,1645100197,"Add PoolFormer (#15531)

* Added all files, PoolFormerFeatureExtractor still failing tests

* Fixed PoolFormerFeatureExtractor not being able to import

* Completed Poolformer doc

* Applied Suggested fixes

* Fixed errors in modeling_auto.py

* Fix feature extractor, convert docs to Markdown, styling of code

* Remove PoolFormer from check_repo and fix integration test

* Remove Poolformer from check_repo

* Fixed configuration_poolformer.py docs and removed inference.py from poolformer

* Ran with black v22

* Added PoolFormer to _toctree.yml

* Updated poolformer doc

* Applied suggested fixes and added on README.md

* Did make fixup and make fix-copies, tests should pass now

* Changed PoolFormer weights conversion script name and fixed README

* Applied fixes in test_modeling_poolformer.py and modeling_poolformer.py

* Added PoolFormerFeatureExtractor to AutoFeatureExtractor API

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/poolformer/__init__.py', 'src/transformers/models/poolformer/configuration_poolformer.py', 'src/transformers/models/poolformer/convert_poolformer_original_to_pytorch.py', 'src/transformers/models/poolformer/feature_extraction_poolformer.py', 'src/transformers/models/poolformer/modeling_poolformer.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/test_feature_extraction_poolformer.py', 'tests/test_modeling_poolformer.py']","Issues with PoolFormerFeatureExtractor test failures; import errors with PoolFormerFeatureExtractor and modeling_auto.py; Inability to integrate PoolFormer into the main repository, and problems adding PoolFormerFeatureExtractor to AutoFeatureExtractor API."
a503012275e8d2fa6e682d11c9bad68aa4c46cd6,1636423205,"Small change to Wav2Vec2 model to support Tensor-Parallelism with DeepSpeed (#14298)

* minor modification to the wav2vec2 modeling file to support tensor-parallelism with DeepSpeed on this HuggingFace model

* refine the comments

* synch changes

* fix comments

* refine comments

* fix format","['src/transformers/models/bart/modeling_bart.py', 'src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py', 'src/transformers/models/blenderbot/modeling_blenderbot.py', 'src/transformers/models/blenderbot_small/modeling_blenderbot_small.py', 'src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/m2m_100/modeling_m2m_100.py', 'src/transformers/models/marian/modeling_marian.py', 'src/transformers/models/mbart/modeling_mbart.py', 'src/transformers/models/pegasus/modeling_pegasus.py', 'src/transformers/models/sew/modeling_sew.py', 'src/transformers/models/speech_to_text/modeling_speech_to_text.py', 'src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py', 'src/transformers/models/unispeech/modeling_unispeech.py', 'src/transformers/models/unispeech_sat/modeling_unispeech_sat.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py']","The current Wav2Vec2 model does not support tensor-parallelism with DeepSpeed, limiting its performance capabilities."
a03d13c83d402a6c13866a756ebfaceb43b8e8b6,1690209259,"Pvt model (#24720)

* pull and push updates

* add docs

* fix modeling

* Add and run test

* make copies

* add task

* fix tests and fix small issues

* Checks on a Pull Request

* fix docs

* add desc pvt.md","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/image_processing_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/pvt/__init__.py', 'src/transformers/models/pvt/configuration_pvt.py', 'src/transformers/models/pvt/convert_pvt_to_pytorch.py', 'src/transformers/models/pvt/image_processing_pvt.py', 'src/transformers/models/pvt/modeling_pvt.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/models/pvt/test_image_processing_pvt.py', 'tests/models/pvt/test_modeling_pvt.py']","Issues with Pvt model updating, documentation, copies and testing, with related tasks requiring fixes and checks on a pull request."
5cd40323684c183c30b34758aea1e877996a7ac9,1659708760,"Use new huggingface_hub tools for download models (#18438)

* Draft new cached_file

* Initial draft for config and model

* Small fixes

* Fix first batch of tests

* Look in cache when internet is down

* Fix last tests

* Bad black, not fixing all quality errors

* Make diff less

* Implement change for TF and Flax models

* Add tokenizer and feature extractor

* For compatibility with main

* Add utils to move the cache and auto-do it at first use.

* Quality

* Deal with empty commit shas

* Deal with empty etag

* Address review comments","['src/transformers/configuration_utils.py', 'src/transformers/feature_extraction_utils.py', 'src/transformers/modeling_flax_utils.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/modeling_utils.py', 'src/transformers/tokenization_utils_base.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/hub.py', 'tests/test_configuration_common.py', 'tests/test_feature_extraction_common.py', 'tests/test_modeling_common.py', 'tests/test_modeling_tf_common.py', 'tests/test_tokenization_common.py']","Running the tests results in failures, the cache does not load when the Internet is down, and encounters issues with compatibility, tokenization, feature extraction, and handling of empty commit shas and etags."
3bd1fe431585e233efb4564d12d751b3174996c3,1686679462,"Stop storing references to bound methods via tf.function (#24146)

* Stop storing references to bound methods in tf.functions

* Remove the gc.collect calls now that we resolved the underlying problem

* Remove the default signature from model.serving entirely, big cleanup

* Remove _prune_signature as self.input_signature can prune itself

* Restore serving docstring

* Update int support test to check the input signature

* Make sure other tests also use model.input_signature and not serving.input_signature

* Restore _prune_signature

* Remove the doctest GC now it's no longer needed

* Correct core tests to use the pruned sig

* order lines correctly in core tests

* Add eager_serving back with a deprecation warning","['src/transformers/modeling_tf_utils.py', 'src/transformers/testing_utils.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py', 'tests/models/rag/test_modeling_tf_rag.py', 'tests/models/sam/test_modeling_tf_sam.py', 'tests/models/xglm/test_modeling_tf_xglm.py', 'tests/test_modeling_tf_common.py', 'tests/utils/test_modeling_tf_core.py']",Storing references to bound methods in tf.functions is causing garbage collection issues and affecting model serving and input signature functionality.
d143087d189a183b153090c8b37daa6c7c031039,1680271655,"Making sure we can use safetensors to serialize all the time. (#22437)

* Making sure we can use safetensors to serialize all the time.

* Expanding the tests for increased coverage.

* Update the test.

* Getting current state of affairs.

* Tentative fix.

* Fixing black version.

* Fixing the worst offenders.

* Try to modify less files.

* Fixing blip_2 (Weird solution right now).

* Fixing deta.

* Fix blip ?

* Missing extra newline.

* No deta modification.

* Adding some comments.

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Addressing comments.

* Addressing comments.

* creating warn_once.

* Warning_once !

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/modeling_utils.py', 'src/transformers/models/blip_2/modeling_blip_2.py', 'src/transformers/models/deta/modeling_deta.py', 'src/transformers/models/llama/modeling_llama.py', 'src/transformers/models/pix2struct/configuration_pix2struct.py', 'tests/test_modeling_common.py']","Inconsistent ability to use safetensors to serialize, causing issues with tests and several detected bugs in different files."
7c7d2ec952405686147c5333c097e90f6763e14a,1632341877,"[GPT-J] Use the `float16` checkpoints in integration tests (#13676)

* Use fp16 checkpoints

* Style

* Fix outputs and disable OOM tests

* Correct another output

* Use a random smaller model for generation tests

* repo quickfix

* fix gradient checkpointing",['tests/test_modeling_gptj.py'],"Integration tests failing due to memory overflow when using 'float32' checkpoints, leading to inconsistent output generation and gradient checkpointing issues."
31c3e7e75b2d71a5aeed6debbfbee169f4fa952b,1624907489,"[Flax] Add T5 pretraining script (#12355)

* fix_torch_device_generate_test

* remove @

* add length computatan

* finish masking

* finish

* upload

* fix some bugs

* finish

* fix dependency table

* correct tensorboard

* Apply suggestions from code review

* correct processing

* slight change init

* correct some more mistakes

* apply suggestions

* improve readme

* fix indent

* Apply suggestions from code review

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* correct tokenizer

* finish

* finish

* finish

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>","['examples/flax/language-modeling/run_mlm_flax.py', 'examples/flax/language-modeling/run_t5_mlm_flax.py', 'examples/flax/language-modeling/t5_tokenizer_model.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/t5/modeling_flax_t5.py']","Issues present in T5 pretraining script including bugs found during testing, incorrect tensorboard, processing errors, and issues with tokenizer. README and code readability also require improvements."
1791ef8df647a38b4fcb96c14ddd83a43861d713,1692272494,"Adds `TRANSFORMERS_TEST_DEVICE` (#25506)

* Adds `TRANSFORMERS_TEST_DEVICE`
Mirrors the same API in the diffusers library. Useful in transformers
too.

* replace backend checking with trying `torch.device`

* Adds better error message for unknown test devices

* `make style`

* adds documentation showing `TRANSFORMERS_TEST_DEVICE` usage.",['src/transformers/testing_utils.py'],"The Transformers library lacks a method to determine and handle different test devices, possibly leading to unexpected results or errors when running tests on various platforms."
fc95386ea12fc11942cc7f2a4f99ef9602d774ef,1670432739,"Add TFBartForSequenceClassification (#20570)

* read to load

* base functionality

* revert init

* fix dummy data

* moving right along

* moving right along

* finally

* cleanup

* pull out comment

* add test

* update docstring for main class

* flake comments and rewriting copies from make repo-consistency`

* remove irrelevant differences/accidental spaces

* put copies back after space removals

* mid

* final test pass

* stray comment

* update test file

* update test file

* fixup

* black

* missed

* black missed one more

* sytle

* add doc update

* fix order of output class

* comment

* Revert ""comment""

This reverts commit 03f86b6948808461939cc8ad4ad74305dfb67700.

* remove redundant function, and redundant reshape

* move change out of common

* style

* put common spaces back

* reorder kwargs in output

* doc style","['src/transformers/__init__.py', 'src/transformers/convert_pytorch_checkpoint_to_tf2.py', 'src/transformers/modeling_tf_outputs.py', 'src/transformers/modeling_tf_utils.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/bart/__init__.py', 'src/transformers/models/bart/modeling_tf_bart.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/models/bart/test_modeling_tf_bart.py']","TFBartForSequenceClassification model missing from the library, leading to issues in handling sequence classification tasks."
9ddf4f4f03608095224cd3354b62c6f7d0d4b009,1677353454,"Fix resume_from_checkpoint for deepspeed (#21735)

* Fix resume_from_checkpoint for deepspeed

Fix resume_from_checkpoint for deepspeed, by ensuring that the deepspeed engine is the one to load the checkpoint.

* Empty commit to trigger CI

* Removed deepspeed skipping 

Removed deepspeed skipping inside the _load_from_checkpoint function, as it is obsolete

* another adjustment

* Trigger CI

* trigger circleci

* style

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas@stason.org>","['src/transformers/deepspeed.py', 'src/transformers/trainer.py']","Deepspeed engine is not correctly loading from checkpoint, causing checkpoint resumption failures."
5b72b3412b8efb3177d2daf90798e2ee20cb21e9,1676038516,"Remove CLI spams with Whisper FeatureExtractor (#21267)

* Remove CLI spams with Whisper FeatureExtractor

Whisper feature extractor representation includes the MEL filters, a list of list that is represented as ~16,000 lines. This needlessly spams the command line. I added a `__repr__` method that replaces this list with a string ""<array of shape (80, 201)>""

* Remove mel_filters from to_dict output  

Credits to @ArthurZucker

* remove unused import

* update feature extraction tests for the changes in to_dict","['src/transformers/models/whisper/feature_extraction_whisper.py', 'tests/models/whisper/test_feature_extraction_whisper.py']","Whisper feature extractor causes excessive output on the command line due to a large array being displayed, complicating its usability."
1609a436eca115853b5a4cfd80b9ec2302bb9fcc,1686784227,"Add MMS CTC Fine-Tuning (#24281)

* Add mms ctc fine tuning

* make style

* More fixes that are needed

* make fix-copies

* make draft for README

* add new file

* move to new file

* make style

* make style

* add quick test

* make style

* make style","['examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py', 'examples/pytorch/test_pytorch_examples.py', 'src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/sew/modeling_sew.py', 'src/transformers/models/sew_d/modeling_sew_d.py', 'src/transformers/models/unispeech/modeling_unispeech.py', 'src/transformers/models/unispeech_sat/modeling_unispeech_sat.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'src/transformers/models/wavlm/modeling_wavlm.py']","Lack of MMS CTC Fine-Tuning capability and a quick test for it in the current implementation, causing usability and testing issues."
a23ac36f8cc091f737a7e3536828293fb7893901,1691474957,"[DOCS] Add descriptive docstring to MinNewTokensLength (#25196)

* Add descriptive docstring to MinNewTokensLength

It addresses https://github.com/huggingface/transformers/issues/24783

* Refine the differences between `min_length` and `min_new_tokens`

* Remove extra line

* Remove extra arguments in generate

* Add a missing space

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Run the linter

* Add clarification comments

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",['src/transformers/generation/logits_process.py'],"The function `MinNewTokensLength` lacks a descriptive docstring causing confusion about its purpose and how it differs from `min_length` and `min_new_tokens`. Also, there are redundant arguments in `generate`."
3d764fe86074661614a14a84607bcba43daa94a2,1684328408,"Return early once stop token is found. (#23421)

Previously even after finding a stop token, other stop tokens were considered, which is unnecessary and slows down processing.

Currently, this unnecessary overhead is negligible since there are usually 2 stop tokens considered and they are fairly short, but in future it may become more expensive.",['src/transformers/tools/agents.py'],"After finding a stop token in the system, the process continues to consider other stop tokens, causing unnecessary delays in processing speed."
57b980a613ea76e8d215035d693c38524b421705,1641925173,"Fix saving FlaubertTokenizer configs (#14991)

All specific tokenizer config properties must be passed to its base
class (XLMTokenizer) in order to be saved. This was not the case for
do_lowercase config. Thus it was not saved by save_pretrained() method
and saving and reloading the tokenizer changed its behaviour.

This commit fixes it.",['src/transformers/models/flaubert/tokenization_flaubert.py'],Saving and reloading FlaubertTokenizer alters its behavior due to the absence of 'do_lowercase' config property in the base class (XLMTokenizer).
9f8bfe703c4e8b88f462b23ff4968b1385bd955b,1649850546,"Fix #16660 (tokenizers setters of ids of special tokens) (#16661)

* Fix setters of *_token_id properties of SpecialTokensMixin

* Test setters of common tokens ids

* Move to a separate test checks of setters of tokens ids

* Add independent test for ByT5

* Add Canine test

* Test speech to text","['src/transformers/tokenization_utils_base.py', 'tests/byt5/test_tokenization_byt5.py', 'tests/canine/test_tokenization_canine.py', 'tests/test_tokenization_common.py']","Setters for *_token_id properties in SpecialTokensMixin are failing, affecting the assignment of special token IDs. Special token ID issue also extends to ByT5 and Canine units."
4cbc797b27c461be3c9c2ae3d95ec47fb898e562,1664986333,"Change `BloomConfig` docstring (#19336)

* change `BloomConfig` docstring

- slightly change the docstring of the `BloomConfig`
- Use correct default vocab size
- Use correct default `hidden_dim`, `n_head`

* Update src/transformers/models/bloom/configuration_bloom.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/bloom/configuration_bloom.py

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* make style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>",['src/transformers/models/bloom/configuration_bloom.py'],"The `BloomConfig` docstring is currently inaccurate, including incorrect default values for vocab size and `hidden_dim`, `n_head`."
1c191efc3abc391072ff0094a8108459bc08e3fa,1631162577,"flax ner example (#13365)

* flax ner example

* added task to README

* updated readme

* 1. ArgumentParser -> HfArgumentParser
2. step-wise logging,eval and save

* added requirements.txt

* added progress bar

* updated README

* added check_min_version

* updated training data permuattion with JAX

* added metric lib to requirements

* updated readme table

* fixed imports",['examples/flax/token-classification/run_flax_ner.py'],"Readme updates, missing requirements and version checks, incorrect import statements, and inadequate logging, evaluation, and saving step-wise in the flax ner example. Also, issue with incorrect training data permutation using JAX."
f3d0866ed999ba3fefef9c9c100c485eb9080f42,1627434100,"Correct validation_split_percentage argument from int (ex:5) to float (0.05) (#12897)

* Fixed train_test_split test_size argument

* `Seq2SeqTrainer` set max_length and num_beams only when non None  (#12899)

* set max_length and num_beams only when non None

* fix instance variables

* fix code style

* [FLAX] Minor fixes in CLM example (#12914)

* readme: fix retrieval of vocab size for flax clm example

* examples: fix flax clm example when using training/evaluation files

* Fix module path for symbolic_trace example

Co-authored-by: cchen-dialpad <47165889+cchen-dialpad@users.noreply.github.com>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>","['examples/tensorflow/language-modeling/run_clm.py', 'examples/tensorflow/language-modeling/run_mlm.py']","The current setup incorrectly interprets the validation_split_percentage argument as int values instead of float, causing errors during training-test splits. Furthermore, max_length and num_beams only work when non-None, leading to potential issues with Seq2SeqTrainer.
"
26ec7928d0077aaf4084a36ee05a253195497f51,1668531523,"Slightly alter Keras dummy loss (#20232)

* Slightly alter Keras dummy loss

* Slightly alter Keras dummy loss

* Add sample weight to test_keras_fit

* Fix test_keras_fit for datasets

* Skip the sample_weight stuff for models where the model tester has no batch_size","['src/transformers/modeling_tf_utils.py', 'tests/test_modeling_tf_common.py']","Issues with Keras dummy loss in models where batch size isn't relatively defined. Test_keras_fit is affected too, especially with handling of the sample weight for datasets."
2b096508852344c1d361f36e251b1b74253120fd,1658823092,"Add ViltForTokenClassification e.g. for Named-Entity-Recognition (NER) (#17924)

* Add ViltForTokenClassification e.g. for Named-Entity-Recognition (NER)

* Add ViltForTokenClassification e.g. for Named-Entity-Recognition (NER)

* provide classifier only text hidden states

* add test_for_token_classification

* Update src/transformers/models/vilt/modeling_vilt.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/vilt/modeling_vilt.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/vilt/modeling_vilt.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/vilt/modeling_vilt.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* add test_for_token_classification

Co-authored-by: gfuchs <gfuchs@ebay.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/vilt/__init__.py', 'src/transformers/models/vilt/modeling_vilt.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/vilt/test_modeling_vilt.py', 'utils/check_repo.py']","ViltForTokenClassification for Named-Entity-Recognition (NER) is missing, impeding token classification processes and tests."
d481b6414d5c07e7ae76ac19e32e1305d3b85b5f,1647623736,"Make Flax pt-flax equivalence test more aggressive (#15841)

* Make test_equivalence_pt_to_flax more aggressive

* Make test_equivalence_flax_to_pt more aggressive

* don't use to_tuple

* clean-up

* fix missing test cases + testing on GPU

* fix conversion

* fix `ValueError: assignment destination is read-only`

* Add type checking

* commit to revert later

* Fix

* fix

* fix device

* better naming

* clean-up

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",['tests/test_modeling_flax_common.py'],"Flax to PyTorch equivalence tests are not comprehensive enough and are failing on various cases including GPU testing, device fixing, missing cases, and conversion issues."
5e2f2d7dd2b72a35fe9e2fe5b55e13674e9a74a2,1658392541,"Better messaging and fix for incorrect shape when collating data. (#18119)

* More informative error message

* raise dynamic error

* remove_excess_nesting application

* incorrect shape assertion for collator & function to remove excess nesting from DatasetDict

* formatting

* eliminating datasets import

* removed and relocated remove_excess_nesting to the datasets library and updated docs accordingly

* independent assert instructions

* inform user of excess nesting",['src/transformers/tokenization_utils_base.py'],"Collating data results in an incorrect shape and lack of informative error message, with issues also noticed in removing excess nesting from the DatasetDict."
4d10de55b4a2a35031cd876b9e3f894b3f96ef1e,1682107986,"Feature to convert videomae huge and small finetuned on kinetics and ssv2 added to the videomae to pytorch converter (#22788)

* Feature to convert videomae huge finetuned kinetics and videomae small finetuned kinetics and ssv2 added to videomae to pytorch converter

* Reformat convert_videomae_to_pytorch using black

* Value exception added for the possible videomae model architectures",['src/transformers/models/videomae/convert_videomae_to_pytorch.py'],The existing videomae to Pytorch converter lacks the capability to convert highly specific model architectures like videomae huge finetuned kinetics and videomae small finetuned kinetics and ssv2.
002a078aff3630e706456b5b615de083eb12c883,1632160761,"Dynamically load model code from the Hub (#13467)

* Dynamic model

* Use defensive flag

* Style

* Doc and arg rename

* Arg rename

* Add tests

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>","['src/transformers/file_utils.py', 'src/transformers/models/auto/auto_factory.py', 'src/transformers/models/auto/dynamic.py', 'tests/test_modeling_auto.py', 'tests/test_modeling_common.py']","Unable to dynamically load models from the Hub, leading to inflexible and inefficient code execution."
fc9e387dc0a61a5742579ba7aed6a75b4fb09efa,1689162309,"Replacement of 20 asserts with exceptions (#24757)

* initial replacements of asserts with errors/exceptions

* replace assert with exception in generation, align and bart

* reset formatting change

* reset another formatting issue

* Apply suggestion

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* don't touch this file

* change to 'is not False'

* fix type

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['examples/flax/language-modeling/run_bart_dlm_flax.py', 'examples/flax/language-modeling/run_clm_flax.py', 'src/transformers/benchmark/benchmark_args_utils.py', 'src/transformers/benchmark/benchmark_tf.py', 'src/transformers/benchmark/benchmark_utils.py', 'src/transformers/data/metrics/__init__.py', 'src/transformers/generation/beam_search.py', 'src/transformers/models/align/convert_align_tf_to_hf.py', 'src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py']","Asserts are currently in use in various places instead of proper exceptions and errors, which could lead to confusion or less informative error handling."
aac4c7996837b77abe91cd1ea734b6ef74117156,1689101704,"Fix non-deterministic Megatron-LM checkpoint name (#24674)

Fix non-deterministic checkpoint name

`os.listdir`'s order is not deterministic, which is a problem when
querying the first listed file as in the code (`os.listdir(...)[0]`).

This can return a checkpoint name such as `distrib_optim.pt`, which does
not include desired information such as the saved arguments originally
given to Megatron-LM.",['src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py'],"`os.listdir` function provides non-deterministic order leading to incorrect first file selection during checkpointing in Megatron-LM, resulting in files with missing key information."
dd523da577f3d1471d570f0bc388af55d026ce95,1666099209,"Add table transformer [v2] (#19614)

* First draft

* Add conversion script

* Make conversion work

* Upload checkpoints

* Add final fixes

* Revert changes of conditional and deformable detr

* Fix toctree, add and remove copied from

* Use model type

* Improve docs

* Improve code example

* Update copies

* Add copied formt

* Don't update conditional detr

* Don't update deformable detr","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/table_transformer/__init__.py', 'src/transformers/models/table_transformer/configuration_table_transformer.py', 'src/transformers/models/table_transformer/convert_table_transformer_original_pytorch_checkpoint_to_pytorch.py', 'src/transformers/models/table_transformer/modeling_table_transformer.py', 'src/transformers/utils/dummy_timm_and_vision_objects.py', 'tests/models/table_transformer/test_modeling_table_transformer.py', 'utils/check_repo.py']","Inability to transform tables in the current setup, with no existing model type or documentation for the same."
923c35b5c5c2657bf46041fb5064c36cd93c42aa,1647261092,"Make TF pt-tf equivalence test more aggressive (#15839)

* Make TF pt-tf equivalence test more aggressive

* Fix for TFConvNextModelTest and TFTransfoXLModelTest

* fix kwargs for outputs

* clean-up

* Add docstring for check_outputs()

* remove: need to rename encoder-decoder

* clean-up

* send PyTorch things to the correct device

* Add back the accidentally removed test case in test_pt_tf_model_equivalence()

* Fix: change to tuple before calling check_outputs()

* Fix: tfo could be a list

* use to_tuple()

* allow tfo only to be tuple or tensor

* allow tfo to be list or tuple for now + style change

* minor fix

* remove np.copy and update comments

* tfo -> tf_output, same for pt

* Add more detailed comment

* remove the incorrect comment

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>",['tests/test_modeling_tf_common.py'],"The current TensorFlow to PyTorch model equivalence tests lack aggressiveness and encounter issues, particularly in the TFConvNextModelTest and TFTransfoXLModelTest."
5b7dcc73427d16218488846a365d10866dca9c3e,1646765141,"Seed _get_train_sampler's generator with arg seed to improve reproducibility (#15961)

* Seed get_train_sampler's generator with arg seed to improve reproducibility

and make the world_size<=1 code path more similar to the others

* move test file into trainer test explicitly

* dumb typo

* make style lint happy

* per discussion, switch to data_seed

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/trainer.py', 'src/transformers/training_args.py', 'tests/trainer/test_trainer.py']",_get_train_sampler's generator lacks an arg seed resulting in inconsistent and non-repeatable outputs during training.
c5bd732ac6d262a0825751634cafbc08971842b0,1638767938,"Add Flax example tests (#14599)

* add test for glue

* add tests for clm

* fix clm test

* add summrization tests

* more tests

* fix few tests

* add test for t5 mlm

* fix t5 mlm test

* fix tests for multi device

* cleanup

* ci job

* fix metric file name

* make t5 more robust","['examples/flax/language-modeling/run_clm_flax.py', 'examples/flax/language-modeling/run_mlm_flax.py', 'examples/flax/language-modeling/run_t5_mlm_flax.py', 'examples/flax/question-answering/run_qa.py', 'examples/flax/summarization/run_summarization_flax.py', 'examples/flax/test_examples.py', 'examples/flax/text-classification/run_flax_glue.py', 'examples/flax/token-classification/run_flax_ner.py', 'src/transformers/testing_utils.py']","Flax example lacks sufficient tests, causing issues with glue, clm, summarization, t5 mlm, multi device functionality, and metric file name."
26dd041c6e45379141302e2d293ab4cd9cf805d4,1671204241,"Add Swin2SR (#19784)

* First draft

* Add more improvements

* Improve forward pass

* Fix layernorm

* Add upscaler

* More improvements

* More improvements

* More improvements

* Improve conversion script

* Add preprocessing

* Make output match original implementation

* Add additional attributes

* Add support for more models

* Support more models

* Add support for real world sr

* Add initial Swin2SRFeatureExtractor

* Add ImageSuperResolutionOutput

* Make more tests pass

* Use BaseModelOutput

* Fix one more test

* Fix more tests

* Fix another test

* Fix all tests

* Rename to Swin2SRImageProcessor

* Fix toctree

* Fix toctree

* Fix rebase

* Improve Swin2SRImageProcessor

* Remove feature extractor file

* Improve model

* Improve conversion script

* Fix integration test

* Fix init

* Fix conversion script

* Address comments

* Improve upsampler

* Add NearestConvUpsampler

* Improve pixel shuffle upsampler

* Improve auxiliary upsampler

* Improve conversion script

* Rename conv_last to final_convolution

* Fix rebase

* Improve upsample module

* Add padding to image processor

* Fix bug

* Update padding

* Remove print statement and fix integration test

* Improve docs

* Add image processor tests

* Convert all checkpoints, fix testsé

* Remove print statements

* Fix import

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/modeling_outputs.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/image_processing_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/swin2sr/__init__.py', 'src/transformers/models/swin2sr/configuration_swin2sr.py', 'src/transformers/models/swin2sr/convert_swin2sr_original_to_pytorch.py', 'src/transformers/models/swin2sr/image_processing_swin2sr.py', 'src/transformers/models/swin2sr/modeling_swin2sr.py', 'src/transformers/models/swinv2/modeling_swinv2.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/models/swin2sr/test_image_processing_swin2sr.py', 'tests/models/swin2sr/test_modeling_swin2sr.py', 'utils/check_repo.py']","There is a need for the implementation of Swin2SR (Swin Transformer for Image Super-Resolution) with continuous improvements, fixes, and enhanced compatibility with real world sr, different models, and integration tests. The implementation requires new elements, optimizations, and bug fixes, ensuring effective super-resolution output."
49e44b216b2559e34e945d5dcdbbe2238859e29b,1660762627,"Update feature extractor methods to enable type cast before normalize  (#18499)

* Update methods to optionally rescale
This is necessary to allow for casting our images / videos to numpy arrays within the feature extractors' call. We want to do this to make sure the behaviour is as expected when flags like  are False. If some transformations aren't applied, then the output type can't be unexpected e.g. a list of PIL images instead of numpy arrays.

* Cast images to numpy arrays in call to enable consistent behaviour with different configs

* Remove accidental clip changes

* Update tests to reflect the scaling logic
We write a generic  function to handle rescaling of our arrays. In order for the API to be intuitive, we take some factor c and rescale the image values by that. This means, the rescaling done in normalize and to_numpy_array are now done with array * (1/255) instead of array / 255. This leads to small differences in the resulting image. When testing, this was in the order of 1e-8, and so deemed OK","['src/transformers/image_utils.py', 'tests/utils/test_image_utils.py']","Inconsistent behaviors observed in feature extraction methods with various configurations, resulting in unexpected output types such as lists of PIL images instead of numpy arrays."
b33ab4eb59f3baa0108d494695bc09b4688960a7,1649178327,"Add global_attention_mask to gen_kwargs (#16485)

If global_attention_mask is found in the models inputs (used by certain
models, like LED) in the prediction_step method of Seq2SeqTrainer,
it is added to the gen_kwargs, which are passed to model.decode().
This allows us to properly set the global attention when decoding.",['src/transformers/trainer_seq2seq.py'],"The Seq2SeqTrainer's prediction_step method doesn't consider the 'global_attention_mask' in the model's inputs for certain models like LED, resulting in erroneous decoding due to improperly set global attention."
83eda6435e7c842e55b42a529e9bf367bf2a126b,1683827542,"Better check for packages availability (#23163)

* Better check for packages availability

* amend _optimumneuron_available

* amend torch_version

* amend PIL detection and lint

* lint

* amend _faiss_available

* remove overloaded signatures of _is_package_available

* fix sklearn and decord detection

* remove unused checks

* revert","['src/transformers/file_utils.py', 'src/transformers/onnx/config.py', 'src/transformers/onnx/convert.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/fx.py', 'src/transformers/utils/import_utils.py', 'tests/onnx/test_onnx_v2.py']","Inadequate system to check for the availability of different packages, leading to false positives/negatives in detection."
865da84abb8d150cb33a913104186a071ea55073,1673978672,"Add Epsilon- and Eta-Sampling (#21121)

* Add epsilon- and eta-sampling.

Add epsilon- and eta-sampling, following the official code from https://github.com/john-hewitt/truncation-sampling and adapting to be more configurable, as required by Huggingface transformers.

* Add unit tests for epsilon- and eta-sampling.

* Black: fix code formatting.

* Fix docstring spacing.

* Clean up newlines.

* Fix implementation bugs and their associated tests.

* Remove epsilon- and eta-sampling parameters from PretrainedConfig.

* Clarify and clean up the documentation.

* Remove parameters for PretrainedConfig test.","['src/transformers/generation/__init__.py', 'src/transformers/generation/configuration_utils.py', 'src/transformers/generation/logits_process.py', 'src/transformers/generation/utils.py', 'tests/generation/test_logits_process.py']",Huggingface transformers lack epsilon- and eta-sampling making it less configurable. The current unit tests do not cover this area and there are discrepancies in documentation and code formatting.
587a19c72595a0f60e71497f87e34b98240658c9,1682087033,"fix: GPTNeoX half inference error (#22888)

* fix: half inference error

norm_factor is still torch.float32 after using model.half

So I changed it to register_buffer so I can change it to torch.float16 after using model.half

* fix: Added a variable ""persistent=False""

* run make style",['src/transformers/models/gpt_neox/modeling_gpt_neox.py'],"After applying model.half on GPTNeoX, norm_factor still remains as torch.float32 causing inference errors."
660e0b97bd652bd3a0dfd5f847e5cf62502d0469,1662750062,"Fix train_step, test_step and tests for CLIP (#18684)

* Fix train_step and test_step, correctly enable CLIP fit test

* Stop using get_args on older Python versions

* Don't use get_origin either

* UnionType is actually even newer, don't use that either

* Apply the same fix to test_loss_computation

* Just realized I was accidentally skipping a bunch of tests!

* Fix test_loss_computation for models without separable labels

* Fix scalar losses in test_step and train_step

* Stop committing your breakpoints

* Fix Swin loss shape

* Fix Tapas loss shape

* Shape fixes for TAPAS, DeIT, HuBERT and ViTMAE

* Add loss computation to TFMobileBertForPreTraining

* make fixup and move copied from statement

* make fixup and move copied from statement

* Correct copied from

* Add labels and next_sentence_label inputs to TFMobileBERT

* Make sure total_loss is always defined

* Update tests/test_modeling_tf_common.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Fix copied from

* Ensure CTC models get labels in tests

* Ensure CTC models get labels in tests

* Fix tests for vit_mae

* Fix tests for vit_mae

* Fix tests for vit_mae

* Reduce batch size for wav2vec2 testing because it was causing OOM

* Skip some TAPAS tests that are failing

* Skip a failing HuBERT test

* make style

* Fix mobilebertforpretraining test

* Skip Wav2Vec2 tests that use huge amounts of mem

* Skip keras_fit for Wav2Vec2 as well

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/modeling_tf_utils.py', 'src/transformers/models/clip/modeling_tf_clip.py', 'src/transformers/models/deit/modeling_tf_deit.py', 'src/transformers/models/hubert/modeling_tf_hubert.py', 'src/transformers/models/mobilebert/modeling_tf_mobilebert.py', 'src/transformers/models/swin/modeling_tf_swin.py', 'src/transformers/models/tapas/modeling_tf_tapas.py', 'src/transformers/models/vit_mae/modeling_tf_vit_mae.py', 'tests/models/hubert/test_modeling_tf_hubert.py', 'tests/models/mobilebert/test_modeling_tf_mobilebert.py', 'tests/models/tapas/test_modeling_tf_tapas.py', 'tests/models/wav2vec2/test_modeling_tf_wav2vec2.py', 'tests/test_modeling_tf_common.py']","Issues with train_step, test_step, and several tests for CLIP, causing incorrect loss computation, memory overflow, and tests to unintentionally skip, fail or give incorrect results."
cd9274d0107079cb4ba5a8d00bba2fcd8236c220,1651569979,"[FlaxBert] Add ForCausalLM (#16995)

* [FlaxBert] Add ForCausalLM

* make style

* fix output attentions

* Add RobertaForCausalLM

* remove comment

* fix fx-to-pt model loading

* remove comment

* add modeling tests

* add enc-dec model tests

* add big_bird

* add electra

* make style

* make repo-consitency

* add to docs

* remove roberta test

* quality

* amend cookiecutter

* fix attention_mask bug in flax bert model tester

* tighten pt-fx thresholds to 1e-5

* add 'copied from' statements

* amend 'copied from' statements

* amend 'copied from' statements

* quality","['src/transformers/__init__.py', 'src/transformers/modeling_flax_outputs.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/bert/__init__.py', 'src/transformers/models/bert/modeling_flax_bert.py', 'src/transformers/models/big_bird/__init__.py', 'src/transformers/models/big_bird/modeling_flax_big_bird.py', 'src/transformers/models/electra/__init__.py', 'src/transformers/models/electra/modeling_flax_electra.py', 'src/transformers/models/roberta/__init__.py', 'src/transformers/models/roberta/modeling_flax_roberta.py', 'src/transformers/utils/dummy_flax_objects.py', 'templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py', 'tests/bert/test_modeling_flax_bert.py', 'tests/big_bird/test_modeling_flax_big_bird.py', 'tests/electra/test_modeling_flax_electra.py', 'tests/encoder_decoder/test_modeling_flax_encoder_decoder.py', 'tests/roberta/test_modeling_flax_roberta.py', 'tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py', 'utils/check_repo.py']","The current FlaxBert model lacks implementation for ForCausalLM, causing issues with attention mask management and pt-fx thresholds in the Flax BERT model tester."
3042c63a9521d7d1c23a3127f78878f7288e18a5,1682586222,"Add methods to PreTrainedModel to use PyTorch's BetterTransformer (#21259)

* fix mess

* better documentation

* typo

* fix doc

* update

* add test

* fix test

* more tests

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* move to utils

* Apply suggestions from code review

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>

* nit

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>","['src/transformers/modeling_utils.py', 'src/transformers/testing_utils.py', 'src/transformers/utils/__init__.py', 'tests/bettertransformer/test_integration.py']","'PreTrainedModel lacks methods that allow it to use PyTorch's BetterTransformer, resulting in problems with transform modeling and source code updates.'"
dbc16f4404eca4a75459683d5135f6accea35a02,1693333464,"Support loading base64 images in pipelines (#25633)

* support loading base64 images

* add test

* mention in docs

* remove the logging

* sort imports

* update error message

* Update tests/utils/test_image_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* restructure to catch base64 exception

* doesn't like the newline

* download files

* format

* optimize imports

* guess it needs a space?

* support loading base64 images

* add test

* remove the logging

* sort imports

* restructure to catch base64 exception

* doesn't like the newline

* download files

* optimize imports

* guess it needs a space?

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/image_utils.py', 'tests/utils/test_image_utils.py']",Pipelines currently do not support loading base64 images leading to functionality limitations.
a021f2b90c56f7b410cea45b701529e5ab2e4533,1652096743,"Add type hints for BigBirdPegasus and Data2VecText PyTorch models (#17123)

* Add type hints for remaining BigBirdPegasus models

Here I added type hints to the BigBirdPegasusForCausalLM class.

* Add missing type hints for Data2VecText models

Added type hints to the Data2VecTextForCausalLM, Data2VecTextForMaskedLM,
Data2VecTextForMultipleChoice, Data2VecTextForQuestionAnswering,
Data2VecTextForSequenceClassification, and
Data2VecTextForTokenClassification classes.","['src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py', 'src/transformers/models/data2vec/modeling_data2vec_text.py']","BigBirdPegasus and Data2VecText PyTorch models classes are missing type hints, which may cause ambiguity in inputs and outputs."
e789418ebe9a32ce217c9e7996dc3ab6b1195dae,1650544750,"Adding support for `array` key in raw dictionnaries in ASR pipeline. (#16827)

* Adding support for `array` key in raw dictionnaries in ASR pipeline.

* ES .

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Making it work by not popping `array` first.

* Black 22.3

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/pipelines/automatic_speech_recognition.py'],"The ASR pipeline does not currently support the 'array' key in raw dictionaries, resulting in compatibility issues."
ee18d4d2a994fe8c70773f0d7a65f71d1e5b1370,1648582525,"TF GPT2: clearer model variable naming with @unpack_inputs (#16311)

* add unpack_inputs decorator to Main Layer

* add unpack_inputs decorator to Model

* add unpack_inputs decorator to LMHead Model

* add unpack_inputs decorator to Double Head Model

* add unpack_inputs decorator to Sequence Classification Model

* run fixup recipe

* make unpack_inputs the first decorator",['src/transformers/models/gpt2/modeling_tf_gpt2.py'],"Confusing variable naming conventions in the Main Layer, Model, LMHead Model, Double Head Model & Sequence Classification Model, have made the code unmanageable and difficult to understand."
70b0d4e193ea3d15effebc7cda534b6b9454abef,1659966788,"Fix compatibility with 1.12 (#17925)

* Fix compatibility with 1.12

* Remove pin from examples requirements

* Update torch scatter version

* Fix compatibility with 1.12

* Remove pin from examples requirements

* Update torch scatter version

* fix torch.onnx.symbolic_opset12 import

* Reject bad version

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['setup.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/models/deberta/modeling_deberta.py', 'src/transformers/models/deberta_v2/modeling_deberta_v2.py', 'src/transformers/models/sew_d/modeling_sew_d.py']",Incompatibility issue with 1.12 that includes errors with outdated torch scatter version and incorrect torch.onnx.symbolic_opset12 import.
576cd45a57501856bf719f2eb6186325fb6d2f88,1695401635,"Add image to image pipeline (#25393)

* Add image to image pipeline

Add image to image pipeline

* remove swin2sr from tf auto

* make ImageToImage importable

* make style

make style

make style

make style

* remove tf support

* remove nonused imports

* fix postprocessing

* add important comments; add unit tests

* add documentation

* remove support for TF

* make fixup

* fix typehint Image.Image

* fix documentation code

* address review request; fix unittest type checking

* address review request; fix unittest type checking

* make fixup

* address reviews

* Update src/transformers/pipelines/image_to_image.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* enhance docs

* make style

* make style

* improve docetest time

* improve docetest time

* Update tests/pipelines/test_pipelines_image_to_image.py

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

* Update tests/pipelines/test_pipelines_image_to_image.py

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

* make fixup

* undo faulty merge

* undo faulty merge

* add image-to-image to test pipeline mixin

* Update src/transformers/pipelines/image_to_image.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update tests/pipelines/test_pipelines_image_to_image.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* improve docs

---------

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/image_to_image.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/pipelines/test_pipelines_image_to_image.py', 'tests/test_pipeline_mixin.py', 'utils/update_metadata.py']","The image pipeline lacks an image component, leading to issues with TensorFlow support, and needs improvements in style, postprocessing, comments, documentation, type checking in unittests and docetest time."
b3ff7c680cf195d33013483cc98f7156de8bf287,1660326652,"[fsmt] deal with -100 indices in decoder ids (#18592)

* [fsmt] deal with -100 indices in decoder ids

Fixes: https://github.com/huggingface/transformers/issues/17945

decoder ids get the default index -100, which breaks the model - like t5 and many other models add a fix to replace -100 with the correct pad index. 

For some reason this use case hasn't been used with this model until recently - so this issue was there since the beginning it seems.

Any suggestions to how to add a simple test here? or perhaps we have something similar already? user's script is quite massive.

* style",['src/transformers/models/fsmt/modeling_fsmt.py'],"The default index of -100 in decoder ids is causing a break in the FSMT model, and this problem appears to be present since the model's creation."
626a0a01471accc32ded29ccca3ed93c4995fcd6,1625556717,"[RoFormer] Fix some issues (#12397)

* add RoFormerTokenizerFast into AutoTokenizer

* fix typo in roformer docs

* make onnx export happy

* update RoFormerConfig embedding_size

* use jieba not rjieba

* fix 12244 and make test_alignement passed

* update ARCHIVE_MAP

* make style & quality & fixup

* update

* make style & quality & fixup

* make style quality fixup

* update

* suggestion from LysandreJik

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* make style

* use rjieba

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>","['src/transformers/file_utils.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/roformer/configuration_roformer.py', 'src/transformers/models/roformer/modeling_roformer.py', 'src/transformers/models/roformer/modeling_tf_roformer.py', 'src/transformers/models/roformer/tokenization_roformer.py', 'src/transformers/models/roformer/tokenization_roformer_fast.py', 'src/transformers/models/roformer/tokenization_utils.py', 'src/transformers/testing_utils.py', 'tests/test_tokenization_roformer.py']","RoFormer's tokenizer, documentation, and onnx exporting have errors. Issues with RoFormerConfig embedding_size and incorrect usage of 'jieba' instead of 'rjieba'. Further, test alignment and ARCHIVE_MAP need fixes."
62d71f4083acccca1c1c9b0eea68db69d9ef759a,1686923023,"Fix functional TF Whisper and modernize tests (#24301)

* Revert whisper change and modify the test_compile_tf_model test

* make fixup

* Tweak test slightly

* Add functional model saving to test

* Ensure TF can infer shapes for data2vec

* Add override for efficientformer

* Mark test as slow","['src/transformers/models/data2vec/modeling_tf_data2vec_vision.py', 'src/transformers/models/whisper/modeling_tf_whisper.py', 'tests/models/data2vec/test_modeling_tf_data2vec_vision.py', 'tests/models/efficientformer/test_modeling_tf_efficientformer.py', 'tests/models/funnel/test_modeling_tf_funnel.py', 'tests/models/lxmert/test_modeling_tf_lxmert.py', 'tests/models/marian/test_modeling_tf_marian.py', 'tests/models/mbart/test_modeling_tf_mbart.py', 'tests/models/mobilevit/test_modeling_tf_mobilevit.py', 'tests/models/pegasus/test_modeling_tf_pegasus.py', 'tests/models/segformer/test_modeling_tf_segformer.py', 'tests/models/vit_mae/test_modeling_tf_vit_mae.py', 'tests/test_modeling_tf_common.py']","Functional TensorFlow model, Whisper, has issues in saving and inferring shapes. Also, the test_compile_tf_model requires modification."
8e5a1b2abb319c0d6e23f4f9c86c9064ac5aae89,1677776923,"Make schedulers picklable by making lr_lambda fns global (#21768)

* Make schedulers picklable by making lr_lambda fns global

* add unused _get_constant_schedule_lr_lambda arg

* remove unneeded _get_constant_schedule_lr_lamda

* add test

* make style

* rebase, remove torch dep, put lambda back

* repo-consistency and style","['src/transformers/optimization.py', 'tests/optimization/test_optimization.py']","Current Schedulers are not picklable, causing issues in distributed settings due to non-global lr_lambda functions."
0c55d47cded0c711a4359afb04d73ffb86b7d913,1647935473,"Add GLPN (#16199)

* First draft

* Fix logits calculation

* Improve tests

* Add copied from statements

* Fix base_model_prefix

* Improve implementation, upload new models

* Update design

* Fix integration test

* Add model to README and toctree

* Add document image

* Apply suggestions from code review

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add decoder_hidden_size attribute

* Update design of decoder

* Add DepthEstimatorOutput class

* Rename in_index to head_in_index and add feature extractor tests

* Apply suggestions from code review

* Apply suggestions from code review

* Update pretrained model name and add to doc tests

* Remove test.py script

* Update copied from statements and clean up

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/modeling_outputs.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/glpn/__init__.py', 'src/transformers/models/glpn/configuration_glpn.py', 'src/transformers/models/glpn/convert_glpn_to_pytorch.py', 'src/transformers/models/glpn/feature_extraction_glpn.py', 'src/transformers/models/glpn/modeling_glpn.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/glpn/test_feature_extraction_glpn.py', 'tests/glpn/test_modeling_glpn.py', 'utils/check_repo.py']",The current implementation lacks GLPN model leading to incorrect logits calculation and inadequate testing of the feature extractor.
6c8f4c9a938a09749ea1b19a5fa2a8dd27e99a29,1656442307,"Adding GroupViT Models (#17313)

* add group vit and fixed test (except slow)

* passing slow test

* addressed some comments

* fixed test

* fixed style

* fixed copy

* fixed segmentation output

* fixed test

* fixed relative path

* fixed copy

* add ignore non auto configured

* fixed docstring, add doc

* fixed copies

* Apply suggestions from code review

merge suggestions

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* resolve comment, renaming model

* delete unused attr

* use fix copies

* resolve comments

* fixed attn

* remove unused vars

* refactor tests

* resolve final comments

* add demo notebook

* fixed inconsitent default

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* rename stage->stages

* Create single GroupViTEncoderLayer class

* Update conversion script

* Simplify conversion script

* Remove cross-attention class in favor of GroupViTAttention

* Convert other model as well, add processor to conversion script

* addressing final comment

* fixed args

* Update src/transformers/models/groupvit/modeling_groupvit.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/groupvit/__init__.py', 'src/transformers/models/groupvit/configuration_groupvit.py', 'src/transformers/models/groupvit/convert_groupvit_nvlab_to_hf.py', 'src/transformers/models/groupvit/modeling_groupvit.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/groupvit/test_modeling_groupvit.py', 'utils/check_config_docstrings.py', 'utils/check_repo.py']","Problems occur when trying to add new GroupViT models including test failures, styling issues, documentation errors, code refactoring needs, naming conflicts, and unused variables."
713eab45d3dff1199b823d10b0bc833d835e91e2,1666098039,":rotating_light: :rotating_light: :rotating_light: [Breaking change] Deformable DETR intermediate representations (#19678)

* [Breaking change] Deformable DETR intermediate representations

- Fixes naturally the `object-detection` pipeline.
- Moves from `[n_decoders, batch_size, ...]` to `[batch_size,
  n_decoders, ...]` instead.

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/deformable_detr/modeling_deformable_detr.py', 'tests/pipelines/test_pipelines_object_detection.py']","The 'object-detection' pipeline is not functioning as intended due to issues in Deformable DETR intermediate representations, which currently use a `[n_decoders, batch_size, ...]` structure."
a4562552eb5efa8a12c61a3a7ebfd687dc72ee19,1659961556,"[DX fix] Fixing QA pipeline streaming a dataset. (#18516)

* [DX fix] Fixing QA pipeline streaming a dataset.

QuestionAnsweringArgumentHandler would iterate over the whole dataset
effectively killing all properties of the pipeline.
This restores nice properties when using `Dataset` or `Generator` since
those are meant to be consumed lazily.

* Handling TF better.","['src/transformers/pipelines/question_answering.py', 'tests/pipelines/test_pipelines_question_answering.py']","`QuestionAnsweringArgumentHandler` in QA pipeline is iterating over the entire dataset, negating the lazy consumption properties of `Dataset` or `Generator`."
55bb4c06f7be141c6d895dbe1f11018dc8580b2d,1624632915,"Fix exception in prediction loop occurring for certain batch sizes (#12350)

* fix distributed_concat for scalar outputs

* Update README.md

* fixed typo (#12356)

* simplify fix with terser syntax

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Trigger CI

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: michal pitr <21157924+MichalPitr@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/trainer_pt_utils.py'],"Exception is thrown in prediction loop when running with specific batch sizes, seemingly related to a distributed_concat function for scalar outputs."
3fefee99108de855f5659679c9d034a3be5ad0f4,1642622666,"Make chuking smartly (long files) work on asr ctc_with_lm. (#15219)

* [WIP] Make chuking smartly (long files) work on asr ctc_with_lm.

* Slow test with functionality.

* Fixing regular test.

* fix for batch size 1

* Handling batch outside `rescale_Stride`.

- Renamed to `rescale_stride`.

* Disable equality in the test.

* Remove print.

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/pipelines/automatic_speech_recognition.py', 'tests/test_pipelines_automatic_speech_recognition.py']","ASR ctc_with_lm fails to handle long files efficiently during chunking, and there are issues with handling batch sizes of 1. A test equality problem exists as well."
ffecfea9495d4aa788e1c05d0612a40bc4b460fc,1630338025,"Correct wrong function signatures on the docs website (#13198)

* Correct outdated function signatures on website.

* Upgrade sphinx to 3.5.4 (latest 3.x)

* Test

* Test

* Test

* Test

* Test

* Test

* Revert unnecessary changes.

* Change sphinx version to 3.5.4""

* Test python 3.7.11","['setup.py', 'src/transformers/dependency_versions_table.py']",The documentation on the website contains outdated function signatures and the sphinx version used is obsolete.
d6b8e9cec7301ba02f642588a6f12e78ec3b9798,1652828863,"Add trajectory transformer (#17141)

* Add trajectory transformer


Fix model init


Fix end of lines for .mdx files

Add trajectory transformer model to toctree

Add forward input docs

Fix docs, remove prints, simplify prediction test

Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Update docs, more descriptive comments

Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Update readme

Small comment update and add conversion script

Rebase and reformat

Fix copies

Fix rebase, remove duplicates

Fix rebase, remove duplicates

* Remove tapex

* Remove tapex

* Remove tapex","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/trajectory_transformer/__init__.py', 'src/transformers/models/trajectory_transformer/configuration_trajectory_transformer.py', 'src/transformers/models/trajectory_transformer/convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py', 'src/transformers/models/trajectory_transformer/modeling_trajectory_transformer.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/trajectory_transformer/test_modeling_trajectory_transformer.py']",The existing system lacks a trajectory transformer model and has issues with model initialization and documentation. There are unwanted print statements along with complex prediction tests. Duplication of code also exists.
250b478a2cb5977286da95a6b93a3a4827135c30,1647879063,"GPT2 TensorFlow Type Hints (#16261)

* Add typing hints for base model class

* Add typing hints for causal LM model class

* Add typing hints for double heads model class

* Add typing hints for sequence classification model class

* Add typing hints for Main Layer

* Run fixup",['src/transformers/models/gpt2/modeling_tf_gpt2.py'],"Absence of typing hints in various model classes like base model, causal LM, double heads, sequence classification and Main Layer in GPT2 TensorFlow implementation."
12240925cfa29fff932e49927eb9744713ab1018,1687924532,"Add bitsandbytes support for gpt2 models (#24504)

* Add bitsandbytes support for gpt2 models

* Guard Conv1D import to pass tensorflow test

* Appease ruff linter

* Fix 4bit test and remove int8 test boilerplate

* Update tests/bnb/test_mixed_int8.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>","['src/transformers/utils/bitsandbytes.py', 'tests/bnb/test_4bit.py', 'tests/bnb/test_mixed_int8.py']","GPT-2 models lack support for bitsandbytes, causing inconsistencies with Conv1D import and failure with tensorflow tests."
fd6a0ade9b89c415ea213ef1aa07c9b2c32a4d75,1697479013,"🚨🚨🚨 [`Quantization`] Store the original dtype in the config as a private attribute 🚨🚨🚨 (#26761)

* First step

* fix

* add adjustements for gptq

* change to `_pre_quantization_dtype`

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fix serialization

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>","['src/transformers/configuration_utils.py', 'src/transformers/modeling_utils.py', 'tests/quantization/bnb/test_4bit.py', 'tests/quantization/bnb/test_mixed_int8.py', 'tests/quantization/gptq/test_gptq.py']","Quantization functionality does not retain information about the original data type before execution, causing issues with serialization and adjustments for gptq."
a26d71d6aed8c8dae31c8ec645a1325d2acb4d6d,1665154383,"Export TensorFlow models to ONNX with dynamic input shapes (#19255)

* validate onnx models with a different input geometry than saved with

* only test working features for now

* simpler test skipping

* rm TODO

* expose batch_size/seq_length on vit

* skip certain name, feature, framework parameterizations known to fail validation

* Trigger CI

* Trigger CI","['src/transformers/models/clip/configuration_clip.py', 'src/transformers/models/groupvit/configuration_groupvit.py', 'src/transformers/models/owlvit/configuration_owlvit.py', 'src/transformers/onnx/convert.py', 'tests/onnx/test_onnx_v2.py']","When trying to export TensorFlow models to ONNX, dynamic input shapes are not correctly handled, causing issues with validation when input geometry differs from saved model."
21a772426dee10003fb0111abec514c9dcefda35,1655488775,"Migrate HFDeepSpeedConfig from trfrs to accelerate (#17623)

* Migrate HFDeepSpeedConfig from trfrs to accelerate

* add `accelerate` to testing dep

* addressing comments

* addressing comments

Using `_shared_state` and avoiding object creation. This is necessary as `notebook_launcher` in `launcers.py` checks `len(AcceleratorState._shared_state)>0` to throw an error.

* resolving comments

1. Use simple API from accelerate to manage the deepspeed config integration
2. Update the related documentation

* reverting changes and addressing comments

* docstring correction

* addressing nits

* addressing nits

* addressing nits 3

* bumping up the accelerate version to 0.10.0

* resolving import

* update setup.py to include deepspeed dependencies

* Update dependency_versions_table.py

* fixing imports

* reverting changes to CI dependencies for ""run_tests_pipelines_tf*"" tests

These changes didn't help with resolving the failures and I believe this needs to be addressed in another PR.

* removing `accelerate` as hard dependency

Resolves issues related to CI Tests

* adding `accelerate` as dependency for building docs

resolves failure in Build PR Documentation test

* adding `accelerate` as dependency in ""dev"" to resolve doc build issue

* resolving comments

1. adding `accelerate` to extras[""all""]
2. Including check for accelerate too before import HFDeepSpeedConfig from there

Co-Authored-By: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* resolving comments

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['setup.py', 'src/transformers/deepspeed.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/training_args.py']","The HFDeepSpeedConfig, currently located in trfrs, causes an error with the notebook_launcher in launchers.py due to unexpected object creations, and there are issues with CI tests and documentation build due to 'accelerate' dependency management."
f85acb4d73a84fe9bee5279068b0430fc391fb36,1662466346,"Fix decode_input_ids to bare T5Model and improve doc (#18791)

* use tokenizer to output tensor

* add preprocessing for decoder_input_ids for bare T5Model

* add preprocessing to tf and flax

* linting

* linting

* Update src/transformers/models/t5/modeling_flax_t5.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/t5/modeling_tf_t5.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/t5/modeling_t5.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/models/t5/modeling_flax_t5.py', 'src/transformers/models/t5/modeling_t5.py', 'src/transformers/models/t5/modeling_tf_t5.py']","Decoder_input_ids for bare T5Model aren't preprocessed, causing inaccurate results. Documentations related to the use of tokenizer and data preprocessing need improvements."
e3342edc4ef2f6046595ab2d5660640a313501fe,1646047356,"Flax Speech-Encoder-Decoder Model (#15613)

* rebase

* Delete shift tokens func

* downsample decoder input seq len for init

* correct attention mask

* add tests

* pt flax cross test

* make fixup

* init file for import

* change pt-flax cross test threshold

* pt-flax test logits only

* move tests

* make repo-consistency

* consistent indentation

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_flax_auto.py', 'src/transformers/models/speech_encoder_decoder/__init__.py', 'src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py', 'src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py', 'src/transformers/utils/dummy_flax_objects.py', 'tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py', 'utils/check_repo.py']","Issues with attention mask and improper initialization in the Flax Speech-Encoder-Decoder Model. In addition, test threshold inconsistencies for pt-flax cross test are present."
b69a62d579ded31d490e0c9a01bf9ecb81cb9b65,1659604083,"[BLOOM] Clean modeling code (#18344)

* Cleanup some code

* Improve signatures

* Try to reduce the number of reshape/copies

* I don't think we actually need the layer_num scaling trick

* No need for duplication

* Try to fix beam_search

* Fix beam search

* Removing layer num normalization seems to be breaking

* Not sure self.layer_number normalization actually matters

* Try and be backward compatible

* Try to fix beam_search

* Revert attempt to be backward compatible

* Improve documentation on past_key_values format

* Optimize the device allocation in case of hidden_states in multiple devices

* No need to manually cast the values to a specific device

* Rename with long version of variables

* Improve type hinting

* Add comment that explains that some methods return views

* Actually i think the attention casting only makes sense when we use torch.float16

* We don't actually need layer_number to be passed anymore

* Fix FX test

* Bypass torch.baddbmm

* Apply suggestions from code review

* Add comment about support for torchScript v1.11

* fix ONNX support for bloom (#18456)

Co-authored-by: Niklas Muennighoff <n.muennighoff@gmail.com>
Co-authored-by: Nouamane Tazi <nouamane98@gmail.com>","['src/transformers/models/bloom/configuration_bloom.py', 'src/transformers/models/bloom/modeling_bloom.py']","Code in Bloom's modeling has redundancy, inefficient use of reshaping/copies, uncertainty about the requirement of layer_num scaling, problems with beam_search, and needs optimization in device allocation for hidden_states across multiple devices."
8f36ab3e22c029018e020ce232a5f5310620b53f,1690311769,"[`T5`, `MT5`, `UMT5`] Add [T5, MT5, UMT5]ForSequenceClassification (#24726)

* Initial addition of t5forsequenceclassification

* Adding imports and adding tests

* Formatting

* Running make fix-copies

* Adding mt5forseq

* Formatting

* run make fix-copies

* Adding to docs

* Add model_parallel

* Fix bug

* Fix

* Remove TODO

* Fixing tests for T5ForSequenceClassification

* Undo changes to dependency_versions_table.py

* Change classification head to work with T5Config directly

* Change seq length to let tests pass

* PR comments for formatting

* Formatting

* Initial addition of UMT5ForSequenceClassification

* Adding to inits and formatting

* run make fix-copies

* Add doc for UMT5ForSeqClass

* Update UMT5 config

* Fix docs

* Skip torch fx test for SequenceClassification

* Formatting

* Add skip to UMT5 tests as well

* Fix umt5 tests

* Running make fix-copies

* PR comments

* Fix for change to sentence_representation

* Rename seq_len to hidden_size since that's what it is

* Use base_model to follow format of the rest of the library

* Update docs

* Extract the decoder_input_ids changes and make one liner

* Make one-liner","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/mt5/__init__.py', 'src/transformers/models/mt5/configuration_mt5.py', 'src/transformers/models/mt5/modeling_mt5.py', 'src/transformers/models/t5/__init__.py', 'src/transformers/models/t5/configuration_t5.py', 'src/transformers/models/t5/modeling_t5.py', 'src/transformers/models/umt5/__init__.py', 'src/transformers/models/umt5/configuration_umt5.py', 'src/transformers/models/umt5/modeling_umt5.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/t5/test_modeling_t5.py', 'tests/models/umt5/test_modeling_umt5.py']","T5, MT5, and UMT5 currently lack implementations for sequence classification, causing tests to fail and limiting comprehensiveness of these models."
3a8de58c5192b620228128430ea52e6eda81c40a,1633347429,"Add Mistral GPT-2 Stability Tweaks (#13573)

* Add layer-wise scaling

* Add reorder & upcasting argument

* Add OpenAI GPT-2 weight initialization scheme

* start `layer_idx` count at zero for consistency

* disentangle attn and reordered and upscaled attn function

* rename `scale_attn_by_layer` to `scale_attn_by_layer_id`

* make autocast from amp compatible with pytorch<1.6

* fix docstring

* style fixes

* Add fixes from PR feedback, style tweaks

* Fix doc whitespace

* Reformat

* First pass scale_attn_by_layer_idx and reorder_and_upcast_attn tests

* Rename scale_attn_by_layer_idx, add tip

* Remove extra newline

* add test for weight initialization

* update code format

* add assert check weights are fp32

* remove assert

* Fix incorrect merge

* Fix shape mismatch in baddbmm

* Add generation test for Mistral flags

Co-authored-by: leandro <leandro.vonwerra@spoud.io>
Co-authored-by: Keshav Santhanam <keshav2@stanford.edu>
Co-authored-by: J38 <jebolton@stanford.edu>","['src/transformers/models/gpt2/configuration_gpt2.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'tests/test_modeling_gpt2.py']","Inconsistencies in scaling, ordering and upcasting during Mistral GPT-2 model initialization causing potential accuracy degradation."
6192549c1fac4721361bbdf57a95b4c886c1acb8,1678294771,"[examples/speech-recognition] Add SpecAugment to run_speech_recognition_seq2seq.py (#21942)

* Add specaugment to run_speech_recognition_seq2seq.py

* Remove useless argument: text_column

* Fix quality

* Update return_attention_mask condition

* Update specaugment arguments only for whisper models

* Remove SpecAugment arguments from ModelArguments, only leave default values for simplicity

* Apply suggestions from code review

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* Update apply_spec_augment only for whisper models

* Apply suggestions from code review

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* Rename return_attention_mask to forward_attention_mask to avoid confusion with wav2vec2 models

---------

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>",['examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py'],"Speech recognition model doesn't support SpecAugment, causing possible sub-optimal results. Furthermore, confusion exists between return_attention_mask and forward_attention_mask in wav2vec2 models."
860d11ff7c4235e0baaeee50d96cf1686781bdd3,1686759893,"Fix Debertav2 embed_proj (#24205)

* MLM prediction head output size from embed_size

Take the output size of the dense projection layer from embedding_size instead of hidden_size since there could be a projection of the input embedding into hidden_size if they are different

* project TFDebertaV2 mlm output to embedding size

embedding size can be different that hidden_size, so the final layer needs to project back to embedding size. like in ELECTRA or DeBERTaV3 style pertaining.

This should solve an error that occurs when loading models like ""almanach/camemberta-base-generator"".

* fix the same issue for reshaping after projection

* fix layernorm size

* add self.embedding_size to scope

* fix embed_proj scope name

* apply the same changes to TF Deberta

* add the changes to deberta

* added self.embedding_size instead of config.embedding_size

* added the same change to debertav2

* added coppied from deberta to deberta2 model

* config.embedding_size fix

* black

* fix deberta config name","['src/transformers/models/deberta/modeling_deberta.py', 'src/transformers/models/deberta/modeling_tf_deberta.py', 'src/transformers/models/deberta_v2/modeling_deberta_v2.py', 'src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py']","Discrepancies between embedding_size and hidden_size in DebertaV2's MLM prediction head output and layer normalization results in incompatibility with models like ""almanach/camemberta-base-generator""."
961732c276241620e80a81ec28e95374963a166e,1638961674,"[Wav2Vec2] PyCTCDecode Integration to support language model boosted decoding (#14339)

* up

* up

* up

* make it cleaner

* correct

* make styhahalal

* add more tests

* finish

* small fix

* make style

* up

* tryout to solve cicrle ci

* up

* fix more tests

* fix more tests

* apply sylvains suggestions

* fix import

* correct docs

* add pyctcdecode only to speech tests

* fix more tests

* add tf, flax and pt tests

* add pt

* fix last tests

* fix more tests

* Apply suggestions from code review

* change lines

* Apply suggestions from code review

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* correct tests

* correct tests

* add doc string

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>","['setup.py', 'src/transformers/__init__.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/file_utils.py', 'src/transformers/models/wav2vec2/__init__.py', 'src/transformers/models/wav2vec2/processing_wav2vec2_with_lm.py', 'src/transformers/testing_utils.py', 'src/transformers/utils/dummy_pyctcdecode_objects.py', 'tests/test_modeling_flax_wav2vec2.py', 'tests/test_modeling_tf_wav2vec2.py', 'tests/test_modeling_wav2vec2.py', 'tests/test_processor_wav2vec2_with_lm.py']","Wav2Vec2 currently does not support language model boosted decoding, potentially impacting the accuracy of its transcriptions. The PyCTCDecode module could provide this functionality, but it's currently not integrated into the system."
f726d53ea30e300feff452f5d0312a408dc8301d,1675428075,"Remove more unused attributes in config classes (#21392)

* * Remove unused type_vocab_size

* Remove unused initializer_factor

* Remove unused n_embd

* Remove unused scale_embedding

* Remove unused scale_attn_weights

* fix

* fix

* Remove unused head_hidden_scale

* Remove unused activation_dropout

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/blip/configuration_blip.py', 'src/transformers/models/bridgetower/configuration_bridgetower.py', 'src/transformers/models/codegen/configuration_codegen.py', 'src/transformers/models/conditional_detr/configuration_conditional_detr.py', 'src/transformers/models/decision_transformer/configuration_decision_transformer.py', 'src/transformers/models/detr/configuration_detr.py', 'src/transformers/models/funnel/configuration_funnel.py', 'src/transformers/models/git/configuration_git.py', 'src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py', 'src/transformers/models/gptj/configuration_gptj.py', 'src/transformers/models/graphormer/configuration_graphormer.py', 'src/transformers/models/led/modeling_led.py', 'src/transformers/models/longformer/modeling_longformer.py', 'src/transformers/models/nezha/configuration_nezha.py', 'src/transformers/models/nezha/modeling_nezha.py', 'src/transformers/models/sew_d/configuration_sew_d.py', 'src/transformers/models/table_transformer/configuration_table_transformer.py', 'src/transformers/models/trajectory_transformer/configuration_trajectory_transformer.py']","Several attributes in config classes are not being utilized, cluttering the codebase and potentially causing misinterpretations or inefficiencies."
30ed3adf474aaf2972ab56f5624089bc24a6adf3,1688982643,"Add Multi Resolution Analysis (MRA) (New PR) (#24513)

* Add all files

* Update masked_language_modeling.md

* fix mlm models

* fix conflicts

* fix conflicts

* fix copies

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Reduce seq_len and hidden_size in ModelTester

* remove output_attentions

* fix conflicts

* remove copied from statements

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/kernels/mra/cuda_kernel.cu', 'src/transformers/kernels/mra/cuda_kernel.h', 'src/transformers/kernels/mra/cuda_launch.cu', 'src/transformers/kernels/mra/cuda_launch.h', 'src/transformers/kernels/mra/torch_extension.cpp', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/mra/__init__.py', 'src/transformers/models/mra/configuration_mra.py', 'src/transformers/models/mra/convert_mra_pytorch_to_pytorch.py', 'src/transformers/models/mra/modeling_mra.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/mra/test_modeling_mra.py']","The software lacks implementation for Multi Resolution Analysis (MRA), limiting its functionality for specific tasks."
2acedf47214d7a634c193846124832a4686cc8fd,1684485191,"feat: Whisper prompting (#22496)

* initial working additions

* clean and rename, add cond stripping initial prompt to decode

* cleanup, edit create_initial_prompt_ids, add tests

* repo consistency, flip order of conditional

* fix error, move the processor fn to the tokenizer

* repo consistency, update test ids to corresponding tokenizer

* use convert_tokens_to_ids not get_vocab...

* use actual conditional in generate

* make sytle

* initial address comments

* initial working add new params to pipeline

* first draft of sequential generation for condition_on_previous_text

* add/update tests, make compatible with timestamps

* make compatible with diff. input kwargs and max length

* add None check

* add temperature check

* flip temp check operand

* refocusing to prev pr scope

* remove the params too

* make style

* edits, move max length incorporating prompt to whisper

* address comments

* remove asr pipeline prompt decoding, fix indexing

* address comments (more tests, validate prompt)

* un-comment out tests (from debug)

* remove old comment

* address comments

* fix typo

* remove timestamp token from test

* make style

* cleanup

* copy method to fast tokenizer, set max_new_tokens for test

* prompt_ids type just pt

* address Amy's comments

* make style","['src/transformers/models/whisper/modeling_whisper.py', 'src/transformers/models/whisper/processing_whisper.py', 'src/transformers/models/whisper/tokenization_whisper.py', 'src/transformers/models/whisper/tokenization_whisper_fast.py', 'tests/models/whisper/test_modeling_whisper.py', 'tests/models/whisper/test_processor_whisper.py', 'tests/models/whisper/test_tokenization_whisper.py']","Issues with whisper prompting functionality due to incorrect handling of prompt decoding, sequential generation and temperature checks. This is causing inconsistencies and errors in different parameters and input scenarios."
ca51499248b986ebf3991848234ef2d8bc81a36a,1692114467,"Make training args fully immutable (#25435)

* Make training args fully immutable

* Working tests, PyTorch

* In test_trainer

* during testing

* Use proper dataclass way

* Fix test

* Another one

* Fix tf

* Lingering slow

* Exception

* Clean","['examples/pytorch/image-pretraining/run_mae.py', 'examples/pytorch/summarization/run_summarization.py', 'examples/research_projects/mlm_wwm/run_mlm_wwm.py', 'examples/tensorflow/language-modeling/run_clm.py', 'examples/tensorflow/language-modeling/run_mlm.py', 'src/transformers/training_args.py', 'tests/trainer/test_trainer.py', 'tests/trainer/test_trainer_distributed.py']",The training arguments mutable state is causing issues in different testing and training stages.
dce33f2150769825ca175df3209441122f85a814,1649708352,"Improve PT/TF equivalence test (#16557)

* add error message

* Use names in the error message

* allow ModelOutput

* rename to check_pt_tf_outputs and move outside

* fix style

* skip past_key_values in a better way

* Add comments

* improve code for label/loss

* make the logic clear by moving the ignore keys out

* fix _postprocessing_to_ignore

* fix _postprocessing_to_ignore: create new outputs from the remaining fields

* ignore past_key_values in TFGPT2 models for now

* make check_pt_tf_outputs better regarding names

* move check_pt_tf_models outside

* rename methods

* remove test_pt_tf_model_equivalence in TFCLIPModelTest

* Reduce TFViTMAEModelTest.test_pt_tf_model_equivalence

* move prepare_pt_inputs_from_tf_inputs outside check_pt_tf_models

* Fix quality

* Clean-up TFLxmertModelTester.test_pt_tf_model_equivalence

* Fix quality

* fix

* fix style

* Clean-up TFLEDModelTest.test_pt_tf_model_equivalence

* Fix quality

* add docstring

* improve comment

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['tests/clip/test_modeling_tf_clip.py', 'tests/led/test_modeling_tf_led.py', 'tests/lxmert/test_modeling_tf_lxmert.py', 'tests/test_modeling_tf_common.py', 'tests/vit_mae/test_modeling_tf_vit_mae.py']","Issues with PT/TF model equivalence tests resulting in unclear error messages, naming inconsistencies, incorrect handling of label/loss logic, and inefficiencies in model comparison for various models like TFGPT2, TFCLIP, TFViTMAE, TFLxmert and TFLED."
3ca18d6d09ee0d1610a400ead6f6041394f66421,1695825931,"[`PEFT`] Fix PEFT multi adapters support (#26407)

* fix PEFT multi adapters support

* refactor a bit

* save pretrained + BC + added tests

* Update src/transformers/integrations/peft.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add more tests

* add suggestion

* final changes

* adapt a bit

* fixup

* Update src/transformers/integrations/peft.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt from suggestions

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/integrations/peft.py', 'src/transformers/modeling_utils.py', 'tests/peft_integration/test_peft_integration.py']","PEFT multi adapters support is not functioning properly, with potential issues in saving pre-trained data and insufficient tests for this functionality."
cd4c5c90605b2e23879fcca484f7079b0fc0c361,1647350360,"TF XLA greedy generation (#15786)

* First attempt at TF XLA generation

* Fix comments

* Update XLA greedy generate with direct XLA calls

* Support attention mask, prepare_inputs_for_generation no longer hardcoded for greedy

* Handle position_ids correctly

* make xla generate work for non xla case

* force using xla generate

* refactor

* more fixes

* finish cleaning

* finish

* finish

* clean gpt2 tests

* add gpt2 tests

* correct more cases

* up

* finish

* finish

* more fixes

* flake 8 stuff

* final rag fix

* Update src/transformers/models/rag/modeling_tf_rag.py

* finish t5 as well

* finish

* Update src/transformers/generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>","['src/transformers/generation_tf_logits_process.py', 'src/transformers/generation_tf_utils.py', 'src/transformers/models/gpt2/modeling_tf_gpt2.py', 'src/transformers/models/rag/modeling_tf_rag.py', 'src/transformers/models/t5/modeling_tf_t5.py', 'tests/gpt2/test_modeling_gpt2.py', 'tests/gpt2/test_modeling_tf_gpt2.py', 'tests/t5/test_modeling_tf_t5.py']","The TF XLA generation doesn't handle position_ids correctly and has hardcoded settings for greedy prepare_inputs_for_generation, also, it fails in non-xla cases."
0c9c8472e65f630fb2a8f389ea5f875f699a7dbc,1676471096,"Add Ernie-M Model to huggingface (#21349)

* config and tokenization(fast too) changed and ErnieEncoder added

* Slow Tokenization Added

* Tokenizer(slow) is now working and Fast Tokenizer removed

* Added Config code

* Added Base Model and utils

* ErnieMModel is now working

* All added except tests

* All tests passed except ErnieUIEM

* All tests passed

* all fixes done

* all fixes done

* fixed MAP

* fixed check_code_quality

* fixed Build PR Documentation issue

* Added changes(comments) and also updated to the latest upstream/main

* Added fixup

* Added # Copied comments

* Added fixup

* Added more comments and some nits

* Added fixup

* Fixed README_hd.md

* Added more fixes

* ErnieMTokenizer (being sentencepiece) protected and other docs edited

* Added code_quality fix

* Fixed for

* Added more fix

* modified AZ

* ernie-m tokenization test added!

* attention mask part fixed(with 0->self.config.pad_token_id)

* applied make fixup","['src/transformers/__init__.py', 'src/transformers/models/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'src/transformers/models/ernie_m/__init__.py', 'src/transformers/models/ernie_m/configuration_ernie_m.py', 'src/transformers/models/ernie_m/modeling_ernie_m.py', 'src/transformers/models/ernie_m/tokenization_ernie_m.py', 'src/transformers/utils/dummy_pt_objects.py', 'src/transformers/utils/dummy_sentencepiece_objects.py', 'tests/models/ernie_m/test_modeling_ernie_m.py', 'tests/models/ernie_m/test_tokenization_ernie_m.py', 'utils/check_repo.py']","Ernie-M Model missing from the supported models in huggingface, associated configurations, tokenization (both fast and slow), and tests need to be added including addressing issues in Ernie UIEM tests."
9393f966bcebd112b25f84e46af6e349525406c7,1663832724,"[fix] Add DeformableDetrFeatureExtractor (#19140)

* Add DeformableDetrFeatureExtractor

* Fix post_process

* Fix name

* Add tests for feature extractor

* Fix doc tests

* Fix name

* Address comments

* Apply same fix to DETR and YOLOS as well

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/deformable_detr/__init__.py', 'src/transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py', 'src/transformers/models/deformable_detr/feature_extraction_deformable_detr.py', 'src/transformers/models/deformable_detr/modeling_deformable_detr.py', 'src/transformers/models/detr/feature_extraction_detr.py', 'src/transformers/models/yolos/feature_extraction_yolos.py', 'src/transformers/utils/dummy_vision_objects.py', 'tests/models/deformable_detr/test_feature_extraction_deformable_detr.py', 'tests/models/detr/test_feature_extraction_detr.py', 'tests/models/yolos/test_feature_extraction_yolos.py']",DeformableDetrFeatureExtractor not existing in DETR and YOLOS causing issues in post processing and failing tests due to incorrect documentation and improper naming convention.
fc6c8b0eaa8755bff60a48757d2e37e3a03a33dd,1683307369,"Add `no_trainer` scripts to pre-train Vision Transformers (#23156)

* Add run_mim_no_trainer.py draft from #20412

Add parse_args method and copy over other dependencies

Add Method call for sending telemetry

Initialize Accelerator

Make one log on every process

Set seed and Handle repository creation

Initialize dataset and Set validation split

Create Config

Adapt Config

Update Config

Create Feature Extractor

Create model

Set column names

Create transforms

Create mask generator

Create method to preprocess images

Shuffle datasets if needed and set transforms

Create Dataloaders

Add optimizer

Add learning rate scheduler

Prepare everything with our accelerator

Tie weights for TPU training

Recalculate training steps and training epochs

Set accelerator checkpointing steps

Initialize trackers and store configuration

Set total batch size

Fix typo: mlm -> mim

Log info at the start of training

Load in the weights and states from previous save

update the progress_bar if load from checkpoint

Define train loop

Add evaluation loop to training

Add to parse_args method

Push repo to hub

Save accelerator state

End training and save model and feature extractor

Remove unused imports

Fix trailing whitespace

* Update code based on comments, Rename feature_extractor to image_processor

* Fix linting

* Add argument for learning rate

* Add argument for setting number of training epochs

* Remove incorrect logger argument

* Convert max_train_steps to int for tqdm

---------

Co-authored-by: Saad Mahmud <shuvro.mahmud79@gmail.com>",['examples/pytorch/image-pretraining/run_mim_no_trainer.py'],"The code lacks a 'no_trainer' script to pre-train Vision Transformers, needing manual handling for numerous tasks like setting epochs, learning rates, creating models, setting transformations, optimizing, and more."
d45fc7da3d43ff29ca597f5ffa8cf3151d705013,1633992392,"[Speech Examples] Add pytorch speech pretraining (#13877)

* adapt wav2vec2

* add example

* add files

* adapt

* remove bogus file

* Apply suggestions from code review

* adapt files more

* upload changes

* del old files

* up

* up

* up

* up

* up

* correct gradient checkpoitning

* add readme

* finish

* finish

* up

* more fixes

* up

* up

* add demo run to readme

* up","['examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py', 'examples/pytorch/test_examples.py', 'src/transformers/models/hubert/modeling_hubert.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'tests/test_modeling_hubert.py', 'tests/test_modeling_wav2vec2.py']","Existing pretraining in the Pytorch speech lacks wav2vec2 support, causing gradient checkpointing issues and missing demo run information in the readme."
667b823b894700ec848c4e0bdcfc65630157ef82,1647452305,"Swin support for any input size (#15986)

* padding done

* correctly return one attention per layer

* almost correct, attentions are not flatten one tuple per stage

* tests green

* doc

* conversations

* reshaping hidden_states

* view in the test

* reshape_hidden_states in Encoder and Model

* new outputs with reshaped_hidden_states

* conversations

* doc

* Update docs/source/model_doc/swin.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* conversations

* fix tests

* minor changes

* resolved conversations

* attentions one per stage

* typo

* typos

* typos

* function signature

* CI

* clean up tests

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>","['src/transformers/models/swin/modeling_swin.py', 'tests/swin/test_modeling_swin.py']","Swin transformer model is unable to handle varying input sizes, causing issues with padding and attention layer outputs."
6d585fe0f0921b86c05723d40d76dce91ba51165,1643728438,"replace assert with exception for padding_side arg in `PreTrainedTokenizerBase` `__init__` (#15454)

* replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`

* add test

* fix kwargs

* reformat test

* format

* format

* fix typo to render the documentation","['src/transformers/tokenization_utils_base.py', 'tests/test_tokenization_common.py']","The `padding_side` argument assertion fails in `PreTrainedTokenizerBase` `__init__`, not clearly communicating the mistake in instantiation to the user."
ed31ab3f103e7b3b0b08658baa9b36174517fdd2,1648563546,"Adding DocTest to TrOCR (#16398)

* docstring still WIP | adding to documentation_tests

* clean version | passes tests

* adding to documentation_test

* adding forward for training pass

* make fixup applied

* address comments

* fix doctest

* apply make fixup

* remove additional blank

* fix file to have correct split for prepare_for_doc_test

* Update src/transformers/models/trocr/modeling_trocr.py

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>

* address comments

* changing text | adding loss check | make fixup

* make fixup

* Update src/transformers/models/trocr/modeling_trocr.py

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>

* Update src/transformers/models/trocr/modeling_trocr.py

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>

* Update src/transformers/models/trocr/modeling_trocr.py

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>

* make fixup

Co-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",['src/transformers/models/trocr/modeling_trocr.py'],"The TrOCR model lacks a DocTest, causing it not to be properly included in the automatic documentation tests."
3028b20a71d566f67d4ef414e4aa32cb3eb53d64,1679065675,"Fix natten (#22229)

* Add kernel size to NATTEN's QK arguments.

The new NATTEN 0.14.5 supports PyTorch 2.0, but also adds an additional
argument to the QK operation to allow optional RPBs.

This ends up failing NATTEN tests.

This commit adds NATTEN back to circleci and adds the arguments to get
it working again.

* Force NATTEN >= 0.14.5","['.circleci/create_circleci_config.py', 'setup.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/models/dinat/modeling_dinat.py', 'src/transformers/models/nat/modeling_nat.py']","NATTEN tests fail due to missing additional argument in QK operation for PyTorch 2.0 support, introduced in NATTEN version 0.14.5."
70e7d1d65627d64ecf71afce88cec3de83eb456a,1658918320,"Fixes torch jit tracing for LayoutLMv2 model (re-open) (#18313)

* Fixes torch jit tracing for LayoutLMv2 model.
Pytorch seems to reuse memory for input_shape which caused a mismatch in shapes later in the forward pass.

* Fixed code quality

* avoid unneeded allocation of vector for shape","['src/transformers/models/layoutlmv2/modeling_layoutlmv2.py', 'tests/models/layoutlmv2/test_modeling_layoutlmv2.py', 'tests/test_modeling_common.py']",Memory reuse for input_shape in Pytorch causes a mismatch in shapes during the forward pass for LayoutLMv2 model when torch jit tracing is used.
2f5507580be2f963afc6cd9c1d2340c81d90a2e9,1676048961,"[from_pretrained] extend `torch_dtype=""auto""` to look up `config.torch_dtype` first, expand docs (#21524)

* [from_pretrained] expand on torch_dtype entry

* fold 4 into 1

* style

* support torch_dtype='config' plus tests

* style

* oops

* fold config into auto, fix bug

* fix check

* better log

* better log

* clean up","['src/transformers/modeling_utils.py', 'src/transformers/models/auto/auto_factory.py', 'tests/test_modeling_common.py']","The ""auto"" option for torch_dtype in the function from_pretrained is not considering the dtype defined in the config, leading to potential type discrepancies.
"
78cda46f17548d8739c354a07b00b3f2996773c7,1681835816,"Generate: Add assisted generation (#22211)

* working mvp

* remove breakpoint

* fix commit

* standardize outputs

* tmp commit

* tests almost ready

* tmp commit

* skip a few models

* Add streaming; Docs and examples

* document limitations

* PR commits

* Amy PR comments","['src/transformers/generation/utils.py', 'tests/generation/test_utils.py', 'tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py', 'tests/models/whisper/test_modeling_tf_whisper.py', 'tests/models/whisper/test_modeling_whisper.py']","The current generation mechanism lacks support for assisted generation, leading to difficulties in standardizing outputs and testing. It also doesn't support streaming with clear documentation and examples."
56af8df35966780f413c0b407ef65af087a497cd,1665150415,"HF <-> megatron checkpoint reshaping and conversion for GPT (#19317)

* HF <-> megatron checkpoint conversion handling reshaping from different tensor and parallel sizes

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* addressing comments

* add doc strings and  🐛 fixes

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py'],Issues exist in reshaping and conversion of checkpoints between HF and Megatron for GPT due to differing tensor and parallel sizes.
6667b0d7bf0a135b9a85d2d21b7eb25ec8ad58cb,1661336876,"add warning to let the user know that the `__call__` method is faster than `encode` + `pad` for a fast tokenizer (#18693)

* add warning to let the user know that the  method is slower that  for a fast tokenizer

* user warnings

* fix layoutlmv2

* fix layout*

* change warnings into logger.warning","['src/transformers/tokenization_utils_base.py', 'tests/models/layoutlmv2/test_tokenization_layoutlmv2.py', 'tests/models/layoutlmv3/test_tokenization_layoutlmv3.py', 'tests/models/layoutxlm/test_tokenization_layoutxlm.py', 'tests/test_tokenization_common.py']","Usage of encode + pad methods in Fast Tokenizer is slower than using the __call__ method, but users are not aware of this potential performance impact."
04b2f13c37791204b02178392671d9dae52065be,1675975586,"🚨🚨🚨 Enforce single model initialization (#21431)

* Enforce single model initialization

* Add OneFormer example for problem 3

* Do it the Stas way

* Actually rename the uses...

* Rewrite test

* Try to change the test this way

* Fix all init slow/fast tests

* Break connection

* Fix more tests

* Fix test for initialization

* Remove custom test

* Quality

* Fix last failing tests

* The end?","['src/transformers/modeling_utils.py', 'src/transformers/models/altclip/modeling_altclip.py', 'src/transformers/models/bart/modeling_bart.py', 'src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py', 'src/transformers/models/fsmt/modeling_fsmt.py', 'src/transformers/models/led/modeling_led.py', 'src/transformers/models/maskformer/modeling_maskformer_swin.py', 'src/transformers/models/mbart/modeling_mbart.py', 'src/transformers/models/mvp/modeling_mvp.py', 'src/transformers/models/oneformer/modeling_oneformer.py', 'src/transformers/models/plbart/modeling_plbart.py', 'src/transformers/models/upernet/modeling_upernet.py', 'src/transformers/models/wav2vec2/modeling_wav2vec2.py', 'src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py', 'src/transformers/models/wavlm/modeling_wavlm.py', 'tests/models/bart/test_modeling_bart.py', 'tests/models/deta/test_modeling_deta.py', 'tests/models/dpt/test_modeling_dpt.py', 'tests/models/dpt/test_modeling_dpt_hybrid.py', 'tests/models/layoutlmv2/test_modeling_layoutlmv2.py', 'tests/models/prophetnet/test_modeling_prophetnet.py', 'tests/models/reformer/test_modeling_reformer.py', 'tests/models/vit_hybrid/test_modeling_vit_hybrid.py', 'tests/test_modeling_common.py']","Multiple initializations of the same model are currently allowed, risking data inconsistencies and performance issues."
3909d7f139affb79f811155ef53868996039a5fd,1659369990,"Add Flax BART pretraining script (#18297)

* add bart pretraining flax script

* fixup

* add bart pretraining flax script

* add BART to README

* add BART to README

* add BART to README

* add BART to README

* add BART to README

* add bos eos document

* Update README.md

* Update README.md

* Update examples/flax/language-modeling/run_bart_dlm_flax.py

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* final

* final

* final

* remove use_auth_token ing from_config

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>","['examples/flax/language-modeling/run_bart_dlm_flax.py', 'examples/flax/language-modeling/run_mlm_flax.py', 'examples/flax/language-modeling/run_t5_mlm_flax.py']",Flax BART model lacks a pretraining script and is not listed in the README; also issue with proper token configuration.
d24e84d9edde889bd56ccd46617208cb118d66fa,1668429558,"Pytorch type hints (#20112)

* initial commit

* Update modeling_whisper.py

* Fixing Tests

* modeling_vision_text_dual_encoder

* modeling_vision_encoder_decoder

* Update modeling_vit.py

* Update modeling_vit_msn.py

* Update modeling_trajectory_transformer.py

* style

* Update modeling_time_series_transformer.py

* Update modeling_time_series_transformer.py

* Update modeling_segformer.py

* Update modeling_plbart.py

* Update modeling_dpt.py

* Update modeling_deit.py

* Update modeling_dpt.py

* Update modeling_esm.py

* Update modeling_fnet.py

* Update modeling_fnet.py

* Update modeling_fnet.py

* Update modeling_flava.py

* Update modeling_flava.py

* Update modeling_layoutlmv3.py

* Update modeling_levit.py","['src/transformers/models/deit/modeling_deit.py', 'src/transformers/models/dpt/modeling_dpt.py', 'src/transformers/models/esm/modeling_esm.py', 'src/transformers/models/flava/modeling_flava.py', 'src/transformers/models/fnet/modeling_fnet.py', 'src/transformers/models/layoutlmv3/modeling_layoutlmv3.py', 'src/transformers/models/levit/modeling_levit.py', 'src/transformers/models/plbart/modeling_plbart.py', 'src/transformers/models/realm/modeling_realm.py', 'src/transformers/models/segformer/modeling_segformer.py', 'src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py', 'src/transformers/models/speech_to_text/modeling_speech_to_text.py', 'src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py', 'src/transformers/models/time_series_transformer/modeling_time_series_transformer.py', 'src/transformers/models/trajectory_transformer/modeling_trajectory_transformer.py', 'src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py', 'src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py', 'src/transformers/models/vit/modeling_vit.py', 'src/transformers/models/vit_msn/modeling_vit_msn.py', 'src/transformers/models/whisper/modeling_whisper.py']","Pytorch's type hints are missing or inconsistent in various modeling files, causing confusion and potential mismatches during development."
634242735367a4ae934776399adba0c8474adfca,1675179338,"Remove more unused attributes in config classes (#21327)

* remove unused classifier_dropout

* remove unused dropout

* remove unused pooler_fn

* remove unnecessary is_encoder_decoder

* remove unnecessary drop_rate

* remove unused classifier_dropout

* remove unused classifier_dropout

* remove unused dropout

* remove unused dropout

* remove unused summary_* attributes

* remove unused tie_word_embeddings

* remove unused summary_* attributes

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/altclip/configuration_altclip.py', 'src/transformers/models/biogpt/configuration_biogpt.py', 'src/transformers/models/blip/configuration_blip.py', 'src/transformers/models/bridgetower/configuration_bridgetower.py', 'src/transformers/models/chinese_clip/configuration_chinese_clip.py', 'src/transformers/models/clip/configuration_clip.py', 'src/transformers/models/clipseg/configuration_clipseg.py', 'src/transformers/models/conditional_detr/configuration_conditional_detr.py', 'src/transformers/models/ctrl/configuration_ctrl.py', 'src/transformers/models/decision_transformer/configuration_decision_transformer.py', 'src/transformers/models/detr/configuration_detr.py', 'src/transformers/models/esm/configuration_esm.py', 'src/transformers/models/git/configuration_git.py', 'src/transformers/models/gpt_neo/configuration_gpt_neo.py', 'src/transformers/models/layoutlm/configuration_layoutlm.py', 'src/transformers/models/longformer/configuration_longformer.py', 'src/transformers/models/owlvit/configuration_owlvit.py', 'src/transformers/models/pegasus_x/configuration_pegasus_x.py', 'src/transformers/models/speech_to_text/configuration_speech_to_text.py', 'src/transformers/models/table_transformer/configuration_table_transformer.py', 'src/transformers/models/trocr/configuration_trocr.py', 'src/transformers/models/whisper/configuration_whisper.py', 'src/transformers/models/x_clip/configuration_x_clip.py']","There are multiple unused and unnecessary attributes present in the Config classes, creating unnecessary clutter."
77713d11f6656314fb06c217cf43c4b8f5c64df8,1693303527,"[DINOv2] Add backbone class (#25520)

* First draft

* More improvements

* Fix all tests

* More improvements

* Add backbone test

* Improve docstring

* Address comments

* Rename attribute

* Remove expected output

* Update src/transformers/models/dinov2/modeling_dinov2.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Fix style

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/dinov2/__init__.py', 'src/transformers/models/dinov2/configuration_dinov2.py', 'src/transformers/models/dinov2/modeling_dinov2.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/dinov2/test_modeling_dinov2.py', 'utils/check_repo.py']","The DINOv2 model lacks a necessary backbone class, impacting functionality and testing."
edb704b26e79b6be03061ba45f340f0acd1f8ab1,1680641994,"Fix inverted conditional in TF common test! (#22540)

* Fix inverted conditional in TF common test!

* Make the same change in the PT tests file

* Make sure hidden states for GPT2 have the same output shape in PT/TF

* Minor fix to PT implementation of token classification loss

* Skip loss equivalence test for TFHubert because it keeps overflowing to inf

* Compute LM loss for TF the (weird) way it's computed in PT

* Skip loss equivalence test for Wav2Vec2 for the same reason as Hubert

* Fix - don't try to access the hidden states property when output is a tuple","['src/transformers/models/esm/modeling_esm.py', 'src/transformers/models/gpt2/modeling_tf_gpt2.py', 'src/transformers/models/xglm/modeling_tf_xglm.py', 'tests/models/hubert/test_modeling_tf_hubert.py', 'tests/models/wav2vec2/test_modeling_tf_wav2vec2.py', 'tests/test_modeling_common.py', 'tests/test_modeling_tf_common.py']","TensorFlow and PyTorch tests failing due to conditionals inverted, discrepancy in output shapes for hidden states across both frameworks, and loss equivalence tests overflowing to infinity."
03ae1f060bbb8cfd8ba691385b35a7ae09adcf33,1669214144,"change the way sentinel tokens can retrived (#20373)

* change the way sentinel tokens can retrived

* Fix line length for doc string

* Fix line length for doc string

* Add more stronger test for t5 tokenization

* Format file changes

* Make a stronger test for filtering sentinel tokens

* fix file format issues","['src/transformers/models/t5/tokenization_t5.py', 'src/transformers/models/t5/tokenization_t5_fast.py', 'tests/models/t5/test_tokenization_t5.py']","Retrieving sentinel tokens is not working as expected, and existing tests for T5 tokenization do not sufficiently cover potential filtering cases."
7ba1d4e51fbf86aa843fc15009c38c9b776919c6,1652876627,"Add type hints for ProphetNet (Pytorch) (#17223)

* added type hints to prophetnet

* reformatted with black

* fix bc black misformatted some parts

* fix imports

* fix imports

* Update src/transformers/models/prophetnet/configuration_prophetnet.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* update OPTIONAL type hint and docstring

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>","['src/transformers/models/prophetnet/configuration_prophetnet.py', 'src/transformers/models/prophetnet/modeling_prophetnet.py', 'src/transformers/models/prophetnet/tokenization_prophetnet.py']",ProphetNet in Pytorch lacks type hints leading to confusion and possible errors while using its functionalities.
7c5eaf9e5a4afae7a6949f653167238ad6ae90ee,1670428915,"Add `dpt-hybrid` support (#20645)

* add `dpt-hybrid` support

* refactor

* final changes, all tests pass

* final cleanups

* final changes

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* fix docstring

* fix typo

* change `vit_hybrid` to `hybrid`

* replace dataclass

* add docstring

* move dataclasses

* fix test

* add `PretrainedConfig` support for `backbone_config`

* fix docstring

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove `embedding_type` and replace it by `is_hybrid`

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/dpt/configuration_dpt.py', 'src/transformers/models/dpt/convert_dpt_hybrid_to_pytorch.py', 'src/transformers/models/dpt/modeling_dpt.py', 'tests/models/dpt/test_modeling_dpt.py', 'tests/models/dpt/test_modeling_dpt_hybrid.py', 'utils/tests_fetcher.py']",The existing application lacks `dpt-hybrid` support leading to limitations in functionality and incompatible `PretrainedConfig` support for `backbone_config`.
61e068e5a2bbfe610b4182ee572c8798a0bae247,1673007739,"Support turning off the model uploading in ClearML (#20969)

* Add support for turning off the model uploading in ClearML

* Add documentation for the CLEARML_LOG_MODEL environment variable

* Adjust new doc addition to the new style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Dudu Lasry <dudu.lasry@viz.ai>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>",['src/transformers/integrations.py'],"ClearML does not provide an option to disable model uploading, forcing all models to be uploaded regardless of user needs."
6dc884abc85c7f9b3b46f536a541d34d9f513d54,1669664029,"[Maskformer] Add MaskFormerSwin backbone (#20344)

* First draft

* Fix backwards compatibility

* More fixes

* More fixes

* Make backbone more general

* Improve backbone

* Improve test

* Fix config checkpoint

* Address comments

* Use model_type

* Address more comments

* Fix special model names

* Remove MaskFormerSwinModel and MaskFormerSwinPreTrainedModel from main init

* Fix typo

* Update backbone

* Apply suggestion

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>","['src/transformers/__init__.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/maskformer/__init__.py', 'src/transformers/models/maskformer/configuration_maskformer_swin.py', 'src/transformers/models/maskformer/modeling_maskformer.py', 'src/transformers/models/maskformer/modeling_maskformer_swin.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/maskformer/test_modeling_maskformer_swin.py', 'utils/check_copies.py', 'utils/check_repo.py']","The MaskFormerSwin backbone is missing from the framework, and there's an issue with the current special model names and their initialization."
06d488061f065f3a8e8f709bce43f3f00eb49052,1667485360,"[Whisper Tokenizer] Make more user-friendly (#19921)

* [Whisper Tokenizer] Make more user-friendly

* use property

* make indexing rigorous

* small clean-up

* tests

* skip seq2seq tests

* remove multilingual arg

* reorder args

* collapse to one function

Co-authored-by: ArthurZucker <arthur@huggingface.co>

* option to override attributes

Co-authored-by: ArthurZucker <arthur@huggingface.co>

* add to docs

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make comment more clear

Co-authored-by: sgugger <sylvain@huggingface.co>

* don't add special tokens in get_decoder_prompt_ids

* add test for set_prefix_tokens

Co-authored-by: ArthurZucker <arthur@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: sgugger <sylvain@huggingface.co>","['src/transformers/models/whisper/processing_whisper.py', 'src/transformers/models/whisper/tokenization_whisper.py', 'tests/models/whisper/test_tokenization_whisper.py']",Issues with user-friendliness in the Whisper Tokenizer; additional problem with rigorous indexing. Inconsistent behavior in adding special tokens in get_decoder_prompt_ids. Also set_prefix_tokens lack sufficient testing.
c1138273d48a06e4258975ae66d0e8bc2baed45c,1640633126,"Fix duplicate call to save_checkpoint when using deepspeed (#14946)

* Fix duplicate call to save_checkpoint when using deepspeed / stage3_gather_fp16_weights_on_model_save

* Revert ""Fix duplicate call to save_checkpoint when using deepspeed / stage3_gather_fp16_weights_on_model_save""

This reverts commit 6a3dec0397723a8417351dc38fdebf14ab17756c.

* Delete correct duplicate invocation of deepspeed save_checkpoint",['src/transformers/trainer.py'],Issue with duplicate invocations of 'save_checkpoint' function when using deepspeed results in unnecessary checkpointing.
382ba670ed2376a9454c3c841fae4819118ec4f5,1695185776,"FSDP tests and checkpointing fixes (#26180)

* add fsdp tests

* Update test_fsdp.py

* Update test_fsdp.py

* fixes

* checks

* Update trainer.py

* fix

* fixes for saving/resuming checkpoints

* fixes

* add tests and delete debug statements

* fixing tests

* Update test_fsdp.py

* fix tests

* fix tests

* minor nits

* fix code style and quality

* refactor and modularize test code

* reduce the time of tests

* reduce the test time

* fix test

* reduce test time

* reduce test time

* fix failing tests

* fix

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* resolve comments

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>","['src/transformers/modeling_utils.py', 'src/transformers/testing_utils.py', 'src/transformers/trainer.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/import_utils.py', 'tests/fsdp/test_fsdp.py']",Issues with FSDP tests leading to failures and problems with saving/resuming checkpoints in the trainer.
0a55d9f7376f72ad3ff296d4249840021b03bcc4,1694794921,"[PEFT] Allow PEFT model dict to be loaded (#25721)

* Allow PEFT model dict to be loaded

* make style

* make style

* Apply suggestions from code review

* address comments

* fixup

* final change

* added tests

* fix test

* better logic for handling if adapter has been loaded

* Update tests/peft_integration/test_peft_integration.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>","['src/transformers/integrations/peft.py', 'tests/peft_integration/test_peft_integration.py']","PEFT model dictionary currently cannot be loaded, which possibly impacts the functionality of the PEFT integration."
395e566a420f2466b32689686a631ecca7ac5c31,1689172705,"gpt-bigcode: avoid `zero_` to support Core ML (#24755)

gpt-bigcode: avoid `zeros_` to support Core ML.

In-place `zeros_` is not supported by the Core ML conversion process.
This PR replaces it with `zeros_like` so conversion can proceed.

The change only affects a workaround for a PyTorch bug on the `cpu`
device.",['src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py'],"The Core ML conversion process fails due to its incompatibility with the in-place `zeros_` function in PyTorch, specifically on the `cpu` device."
4edb3e49f6bd3d1a4f6862452ecaf07108d62ff7,1665589836,"Make `MobileBert` tokenizers independent from `Bert` (#19531)

* Make `MobileBert` tokenizers independent from `Bert`

* Update src/transformers/models/mobilebert/tokenization_mobilebert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fixed the name in the error message

* Update src/transformers/models/mobilebert/tokenization_mobilebert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Removed extra space from the ""copied"" comment

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['src/transformers/models/mobilebert/tokenization_mobilebert.py', 'src/transformers/models/mobilebert/tokenization_mobilebert_fast.py']",`MobileBert` tokenizer currently has dependence on `Bert` which affects its functionality and independence.
5fe06b9bdda55cb3ed891c6989414a1c5f26431b,1648822886,"Adding missing type hints for mBART model (PyTorch) (#16429)

* added type hints for mbart tensorflow tf implementation

* Adding missing type hints for mBART model 

Tensorflow Implementation model added with missing type hints

* Missing Type hints - correction

For TF model

* Code fixup using make quality tests

* Hint types - typo error

* make fix-copies and make fixup

* type hints

* updated files

* type hints update

* making dependent modesls coherent

Co-authored-by: matt <rocketknight1@gmail.com>","['src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py', 'src/transformers/models/blenderbot/modeling_blenderbot.py', 'src/transformers/models/m2m_100/modeling_m2m_100.py', 'src/transformers/models/mbart/modeling_mbart.py', 'src/transformers/models/pegasus/modeling_pegasus.py', 'src/transformers/models/xglm/modeling_xglm.py']",The mBART model implementation in PyTorch is missing type hints leading to ambiguous data type usage and compliance issues with other dependent models.
d5b8fe3b90bd6daee60c1e4c5f08075caba90707,1625160162,"Validation split added: custom data files @sgugger, @patil-suraj (#12407)

* Validation split added: custom data files

Validation split added in case of no validation file and loading custom data

* Updated documentation with custom file usage

Updated documentation with custom file usage

* Update README.md

* Update README.md

* Update README.md

* Made some suggested stylistic changes

* Used logger instead of print.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Made similar changes to add validation split

In case of a missing validation file, a validation split will be used now.

* max_train_samples to be used for training only

max_train_samples got misplaced, now corrected so that it is applied on training data only, not whole data.

* styled

* changed ordering

* Improved language of documentation

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Improved language of documentation

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fixed styling issue

* Update run_mlm.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['examples/tensorflow/language-modeling/run_clm.py', 'examples/tensorflow/language-modeling/run_mlm.py']","No validation split is present when loading custom data files with a missing validation file, and `max_train_samples` is being incorrectly applied to the entire dataset instead of just the training data."
7e22609e0f8ae63b5c223fe6633a01d0800adf00,1624905104,"Tensorflow LM examples (#12358)

* Tensorflow MLM example

* Add CLM example

* Style fixes, adding missing checkpoint code from the CLM example

* Fix TPU training, avoid massive dataset warnings

* Fix incorrect training length calculation for multi-GPU training

* Fix incorrect training length calculation for multi-GPU training

* Refactors and nitpicks from the review

* Style pass

* Adding README","['examples/tensorflow/language-modeling/run_clm.py', 'examples/tensorflow/language-modeling/run_mlm.py']","Multi-GPU training on Tensorflow has issues with incorrect training length calculation leading to dataset warnings and TPU training problems.
"
f16d29b337f31d101685f3f10e2b98bdbc42d777,1676584260,"Adapt PerceiverIO Multimodal class to work with arbitrary modalities (#20054)

* * Properly register parameters in PerceiverMultimodalPreprocessor
* Adapt PerceiverTextPreprocessor to work with PerceiverMultimodalPreprocessor
* Change a few type hints

* Fix formatting; incorrect return type

* Return embeddings_wo_pos

---------

Co-authored-by: Steven Anton <antonstv@amazon.com>",['src/transformers/models/perceiver/modeling_perceiver.py'],"PerceiverIO Multimodal class doesn't function with arbitrary modalities, leading to issues in registering parameters within PerceiverMultimodalPreprocessor and adapting PerceiverTextPreprocessor."
580dd87c55bf3c3b2387b89832ca5f724c5ec424,1647050033,"[Deepspeed] add support for bf16 mode (#14569)

* [WIP] add support for bf16 mode

* prep for bf16

* prep for bf16

* fix; zero2/bf16 is ok

* check bf16 is available

* test fixes

* enable zero3_bf16

* config files

* docs

* split stage_dtype; merge back to non-dtype-specific config file

* fix doc

* cleanup

* cleanup

* bfloat16 => bf16 to match the PR changes

* s/zero_gather_fp16_weights_on_model_save/zero_gather_16bit_weights_on_model_save/; s/save_fp16_model/save_16bit_model/

* test fixes/skipping

* move

* fix

* Update docs/source/main_classes/deepspeed.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* backticks

* cleanup

* cleanup

* cleanup

* new version

* add note about grad accum in bf16

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>","['setup.py', 'src/transformers/deepspeed.py', 'src/transformers/dependency_versions_table.py', 'src/transformers/trainer.py', 'tests/deepspeed/test_deepspeed.py', 'tests/deepspeed/test_model_zoo.py']","Deepspeed lacks support for bf16 mode, causing compatibility and optimization issues."
9088fcae82f4e23021e600966626188ce6fbe6df,1683748848,"Bring back the PR `Refactor doctests + add CI` to `main` (#23271)

* Revert ""Revert ""[Doctests] Refactor doctests + add CI"" (#23245)""

This reverts commit 69ee46243c40ea61f63d4b8f78d171ad27b4a046.

* try not expose HfDocTestParser

* move into testing_utils.py

* remove pytest install

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['.circleci/create_circleci_config.py', 'conftest.py', 'src/transformers/testing_utils.py', 'utils/prepare_for_doc_test.py']","Original refactor of doctests and addition to CI caused unintended exposure of HfDocTestParser, necessitating reversion of commits."
ce6d153a533632138be70ba983f009fd65053b35,1691154794,"Make `bark` could have tiny model (#25290)

* temp

* update

* update

* update

* small dim

* small dim

* small dim

* fix

* update

* fix

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>","['src/transformers/models/bark/configuration_bark.py', 'tests/models/bark/test_modeling_bark.py', 'utils/create_dummy_models.py']",The `bark` function is exhibiting issues when used with a model of a small dimension.
3223d49354e41dfa44649a9829c7b09013ad096e,1661409282,"Add ONNX support for Longformer (#17176)

* Implement ONNX support for Longformer

Fix repo consistency check complaints

Fix value mismatches

Add pooler output for default model

Increase validation atol to accommodate multiple-choice error

Fix copies

Fix chunking for longer sequence lengths

Add future comment

* Fix issue in mask_invalid_locations

* Remove torch imports in configuration_longformer

* Change config access to fix LED

* Push opset version to support tril

* Work in review comments (mostly style)

* Add Longformer to ONNX tests","['src/transformers/models/led/modeling_led.py', 'src/transformers/models/longformer/configuration_longformer.py', 'src/transformers/models/longformer/modeling_longformer.py', 'src/transformers/onnx/features.py', 'tests/onnx/test_onnx_v2.py']","The project lacks ONNX support for Longformer, causing issues with longer sequence lengths and mask invalid locations. Inconsistencies and value mismatches are observed in the repo."
2b0c92456877c0cc151dcfcd69a31e3ac9eaa336,1683033946,"GPT2ForQuestionAnswering (#23030)

* first draft - gives index error in question_answering.py

* maturing

* no labels

* pipeline should know about QA

* fixing checks

* formatting

* fixed docstring

* make sure legacy code executes

* comment

* like this

---------

Co-authored-by: Prof. Peter Schneider-Kamp <jps@ordbogen.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/gpt2/__init__.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/gpt2/test_modeling_gpt2.py']","Index error in question_answering.py when attempting to use GPT2ForQuestionAnswering, causing failures in QA pipeline and checks.
"
872e6be03dc84c9798a1b517893c2b152a9233b9,1630565156,"Update clip loss calculation (#13217)

* Update clip loss calculation

Hello, I'm the author of the blog you took the snippet from. I think this way of calculating is possibly slightly more accurate for calculation.

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>",['src/transformers/models/clip/modeling_clip.py'],"The current method of clip loss calculation in the system may be inaccurate, leading to potential discrepancies."
b48ac1a094e572d6076b46a9e4ed3e0ebe978afc,1653303335,"Fix CodeParrot training script (#17291)

* average loss over batches and accumulated steps for tracking

* fix layernorm weight decay

* use AdamW from Pytorch instead of Transformers

* add shuffling of sequences inside the batches

* add shuffling of sequences inside the batches

* add logging dir and reformat code

* fix lr tracking

* remove Mistral scaling

* keep Mistral scaling

* reformat code

* fix error

* fix error

* use shuffling function from Pytorch

* remove argument for shuffling batch sequences as it isn't optional

* update package versions and install accelerate from source

* remove unused package

* Update loss average over accumulated steps

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Update loss average over accumulated steps

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* use one shuffle buffer argument

* compute avg_loss in one line

Co-authored-by: Loubna ben allal <loubnabenallal@gmail.com>
Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>","['examples/research_projects/codeparrot/scripts/arguments.py', 'examples/research_projects/codeparrot/scripts/codeparrot_training.py', 'examples/research_projects/codeparrot/scripts/initialize_model.py']","Issues with CodeParrot training script including incorrect loss tracking, inconsistent weight decay in layernorm, incorrect use of AdamW, lack of sequence shuffling in batches, issues with learning rate tracking, and outdated package versions. Incorrect handling of batch sequence shuffling arguments."
6b09328368324d170504c14bcd202856d0f851a3,1646939985,"Fix duplicate arguments passed to dummy inputs in ONNX export (#16045)

* Fix duplicate arguments passed to dummy inputs in ONNX export

* Fix M2M100 ONNX config

* Ensure we check PreTrained model only if torch is available

* Remove TensorFlow tests for models without PyTorch parity","['src/transformers/models/m2m_100/configuration_m2m_100.py', 'src/transformers/onnx/convert.py', 'tests/onnx/test_onnx_v2.py']","Duplicate arguments are being passed to dummy inputs during ONNX export, leading to incorrect configurations."
83b38fbea8c2a6fbd9af0d9561db3afde2f4f4e2,1683209715,"GPTNeoXForQuestionAnswering (#23059)

* first draft - gives index error in question_answering.py

* maturing

* no labels

* pipeline should know about QA

* fixing checks

* formatting

* fixed docstring

* initial commit

* formatting

* adding the class to many places

* towards less unhappy checks

* nearly there

* and gpt neox for qa

* use right model

* forgot this one

* base_model_prefix is ""gpt_neox"" for GPTNeoX* models

* unnecessary stuff

* Update src/transformers/models/gpt_neox/modeling_gpt_neox.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* format

* Update src/transformers/models/gpt_neox/modeling_gpt_neox.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* removed gpt2 stuff

---------

Co-authored-by: Prof. Peter Schneider-Kamp <jps@ordbogen.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_auto.py', 'src/transformers/models/gpt2/modeling_gpt2.py', 'src/transformers/models/gpt_neox/__init__.py', 'src/transformers/models/gpt_neox/modeling_gpt_neox.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/models/gpt_neox/test_modeling_gpt_neox.py']",Index error occurring in `question_answering.py` when using GPTNeoX for question answering.
5de2a6d5e51a455c50bf047fa727f41dcf54613d,1684774665,"Fix wav2vec2 is_batched check to include 2-D numpy arrays (#23223)

* Fix wav2vec2 is_batched check to include 2-D numpy arrays

* address comment

* Add tests

* oops

* oops

* Switch to np array

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* Switch to np array

* condition merge

* Specify mono channel only in comment

* oops, add other comment too

* make style

* Switch list check from falsiness to empty

---------

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>","['src/transformers/feature_extraction_sequence_utils.py', 'src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py', 'src/transformers/models/wav2vec2/tokenization_wav2vec2.py', 'tests/models/wav2vec2/test_feature_extraction_wav2vec2.py', 'tests/models/wav2vec2/test_tokenization_wav2vec2.py']","'in wav2vec2, batched checks don't appropriately handle 2-D numpy arrays leading to potential discrepancies in data handling.'"
25e651a2de64236c62f07eeb933c2ec33ac65e0d,1660670757,"Update run_translation_no_trainer.py (#18637)

* Update run_translation_no_trainer.py

found an error in selecting `no_decay` parameters and some small modifications when the user continues to train from a checkpoint

* fixs `no_decay` and `resume_step` issue

1. change `no_decay` list
2. if use continue to train their model from provided checkpoint, the `resume_step` will not be initialized properly if `args.gradient_accumulation_steps != 1`","['examples/pytorch/language-modeling/run_clm_no_trainer.py', 'examples/pytorch/language-modeling/run_mlm_no_trainer.py', 'examples/pytorch/translation/run_translation_no_trainer.py']",The 'no_decay' parameters are incorrectly selected in run_translation_no_trainer.py and there's an issue with 'resume_step' initialization when users continue to train a model from a checkpoint with non-1 gradient accumulation steps.
9e71d4645526911f2ea9743aa4cf8e9d479fc840,1645633226,"Enable `image-segmentation` on `AutoModelForSemanticSegmentation` (#15647)

* Enabling Beit SegFormer to `image-segmentation`.

* Fixing the score.

* Fix import ?

* Missing in type hint.

* Multiple test fixes:

- Add `raw_image` support. It should be the default IMHO since in Python
  world it doesn't make any sense to base64 encode the image (Sorry
  @mishig, didn't catch that in my review). I really think we should
  consider breaking BC here.
- Add support for Segformer tiny test (needed
  `SegformerModelTester.get_config` to enable TinyConfig
  @NielsRogge)
- Add the check that `batch_size` works correctly on that pipeline.
  Uncovered that it doesn't for Detr, which IMO is OK since images
  after `feature_extractor` don't have the same size. Comment should
  explain.

* Type hint as a string.

* Make fixup + update black.

* torch+vision protections.

* Don't use torchvision, use F.interpolate instead (no new dep).

* Last fixes for Segformer.

* Update test to reflect new image (which was broken)

* Update tests.

* Major BC modification:

- Removed the string compressed PNG string, that's a job for users
`transformers` stays in python land.
- Removed the `score` for semantic segmentation. It has hardly a meaning
  on its own in this context.
- Don't include the grayscale with logits for now (which could enable
  users to get a sense of confidence). Might be done later.
- Don't include the surface of the mask (could be used for sorting by
  users, to filter out small masks). It's already calculable, and
  it's easier to add later, than to add now and break later if we need.

* `make fixup`.

* Small changes.

* Rebase + doc fixup.","['src/transformers/__init__.py', 'src/transformers/pipelines/__init__.py', 'src/transformers/pipelines/image_segmentation.py', 'src/transformers/utils/dummy_pt_objects.py', 'tests/test_modeling_segformer.py', 'tests/test_pipelines_common.py', 'tests/test_pipelines_image_segmentation.py']","The `image-segmentation` feature is not enabled on `AutoModelForSemanticSegmentation`. Also, there are errors in type hinting, multiple test fails, and issues with batch sizes for the pipeline. Base64 encoding of images appears problematic and unnecessary in Python environment. In addition, PNG string compression, semantic segmentation score, and grayscale with logits are currently misplaced or unnecessary features."
390e121fb5233aa553d7a456c40ad5c95a72b7f1,1681449061,"[Examples] TPU-based training of a language model using TensorFlow (#21657)

* add: tokenizer training script for TF TPU LM training.

* add: script for preparing the TFRecord shards.

* add: sequence of execution to readme.

* remove limit from the tfrecord shard name.

* Add initial train_model.py

* Add basic training arguments and model init

* Get up to the point of writing the data collator

* Pushing progress so far!

* Complete first draft of model training code

* feat: grouping of texts efficiently.

Co-authored-by: Matt <rocketknight1@gmail.com>

* Add proper masking collator and get training loop working

* fix: things.

* Read sample counts from filenames

* Read sample counts from filenames

* Draft README

* Improve TPU warning

* Use distribute instead of distribute.experimental

* Apply suggestions from code review

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* Modularize loading and add MLM probability as arg

* minor refactoring to better use the cli args.

* readme fillup.

* include tpu and inference sections in the readme.

* table of contents.

* parallelize maps.

* polish readme.

* change script name to run_mlm.py

* address PR feedback (round I).

---------

Co-authored-by: Matt <rocketknight1@gmail.com>
Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>","['examples/tensorflow/language-modeling-tpu/prepare_tfrecord_shards.py', 'examples/tensorflow/language-modeling-tpu/run_mlm.py', 'examples/tensorflow/language-modeling-tpu/train_unigram.py', 'src/transformers/data/data_collator.py']","Training a language model using TensorFlow on TPU is not currently possible because the necessary scripts, preparation steps, and proper masking collator in the training loop are lacking."
6eb51450fa2a440a45e02b29f01e4f2aa4f70a4d,1660146591,"TF Examples Rewrite (#18451)

* Finished QA example

* Dodge a merge conflict

* Update text classification and LM examples

* Update NER example

* New Keras metrics WIP, fix NER example

* Update NER example

* Update MC, summarization and translation examples

* Add XLA warnings when shapes are variable

* Make sure batch_size is consistently scaled by num_replicas

* Add PushToHubCallback to all models

* Add docs links for KerasMetricCallback

* Add docs links for prepare_tf_dataset and jit_compile

* Correct inferred model names

* Don't assume the dataset has 'lang'

* Don't assume the dataset has 'lang'

* Write metrics in text classification

* Add 'framework' to TrainingArguments and TFTrainingArguments

* Export metrics in all examples and add tests

* Fix training args for Flax

* Update command line args for translation test

* make fixup

* Fix accidentally running other tests in fp16

* Remove do_train/do_eval from run_clm.py

* Remove do_train/do_eval from run_mlm.py

* Add tensorflow tests to circleci

* Fix circleci

* Update examples/tensorflow/language-modeling/run_mlm.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update examples/tensorflow/test_tensorflow_examples.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update examples/tensorflow/translation/run_translation.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Update examples/tensorflow/token-classification/run_ner.py

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>

* Fix save path for tests

* Fix some model card kwargs

* Explain the magical -1000

* Actually enable tests this time

* Skip text classification PR until we fix shape inference

* make fixup

Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>","['examples/tensorflow/language-modeling/run_clm.py', 'examples/tensorflow/language-modeling/run_mlm.py', 'examples/tensorflow/multiple-choice/run_swag.py', 'examples/tensorflow/question-answering/run_qa.py', 'examples/tensorflow/summarization/run_summarization.py', 'examples/tensorflow/test_tensorflow_examples.py', 'examples/tensorflow/text-classification/run_glue.py', 'examples/tensorflow/text-classification/run_text_classification.py', 'examples/tensorflow/token-classification/run_ner.py', 'examples/tensorflow/translation/run_translation.py', 'src/transformers/optimization_tf.py', 'src/transformers/training_args.py', 'src/transformers/training_args_tf.py']","Assuming consistent batch size scaling by number of replicas in TF examples fails, causing problems in language modeling, translation, and token classification examples. Dataset 'lang' field arbitrarily presumed. Shape inference for text classification PR is not currently optimised."
98122794d45b124b7e9ef75d58686a4f156156bd,1669646643,"Replace assertions with value errors on distilbert model (#20463)

* Changed assert into 7-8 exceptions

* updated syntax error

* updated error

* updated file (Co-autho: Batese2001)

* Successful test on test_modeling_distilbert.py 

Successful raising errors and exceptions on the revised code in test_modeling_distilbert.py .

Co-credit: @batese2001

* Delete test_modeling_distilbert.ipynb

* Update modeling_distilbert.py

* Successful raising of exceptions with the conditions that are contrary to defined condition that asserts statements (Co-author: Batese2001)

* Successful raising of exceptions with the conditions that are contrary to defined condition that asserts statements (Co-author: Batese2001)

* committing the reformatted distilbert model

* reformatted distilbert model

* reformatted distilbert model

* reformatted distilbert model

* reformatted distilbert model with black

* Changed comments that explain better about raising exceptions for not having the even number of multi heads

* Changed comments that explain better about raising exceptions for not having the even number of multi heads

* changed based on the feedback

* Changed line 833 based on the suggestion made from @younesbelkada

* Changed line 833 based on the suggestion made from @younesbelkada draft2

* reformatted file

* Update src/transformers/models/distilbert/modeling_distilbert.py

* Update src/transformers/models/distilbert/modeling_distilbert.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>",['src/transformers/models/distilbert/modeling_distilbert.py'],"The distilbert model uses assertions that don't provide meaningful feedback to users when requirements are not met, particularly regarding even numbers of multi-heads."
105c3a48bec363b99e225614afaaf53730921046,1669815826,"Support extraction of both train and eval XLA graphs (#20492)

Neuron supports extraction of XLA graphs for compilation.
However, when both do_train and do_eval options are enabled,
sizes returned by tensor operator can be 0. To avoid
INVALID_ARGUMENT error, we use inequality in the check whether
a tensor needs padding or not.",['src/transformers/trainer.py'],"When both do_train and do_eval options are enabled in Neuron for the extraction of XLA graphs, sizes returned by tensor operator can be zero causing an INVALID_ARGUMENT error."
f1660d7e23d4432513fe060bde4f9b7b29f05204,1686076274,"Remote code improvements (#23959)

* Fix model load when it has both code on the Hub and locally

* Add input check with timeout

* Add tests

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

* Some non-saved stuff

* Add feature extractors

* Add image processor

* Add model

* Add processor and tokenizer

* Reduce timeout

---------

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>","['src/transformers/dynamic_module_utils.py', 'src/transformers/models/auto/auto_factory.py', 'src/transformers/models/auto/configuration_auto.py', 'src/transformers/models/auto/feature_extraction_auto.py', 'src/transformers/models/auto/image_processing_auto.py', 'src/transformers/models/auto/processing_auto.py', 'src/transformers/models/auto/tokenization_auto.py', 'tests/models/auto/test_configuration_auto.py', 'tests/models/auto/test_feature_extraction_auto.py', 'tests/models/auto/test_image_processing_auto.py', 'tests/models/auto/test_modeling_auto.py', 'tests/models/auto/test_processor_auto.py', 'tests/models/auto/test_tokenization_auto.py']","Model load fails when code is present both on the Hub and locally. Additionally, timeouts lack input checks, and there is a need for feature extractors, image processor, model, processor and tokenizer."
48d4827697084930c13818f82868d2cf255fe9bf,1639580272,"TF model cards (#14720)

* Initial commit for Keras model cards

* Revert accidental change

* make style

* make style

* make style

* Fix PR comments

* Move repo creation to __init__

* Fixes to README.md creation

* Partial progress for proper card creation on `push_to_hub`

* Proper card creation from `push_to_hub` plus fixes for malformed model cards

* Fixes for model card creation outside the callback

* Adding a model card creation test

* Putting the model card creation test in the right file.
Good job, Matt.

* make style

* Fix model card test temp dir usage

* Fix model card creation when no optimizer present

* Fixes for when training history not present

* Fix accidental edit to test_modeling_common","['src/transformers/file_utils.py', 'src/transformers/keras_callbacks.py', 'src/transformers/modelcard.py', 'src/transformers/modeling_tf_utils.py', 'tests/test_modeling_tf_common.py']","Model card creation in `push_to_hub` has inconsistencies and fails under certain conditions, such as when no optimizer or training history is present. Additionally, model card creation outside the callback is flawed."
da2a4d95a24ccc43f452389884f9f017fe4ec3fb,1675176695,"Add support of backward_prefetch and forward_prefetch (#21237)

* Add support of backward_prefetch and forward_prefetch

* Fix format issue

* Fix isort issue

* Fix doc style issue

* Update src/transformers/trainer.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix black issue

* Fix doc-style issue

* Make additional fsdp parameters into fsdp config

* Fix black issue

* Remove unused imports

* Fix doc style issues

* Incorporate PR feedbacks

* Remove unused imports

* Fix tests

* Fix tests

* Fix tests

* Fix tests

* Fix tests

* Update src/transformers/training_args.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix tests

* Incorporate PR feedbacks

* Incorporate PR feedbacks

* Fix black issues

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>","['src/transformers/trainer.py', 'src/transformers/training_args.py']",Transformer trainer lacks support for backward_prefetch and forward_prefetch features.
5c17918fe4cda80dae5b7ec8f0b2d23a813c4a05,1654247315,"Allow from transformers import TypicalLogitsWarper (#17477)

* Allow from transformers import TypicalLogitsWarper

* Added TypicalLogitsWarper

* Allow from transformers import TypicalLogitsWarper

* Allow from transformers import TypicalLogitsWarper

* Allow from transformers import TypicalLogitsWarper

* Allow from transformers import TypicalLogitsWarper

Added TypicalLogitsWarper

Allow from transformers import TypicalLogitsWarper

Allow from transformers import TypicalLogitsWarper

Allow from transformers import TypicalLogitsWarper","['src/transformers/__init__.py', 'src/transformers/utils/dummy_pt_objects.py']","'Unable to import TypicalLogitsWarper from transformers, causing issues in code requiring this functionality.'"
049e7917587aba45617cae0824d73f87009ce8e3,1651666105,"Add Data2Vec for Vision in TF (#17008)

* add utilities till TFData2VecVisionLayer.

* chore: pass window_size to attention layer.

* feat: add TFData2VecVisionRelativePositionBias.

* feat: initial implementation ready for tf data2vec.

* fix: relative position bias index, table to be fixed.

* chore: implementation added, tests remaining.

* add: tests, other PR files.

* fix: code quality.

* fix: import structure in init.

* chore: run make fix-copies.

* chore: address PR feedback (round I).

* chore: styling nit.

* fix: tests due to removal of to_2tuple().

* chore: rebase with upstream main and move the test.

* Update src/transformers/models/auto/modeling_tf_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/auto/modeling_tf_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix: layer call.

* chore: remove from_pt=True and rerun test.

* chore: remove cast and tf.divide.

* chore: minor edits to the test script.

* Update src/transformers/models/data2vec/modeling_tf_data2vec_vision.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* fix: expand() on TF tensors with broadcast_to().

* fix: test import.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>","['src/transformers/__init__.py', 'src/transformers/models/auto/modeling_tf_auto.py', 'src/transformers/models/data2vec/__init__.py', 'src/transformers/models/data2vec/modeling_tf_data2vec_vision.py', 'src/transformers/utils/dummy_tf_objects.py', 'tests/models/data2vec/test_modeling_tf_data2vec_vision.py']","The Vision component for Tensorflow Data2Vec layer is missing, causing inconsistencies in relative position bias, testing, and layer calls. Furthermore, issues have been identified in handling TF tensors expansion."
2b81f72be9fa6d69734ae27cfcbfd72b04988fe4,1658934941,"start from 1.12, torch_ccl is renamed as oneccl_bindings_for_pytorch … (#18229)

* start from 1.12, torch_ccl is renamed as oneccl_bindings_for_pytorch and should import it before use

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* add doc for perf_train_cpu_many

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* update doc

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>","['src/transformers/training_args.py', 'src/transformers/utils/__init__.py', 'src/transformers/utils/import_utils.py']","After version 1.12, attempting to import torch_ccl results in a failure due to renaming to oneccl_bindings_for_pytorch, leading to import errors."
667ccea72235504ab7876024e4f8c113ca62190f,1669647985,"Replace assertion with ValueError exceptions in run_image_captioning_flax.py (#20365)

* replace 4 asserts with ValueError exception for control flow

* Update examples/flax/image-captioning/run_image_captioning_flax.py

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* Update examples/flax/image-captioning/run_image_captioning_flax.py

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* reformatted file

* uninstalled trasformers and applied make style

Co-authored-by: Bibi <Bibi@katies-mac.local>
Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>",['examples/flax/image-captioning/run_image_captioning_flax.py'],The use of assertions for control flow in run_image_captioning_flax.py could potentially raise ambiguous error messages during runtime.
52e2c13da3c56b4d87996e0ff1cd85ad068fe1c5,1695891396,"[VITS] Fix speaker_embed device mismatch (#26115)

* [VITS] Fix speaker_embed device mismatch

- pass device arg to speaker_id tensor

* [VITS] put speaker_embed on device when int

* [VITS] device=self.device
instead of self.embed_speaker.weight.device

* [VITS] make tensor directly on device
using torch.full()",['src/transformers/models/vits/modeling_vits.py'],Discrepancy in device placement for speaker_embed causing issues in VITS model execution.
ab0ddc99e853c974949d823dbfaa732202696f3e,1697100518,"Warnings controlled by logger level (#26527)

* Logger level

Co-authored-by: Sahil Bhosale <sahilbhosale63@live.com>
Co-authored-by: Adithya4720 <hegdeadithyak@gmail.com>
Co-authored-by: Sachin Singh <sachinishu02@gmail.com>
Co-authored-by: Riya Dhanduke <113622644+riiyaa24@users.noreply.github.com>

* More comprehensive documentation

---------

Co-authored-by: Sahil Bhosale <sahilbhosale63@live.com>
Co-authored-by: Adithya4720 <hegdeadithyak@gmail.com>
Co-authored-by: Sachin Singh <sachinishu02@gmail.com>
Co-authored-by: Riya Dhanduke <113622644+riiyaa24@users.noreply.github.com>",['src/transformers/utils/logging.py'],The logger level is not controlling the warnings as expected.
