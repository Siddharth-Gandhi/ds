commit_id,commit_date,commit_message,actual_files_modified,transformed_message_gpt4
621ff0f9735cd8c4c5d6becb291ad050c35e01c0,1651826654,"Add linalg.vander

This PR adds `linalg.vander`, the linalg version of `torch.vander`.

We add autograd support and support for batched inputs.

We also take this chance to improve the docs (TODO: Check that they
render correctly!) and add an OpInfo.

**Discussion**: The current default for the `increasing` kwargs is extremely
odd as it is the opposite of the classical definition (see
[wiki](https://en.wikipedia.org/wiki/Vandermonde_matrix)). This is
reflected in the docs, where I explicit both the odd defaults that we
use and the classical definition. See also [this stackoverflow
post](https://stackoverflow.com/a/71758047/5280578), which shows how
people are confused by this defaults.

My take on this would be to correct the default to be `increasing=True`
and document the divergence with NumPy (as we do for other `linalg`
functions) as:

- It is what people expect
- It gives the correct determinant called ""the Vandermonde determinant"" rather than (-1)^{n-1} times the Vandermonde det (ugh).
- [Minor] It is more efficient (no `flip` needed)
- Since it's under `linalg.vander`, it's strictly not a drop-in replacement for `np.vander`.

We will deprecate `torch.vander` in a PR after this one in this stack
(once we settle on what's the correct default).

Thoughts? mruberry

cc kgryte rgommers as they might have some context for the defaults of
NumPy.

Fixes https://github.com/pytorch/pytorch/issues/60197

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76303

Approved by: https://github.com/albanD, https://github.com/mruberry
","['aten/src/ATen/native/BatchLinearAlgebra.cpp', 'torch/linalg/__init__.py', 'torch/overrides.py', 'torch/testing/_internal/common_methods_invocations.py']","The current `torch.vander` function has unusual defaults for the `increasing` parameter which lead to confusion, its results differ from the classical definition, and it lacks autograd support and batched inputs handling. It also needs better documentation."
ce7751188afb42263ebda159d6ee7a343a833cc1,1665438659,"[DDP] Add `PackedSequence` support when `device_ids` is specified (#86614)

Before this PR, if a user runs DDP with `device_ids` specified and with a `PackedSequence` input, then the execution will error with something like:
```
raise ValueError(
  ValueError: batch_sizes should always be on CPU. Instances of PackedSequence should never be created manually. They should be instantiated by
 functions like pack_sequence and pack_padded_sequences in nn.utils.rnn. https://pytorch.org/docs/stable/nn.html...
```
This is because the DDP forward calls `_to_kwargs()`, which calls `_recursive_to()`, which moves the inputs to GPU. However, `_is_namedtuple(packed_sequence)` returns `True`, leading to the branch `return [type(obj)(*args) for args in zip(*map(to_map, obj))]`, which tries to construct a `PackedSequence` directly via `type(obj)(*args)`, leading to the error.

Repro for `_is_namedtuple(packed_sequence)` returning `True`:
```
import random

import torch
import torch.nn.utils.rnn as rnn_utils
from torch.nn.parallel.scatter_gather import _is_namedtuple

def _ordered_sequence(tensor_type):
    seqs = [tensor_type(random.randint(1, 256))
            for _ in range(32)]
    seqs = [s.random_(-128, 128) for s in seqs]
    ordered = sorted(seqs, key=len, reverse=True)
    return ordered

def _padded_sequence(tensor_type):
    ordered = _ordered_sequence(tensor_type)
    lengths = [len(i) for i in ordered]
    padded_tensor = rnn_utils.pad_sequence(ordered)
    return padded_tensor, lengths

padded, lengths = _padded_sequence(torch.Tensor)
packed = rnn_utils.pack_padded_sequence(
    padded, lengths, enforce_sorted=False)
print(type(packed), packed.data.device)
print(_is_namedtuple(packed))
```

Test Plan:
```
python test/distributed/test_c10d_nccl.py -k test_ddp_packed_sequence
```
Without the fix, the added unit test fails with the expected error.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86614
Approved by: https://github.com/rohan-varma
","['test/distributed/test_c10d_nccl.py', 'torch/distributed/utils.py']","When running DDP with `device_ids` specified and a `PackedSequence` input, the forward call results in an error indicating that `batch_sizes` should always be on CPU while the instantiation process tries to construct a `PackedSequence` directly on the GPU."
b00afe135d50a325eb01fec0c573562f13bb9860,1628916014,"[Pytorch Profiler] Add debug_handles to KinetoEvent (#62228)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62228

This diff adds debug handles to events and provides a way to use
RECORD_FUNCTIONs that will pass debug_handles down to profiler, which
will record it in the events.

Why add debug_handles?
For pytorch mobile, with lite interpreter, we generate debug handles
that can be used for lazily symbolicate exception traces to model level
stack trace. Similar to the model level stack trace you get in
TorchScript models. The debug_handles also enable getting module
hierarchy for lite interpreter model, support for which was added to
KinetoProfiler in previous diffs.

Followup plan:
1. Enabled scope callbacks such that lite interpreter can use it to
profiler only top level ops.
2. Enable post processing callbacks that take KinetoEvents and populate
module hierarchy using debug handles.

This will let us use KinetoProfiler for lite interpter use cases on
mobile. Aim is to use RAII guard to similarly generate chrome trace for
mobile usecases as well, although only for top level ops.

Test Plan:
test_misc : RecordDebugHandles.Basic

Imported from OSS

Reviewed By: ilia-cher

Differential Revision: D29935899

fbshipit-source-id: 4f06dc411b6b5fe0ffaebdd26d3274c96f8f389b
","['aten/src/ATen/record_function.h', 'test/cpp/jit/test_misc.cpp', 'torch/csrc/autograd/profiler_kineto.cpp', 'torch/csrc/autograd/profiler_kineto.h']","KinetoEvents lack debug handles for recording purposes, leading to difficulties in lazily symbolicate exception traces for pytorch mobile with the lite interpreter."
6e2d020037d006cbe31a12c0041c6bd659201b83,1617390696,"Add interpolation kwarg to torch.quantile (#49267)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/49267

This PR builds upon the PR https://github.com/pytorch/pytorch/pull/48711 by RockingJavaBean. The original PR introduced a BC breaking change by making the interpolation parameter positional. Thus, previous invocations of torch.quantile that did not include the interpolation parameter failed after the PR landed.

To avoid BC breaking changes, we preserve the original signatures and make the interpolation parameter in the new signatures kwarg only. For now, interpolation cannot have a default value to avoid ambiguity with the deprecated signature. However, due to limitations of codegen and C++, we cannot have a required arg after optional ones. Thus, this PR also makes dim and keepdim requires args. Once we can remove the old signatures, dim, keepdim and interpolation parameters in the new signature will get the default values back.

__TODO__
 ---
- [ ] Run backward compat tests

This reverts commit 2f1d1eb7df5e8032392b73751c84025a2aa3d1ee.

Test Plan: Imported from OSS

Reviewed By: glaringlee

Differential Revision: D27337117

Pulled By: heitorschueroff

fbshipit-source-id: 7fe31f22027645e0d6cb3cab0392d532a4b362c9
","['aten/src/ATen/native/Sorting.cpp', 'aten/src/ATen/native/Sorting.h', 'test/test_reductions.py', 'torch/_tensor_docs.py', 'torch/_torch_docs.py', 'torch/testing/_internal/common_methods_invocations.py']","The interpolation parameter in torch.quantile unexpectedly became positional after a PR, causing previous invocations without this parameter to fail."
4af5939d7ac70cba7f130ab74705080cbda68d7b,1673656514,"[optim] Improve adadelta foreach, group tensors to maximize fast path (#92048)

Old behavior would have adadelta foreach sending tensors to the slow path if they were not all the same dtype nor on the same device.

This PR adds grouping for adadelta optimizer so that it would run foreach in batches, allowing more users to benefit from foreach perf.

Of course, we should ensure that the new implementation works, so there are new tests to ensure this behavior is not broken.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92048
Approved by: https://github.com/albanD
","['.jenkins/pytorch/multigpu-test.sh', 'test/test_optim.py', 'torch/optim/adadelta.py']","Adadelta optimizer's foreach function sends tensors to the slower path when they are not the same dtype or on the same device, limiting performance benefits."
f4ee37453cc8ad9e0b7eafeaabf11d22ba0c50fd,1659546012,"[dist.checkpoint] Change metadata format and improve error reporting (#82078)

This PR implements the following changes.

Move to new checkpoint metadata format with split between logical and storage data.
This is a step in the direction of supporting extensible checkpointing as it moves us away from the hardcoded storage model enforced by the FileSystem storage layer.

Change CheckpointException to include exception traceback. Exception tracebacks are not serializable so we need to take care of that otherwise we provide horribly bad errors to users.

Finally, remove `validate_state_dict` as it lost its usefulness. Loading is becoming more and more flexible to the point that the only reasonable way to verify if it's possible to load a given configuration is to actually try it.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82078
Approved by: https://github.com/wanchaol, https://github.com/fduwjj
","['test/distributed/_shard/checkpoint/test_checkpoint.py', 'torch/distributed/_shard/checkpoint/__init__.py', 'torch/distributed/_shard/checkpoint/api.py', 'torch/distributed/_shard/checkpoint/metadata.py', 'torch/distributed/_shard/checkpoint/resharding.py', 'torch/distributed/_shard/checkpoint/state_dict_loader.py', 'torch/distributed/_shard/checkpoint/state_dict_saver.py', 'torch/distributed/_shard/checkpoint/utils.py']","Checkpoint metadata format is hard-coded and inflexible, limiting extensibility. In addition, `CheckpointException` does not include traceback information, leading to vague error messages. The `validate_state_dict` function is no longer useful due to increasing load flexibility."
473d1939661a4b49d600bc213859175dc3c69d41,1617888939,"Use mkldnn copy for copy_ when self and src are Mkldnn layout (#54248)

Summary:
Currently, when copy_ is called with Mkldnn layout, a RuntimeError is raised.

**Environment**
- CPU : Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz
- PyTorch master(1772e26f6380d1)
- build with USE_MKLDNN=1

**Sample code to reproduce:**
```python
import torch

x = torch.randn(4, 5, dtype=torch.float32)
mkldnn_x = x.to_mkldnn()
mkldnn_y = torch.randn(4, 5, dtype=torch.float32).to_mkldnn()
mkldnn_y.copy_(mkldnn_x)

print(x)
print(mkldnn_y.to_dense())
```

**Results:**
Actual:
```sh
Traceback (most recent call last):
  File ""mkldnn_copy.py"", line 6, in <module>
    mkldnn_y.copy_(mkldnn_x)
RuntimeError: unsupported tensor layout: Mkldnn
```

Expected:
```sh
# x
tensor([[ 0.1276, -0.1179,  1.1970,  2.4836,  1.9059],
        [-1.9647,  0.8613, -0.5060,  0.1555,  0.3661],
        [-0.1560, -0.2133,  0.3414, -1.7095, -2.3431],
        [ 1.3291,  0.3083,  0.5523, -2.0577, -0.4740]])
# mkldnn_y
tensor([[ 0.1276, -0.1179,  1.1970,  2.4836,  1.9059],
        [-1.9647,  0.8613, -0.5060,  0.1555,  0.3661],
        [-0.1560, -0.2133,  0.3414, -1.7095, -2.3431],
        [ 1.3291,  0.3083,  0.5523, -2.0577, -0.4740]])
```

This is because `copy_` does not support Mkldnn layout.
So I modified to call `copy_mkldnn_` in `copy_` when both `self` and `src` are Mkldnn layout.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54248

Reviewed By: mrshenli

Differential Revision: D27641352

Pulled By: ezyang

fbshipit-source-id: 70a37cdacb4a40b250ca16f2f6ddb6b71ff52d90
","['aten/src/ATen/native/mkldnn/Copy.cpp', 'test/test_mkldnn.py', 'tools/build_variables.bzl']","When 'copy_' is invoked with Mkldnn layout, a RuntimeError is encountered due to unsupported tensor layout."
38169c2287a6982946f3f7b3764dada48e5ec23f,1657125078,"[Static Runtime] Fix precision error in test cases (#80935)

Summary:
- Test cases related to DeepAndWideSciptModel() was crashing at random due to precision issue
- test cases related for precision: DeepWide, KWargsAPI_1, KWargsAPI_2, KWargsAPI_Optional, FusionPass
- test failure was not observed always due to random input to the model (via torch::randn)
- Increasing the absolute tolerance for test cases

Differential Revision: D37639067

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80935
Approved by: https://github.com/mikeiovine
",['benchmarks/static_runtime/test_static_module.cc'],"Test cases related to DeepAndWideSciptModel() have inconsistent behavior, failing intermittently due to precision errors from randomized inputs via torch::randn."
30e74be784ad6c4d5691cb49dbba8327943ee9cb,1658867947,"a new section for ir generation (#81847)

This is to get a conversation started.

* @JackCaoG we could add attributes to items in `ir_codegen` section to customize IR generation logic (e.g. not generating `::Lower`). Though it could be a bit tricky to thread it through.
* Adding an extra argument to `map_codegen` to filter native functions out seems like a step in the right direction. Otherwise, it's a bit confusing how do we go from a full list to a codegen list.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81847
Approved by: https://github.com/JackCaoG, https://github.com/wconstab, https://github.com/bdhirsh
","['tools/test/test_gen_backend_stubs.py', 'torchgen/gen_backend_stubs.py', 'torchgen/gen_lazy_tensor.py']","There's no explicit way to customize IR generation logic in `ir_codegen`, potentially leading to unneeded generation steps like `::Lower`. Additionally, filtering native functions from the full list to a codegen list in `map_codegen` is unclear."
0c0694495be47d3561a902cf2e9cad0c574d8133,1677176533,"Fix a bug in nesting check_sparse_tensor_invariants context managers (#95372)

As in the title. The bug was reported in https://github.com/pytorch/pytorch/pull/94728#discussion_r1108892366 and has the following reproducer:
```python
>>> import torch
>>> check_ctx = torch.sparse.check_sparse_tensor_invariants(True)
>>> no_check_ctx = torch.sparse.check_sparse_tensor_invariants(False)
>>> with check_ctx:
...   assert torch.sparse.check_sparse_tensor_invariants.is_enabled()
...   with no_check_ctx:
...     assert not torch.sparse.check_sparse_tensor_invariants.is_enabled()
...   assert torch.sparse.check_sparse_tensor_invariants.is_enabled()
...
Traceback (most recent call last):
  File ""<stdin>"", line 5, in <module>
AssertionError
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95372
Approved by: https://github.com/cpuhrsch
","['test/test_sparse.py', 'torch/sparse/__init__.py']","The context manager `check_sparse_tensor_invariants` doesn't correctly maintain its state when nested, failing assertions in PyTorch's sparse package."
3adc8f8cf76844741f403ffa6f3f10653776a1fd,1613718831,"Enable min & max for Float16 & BFloat16 (#51244)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/50790.

Added `min()` & `max()` support for `Float16` & `BFloat16`.
CUDA already supported these ops on `Float16`, so the other three combinations had to be enabled.
`OpInfo`s for `min` & `max` were also added, and their sample inputs were removed from `method_tests()`.

### MORE INFO
The (slightly) long-term goal is to add dispatch for `min()` & `max()` related operations on CPU & CUDA for `Float16` & `BFloat16`,
wherever they aren't present already:
1. `amin()`
2. `argmax()`
3. `amax()`
4. `argmin()`
5. `torch._aminmax()`
6. `torch.clamp()` on CPU. Was already supported on CUDA
7. `min()` (in this PR)
8. `max()` (in this PR)
9. `minimum()`
10. `maximum()`

I'll submit separate PRs for the other ops.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51244

Reviewed By: jbschlosser

Differential Revision: D26503455

Pulled By: anjali411

fbshipit-source-id: c32247f214e9272ca2e4322a23337874e737b140
","['aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp', 'aten/src/ATen/native/cpu/TensorCompareKernel.cpp', 'aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu', 'test/test_reductions.py', 'torch/testing/_internal/common_methods_invocations.py']","Min and max operations are not supported for Float16 and BFloat16 data types, resulting in limited functionality for these types."
4135295a7681fa53b0712997bb4c15ece2c84c69,1682689539,"Excise yaml dependency in torchgen.model (#100203)

The problem:
- The new CustomOp API depends on torchgen.model
- torchgen.model imports `yaml`
- `yaml` is not a PyTorch runtime dependency

To unblock myself, because I'm not sure how long it'll take to
convince people yaml should be a PyTorch runtime dependency
(unless one of you wants to approve #100166), this PR removes the
yaml dependency from torchgen.model.

It does so by splitting torchgen.utils (the offender) into
torchgen.utils (no yaml) and torchgen.yaml (which uses yaml).

Test Plan:
- CI
Pull Request resolved: https://github.com/pytorch/pytorch/pull/100203
Approved by: https://github.com/ezyang, https://github.com/Skylion007
","['test/test_meta.py', 'tools/autograd/gen_python_functions.py', 'tools/autograd/load_derivatives.py', 'tools/onnx/gen_diagnostics.py', 'torchgen/gen.py', 'torchgen/gen_backend_stubs.py', 'torchgen/gen_lazy_tensor.py', 'torchgen/utils.py', 'torchgen/yaml_utils.py']","The new CustomOp API's dependency on torchgen.model is causing an issue due to `yaml` not being a PyTorch runtime dependency, causing potential blocks in the workflow."
6b7b9c796e82a68adfd412a3d6275ca6ec77e395,1695314211,"Fix registering jit decompositions for jvp for out wrapped decomps (#109367)

Python decompositions wrapped by `out_wrapper` need to be unwrapped before compiling with TorchScript since:
- `out_wrapper` extends the decompositions signature with an out parameter, however this `out` parameter is not present in the source code of the original decomposition so the resulting `ScriptFunction` will not have an `out` parameter
- `out_wrapper` is in the `torch._prims_common.wrappers` module so its `globals()` are different to the globals of the decomposition to be wrapped. This may cause symbol resolution to fail with the TorchScript compiler since it is compiling the unwrapped decomps source code rather than the wrapper

The python decomposition for `aten.trace` is wrapped as an example, other decompositions are to be fixed in https://github.com/pytorch/pytorch/pull/107707
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109367
Approved by: https://github.com/lezcano
","['test/test_ops.py', 'torch/_decomp/__init__.py', 'torch/_decomp/decompositions_for_jvp.py', 'torch/_prims_common/wrappers.py', 'torch/_refs/__init__.py']",TorchScript compilation fails when registering Python decompositions because of mismatches in signatures and globals between the original decomposition and the `out_wrapper`.
3aa7a528553cca8d473805caa0e52fe63fb11f52,1667267312,"[xnnpack][lite-int][4/n] introduce serialization to delegate (#87908)

We introduced the serializer we created in the previous diff to our XNNGraph builder, the purpose of this is to serialize parts of the graph as we build this. At the end, we are able to finish and serialize the xnngraph into a std::string for use when we forward this along to on-device runtime.

The next diff will rebuild the xnngraph from the serialization we introduce here, so testing the serialization of the graph will be done in the next diff

Differential Revision: [D39335580](https://our.internmc.facebook.com/intern/diff/D39335580/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39335580/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87908
Approved by: https://github.com/digantdesai
","['torch/csrc/jit/backends/xnnpack/xnnpack_backend_preprocess.cpp', 'torch/csrc/jit/backends/xnnpack/xnnpack_graph_builder.cpp', 'torch/csrc/jit/backends/xnnpack/xnnpack_graph_builder.h']","XNNGraph builder lacks a serializer, leading to difficulty in forwarding the built graph to on-device runtime as it can't be serialized into a comprehensible format (like std::string)."
50fa5880e831c0a1d5e0c2469dacfd0b0b8ff85d,1693469313,"[vmap] symintify alias and squeeze (#107577)

Following tests now pass (both ops call into `alias` on certain paths)

```
PYTORCH_TEST_WITH_DYNAMO=1 pytest test/functorch/test_vmap.py -k test_squeeze -v
PYTORCH_TEST_WITH_DYNAMO=1 pytest test/functorch/test_vmap.py -k test_conj -v
```

NOTE: Ideally, this symint version should work with non symint version as well but that would mean changes at multiple places. Wanted to get a review for this fix before-hand.

Other sites which use the `IntArrayRef` overload.
https://github.com/pytorch/pytorch/blob/5f56c4fb32dbb5dd4e75a3a3a9726ae95931926d/aten/src/ATen/native/TensorShape.cpp#L1707-L1713

`view_impl` is called from `view` and `_unsafe_view`.
https://github.com/pytorch/pytorch/blob/5f56c4fb32dbb5dd4e75a3a3a9726ae95931926d/aten/src/ATen/native/TensorShape.cpp#L3295-L3306

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107577
Approved by: https://github.com/zou3519
","['aten/src/ATen/functorch/BatchRulesViews.cpp', 'aten/src/ATen/native/TensorShape.cpp']","Certain operations in vmap like `alias` and `squeeze` are failing in the existing tests, with issues related to the compatibility of symint versions."
3f09485d7e0c7466862cd6647b3668e119590716,1628272740,"[WIP] Gate DistributedOptimizers on RPC availability (#62774)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62774

Gates DistributedOptimizer which relies on RRef based on if RPC is available. This should enable ZeRo to work with Windows as Windows should not try to import the DIstributedOptimizer. If this works as expected we can enable the windows tests for functional/local sgd optimizers as well.
ghstack-source-id: 135216642

Test Plan: CI

Reviewed By: pbelevich

Differential Revision: D30117838

fbshipit-source-id: e6365a910a3d1ca40d95fa6777a7019c561957db
","['test/run_test.py', 'torch/distributed/optim/__init__.py', 'torch/distributed/optim/optimizer.py', 'torch/distributed/optim/zero_redundancy_optimizer.py']",DistributedOptimizer's reliance on RRef is causing compatibility issues with Windows as it tries to import the DistributedOptimizer.
b1adaa87772eb7476a9544e4832af91a0db02491,1686760337,"[inductor] Fix no-xdim reductions (#103527)

Fixes #103481

Normally triton tensors have shape `[XBLOCK, RBLOCK]`, or some variation where
the lengths are 1 but the number of dimensions is the same. The `no_x_dim`
change in addition to removing the x dimension, also removed the r dimension
from certain values such as the results of reductions and the `xindex` variable.

This fixes those two cases to correctly produce tensors of shape `[1]`,
equivalent to the old shape `[XBLOCK, 1]` with the x-dimension dropped.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/103527
Approved by: https://github.com/ngimel
","['test/inductor/test_cuda_repro.py', 'torch/_inductor/codegen/triton.py']",The `no_x_dim` change in triton tensors is causing an issue where it also removes the r dimension from values such as the results of reductions and the `xindex` variable.
0ec1af4b7e49bec1030dd78580279263bbea8113,1617597505,"[c10d] Enforce order of waited ranks in monitored barrier. (#55009)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55009

Changes monitoredBarrier so that we await acknowledgemenet from ranks
in a consistent order (from least to greatest). This will reduce confusion
around the order the ranks are awaited. We are still planning to add support
for awaiting all ranks in follow up changes.
ghstack-source-id: 125699838

Test Plan: CI

Reviewed By: SciPioneer

Differential Revision: D27405417

fbshipit-source-id: b9a3e72742cbffdd9bf890ab2c94103b768a7b71
","['torch/lib/c10d/ProcessGroupGloo.cpp', 'torch/testing/_internal/distributed/distributed_test.py']","Monitored barrier awaits acknowledgement from ranks inconsistently, potentially causing confusion around the order the ranks are awaited."
17889ad26e007f6eec30e9129d1dc4fb1ac3ba9c,1634771589,"Add support for cat in output stitching (#66098)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66098

`cat` is somewhat special-cased right now because currently we only have list of Tensor inputs where the list is constructed in the JIT IR graph. While that is generally true for Fusion (e.g. why we have ConstantChunk) that may not be true for shape analysis generally, so I'm waiting a bit to generalize.

Test Plan: Imported from OSS

Reviewed By: navahgar, anjali411

Differential Revision: D31797467

Pulled By: eellison

fbshipit-source-id: ca761e214dfd7f3bba8d189f3b3f42ffec064f63
","['test/jit/test_symbolic_shape_analysis.py', 'torch/csrc/jit/passes/symbolic_shape_analysis.cpp']","The 'cat' function lacks support in output stitching, constraining it to deal only with a list of Tensor inputs constructed in the JIT IR graph."
76bcc87277fcd5030ee844cc783a9e1524012423,1682769947,"fix TIMM mobilevit_s complier issue for dynamic CPU path (#100230)

For TIMM ```mobilevit``` dynamic path, there has a compiler issue(```
python -m torch.backends.xeon.run_cpu --node_id 0 benchmarks/dynamo/timm_models.py --performance --float32 -dcpu -n2 --inductor --no-skip --dashboard --only mobilevit_s --inference --dynamic-shapes```
):

```
/tmp/torchinductor_xiaobing/xy/cxyslqzcsxkco4ieph7t63kn5q74ka35ak75lwfon32nlalxmru5.cpp:29:130: error: invalid operands of types ‘long int’ and ‘double’ to binary ‘operator%’
   29 |                             auto tmp0 = in_ptr0[static_cast<long>((((((-1L) + ks1) / 8L)*(((-1L) + ks1) / 8L))*((((2L*((i2 / 1L) % (std::ceil((1.0/2.0) + ((1.0/2.0)*(((-1L) + ks1)
```

There has a modulo for ```long % double```, this PR will convert inputs to long before do this operation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100230
Approved by: https://github.com/jansel
",['torch/_inductor/codegen/cpp.py'],"In the TIMM mobilevit dynamic path, there's a compiler issue resulting from a module operation trying to operate on 'long' and 'double' types together."
bea83e2e4643a4e28c620da267e0f65bc1b6562d,1624471994,"Add `NoChunk` wrapper for pipeline args. (#57325)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57325

As per the design outlined in
https://github.com/pytorch/pytorch/issues/53952, adding a `NoChunk` wrapper for
pipeline parallelism inputs.

If a Tensor is wrapped with this wrapper, the pipeline implementation does not
split this Tensor across micro-batches and instead just replicates this tensor
as-is similar to non-tensors.
ghstack-source-id: 132009305

Test Plan:
1) unit tests.
2) waitforbuildbot.

Reviewed By: SciPioneer

Differential Revision: D28109277

fbshipit-source-id: ee78c814c715d207d2796aba40b756a8e1834898
","['test/distributed/pipeline/sync/test_pipe.py', 'torch/distributed/pipeline/sync/__init__.py', 'torch/distributed/pipeline/sync/microbatch.py', 'torch/distributed/pipeline/sync/pipe.py']","Pipeline parallelism inputs are always split across micro-batches, causing issues when a Tensor should not be split but replicated as-is."
051802e4f4fc6c03b653dd4552998354730fdef2,1650073078,"[quant][core][gpu][bug fix] Fixed off by one index issue in broadcasted_bias

Summary:
There was an off by one index issue in new_size (used for broadcasted_bias).
This has now been corrected.

The matrix multiplication's output's last dimension is the number of out features, which is the
size of bias. We create `new_size` for `broadcasted_bias`, whom we want to have the same number
of dimensions as the matmul output and for it have the same size for the last dimension for broadcasting purposes.
Previously, we had `new_size[1] = bias_.value().size(0);`, but this is wrong, in general.

Test plan:
```
python test/test_quantization.py -k test_qlinear_cudnn
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75483

Approved by: https://github.com/jerryzh168
",['aten/src/ATen/native/quantized/cudnn/Linear.cpp'],The bias size assignment for `new_size` in `broadcasted_bias` off by one index resulting in an incorrect dimensional match with the matrix multiplication output.
44a948c82000dd7cf4cf23ff3aa0da92ff6812ef,1675192488,"Fix MSVC compiler error in basic_ops.h (#93322)

https://github.com/pytorch/pytorch/pull/93069 introduces a compiler error in some internal Windows builds using MSVC:

```
stderr: d:\full-fbsource\xplat\caffe2\torch\csrc\autograd\functions\basic_ops.h(43): fatal error C1001: An internal error has occurred in the compiler.
```
This may be related to older versions of MSVC not recognizing the `[[maybe-unused]]` attribute: https://developercommunity.visualstudio.com/t/compiler-bug-on-parsing-maybe-unused-in-range-base/209488. This PR reverts the changes in `basic_ops.h` that resolves those errors.

Verified this fixes the internal jobs, and landed as [D42854205](https://www.internalfb.com/diff/D42854205).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93322
Approved by: https://github.com/Skylion007, https://github.com/albanD
",['torch/csrc/autograd/functions/basic_ops.h'],"The code in basic_ops.h is throwing a fatal compiler error C1001 in internal Windows builds utilizing MSVC, potentially due to older versions of MSVC not recognizing the `[[maybe-unused]]` attribute."
26c123efbd4e96a616ca4e7ae7d7697a54cb1cf0,1642734698,"empty_cuda: Add functions that don't depend on Tensor (#70616)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70616

This adds `at::detail::empty_cuda` and
`at::detail::empty_strided_cuda` to complement the cpu and meta APIs.

These functions also include the `lazyInitCUDA` and `DeviceGuard` that
are missing from the `at::native::empty_cuda` interface and so is
safer to use.

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D33623677

Pulled By: ngimel

fbshipit-source-id: 1c38e84881083df8e025250388f0c8f392974b92
(cherry picked from commit 4bc48c7008acf2394db7d02dee69dd7a8cfb87b8)
","['aten/src/ATen/cuda/EmptyTensor.cpp', 'aten/src/ATen/cuda/EmptyTensor.h', 'aten/src/ATen/native/cuda/TensorFactories.cu']",The current `at::native::empty_cuda` interface lacks the `lazyInitCUDA` and `DeviceGuard` functions. There's a risk of issues occurring due to these missing elements.
34c91a70513276ef3c99f7ac91f874b7eb2def6f,1689593645,"Prefer bound_sympy over sympy_interp (#105138)

This is the first PR towards simplifying sympy_interp, and more
generally, simplifying the implementation of ValueRangeAnalysis for
SymPy expressions.

In general, it would be conteptually good to have a minimal subset of
operations that conform our SymPy expressions, let that be guards or
indexing expressions. This would allow us to reason better about SymPy
guards and potentially have invariants like knowing that guards are
continuous piecewise rational functions. If this were the case,
we could operate on them using exact arithmetic and completely avoid
precision errors like the one found in https://github.com/pytorch/pytorch/issues/105097
Pull Request resolved: https://github.com/pytorch/pytorch/pull/105138
Approved by: https://github.com/ezyang
","['torch/fx/experimental/symbolic_shapes.py', 'torch/utils/_sympy/value_ranges.py']",The current implementation of ValueRangeAnalysis for SymPy expressions is complex and has caused precision errors while operating on SymPy guards.
5429f68f00a3525aca7dc2e1aeb913a8e0e2f406,1627693624,"[DDP] log bucket sizes (#62232)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62232

Logs the bucket sizes in DDP logging so that we know which workflow ran with what bucket size config. Will be used to verify how changing bucket sizes in DDP affects perf.

Based on the test, we can see inconsistency where the ""first"" bucket size actually is (last before rebuild buckets, first after).
ghstack-source-id: 134663867

Test Plan: CI

Reviewed By: SciPioneer

Differential Revision: D29922299

fbshipit-source-id: 538b331c96e77048164ad130b377433be100a761
","['test/distributed/test_c10d_gloo.py', 'torch/csrc/distributed/c10d/init.cpp', 'torch/csrc/distributed/c10d/logger.cpp', 'torch/csrc/distributed/c10d/logger.hpp', 'torch/csrc/distributed/c10d/reducer.cpp', 'torch/csrc/distributed/c10d/reducer.hpp', 'torch/nn/parallel/distributed.py', 'torch/testing/_internal/distributed/distributed_test.py']","In Distributed Data Parallel (DDP) logging, there is no record of the bucket size configuration for each workflow, making it difficult to understand the effect of changing bucket sizes on performance."
f067972527ab9e163c57bc68bd09a0540769b307,1616871350,"Make memory overlap a little less precise so it works with null data ptr (#54710)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54710

I'm going to make meta tensors have storage (but DataPtr is always
null) so that I can accurately report memory overlap error checking, but
I now have a problem which is that if memory overlap test looks at the
actual data pointer, everything is going to look like it aliases!  A
more conservative test is to just see if the Storage objects themselves
alias, and assume that the data pointers are unique if they don't.

The loss of precision stems from if you unsafely have two distinct
storage objects that point to the same data pointer.  This situation
is pretty rare and so I think it is worth it (and I am hoping no tests
trigger by this.)

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: bdhirsh

Differential Revision: D27338810

Pulled By: ezyang

fbshipit-source-id: 5ebaf81c22824494c47c1ae78982d9c0e5cba59f
",['aten/src/ATen/MemoryOverlap.cpp'],Meta tensors with null DataPtr causing false positives in memory overlap error checking due to storage objects appearing to alias.
98933866a984d104e9f2bf1a91f3a38d3cbe0b99,1618614043,"[quant][graphmode][fx] Optimize cat (#54813)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54813

Previously we have a cat that takes a list of Tensors with different qparams and dequantize them
cacatenate them and requantize with the output qparams. This adds some unnecessary overhead in dequantizing
and quantizing Tensors.

This PR adds an optimization for cat operator, we'll make sure inputs and output of cat
uses same observer/fake_quant and produce a cat that does not do rescaling.

Test Plan: Imported from OSS

Reviewed By: vkuzo

Differential Revision: D27408377

fbshipit-source-id: 6a4bdcfd15e57ea1fe0f7e72d1e1288eb3ece4db
","['test/quantization/test_quantize_fx.py', 'torch/quantization/fx/quantize.py', 'torch/testing/_internal/common_quantization.py']","`cat` operator causes unnecessary overhead by dequantizing tensors, concatenating them and then requantizing with different output qparams."
13125247595b8f1f1d79f1e61f52d2c6932282b2,1644630776,"Remove un-used function in autograd engine (#72687)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72687

Not sure who would use that. It is not used in the code base as far as I can see. And I don't know of anyone working with the engine directly out of tree. So tentatively removing it.

Test Plan: Imported from OSS

Reviewed By: soulitzer

Differential Revision: D34180244

Pulled By: albanD

fbshipit-source-id: 678ba1c4a1cbd9a0458d33be97664d1e3d1bd86b
(cherry picked from commit 3968ca3a380eca41264664f197d12c8da2c27002)
","['torch/csrc/autograd/engine.cpp', 'torch/csrc/autograd/engine.h']",Unused function discovered within the autograd engine of PyTorch; potential impact unknown due to lack of explicit usages in the codebase.
b09c0b6550b05c574c6cb91c0fbdf2fb8998783d,1624511502,"[caffe2] update the BlobSerializer acceptor to allow moving in the data (#60207)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/60207

Update the `BlobSerializerBase` API so that the serizialized blob data is
passed as a `std::string&&` rather than `const std::string&`.  This allows the
acceptor to take ownership of the string data.  This allows the acceptor to do
things like queue it for storing asynchronously, rather than having to make a
copy of the data if they need it to remain valid after returning.

All existing `BlobSerializerBase` implementations already pass in a valid
rvalue reference to the data, so this change did not require updating any of
the existing serializer implementations.
ghstack-source-id: 132216750

Test Plan:
Examined all ~46 `BlobSerializerBase` subclasses in fbsource to confirm they
already pass in an rvalue reference for this argument.  Also searched for
`BlobSerializerBase` on google and did not find any external references to
this class in other open source projects that might be affected.

Differential Revision: D29204426

fbshipit-source-id: b1d567e52a5c17a01d651c70bbfa2fddbaea6cd9
","['caffe2/core/blob_serializer_base.h', 'caffe2/core/blob_test.cc', 'caffe2/operators/load_save_op.cc']","`BlobSerializerBase` makes a copy of the data for async operations, which could potentially slow down the process with large data."
59ac451ba314bbfb97435b186f236178d294104a,1628552866,"Simplify the logic of running ci workflow codegen (#62853)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62853

wanted to simplify the logic in the `__post_int__`, and delegate the settings back to individual workflows, this gives us more flexibility in changing individual workflows, as well as reducing the complexity of understanding the mutation conditions.

Test Plan: Imported from OSS

Reviewed By: walterddr, seemethere

Differential Revision: D30149190

Pulled By: zhouzhuojie

fbshipit-source-id: 44df5b1e14184f3a81cb8004151525d0e0fb20d9
",['.github/scripts/generate_ci_workflows.py'],"Complex and inflexible logic within `__post_int__` is causing difficulty in modifying individual workflows, increasing the complexity of understanding mutation conditions."
0c9d72b5e11e9d3e706eaf1c07406a0cebfe27c7,1613121068,"[StaticRuntime] Clean up output references and remove dead code (#51991)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51991

- Clean up references of outputs, including Tuples/Lists, by using move semantics
- Clean up references of elements in output Tuples/Lists by adding them to `unmanaged_values_` in MemoryPlanner. Check for corner case of Tuple/List element being inputs.
- Modify unit tests to check for use_counts of outputs
- Clean up dead code. A bit overlap with D25592967, but shouldn't be a problem.

This diff does not try to fix the alias problem with the MemoryPlanner.

(Note: this ignores all push blocking failures!)

Test Plan:
```
buck test //caffe2/benchmarks/static_runtime:static_runtime_cpptest
buck test mode/opt-clang caffe2/caffe2/fb/predictor:ptvsc2_predictor_bench_test
```

Reviewed By: bwasti

Differential Revision: D26333953

fbshipit-source-id: cadc0595ad6ab754c4f1f7a5a3733b2c16b3102f
","['benchmarks/static_runtime/test_static_runtime.cc', 'torch/csrc/jit/runtime/static/fusion.cpp', 'torch/csrc/jit/runtime/static/impl.cpp', 'torch/csrc/jit/runtime/static/impl.h']","References to outputs, including Tuples/Lists, are not properly cleaned up. There's also dead code present that needs removal."
35093fc1ab9749e6b763acead007e56b54c6375b,1668548447,"Enable correct supported activities for kineto on rocm (#88207)

A compile time guard was preventing ActivityType::CUDA from being available on rocm.  This caused both the GPU_FALLBACK and CUDA modes to be active at the same time.  So operators were being charged gpu time for the hipEventRecord ranges and the actual kernel execution times.  This caused incorrect (and often negative) cuda times, in e.g. table().

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88207
Approved by: https://github.com/malfet, https://github.com/jeffdaily
",['torch/csrc/autograd/init.cpp'],"On ROCm, both GPU_FALLBACK and CUDA modes are active simultaneously, leading to inaccurate and oftentimes negative CUDA times due to a misconfiguration with ActivityType::CUDA."
54eedf6fa6a5104dbdf92f868b0236c43c90dd21,1675306167,"Fix test_jit_cuda_archflags on Windows (#93332)

Fixes https://github.com/pytorch/pytorch/issues/61655

The test is flaky and fails whenever `test_jit_cuda_archflags` is run.  The latter `test_jit_cuda_archflags` was slow test in the old Windows runner.  It's currently running again on trunk due to the problem with populating slow-test JSON file ~Interestingly, its performance is getting better in the new Windows G5 runner and it becomes a borderline slow test, where it run sometimes~.  Whenever it runs, the next test `test_jit_cuda_extension` will fail.

* Build and load different CUDA arch modules from `test_jit_cuda_archflags` in separate processes to avoid importing them into the current one.  The test only checks the build artifacts.  Importing them cause `test_jit_cuda_extension` to fail as describe in https://github.com/pytorch/pytorch/issues/61655
* Clean up the temp build dir on Windows.  Windows CUDA runner is non-ephemeral, so it's better to clean thing up properly to avoid any funny business the next time the runner is used
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93332
Approved by: https://github.com/davidberard98
",['test/test_cpp_extensions_jit.py'],"The test `test_jit_cuda_archflags` fails intermittently, and seems to cause subsequent test `test_jit_cuda_extension` to fail when run on the Windows platform due to issues in importing build artifacts."
0a0f107b50e88df8b7c0f626b17156f86e63c1ca,1680906039,"Retry ONNX tests (the quick way) (#98627)

This is to mitigate a flaky ONNX test in trunk and also improve its reliability till we have https://github.com/pytorch/pytorch/issues/98626  (I figure that this is better than moving the job to unstable).

I try to disable the flaky test https://github.com/pytorch/pytorch/issues/98622, but that won't work as @clee2000 points out because ONNX isn't part of `run_test.py` to download and apply the list of disabled tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98627
Approved by: https://github.com/BowenBao
",['.ci/onnx/test.sh'],"ONNX test in trunk is flaky, affecting overall test reliability. Disabling the unstable test isn't working due to ONNX not being part of `run_test.py`."
3336aa191c3df3027a550f7ac87c1d98b31cf197,1692638413,"Adding allocated and reserved memory values to memory timline view. (#107056)

Summary: This diff adds the max allocated and max reserved memory values to the memory timeline plot.

Test Plan:
Executed

`buck run mode/dev-nosan kineto/libkineto/fb/integration_tests:pytorch_resnet_integration_test -- --enable_profiling --profile_memory --trace_handler=auto_trace --with_stack --record_shapes` on my devgpu.

The generated output is at
https://www.internalfb.com/manifold/explorer/ai_efficiency/tree/traces/dynocli/devgpu020.odn1.facebook.com/rank-0/rank-0.Aug_10_16_50_50.236946.pt.memorytl.html

 {F1067885545}
Screenshot of the html above
 {F1067886350}

Reviewed By: aaronenyeshi

Differential Revision: D48251791

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107056
Approved by: https://github.com/aaronenyeshi, https://github.com/davidberard98
",['torch/profiler/_memory_profiler.py'],"Memory timeline plot lacks max allocated and max reserved memory values, impacting visibility and monitoring of memory usage."
89072177e10b3cf9c8fdf204381db17fe1fde068,1661299010,"[fx][pass infra] Adding error catching (#83933)

Example:

```
======================================================================
ERROR: test_pass_manager_error (fx.test_pass_infra.TestPassManager)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/angelayi/Projects/pytorch/torch/fx/passes/infra/pass_manager.py"", line 285, in __call__
    res = fn(module)
  File ""/Users/angelayi/Projects/pytorch/test/fx/test_pass_infra.py"", line 164, in pass_fail
    raise RuntimeError(""bad"")
RuntimeError: bad

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/angelayi/Projects/pytorch/test/fx/test_pass_infra.py"", line 170, in test_pass_manager_error
    pm(traced_m)
  File ""/Users/angelayi/Projects/pytorch/torch/fx/passes/infra/pass_manager.py"", line 289, in __call__
    raise RuntimeError(msg) from e
RuntimeError: An error occured when running the 'pass_fail' pass after the following passes: ['replace_add_with_mul_pass', 'replace_mul_with_div_pass']
```

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83933
Approved by: https://github.com/SherlockNoMad
","['test/fx/test_pass_infra.py', 'torch/fx/passes/infra/pass_manager.py']","The 'pass_fail' pass in the pass manager throws a Runtime error, leading to a failure in the fx_test_pass_infra test, after executing the 'replace_add_with_mul_pass' and 'replace_mul_with_div_pass' passes."
ee4cb4b1e71facf3d65e94f2b1c539825ef2d448,1683264303,"Add --offload-to-disk support to minifier (#100546)

When minifying extremely large repros, the minifier can run out of memory. This is because, for delta debugging, the minifier keeps a copy of every intermediate output in the network. This can easily put you over the memory limit for your GPU. To make matters worse, we cannot easily delta debug in such a situation, as delta debugging involves replacing intermediates with inputs, but doing so can cause an intermediate to become live longer than its actual extent in the original model (since inputs all have to be allocated up front).

The strategy in this PR is to use `load_tensor` from the previous PR to offer a low memory mode for delta debugging. Instead of putting intermediates as inputs, we instead load them in the middle of the graph in question.  If, through DCE, the load_tensor ends up floating to the top of the graph, we can input-ify it. We now no longer save all intermediates in memory, but instead save them to disk. I used this to successfully minify the repro that helped us solve https://github.com/pytorch/pytorch/pull/100332

The testing is not very good. I can try to add more robust testing but it will involve a more involved refactor to FX minifier. Let me know if that's what you want.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100546
Approved by: https://github.com/anijain2305, https://github.com/voznesenskym
","['test/dynamo/test_minifier.py', 'test/inductor/test_minifier.py', 'torch/_dynamo/repro/after_aot.py', 'torch/_functorch/fx_minifier.py', 'torch/hub.py']",Minifying large repros leads to memory exhaustion as minifier preserves every intermediate output in memory. This situation restricts delta debugging and prolongs the lifespan of an intermediate beyond its actual extent in the original model.
187a5242494221c39f1c7e0302fff10d88f5b8b7,1619112427,"Re-order tests based on changed files (#56666)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56666

Addresses some of #56557 by checking for changed files when running tests. This will help deliver signal faster when a failing test is run. It should always be safe to at least try to re-order the tests, so there's no option to turn it off, and any error ends up bailing out of the sorting process. Time saved will change between tests, with more improvement for things that are further down the static list here:

https://github.com/pytorch/pytorch/blob/1e9c7ad4cb1869ea3769e1c563c78bce95da5945/test/run_test.py#L32

The results vary from not much improvement ([before: 11m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307580/workflows/6ab3def6-8d63-4f41-9b8d-9c2c50f6266b/jobs/12712819/steps), [after: 10m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307578/workflows/157407b4-f850-431c-b641-d2ac97916a04/jobs/12712802/steps)) to a lot ([before: 75m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307580/workflows/6ab3def6-8d63-4f41-9b8d-9c2c50f6266b/jobs/12712884/steps), [after: 8m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307578/workflows/157407b4-f850-431c-b641-d2ac97916a04/jobs/12712865/steps)), but overall there shouldn't be any regression in test timing. These results are also probably a little confounded since the test sharding will be different after re-ordering.

As a follow up we can use the target determination logic to figure out which tests to bring to front based on the actual code instead of just edits to test files

Test Plan: Imported from OSS

Reviewed By: samestep

Differential Revision: D27934076

Pulled By: driazati

fbshipit-source-id: 747d09ad732289d7693101803d46e9fa8e6d2f59
",['test/run_test.py'],Static ordering of tests leads to potential inefficiency in time taken for test execution due to lack of prioritization of tests related to recently changed files.
9eee782cb64d10b9c0a35bfdf72aa2ecabf0c0f7,1621379420,"[nnc][scripts] Add a script for bisecting the TE fuser pass (#58357)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58357

Finding a miscompilation in a large program can be tedious; this
script automates the process of bisecting based on the number of fused
instructions.  Since fusing aten::cat without the corresponding
prim::ListConstruct will cause an assertion failure, we treat that case as a
""skip"" and ignore it for the purpose of bisection.
ghstack-source-id: 129079484

Test Plan:
Tried it on some failing testcases, plus I wrote a simple bash
script to simulate ""failure"" and ""skip"" and verified a few different cases.

Reviewed By: huiguoo

Differential Revision: D28463808

fbshipit-source-id: 64836f1d37a573549179410316ea7168e3dc1f23
",['torch/csrc/jit/tensorexpr/scripts/bisect.py'],"Miscompilation in large programs with TE fuser pass is tedious to find. Fusing aten::cat without the corresponding prim::ListConstruct results in an assertion failure and is wrongfully categorized as a ""skip""."
82091d666c28a511fb27cf7ee30a3f5c97eb21df,1683388909,"[ONNX] Refactor Input/Output Adapter (#100490)

This PR refactors how InputAdapter and OutputAdapter is used throughout the exporter.

During refactoring, API issues with passes (torch.onnx._internal.fx._pass.Transform) were identified and should be tackled on another API. In short, some passes can modify the input/output of the model and the input/output adapter must be in sync with such change, otherwise, the adapters will not reflect the actual model input/output. The first instance of this issue was with `ReplaceGetAttrWithPlaceholder` pass that adds new inputs to the model. In order to work this around, a new input adapt step to append new inputs (generated by the pass) was introduced. That resulted in the number of inputs of the ONNX model to mismatch the numer of inputs of the pytorch model, though.

Follow up on https://github.com/pytorch/pytorch/pull/98421
Pull Request resolved: https://github.com/pytorch/pytorch/pull/100490
Approved by: https://github.com/BowenBao
","['test/onnx/dynamo/test_exporter_api.py', 'test/onnx/onnx_test_common.py', 'test/onnx/test_fx_to_onnx_with_onnxruntime.py', 'torch/onnx/_internal/exporter.py', 'torch/onnx/_internal/fx/dynamo_graph_extractor.py', 'torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py', 'torch/onnx/_internal/io_adapter.py']","Issues with InputAdapter and OutputAdapter usage across exporter. Some passes that modify input/output lead to discrepancies between the actual model and the representation in the adapters. Notably, inconsistencies between the number of inputs in the ONNX model vs the PyTorch model."
d30db9a251f343c163b38831cd0ac66448e52718,1678826316,"Replace non-reentrant checkpoint with a rewrite that can be nested and contain grad (#90105)

Changes:
- bc-breaking change: The main difference between this and the old non-reentrant impl that it replaces is that we clear recomputed tensors on backward immediately upon unpack, even if retain_graph=True. This has the following additional implications:
   - Accessing _saved_tensors multiple times will silently recompute forward multiple times.
   - Accessing ctx.saved_tensor twice in the same backward will now raise an error.
- To avoid dealing with the potential consequences, early stopping has been hidden behind a global flag that is by default False, and can be enabled via a context manager. We can remove this in a follow up. Some features of nesting as a result do not work by default.

Before land:
- import to check for more bc-breakingness
- implement any workarounds for the bc-breaking-ness, if we decide on any
- update docs to reflect new lifetime of recomputed variables
- update docs to mention the early stop feature

Follow ups:
- enable early-stopping by default
- update docs/tutorial to feature nested use cases

Related docs:
  - code comment: https://github.com/pytorch/pytorch/pull/90105/files#diff-9dcd955620b52ce128e18e3567be88edbb238810460d1288a86fabc20e483b30R448
  - design doc: https://docs.google.com/document/d/1UDLhTNv6_kvuDTRlsjfj9WdqtNaQNr8ahrvdBIB6914/edit#
  - retains_grad <> checkpiont https://docs.google.com/document/d/1maiGmuFUxysQL0AdYUU88kngAaXh_L0XpDcLDh_5Ors/edit

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90105
Approved by: https://github.com/albanD
","['test/test_autograd.py', 'test/test_utils.py', 'torch/utils/checkpoint.py']","The non-reentrant checkpoint is unable to be nested and contain gradient, causing errors and multiple re-computations when accessing saved tensors in the backward pass, even with retain_graph=True."
652592efa9949e824cc5712ae5d2b3e010926a2f,1679954065,"[inductor] use torch.prifiler in the triton wrapper (#97405)

I think it's helpful to use torch.profiler to profile the triton wrapper.

E.g., I tried it for nvidia_deeprecommender's infernece graph.

Even with max-autotune, we see the majority of the time the GPU is running 2 mm/addmm op. That's why max autotune does not help for this model since tuning does not affect the external mm ops.

<img width=""711"" alt=""Screenshot 2023-03-22 at 5 49 28 PM"" src=""https://user-images.githubusercontent.com/52589240/227072474-2f0d7205-4a10-4929-b1b7-551214788c61.png"">

next step I'll check why the triton mm kernels are not picked.

EDIT: the above screenshot is captured without max-autotune due to a typo. below is the trace with max-autotune enabled:
<img width=""712"" alt=""Screenshot 2023-03-22 at 6 43 26 PM"" src=""https://user-images.githubusercontent.com/52589240/227077624-fdccf928-be08-4211-871b-a9e3d7b76fbe.png"">

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97405
Approved by: https://github.com/ngimel
","['benchmarks/dynamo/common.py', 'torch/_inductor/codegen/wrapper.py', 'torch/_inductor/utils.py']","The Triton wrapper's performance profiling is inadequate, making it difficult to identify bottlenecks in the execution (observed in the nvidia_deeprecommender inference graph) like the overuse of certain operations like mm/addmm."
288b94a8ee44c7746eff8f8ea45741653b1304cc,1611851746,"[quant][fx] Make scale, zero_point buffers in the model, use FQN (for quantize_per_tensor ops) (#51171)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51171

Following up on previous PR, this PR makes scale and zero_point for quantize_per_tensor to be
registered as buffers in the module.
Currently the dtype is still stored as attr (not registered as buffer) since we can only register tensor types.

Test Plan:
python test/test_quantization.py test_qparams_buffers

Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D26092964

fbshipit-source-id: a54d914db7863402f2b5a3ba2c8ce8b27c18b47b
","['test/quantization/test_quantize_fx.py', 'torch/quantization/fx/quantization_patterns.py', 'torch/quantization/fx/quantize.py', 'torch/quantization/fx/utils.py']","Scale and zero_point for quantize_per_tensor are not being registered as buffers in the module, causing dtype to be stored as an attribute not a tensor type."
44b3dc4eac725f5d330f4a3f36ad46995291d2ad,1625041890,"resolve conjugate bit in `torch.testing.assert_close` (#60522)

Summary:
We need to resolve the conjugate bit for complex tensors, because otherwise we may not be able to access the imaginary component:

```python
>>> torch.tensor(complex(1, 1)).conj().imag
RuntimeError: view_as_real doesn't work on unresolved conjugated tensors.  To resolve the conjugate tensor so you can view it as real, use self.resolve_conj(); however, be warned that the resulting tensor will NOT alias the original.
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60522

Reviewed By: ngimel

Differential Revision: D29353095

Pulled By: mruberry

fbshipit-source-id: c36eaf883dd55041166f692f7b1d35cd2a34acfb
","['test/test_testing.py', 'torch/testing/_asserts.py']",Accessing the imaginary component of a conjugated complex tensor in `torch.testing.assert_close` raises a RuntimeError due to unresolved conjugate bit.
f73d9a79fe8d52be27c3c28cd93ce690bdc4f9b7,1668653013,"[torch][fx] Fix PassManager to not use a class variable mutable list (#89108)

Summary:
I found a confusing bug in the PassManager that only happens
when you instantiate one multiple times: it will use old passes and
constraints!

This occurs because the class-level declarations initialize it to an empty list,
but the problem is that class initializers only run once, and are creating class
variables. This means the same empty list was being reused every time, except
after the first time it isn't empty.

The empty list has to be created in `__init__` newly each time or else it'll be shared.
Note that this is the same type of bug as using an empty list as a default parameter, where
it'll reuse the same list pointer and not make it empty each time.

The better way to do this is with either:
* An immutable default parameter like an empty tuple, that you create a new list from: `self.passes = list(passes)`
* Use None and then create the empty list inside `__init__`

I chose the latter as it's less likely to cause a behavior change due to the changed default.

Note that for immutable values like `False` and `1` this doesn't apply as you can't mutate that
value for everyone.

Test Plan:
Added a test to ensure that the pass state is not saved.
Without my change, this test would fail as it would run all of the `2 * x` passes first,
then all of the `3 * x` passes.

Differential Revision: D41327056

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89108
Approved by: https://github.com/angelayi
","['torch/fx/passes/infra/pass_manager.py', 'torch/fx/passes/pass_manager.py', 'torch/fx/passes/tests/test_pass_manager.py']","Multiple instances of PassManager reuse old passes and constraints due to class-level declarations running only once, leading to use of a same non-empty list for all instances."
385e5ba561ff3f0fd834c935cf01d0d5f01383d5,1651557526,"ns for fx: more meaningful error message when creating shadow model (#76468)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76468

This makes the error message when copying an unsupported node more verbose.
This is useful to debug where specifically in a user model this is failing.

Test Plan:
1. hardcode this condition to hit
2. run NS tests
3. verify the exception now prints details about the offending node

Reviewed By: jerryzh168

Differential Revision: D35978652

Pulled By: vkuzo

fbshipit-source-id: 9cc93dfa46469bf6ef60aa38d4011041b6709df9
(cherry picked from commit c6e382c2a69aba6ba66740f238bc14446521a433)
",['torch/ao/ns/fx/graph_passes.py'],"Creating a shadow model with an unsupported node provides insufficient error detail, making user model debugging difficult."
65601f5ef3b86231ce886f534fbc8c1c4de9f11d,1666712635,"[ONNX] Add Support on 0d tensor Broadcast (#87211)

I am not sure if this will break things ...

Although 0d tensor is an undefined behavior in ONNX spec, I did some experiments and found that ONNX shape inference actually provides 0d as inference from 0d and 1d Op calculations, and the bug happened in Broadcast function. But still, if this breaks things really bad, I think we can put 0d tensor handling on hold, as it's not very common usage on models?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87211
Approved by: https://github.com/jcwchen, https://github.com/BowenBao
",['torch/csrc/jit/passes/onnx/shape_type_inference.cpp'],"ONNX shape inference provides 0d as inference from 0d and 1d Op calculations, leading to a bug in Broadcast function when dealing with 0d tensor, a case not clearly defined in ONNX spec."
98c8550158a4a79c4d39533a5331c5953f6ea279,1696364773,"Fix Triplet Margin Loss Opinfo (#110302)

Triplet Margin Loss takes in a Callable `distance_function` parameter which is not supported as an argument on the fx graph. See previous error:

> File ""/scratch/eellison/work/pytorch/torch/_dynamo/symbolic_convert.py"", line 562, in call_function
self.push(fn.call_function(self, args, kwargs))
File ""/scratch/eellison/work/pytorch/torch/_dynamo/variables/torch.py"", line 723, in call_function
*proxy_args_kwargs(args, kwargs),
File ""/scratch/eellison/work/pytorch/torch/_dynamo/utils.py"", line 504, in proxy_args_kwargs
f""call_function args: {typestr(*args)} {typestr(*list(kwargs.values()))}""
File ""/scratch/eellison/work/pytorch/torch/_dynamo/exc.py"", line 143, in unimplemented
raise Unsupported(msg)
torch._dynamo.exc.Unsupported: call_function args: TensorVariable() TensorVariable() TensorVariable() ConstantVariable(float) NNModuleVariable()

This is fixable by just inlining into `triplet_margin_loss` and continuing to compile it. This required support for `has_torch_function_variadic`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110302
Approved by: https://github.com/mlazos
","['test/dynamo/test_subclasses.py', 'test/inductor/test_torchinductor_opinfo.py', 'torch/_dynamo/allowed_functions.py', 'torch/_dynamo/utils.py', 'torch/_dynamo/variables/torch.py']","The Callable `distance_function` parameter in Triplet Margin Loss is not supported as an argument on the fx graph, resulting in Unsupported errors during the call function process."
64d5851b1f9f5101138586e16a58f71a69388772,1692890347,"make python decomp for native_batch_norm CompositeImplicitAutograd, remove native_batch_norm from core aten opset (#107791)

Summary:
(From Brian Hirsh)

Description copied from what I put in a comment in this PR: https://github.com/pytorch/pytorch/pull/106329

So, the slightly-contentious idea behind this PR is that lower in the stack, I updated torch._decomps.get_decomps() to check not only the decomp table to see if a given op has a decomposition available, but to also check the dispatcher for any decomps registered to the CompositeImplicitAutograd key (link: https://github.com/pytorch/pytorch/pull/105865/files#diff-7008e894af47c01ee6b8eb94996363bd6c5a43a061a2c13a472a2f8a9242ad43R190)

There's one problem though: we don't actually make any hard guarantees that a given key in the dispatcher points does or does not point to a decomposition. We do rely pretty heavily, however, on the fact that everything registered to the CompositeImplicitAutograd key is in fact a decomposition into other ops.

QAT would like this API to faithfully return ""the set of all decomps that would have run if we had traced through the dispatcher"". However, native_batch_norm is an example of an op that has a pre-autograd decomp registered to it (through op.py_impl(), but the decomp is registered directly to the Autograd key instead of being registered to the CompositeImplicitAutograd key.

If we want to provide a guarantee to QAT that they can programatically access all decomps that would have run during tracing, then we need to make sure that every decomp we register to the Autograd key is also registered to the CompositeImplicitAutograd key.

This might sound kind of painful (since it requires auditing), but I think in practice this basically only applies to native_batch_norm.

Test Plan: python test/test_decomp.py

Differential Revision: D48607575

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107791
Approved by: https://github.com/jerryzh168, https://github.com/SherlockNoMad
",['torch/_decomp/decompositions.py'],"The current design doesn't guarantee that decompositions registered to the Autograd key will also be registered to the CompositeImplicitAutograd key, causing issues when operations like native_batch_norm attempts to access all decompositions during tracing."
9f873ed7c8fa13c7f1ac6870fa21c1dec78a764f,1658216796,"[torchgen] support codegen'd C++ API for a mixture of namespaces (#81581)

Summary:
In #77710 I introduces some hack to allow static dispatch to take namespaces. After we introduced namespace into ops and kernels, we don't have to pass namespace into `static_dispatch()`; instead we will generate ops with the kernel namespace for `Functions.h`. After this diff:

If we have a yaml file looking like this:
```
- func: op_1(Tensor(a) self) -> Tensor(a)
  dispatch:
    CPU: at::op_1_kernel # ATen kernel

- func: op_2(Tensor(a) self) -> Tensor(a)
  dispatch:
    CPU: custom::op_2_kernel # custom kernel
```
`Functions.h` will contain the following C++ APIs:
```
TORCH_API inline at::Tensor & op_1(at::Tensor & self) {
  return at::cpu::op_1_kernel(self);
}

TORCH_API inline at::Tensor & op_2(at::Tensor & self) {
  return custom::cpu::op_2_kernel(self);
}
```

Test Plan: Rely on CI

Differential Revision: D37900753

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81581
Approved by: https://github.com/iseeyuan
",['torchgen/gen.py'],"Static dispatch does not properly handle a mix of namespaces, leading to incorrect generation of ops with the kernel namespace for `Functions.h` from yaml configuration."
c585d354638259f7b1906eff0b51515288a77c2a,1643852119,"CUDACachingAllocator: Keep one event queue per stream (#71745)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/71616

This fixes the leaks in my test case. I have not tested it on our big models yet, but will report back if we can.

This potentially impacts allocator performance in that it slightly increases the amount of CPU memory we allocate for data structures, and it means that `process_events` may look at a larger number of events in the case where there are multiple streams with long-running ops on them.

However, I suspect that in general, either:
- An application isn't using very many streams or very many long-running ops, in which case the performance is essentially the same
- Or, they are, which is precisely the case where https://github.com/pytorch/pytorch/issues/71616 bites you, and so freeing memory faster is probably more valuable than the slight CPU overhead here.

I'm not attached to this approach or any of its details, but figured it was worth throwing up for discussion.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/71745

Reviewed By: soulitzer

Differential Revision: D33948288

Pulled By: ngimel

fbshipit-source-id: 73e95f8a9bbe385a77de483d1c58b857b5d84e81
(cherry picked from commit d233719c072341607e6dab226b5cbfe8d316d91f)
",['c10/cuda/CUDACachingAllocator.cpp'],Memory leaks are observed in PyTorch during certain operations due to issues regarding how the CUDACachingAllocator handles event queues for multiple streams.
7b376bf844e88a8583d88772c30797ac4215c7a4,1636662535,"Remove ProcessGroup from TensorPipeAgent initialization (#68128)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68128

Reland of D31762735 (https://github.com/pytorch/pytorch/commit/0cbfd466d26b7de58e5a1ae1988f149eaa761320).

This diff was originally reverted due to failure in test_send_export_type_through_rpc_with_custom_pickler.

I updated rpc_pickler_test.py to prevent a race condition where processes were not registering their pickler before handling their rpc_sync calls.

Test Plan:
rpc_pickler_test file:

buck test mode/dev-nosan -c 'cxx.coverage_only=caffe2' //caffe2/torch/fb/training_toolkit/backend/metrics/tests:rpc_pickler_test //caffe2/torch/fb/training_toolkit/backend/metrics/collectors/fbdata_aggregator/tests:batch_collector_test -- --run-disabled --collect-coverage '--code-coverage-session=test_session' --force-tpx

rpc_pickler stress test:

buck test mode/dev-nosan -c 'cxx.coverage_only=caffe2' //caffe2/torch/fb/training_toolkit/backend/metrics/tests:rpc_pickler_test -- --exact 'caffe2/torch/fb/training_toolkit/backend/metrics/tests:rpc_pickler_test - test_send_export_type_through_rpc_with_custom_pickler (caffe2.torch.fb.training_toolkit.backend.metrics.tests.rpc_pickler_test.CythonTypeRpcSpawnTest)' --run-disabled --collect-coverage '--code-coverage-session=test_session' --force-tpx --jobs 18 --stress-runs 10 --record-results

Reviewed By: mrshenli

Differential Revision: D32316077

fbshipit-source-id: e58de2335fbaa3ab46d46fe222c659197633a5e4
","['test/cpp/rpc/test_e2e_tensorpipe.cpp', 'torch/csrc/distributed/rpc/init.cpp', 'torch/csrc/distributed/rpc/tensorpipe_agent.cpp', 'torch/csrc/distributed/rpc/tensorpipe_agent.h', 'torch/csrc/distributed/rpc/testing/faulty_tensorpipe_agent.cpp', 'torch/csrc/distributed/rpc/testing/faulty_tensorpipe_agent.h', 'torch/csrc/distributed/rpc/testing/init.cpp', 'torch/distributed/rpc/_testing/faulty_agent_backend_registry.py', 'torch/distributed/rpc/backend_registry.py']",ProcessGroup object unnecessarily present during TensorPipeAgent initialization causes non-registration of processes' pickler before handling rpc_sync calls leading to test failures.
40d3e55b7d20d03d2da5c94d8f6c12a8c64fdbfa,1663975754,"Temporary fix to skip NVIDIA driver installation from RHEL repo (#85569)

This is a temporary fix until torchrec and FBGEMM are updated to use PyTorch NVIDIA installation script instead of using the latest driver from RHEL repo.  It might take a day or so to finish updating the 2 repos, so I want to have this in place to avoid any issue with NVIDIA driver till then.  The driver from RHEL repo `515.65.01` is even newer than what we are using in PyTorch CI `515.57`.  So everything should just work with both of them
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85569
Approved by: https://github.com/clee2000
",['.github/scripts/install_nvidia_utils_linux.sh'],"Driver from RHEL repository is newer than what's used in PyTorch CI, potentially causing compatibility issues until pertinent repositories utilize PyTorch's NVIDIA installation script."
a0a23c6ef8b2e5d23d930f52dc32252dcf2861c1,1654552727,"[bazel] make it possible to build the whole world, update CI (#78870)

Fixes https://github.com/pytorch/pytorch/issues/77509

This PR supersedes https://github.com/pytorch/pytorch/pull/77510.
It allows both `bazel query //...` and `bazel build --config=gpu //...` to work.

Concretely the changes are:
1. Add ""GenerateAten"" mnemonic -- this is a convenience thing, so anybody who uses [Remote Execution](https://bazel.build/docs/remote-execution) can add a

```
build:rbe --strategy=GenerateAten=sandboxed,local
```

line to the `~/.bazelrc` and build this action locally (it doesn't have hermetic dependencies at the moment).

2. Replaced few `http_archive` repos by the proper existing submodules to avoid code drift.
3. Updated `pybind11_bazel` and added `python_version=""3""` to `python_configure`. This prevents hard-to-debug error that are caused by an attempt to build with python2 on the systems where it's a default python (Ubuntu 18.04 for example).
4. Added `unused_` repos, they purpose is to hide the unwanted submodules of submodules that often have bazel targets in them.
5. Updated CI to build //... -- this is a great step forward to prevent regressions in targets not only in the top-level BUILD.bazel file, but in other folders too.
6. Switch default bazel build to use gpu support.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78870
Approved by: https://github.com/ezyang
","['.jenkins/pytorch/build.sh', '.jenkins/pytorch/test.sh', 'aten.bzl']","Unable to build the entire repository due to issues with 'bazel query //...' and 'bazel build --config=gpu //...', along with attempts to build causing hard-to-debug errors when using default Python on certain systems.
"
03cc46a0acadcba618402a5b366f1d02bc3e21af,1629481962,"[fx2trt] Add layernorm plugin for dynamic shape (#63620)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63620

Added layernorm dynamic plugin, so that it works when explicit batch dim is required. Needed for ig model.

Changed the way of how we creating a plugin layer from instantiating the plugin directly to use plugin creator with `PluginFieldCollection`.

Follow ups:
Another way to convert layernorm is by breaking it down to supported trt layers. T97398182

Test Plan: layernorm unittest

Reviewed By: yinghai

Differential Revision: D30138205

fbshipit-source-id: aebe021d8de818e20376634f30e84579b9807f9b
",['torch/fx/experimental/fx2trt/converters/acc_ops_converters.py'],"The current implementation of the layernorm plugin is unable to handle cases with explicit batch dimensions, causing failures with certain models like the ig model."
cfc389f49624e7177cb02519f12412fb38ae6996,1646971101,"[vulkan] Enable Pytorch Vulkan to build in FBCode (#73872)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73872

This diff adds an equivalent target for [`aten_vulkan`](https://fburl.com/code/h9ybej5u) in FBCode as the `ATen-vulkan` target. This diff simply creates equivalent fbcode targets for all the xplat targets needed to build `aten_vulkan`:

The following targets in `xplat/caffe2` have had equivalent targets created in `fbcode/caffe2/aten`
* `aten_vulkan_glsl_src_path`
  * filegroup containing all Vulkan glsl files
* `gen_aten_vulkan_spv_lib`
  * python library containing script to generate vulkan spv files
* `gen_aten_vulkan_spv_bin`
  * python binary wrapping the above target
* `gen_aten_vulkan_spv`
  * genrule to execute the above python script and create C++ headers containing the SPIR-V shader code
* `generated_aten_headers_vulkan`
  * C++ library that points to the generated SPIR-V headers from above
* `aten_vulkan`
  * Contains the Pytorch Vulkan backend

FBCode targets have also been added for:
* `Vulkan-Headers` which contains Vulkan API function signatures
* `vulkan_wrapper` which loads the vulkan library
* `dotslash:glslc` which wraps the glsl compiler in a target that can be executed by genrules

Test Plan:
Try building the new `ATen-vulkan` target:

```
cd fbsource/fbcode/caffe2/aten
buck build :ATen-vulkan
```

Also tested in the next diff which tries to use this target in a Python script in FBCode.

Reviewed By: beback4u

Differential Revision: D34647445

fbshipit-source-id: 7330df1e3858c88b934b06e8e75f4fdcfa88068e
(cherry picked from commit 25251bed83e97bb9ef96a5f611c6ed72ba4219fc)
",['torch/csrc/jit/passes/vulkan_rewrite.cpp'],"Attempting to build Pytorch's Vulkan backend in FBCode ('ATen-vulkan') lacks equivalent FBCode targets for critical xplat targets, resulting in unsuccessful builds."
283ce12aa9c4ab0f734dba63007b2c6de7c655b8,1693364010,"Add channels_last3d support for mkldnn conv and mkldnn deconv (#95271)

### Motivation

- Add channels_last3d support for mkldnn conv and mkldnn deconv.
- Use `ideep::convolution_transpose_forward::compute_v3` instead of `ideep::convolution_transpose_forward::compute`.  compute_v3 uses `is_channels_last` to notify ideep whether to go CL or not to align with the memory format check of PyTorch.

### Testing
1 socket (28 cores):

- memory format: torch.contiguous_format

module | shape | forward / ms | backward / ms
-- | -- | -- | --
conv3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 64.56885 | 150.1796
conv3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3) | 100.6754 | 231.8883
conv3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 19.31751 | 68.31131

module | shape | forward / ms | backward / ms
-- | -- | -- | --
ConvTranspose3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 122.7646 | 207.5125
ConvTranspose3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3) | 202.4542 | 368.5492
ConvTranspose3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 122.959 | 84.62577

- memory format: torch.channels_last_3d

module | shape | forward / ms | backward / ms
-- | -- | -- | --
conv3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 40.06993 | 114.317
conv3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3 | 49.08249 | 133.4079
conv3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 5.873911 | 17.58647

module | shape | forward / ms | backward / ms
-- | -- | -- | --
ConvTranspose3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 88.4246 | 208.2269
ConvTranspose3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3 | 140.0725 | 270.4172
ConvTranspose3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 23.0223 | 37.16972

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95271
Approved by: https://github.com/jgong5, https://github.com/cpuhrsch
","['aten/src/ATen/native/ConvUtils.h', 'aten/src/ATen/native/Convolution.cpp', 'aten/src/ATen/native/mkldnn/Conv.cpp', 'test/test_mkldnn.py', 'test/test_mkldnn_fusion.py']",Lack of channels_last3d support in the MKLDNN convolution and deconvolution can lead to inefficient compute and memory usage.
95c3e2370a9cf03c55b3ce2d97f46231ed7264c8,1659462246,"[mergebot] Amend messages for land checks (#82649)

### Description
<!-- What did you change and why was it needed? -->
We forgot that the < was for comments in markdown. Also added a link to the wiki to the start land checks message so users can see why their PR is taking extra time to land.

### Issue
<!-- Link to Issue ticket or RFP -->
n/a

### Testing
<!-- How did you test your change? -->
n/a
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82649
Approved by: https://github.com/janeyx99, https://github.com/ZainRizvi
",['.github/scripts/trymerge.py'],"Markdown symbols are misinterpreted in mergebot's messages for land checks, obscuring important information and preventing clear communication with users."
1a51efd8bbdeec6cead5bac1212612bd039ecd0c,1660174922,"dispatch API for checking computed table, use it in prim decomps (#82358)

Fixes https://github.com/pytorch/pytorch/issues/82331

Expose a `torch._C._dispatch_has_computed_kernel_for_dispatch_key` to check if an operator has a kernel registered to the given dispatch key in the **computed table**.

Use it in the prim registration logic, making it more accurate and robust (so that it e.g. picks up `CompositeExplicitAutograd` kernels.

It looks like before this change we'd register 134 prim ops to the meta key, and after we only register 62. So that's 72 ops that now use an existing C++ decomp to get meta working, instead of going directly through the prim decomp.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82358
Approved by: https://github.com/ezyang
","['aten/src/ATen/core/dispatch/Dispatcher.h', 'aten/src/ATen/core/dispatch/OperatorEntry.cpp', 'aten/src/ATen/core/dispatch/OperatorEntry.h', 'aten/src/ATen/native/LinearAlgebra.cpp', 'aten/src/ATen/native/layer_norm.cpp', 'torch/_decomp/__init__.py', 'torch/csrc/utils/python_dispatch.cpp']","Operator kernels in computed table are not being accurately registered to proper dispatch keys, causing inaccurate and unstable prim registration logic, alongside excessive and unnecessary registration of operations to the meta key."
8dbf6ae8fa97fa56ceeac106557ae1a170b82e17,1619481798,"ns for fx: handling for user functions in weight and unshadowed act APIs (#56292)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56292

Adds hooks for specifying user defined functions to NS weight and
unshadowed activation APIs.

Adding it to shadowed activation APIs will be a bit more work, upcoming
in a separate PR.

Test Plan:
```
python test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function
```

Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D27830409

fbshipit-source-id: 6bbddc3062c0b3e412a3147244795319c0785a92
","['test/quantization/test_numeric_suite_fx.py', 'torch/quantization/_numeric_suite_fx.py']",Missing hooks for user-defined functions in numeric suite (NS) weight and unshadowed activation APIs can be problematic.
f2582a59d0835323ebf143726ea79ba52e7cceff,1634925107,"[SR] Add rvalue overload for operator() (#66648)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66648

Currently, SR shallow-copies its `IValue` inputs when running inferences. We can avoid refcount bumps by `std::move`-ing the inputs into their slots. To achieve this, I've made the following changes:

1. Add an overload for `set_inputs` that takes a `std::vector<IValue>&&`.
2. Change the signatures of `StaticModule::operator()` and `StaticRuntime::operator()`.
Old:
```
operator()(const std::vector<IValue>& args, const std::unordered_map<std::string, IValue>& kwargs)
```
New:
```
template <class IValueList>
operator()(IValueList&& args, const std::unordered_map<std::string, IValue>& kwargs)
```

The implementations use perfect forwarding to invoke the correct overload of `set_inputs`.

Test Plan: Added a short new unit test to exercise the new code path. All other unit tests still pass.

Reviewed By: hlu1

Differential Revision: D31659973

fbshipit-source-id: b8c194405b54a5af1b418f8edaa1dd29a061deed
","['benchmarks/static_runtime/test_static_module.cc', 'benchmarks/static_runtime/test_static_runtime.cc', 'torch/csrc/jit/runtime/static/impl.cpp', 'torch/csrc/jit/runtime/static/impl.h']","SR copies `IValue` inputs resulting in unnecessary refcount bumps during inference, potentially impacting performance."
89a145fd91bb6970b331efb121db8a5dca3ef117,1638229409,"Sparse CSR CUDA: Add torch.sparse.sampled_addmm (#68007)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68007

This PR adds a new function to the sparse module.
`sampled_addmm` computes α*(A @ B) * spy(C) + β*C, where C is a sparse CSR matrix and A, B are dense (strided) matrices.
This function is currently restricted to single 2D matrices, it doesn't support batched input.

cc nikitaved pearu cpuhrsch IvanYashchuk

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D32435799

Pulled By: cpuhrsch

fbshipit-source-id: b1ffac795080aef3fa05eaeeded03402bc097392
","['aten/src/ATen/cuda/CUDASparse.h', 'aten/src/ATen/native/sparse/cuda/SparseBlas.cpp', 'aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp', 'aten/src/ATen/native/sparse/cuda/SparseBlasImpl.h', 'test/test_sparse_csr.py', 'torch/sparse/__init__.py']","The PyTorch sparse module is lacking a function to efficiently compute α*(A @ B) * spy(C) + β*C, where C is a sparse CSR matrix and A, B are dense strided matrices. Current methods do not support single 2D non-batched matrices."
a5895f85be0f10212791145bfedc0261d364f103,1633512224,"[PyTorch Edge][type] Add type check in compatibility api (#63129)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63129

1. Add an api to get `supported_types` from runtime, expose in c++ only.
2. Add an api to get `contained_types` from model, expose in both c++ and PyThon.
3. Add a field `contained_types_` in `type_parser.cpp` to track the contained types when parsing python string.
4. Expand `is_compatible` api to check type. When checking type, it will check the contained type list from the model with the support type list from runtime.
5. Expand the unittest for compatibility to cover type
6. Add unit test in python to check type list
ghstack-source-id: 139826944

Test Plan:
```
buck test mode/dev //caffe2/test/cpp/jit:jit -- --exact 'caffe2/test/cpp/jit:jit - LiteInterpreterTest.GetContainTypes'

buck test mode/dev //caffe2/test/cpp/jit:jit -- --exact 'caffe2/test/cpp/jit:jit - LiteInterpreterTest.isCompatibleSuccess'
buck test mode/dev //caffe2/test/cpp/jit:jit -- --exact 'caffe2/test/cpp/jit:jit - LiteInterpreterTest.isCompatibleFail'

buck test //caffe2/test:mobile
```

Reviewed By: iseeyuan

Differential Revision: D30231419

fbshipit-source-id: 8427f423ec28cc5de56411f15fd960d8595d6947
","['test/cpp/jit/test_lite_interpreter.cpp', 'test/mobile/test_bytecode.py', 'torch/csrc/jit/mobile/model_compatibility.cpp', 'torch/csrc/jit/mobile/model_compatibility.h', 'torch/csrc/jit/mobile/runtime_compatibility.cpp', 'torch/csrc/jit/mobile/runtime_compatibility.h', 'torch/csrc/jit/mobile/type_parser.cpp', 'torch/csrc/jit/mobile/type_parser.h', 'torch/csrc/jit/python/script_init.cpp', 'torch/jit/mobile/__init__.py']","The compatibility API does not currently check for type compatibility between the model's contained types and the runtime's supported types, potentially leading to compatibility issues during execution."
ba26bc0fc266ddb58ec199349d2c93c7a905dfd0,1667250676,"Fix random ""C1041: cannot open program database"" errors when compiling on Windows (#88084)

Adds `/FS` option to `CMAKE_CXX_FLAGS` and `CMAKE_CUDA_FLAGS`.

So far I've encountered this kind of errors:

```
C:\Users\MyUser\AppData\Local\Temp\tmpxft_00004728_00000000-7_cuda.cudafe1.cpp: fatal error C1041: cannot open program database 'C:\Projects\pytorch\build\third_party\gloo\gloo\CMakeFiles\gloo_cuda.dir\vc140.pdb'; if multiple CL.EXE write to the same .PDB file, please use /FS
```
when building with VS 2022.

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm

Related issues:
- https://github.com/pytorch/pytorch/issues/87691
- https://github.com/pytorch/pytorch/issues/39989
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88084
Approved by: https://github.com/ezyang
",['setup.py'],"Random ""C1041: cannot open program database"" errors occur when compiling on Windows. These appear frequently when building with VS 2022."
3c5a825f3c0f706c9d75e18307ccaf220524ecd1,1681338170,"[AOTAutograd] Fix is-duplicate check in de-dup guard logic (#98932)

**Context**
The existing check to see if an arg is duped is `if dupe_arg_pos != kept_pos:`. However, this incorrectly considers every arg after a true duped arg to also be a duped arg.

Consider `flat_args = [a, b, b, c]`, where indices `1` and `2` are duped.
- `add_dupe_map = {0: 0, 1: 1, 2: 1, 3: 2}`
- For `dupe_arg_pos=2, kept_pos=1`, `2 != 1`, so the check correctly identifies the second `b` to be a duped arg.
- For `dupe_arg_pos=3, kept_pos=2`, `3 != 2`, so the check incorrectly identifies the `c` to be a duped arg.

Indeed, if there were more args like `[a, b, b, c, d, e, ...]`, every arg after the second `b` will be considered a duped arg since its `kept_pos` will always be 1 lower than its `dupe_arg_pos`.

**Overview**
This PR changes `add_dupe_map` to be implemented as a `List[int]`, where the list index implicitly represents the `dupe_arg_pos` and the list element represents the `kept_pos`. We use a list to have stable in-order iteration and because we know the keys to be in `{0, 1, ..., len(flat_args) - 1}`.

With `add_dupe_map` as a list, the `is_dupe_arg` condition is whether the entry in `add_dupe_map` shows a new not-yet-seen index in the iteration. One way to do this is to count the number of unique args so far and compare against that.

This closes https://github.com/pytorch/pytorch/issues/98883, where now the guards change from
```
GUARDS ___guarded_code.valid
and ___check_type_id(L['self'], 93996836333040)
and ___check_obj_id(L['self'], 140119034997536)
and not ___are_deterministic_algorithms_enabled()
and ___check_tensors(L['x'])
and L['self']._buf is L['self']._buf_module._buf
and L['self']._buf_module._buf is L['self']._param
```
to without the final incorrect `L['self']._buf_module._buf is L['self']._param` guard.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98932
Approved by: https://github.com/ezyang
","['test/distributed/test_dynamo_distributed.py', 'torch/_functorch/aot_autograd.py']","The de-dup check mechanism incorrectly identifies all arguments past an actual duplicate as duplicates too, causing inappropriate guard conditions to be generated."
9ed49449b3f70df673b27f4d170107299cc492b2,1637019548,"[SR] Add net level record functions (#68091)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68091

Add record functions for recording perf stats on the entire network.

Note that this is backed by the same pre-sampling mechanism as the op record functions, so net level stats get logged relatively infrequently. (If this is not acceptable, we can not use pre-sampling at the cost of a little bit of perf, every inference will require an RNG call)

Reviewed By: hlu1

Differential Revision: D32296756

fbshipit-source-id: 09ff16c942f3bfc8f4435d6cca2be4a6b8dc6091
","['torch/csrc/jit/runtime/static/impl.cpp', 'torch/csrc/jit/runtime/static/impl.h']","Entire network performance stats are not recorded, and op record functions use a pre-sampling mechanism which logs stats infrequently, potentially affecting performance accuracy."
d4f8a6d05b4054989beac98915c6c41839e9d324,1656379820,"[ao] Implemented report generation for InputWeightEqualization Detector (#80191)

Summary: This adds the implementation for the InputWeightEqualization
detector. This includes both the implementation and the relavent test
cases. This detector is meant to be added to initialize a ModelReport
instance and it will keep track of the necessary statistics to decide if
for certain layers of interest (linear and conv for now), it makes sense
to use input weight equalization and gives the suggestion to the user.

This includes the implementation and subsequent tests for the report
generation functionality of the detector. The full detector should now
be fleshed out and complete with this addition. This included
modifications to the ModelReportObserver class as well to capture min
and max per channel values. In addition, instead of passing in the
observer class to instantiate, the detectors now pass the ModelReport
instance the observer instance that they themselves instantiate.

Test Plan: python test/test_quantization.py TestFxDetectInputWeightEqualization

Reviewers:

Subscribers:

Tasks:

Tags:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80191
Approved by: https://github.com/HDCharles, https://github.com/andrewor14
","['test/quantization/fx/test_model_report_fx.py', 'torch/ao/quantization/fx/_model_report/detector.py', 'torch/ao/quantization/fx/_model_report/model_report.py', 'torch/ao/quantization/fx/_model_report/model_report_observer.py']","The InputWeightEqualization detector lacks implementation and testing, and cannot maintain necessary statistics or provide suggestions for using input weight equalization on certain layers (linear and conv for now)."
17fec516fe658ad96179039907d6e7d6d7b1d7eb,1683664259,"[Vulkan] Test conv2d after division (#100910)

Summary: This tests running a conv2d with clamp after dividing the input tensor by another tensor. Both tensors have number channels = 3 (i.e. not a multiple of 4) and therefore, the channel dimension was padded. Hence, we are testing our divide-by-zero fix (D44392406)

Test Plan:
```
buck run --target-platforms ovr_config//platform/macos:arm64-fbsource -c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_api_test_binAppleMac\#macosx-arm64 -- --gtest_filter=""VulkanAPITest.conv2d_clamp_after_div""
```

Reviewed By: SS-JIA

Differential Revision: D44550026

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100910
Approved by: https://github.com/SS-JIA
",['aten/src/ATen/test/vulkan_api_test.cpp'],"Conv2d operation with clamp after dividing an input tensor by another could cause an issue when tensors have number channels = 3, potentially leading to divide-by-zero errors."
bf09ece782921933e88ca6f69677db50e873fb88,1643858482,"Make svd / svdvals fully functorch compatible (#72181)

Summary:
This should (hopefully) make all the CI from `functorch` go green (including jvp's!) after changing `VARIADIC_BDIMS_BOXED(_svd_helper);` with `VARIADIC_BDIMS_BOXED(_linalg_svd);` and removing all the skip and xfails associated to `linalg.svdvals`.

Locally, there's just one test that started failing because of this, and that is `test_vmapjvpall_norm_nuc_cpu_float32`. I have no idea what's going on here, but it's a jvp product, so not a regression, and it might very well be caused by the jvp of other operation within `norm_nuc` as this is a composite operation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/72181

Reviewed By: ngimel

Differential Revision: D33952744

Pulled By: zou3519

fbshipit-source-id: 2a2510d97eed4a0bfc25615264ddd36e38856efe
(cherry picked from commit 5805fa107c3a91c58f8ecc9778cfc87aa7f64233)
","['aten/src/ATen/native/BatchLinearAlgebra.cpp', 'torch/csrc/autograd/FunctionsManual.cpp']","The svd and svdvals operations are not fully compatible with functorch, resulting in failures in the test_vmapjvpall_norm_nuc_cpu_float32."
be17d6eadf5db7b5742e50c8de71f0e652771d29,1627312472,"Add default Saved Variable hooks (#61834)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61834

Expose a pair of functions to Python users: torch.autograd.graph.set_saved_tensors_default_hooks(pack, unpack) and torch.autograd.graph.reset_saved_tensors_default_hooks().
These functions control the hooks applied to saved tensors: all tensors saved in that context will be packed using the pack function, then unpacked accordingly when needed.

Currently, this works by simply calling register_hooks (cf #60975) directly at the end of the constructor of a SavedVariable. This could be optimized further by not performing the copy before registering default hooks, but this would require a small refactor. Edit: the refactor is done in #61927.

A current limitation is that if users create tensors in this context, they will not be able to register additional hooks on the saved tensor.

For instance, to perform something like #28997, one could define a pack function that saves to disk whenever the tensor size is too big and returns a filename, then unpack simply reads the content of the file and outputs a tensor, e.g.:

```
def pack(x):
    name = os.path.join(tmp_dir, str(uuid.uuid4()))
    torch.save(x, name)
    return name

def unpack(name):
    return torch.load(name)
```

Test Plan: Imported from OSS

Reviewed By: zou3519

Differential Revision: D29792193

Pulled By: Varal7

fbshipit-source-id: 33e931230ef59faa3ec8b5d11ef7c05539bce77c
","['test/test_autograd.py', 'torch/autograd/__init__.py', 'torch/autograd/saved_variable_default_hooks.py', 'torch/csrc/autograd/engine.h', 'torch/csrc/autograd/init.cpp', 'torch/csrc/autograd/python_engine.cpp', 'torch/csrc/autograd/python_engine.h', 'torch/csrc/autograd/python_saved_variable_hooks.cpp', 'torch/csrc/autograd/python_saved_variable_hooks.h', 'torch/csrc/autograd/saved_variable.cpp']","Default hooks for Saved Variables are not exposed to Python users. This limits the ability for users to control or modify the behaviour of these hooks, specifically for saved tensors within this context."
6db8f7a70920f91418078fe09477eed0b0adefdb,1630571783,"Fix TRTModule not adding outputs in order (#64418)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64418

In T99368564, we found that when running TRT lowered module, the output tensors are out-of-order, as compared to the output from the original, non-lowered module. It turns out that in `TRTModule.forward()`, we cannot rely on `ICudaEngine` bindings natural order indices to create the output tensors, but rather, we should explicitly construct the output tensor from the bindings' names, in an ordered that we supply.

Test Plan:
* Arc lint
* Run CI/sandcastle tests
* Run GPU lowering using commands and code changes in D30171741 and ensure we don't observe out-of-order outputs

Reviewed By: yinghai

Differential Revision: D30693545

fbshipit-source-id: 32a894ceeb148fcf4e8d279be3835c7d1f1aa2ba
",['torch/fx/experimental/fx2trt/fx2trt.py'],"TRT lowered module is generating output tensors in incorrect order when compared with the output from the original, non-lowered module."
13ad4739a6e9402e2039a1ce521b9aed595760b3,1658992891,"[quant] Implement PTQ for APoT FakeQuant (#81040)

### Summary:
This PR implements PTQ for APoT FakeQuant. It runs models (Resnet-18 pre-trained model, ImageNet dataset) to compare accuracy metrics for different qconfig settings of uniform vs. APoT quantized activation and weight.

According to the collected accuracy stats, model #2 (uniform activation and APoT weight) appears to have a slight improvement in accuracy compared to model #1 (uniform activation and uniform weight) for 8-bit and significant improvement for 4-bit (see ""Accuracy Stats"" section below).

### Test Plan:
Run models with: `python test/quantization/core/experimental/fx_graph_mode_apot.py`

### Accuracy Stats:
8-bit (Uniform int8, APoT b = 8 k = 2)

**Model #1:** Uniform activation, uniform weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 64.43% (Top-1), 85.62% (Top-5)

**Model #2:** Uniform activation, APoT weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 64.51% (Top-1), 85.78% (Top-5)

**Model #3:** APoT activation, APoT weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 64.32% (Top-1), 85.78% (Top-5)

4-bit (Uniform int4, APoT b = 4 k = 2)

**Model #1:** Uniform activation, uniform weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 45.63% (Top-1), 71.96% (Top-5)

**Model #2:** Uniform activation, APoT weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 64.24% (Top-1), 85.56% (Top-5)

**Model #3:** APoT activation, APoT weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 45.40% (Top-1), 76.21% (Top-5)

**Full Precision model (FX Graph Mode quantized)**
Evaluation accuracy on test dataset: 69.76% (Top-1), 89.08% (Top-5)

**Eager mode quantized model**
Evaluation accuracy on test dataset: 69.49% (Top-1), 88.90% (Top-5)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81040
Approved by: https://github.com/jerryzh168
","['test/quantization/core/experimental/fx_graph_mode_apot.py', 'test/quantization/core/experimental/test_fake_quantize.py', 'torch/ao/quantization/experimental/apot_utils.py', 'torch/ao/quantization/experimental/fake_quantize.py', 'torch/ao/quantization/experimental/observer.py', 'torch/ao/quantization/experimental/qconfig.py', 'torch/ao/quantization/experimental/quantizer.py']","There is no current implementation of Post Training Quantization (PTQ) for APoT FakeQuant, leading to unsatisfactory accuracy metrics when comparing different quantization configurations in some pre-trained models."
a3fc3531f514d4c01de9c4a60f978d704d615494,1680570798,"Add all_reduce_coalesced functional collective (#97157)

Inductor codegen is suboptimal when calling all_reduce_coalesced with input args. We need to fix inductor's calling convention for that, or something else.

Might not work if any outputs is unused.

Test code:

```python
import torch
import torch.distributed as dist
import torch.nn.functional as F
from functorch import make_fx
import os

import torch.distributed._functional_collectives as ft_c
from torch.testing._internal.common_distributed import (
    spawn_threads_and_init_comms,
)
from torch._inductor.compile_fx import compile_fx_inner

def my_fun(a, b):
    c = a * 3
    tensors = ft_c.all_reduce_coalesced([a, c, b], ""sum"", [0])
    return ((tensors[1] + tensors[0] + tensors[2]).sum(), )

@spawn_threads_and_init_comms(world_size=1)
def inductor_main(self):

    x = torch.arange(4).cuda() * (dist.get_rank() + 1)
    y = torch.arange(4).cuda() * (dist.get_rank() + 1)
    x = x.to(torch.float)
    y = y.to(torch.float) * 0.5
    res = make_fx(my_fun)(x, y)
    print(f""fx graph:\n{res.graph}"")
    ind = compile_fx_inner(res, [x, y])
    print(f""inductor done:\n{ind}"")

os.environ[""PROXY_TENSOR_TRACING""] = ""1""
os.environ[""TORCH_COMPILE_DEBUG""] = ""1""
torch._dynamo.config.output_code = True

if __name__ == ""__main__"":
    inductor_main(None)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97157
Approved by: https://github.com/fegin
","['aten/src/ATen/native/Collectives.cpp', 'test/distributed/test_functional_api.py', 'tools/autograd/gen_variable_type.py', 'torch/_inductor/ir.py', 'torch/_inductor/lowering.py', 'torch/_inductor/scheduler.py', 'torch/_meta_registrations.py', 'torch/distributed/_functional_collectives.py', 'torch/testing/_internal/distributed/multi_threaded_pg.py']","Suboptimal codegen in Inductor when calling all_reduce_coalesced with input arguments, potentially failing if any output is unused."
46b83f66ece4083589f05c58bd8a9f4917af1206,1658949075,"functionalization: fix striding for out-of-place copy() (#82009)

Previously we'd implemented `at::native::copy()` using `expand()` for efficiency, but doing it that way actually prevents `copy()` from following the same semantics as `copy_()`.

The output of both `copy_()` and `copy()` should have the same amount of storage as the original tensor, so that you can use it in the same set of operations.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82009
Approved by: https://github.com/ezyang
",['aten/src/ATen/native/Copy.cpp'],"The `copy()` function doesn't maintain the same amount of storage as the original tensor, breaking symmetry with `copy_()` and limiting its usability in certain operations."
13640bf9253ed2f451ab7f059ea8f692f36fa48e,1684607952,"disableing quantizing gradient in 8bw (#101739)

Summary:
Quantizing a *gradient* is not applicable to complex ASR model.

Gradient in INT8
f438266519
Gradient in FP32
f438109197
Clearly two WER shows the limitation with quantizing a gradient.

As of now, we are okay with simply enabling quantized backpropagation but computing gradient in FP32.
It already saves a memory due to model size.

Test Plan: Signals

Differential Revision: D45965552

Pull Request resolved: https://github.com/pytorch/pytorch/pull/101739
Approved by: https://github.com/izaitsevfb
",['torch/csrc/quantized/quantized_backward.cpp'],"Quantizing the gradient for complex ASR models seems to be causing limitations, particularly relating to the Word Error Rate (WER)."
59b8d5be7405c6f8a445b504b73a7e8c7812e860,1688660539,"[inductor] Split ops.reduction into reduction and store_reduction (#102737)

This is intended as a first step towards reductions with multiple outputs. This
also incidentally improves CSE of reductions under C++ codegen. For example,
```python
def fn(x):
    return torch.argmin(x, dim=-1), torch.argmin(x, dim=-1)
```

Currently this generates two reductions, where the common load is CSEd
```cpp
for(long i1=static_cast<long>(0L); i1<static_cast<long>(10); i1+=static_cast<long>(1L))
{
    auto tmp0 = in_ptr0[static_cast<long>(i1 + (10L*i0))];
    if (tmp_acc0.value > tmp0) {
        tmp_acc0.index = i1; tmp_acc0.value = tmp0;
    }
    if (tmp_acc1.value > tmp0) {
        tmp_acc1.index = i1; tmp_acc1.value = tmp0;
    }
}
auto tmp1 = tmp_acc0.index;
out_ptr0[static_cast<long>(i0)] = tmp1;
auto tmp2 = tmp_acc1.index;
out_ptr1[static_cast<long>(i0)] = tmp2;
```

but with this change it gets CSEd to a single accumulator

```cpp
for(long i1=static_cast<long>(0L); i1<static_cast<long>(10L); i1+=static_cast<long>(1L))
{
    auto tmp0 = in_ptr0[static_cast<long>(i1 + (10L*i0))];
    if (tmp_acc0.value > tmp0) {
        tmp_acc0.index = i1; tmp_acc0.value = tmp0;
    }
}
auto tmp1 = tmp_acc0.index;
out_ptr0[static_cast<long>(i0)] = tmp1;
out_ptr1[static_cast<long>(i0)] = tmp1;
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102737
Approved by: https://github.com/jgong5, https://github.com/lezcano
","['torch/_inductor/codegen/common.py', 'torch/_inductor/codegen/cpp.py', 'torch/_inductor/codegen/triton.py', 'torch/_inductor/dependencies.py', 'torch/_inductor/ir.py', 'torch/_inductor/sizevars.py']",Reductions with multiple outputs aren't supported and duplicate reductions aren't optimized into a single accumulator in C++ codegen.
2fb328eb46a6fa71bfb38d457959887eeb41deda,1674239019,"[Dynamo] Preserve source_fn in node.meta (#92399)

Sample value from the test case `test_export_with_stack_trace`

node.target | node.meta[""source_fn""]
-- | --
aten.randn.default | <built-in method randn of type object at 0x7f8683263108>
aten.t.default | < built-in function linear >
aten.mm.default | < built-in function linear >
aten.cos.default | <built-in method cos of type object at 0x7f8683263108>
aten.relu.default | relu
aten.add.Tensor | < built-in function add >

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92399
Approved by: https://github.com/jerryzh168, https://github.com/yanboliang
","['test/dynamo/test_export.py', 'torch/_dynamo/output_graph.py']","'node.meta[""source_fn""] is not being preserved in Dynamo, disrupting traceability and consistency in test cases like `test_export_with_stack_trace`.'"
c4f718cb723987b73157b833e7f515b4369d65b2,1625030460,"[nnc] Serialize initialization of LLVM targets (#60996)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/60996

We've had a bug report of weird LLVM initialization errors, e.g.,
```
Unexpected failure in LLVM JIT: Cannot choose between targets ""x86-64"" and ""x86-64""
```

While I haven't repro'ed that exact message, I did run a stress-test that
compiles on many threads simultaneously, and it deadlocks in
TargetRegistry::lookupTarget.  And in fact I remember debugging this before in
a different system, and finding ""Clients are responsible for avoid race
conditions in registration"" in
https://llvm.org/doxygen/TargetRegistry_8cpp_source.html.

So yeah, let's lock this thing.
ghstack-source-id: 132719018

Test Plan: Heavy multithreaded compilation.  Not sure if it's suitable for landing.

Reviewed By: ZolotukhinM

Differential Revision: D29471343

fbshipit-source-id: b495e468b57e77796a08b627884d3efeca2d1f7c
",['torch/csrc/jit/tensorexpr/llvm_codegen.cpp'],Simultaneous LLVM target initialization across multiple threads results in deadlock during compilation and reported unexpected LLVM JIT failures.
774ae0851d98829b412e46dde85e716dad065a06,1629983198,"[OpInfo] Added ReductionOpInfo subclass of OpInfo and ported sum test (#62737)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62737

ReductionOpInfo is a specialization of OpInfo for reduction operators. For now, it is designed to work with reductions that return a single tensor and that reduce all elements along one or more dimensions to a single value. In particular this excludes operators such as `max` and `min` that return multiple tensors and `quantile` that can return multiple values.

fixes https://github.com/pytorch/pytorch/issues/49746

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D30406568

Pulled By: heitorschueroff

fbshipit-source-id: 218b1da1902f67bcf4c3681e2a0f0029a25d51f1
","['test/test_ops.py', 'test/test_reductions.py', 'torch/testing/_internal/common_methods_invocations.py']",Lack of specific OpInfo subclass for reduction operators in PyTorch leading to insufficient testing coverage for cases with single tensor return type and reduction along dimensions.
09157c76c04546f9f55b15a37dda938d6327df8a,1661175767,"[Static Runtime] Add schema checks for aten::list (#83753)

Summary:
The previous implementation assumed that there was only one overload and unconditionally tried to convert its input into a string. Some users were running into crashes because of this. Added a new overload for the list overload and schema checks.

Also, I managed to uncover another bug when writing tests for this case (yikes). Returning inputs didn't work because the input cleanup process would destroy the output. Extended `CreateOwnedRefsForSpecialIValues` to fix that.

Test Plan: CI + new unit tests

Differential Revision: D38870803

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83753
Approved by: https://github.com/tenpercent, https://github.com/albanD
","['benchmarks/static_runtime/test_static_runtime.cc', 'torch/csrc/jit/runtime/static/native_ops.cpp', 'torch/csrc/jit/runtime/static/passes.cpp']","The previous implementation of aten::list incorrectly assumed a single overload, leading to potential crashes when trying to convert its input into a string. Additionally, returning inputs is malfunctioning as the input cleanup process destroys the output."
353c2e7d39c2c4d0c3e1b8c4d7338e19c7b02f57,1671155650,"[Quant] Add fused LinearLeakyReLU module for onednn backend (#88661)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `QLinearLeakyReLU` module for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this module with other quantization backends otherwise an error is thrown.

**Test plan**
python test_quantization.py TestStaticQuantizedModule

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88661
Approved by: https://github.com/jgong5, https://github.com/jerryzh168
","['test/quantization/core/test_quantized_module.py', 'torch/ao/nn/intrinsic/__init__.py', 'torch/ao/nn/intrinsic/modules/__init__.py', 'torch/ao/nn/intrinsic/modules/fused.py', 'torch/ao/nn/intrinsic/quantized/__init__.py', 'torch/ao/nn/intrinsic/quantized/modules/__init__.py', 'torch/ao/nn/intrinsic/quantized/modules/linear_relu.py']",Absence of a fused `QLinearLeakyReLU` module for onednn backend results in more data movement and lower inference performance in int8 inference. Using this module with other quantization backends results in an error.
e6fd8ca3eef2b85b821936829e86beb7d832575c,1689883150,"Fix test failure in TestCudaMultiGPU.test_cuda_device_memory_allocated (#105501)

The test

https://github.com/pytorch/pytorch/blob/f508d3564c8a472ba2f74878dbdf67f09eaae2d3/test/test_cuda_multigpu.py#L1282-L1290

Torch cuda caching allocator may cache the allocation and cause the ""new_alloc"" being the same as the ""old_alloc"".
```python
     self.assertGreater(memory_allocated(0), current_alloc[0])
```

I suggest that we use `assertGreaterEqual` instead of `assertGreater` in the test.

Individually running only this test does not make it fail but running it together with other tests from the same test module will make it fail.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/105501
Approved by: https://github.com/zou3519
",['test/test_cuda_multigpu.py'],"TestCudaMultiGPU's test_cuda_device_memory_allocated is inconsistent and fails when run with other tests in the module due to caching causing ""new_alloc"" to be the same as ""old_alloc""."
91c7015426f6b57b87bd92a63e9c08d9fd46a020,1665815030,"[einsum] Fix opt_einsum defaults to be more reasonable (#86985)

Fixes the confusing situation mentioned here https://github.com/pytorch/pytorch/issues/85224#issuecomment-1278628262 by

- setting better OG defaults
- changing warnings to errors now that we have better defaults

Test plan:
- Ran einsum tests locally + CI
- Uninstalled opt-einsum and ran through setting
     - `enabled` to False (doesn't throw error)
     - `strategy` to anything that's not None (errors)
     - `strategy` to None (noops)
- Installed opt-einsum and ran through setting
     - `enabled` to False (doesn't throw error)
     - `enabled` to True (doesn't throw error, no ops + defaults to 'auto')
     - `strategy` to random string (errors)
     - `strategy` to None (noops, still is 'auto')
     - `strategy` to 'greedy' (is set to 'greedy')
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86985
Approved by: https://github.com/soulitzer
",['torch/backends/opt_einsum/__init__.py'],The einsum function with opt_einsum enables confusing and potentially faulty outcomes due to flawed original defaults and no error messages for improper strategy settings.
0142fd0b5798f1d21960ef022097ac401445d382,1620196799,"[JIT][NNC] add hardswish symbolic gradient and NNC lowering (#57383)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57383

Notes: I picked up an activation from https://github.com/pytorch/pytorch/issues/56969. You can look at the [activations.cpp](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/Activation.cpp#L429) file which has both forward and backward kernel code to help you write the NNC lowering and the symbolic gradient.

I added a test in test_jit_fuser_te for the fusion, and I added an OpInfo and asserted that we expect to see autodiffable nodes to test the symbolic gradient.

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D28197820

Pulled By: eellison

fbshipit-source-id: 05305d85c5bb0847c8f911b95ba47b137dca7e90
","['test/test_fx.py', 'test/test_jit_fuser_te.py', 'test/test_ops.py', 'torch/csrc/jit/passes/tensorexpr_fuser.cpp', 'torch/csrc/jit/runtime/symbolic_script.cpp', 'torch/csrc/jit/tensorexpr/kernel.cpp', 'torch/testing/_internal/common_methods_invocations.py']",Hardswish activation lacks symbolic gradient and NNC lowering support in JIT which prevents optimal processing of autodiffable nodes and fusion operations.
674d18c12410ac51fe573c6d699a4f2d37cf8a37,1686718759,"inductor: using int64 as index dtype for slice_scatter (#103511)

For the given test case from HF AllenaiLongformerBase, there has an accuracy issue for the dynamic shape case, the reason is that we are using int32 as the index type but there has a default value ```9223372036854775807```  out of range of int32, see the IR:
```
def masked_subblock1(self, ops):
    get_index = self.get_index('index1')
    index_expr = ops.index_expr(get_index, torch.int32)
    get_index_1 = self.get_index('index2')
    index_expr_1 = ops.index_expr(get_index_1, torch.int32)
    ge = ops.ge(index_expr, index_expr_1)
    get_index_2 = self.get_index('index1')
    index_expr_2 = ops.index_expr(get_index_2, torch.int32)
    constant = ops.constant(9223372036854775807, torch.int32)
    lt = ops.lt(index_expr_2, constant)
    and_ = ops.and_(ge, lt)
    masked_subblock2 = self.masked_subblock2(and_, 0.0)
    get_index_3 = self.get_index('index4')
    load = ops.load('arg4_1', get_index_3)
    where = ops.where(and_, masked_subblock2, load)
    return where
```
and the CPU codegen will generate the cpp code according to the node type:
```
auto tmp3 = [&]
{
    auto tmp4 = static_cast<int>(i3);
    auto tmp5 = static_cast<int>(ks2);
    auto tmp6 = tmp4 >= tmp5;
    auto tmp7 = static_cast<int>(9223372036854775807);
    auto tmp8 = tmp4 < tmp7;
    auto tmp9 = tmp6 & tmp8;
    auto tmp10 = [&]
    {
        auto tmp11 = in_ptr0[static_cast<long>(i2 + i3 + ((-1L)*ks2) + (i1*ks3) + (2L*i2*ks2) + (3L*i0*ks3) + (2L*i1*ks2*ks3) + (                       6L*i0*ks2*ks3))];
        return tmp11;
    }
    ;
    auto tmp12 = tmp9 ? tmp10() : static_cast<decltype(tmp10())>(0.0);
    auto tmp13 = in_ptr1[static_cast<long>(i2 + i3 + (i1*ks2) + (2L*i1*(static_cast<long>(ks2*ks2))) + (2L*i2*ks2) + (i0*ks1*ks2)                        + (2L*i0*ks1*(static_cast<long>(ks2*ks2))))];
    auto tmp14 = tmp9 ? tmp12 : tmp13;
    return tmp14;
}
```
For ```auto tmp7 = static_cast<int>(9223372036854775807);```, ```tmp7``` is always ```-1```, this is wrong.

After This PR, HF AllenaiLongformerBase CPU dynamic shape path can be passed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/103511
Approved by: https://github.com/desertfire
","['test/inductor/test_cpu_repro.py', 'torch/_inductor/lowering.py']","Int32 index type used in slice_scatter operation leads to out-of-range errors due to default value exceeding max int32 limit, causing accuracy issues in dynamic shape test cases."
f7574ea43f4f3ae0deda5b94ec8aa0468dd07225,1694720350,"`torch.load`: Replaced multiple one byte read() calls during the `_is_zipfile` check with a single call (#109119)

Fixes #108955.

Right now, the `_is_zipfile` check in `torch.load` performs multiple `read()` calls, reading 1 byte at a time in a loop. This is rather wasteful and leads to performance problems when accessing files on a network share (see #108955) .
This PR replaces those 1 byte calls with a single big call. Functionally, this is equivalent as `read(n)` only reads up to `n` bytes, so even if the file is shorter there should not be any problems.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109119
Approved by: https://github.com/mikaylagawarecki
",['torch/serialization.py'],"The `_is_zipfile` check in `torch.load` results in performance issues due to multiple one-byte `read()` calls, especially when accessing files on a network share."
edb99df2e086fd22068f877c526b9424771bef0f,1664422732,"[ONNX] Fix reduce node shape inference (#85765)

Fix logic in `ProcessReduceNode`. Previously a scalar was assigned for output shape of reduce nodes
when `axes` attribute was not provided, regardless of the value of `keepdims_i` attribute. Hence it is
incorrectly assuming all output axes should be folded.
Since input rank is known, this fix populates axes to be `[0, 1, ..., input_rank - 1]` if axes is not
provided.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85765
Approved by: https://github.com/abock
","['test/onnx/test_pytorch_onnx_shape_inference.py', 'torch/csrc/jit/passes/onnx/shape_type_inference.cpp']","The reduce node's shape inference logic incorrectly assigns a scalar for output shape when 'axes' attribute is not provided, overlooking the 'keepdims_i' attribute value."
d47b94fa8e17ad805f1283943dd2b1bc46b309b8,1668546208,"[inductor] Added bucketize to decomp table (#88348)

These are the benchmark results vs eager

```
[--------------------------- bucketize ----------------------------]
                                                 |  eager  |  decomp
32 threads: --------------------------------------------------------
      ((16384, 1024), (16,)), (True, True)       |    600  |    464
      ((16384, 1024), (16,)), (True, False)      |    542  |    464
      ((16384, 1024), (16,)), (False, True)      |    780  |    731
      ((16384, 1024), (16,)), (False, False)     |    777  |    731
      ((16384, 1024), (64,)), (True, True)       |    624  |    515
      ((16384, 1024), (64,)), (True, False)      |    603  |    515
      ((16384, 1024), (64,)), (False, True)      |    789  |    718
      ((16384, 1024), (64,)), (False, False)     |    786  |    718
      ((16384, 1024), (256,)), (True, True)      |    878  |    820
      ((16384, 1024), (256,)), (True, False)     |    891  |    830
      ((16384, 1024), (256,)), (False, True)     |    897  |    900
      ((16384, 1024), (256,)), (False, False)    |    900  |    900
      ((16384, 1024), (1024,)), (True, True)     |   2000  |   1890
      ((16384, 1024), (1024,)), (True, False)    |   1950  |   1892
      ((16384, 1024), (1024,)), (False, True)    |   1990  |   1962
      ((16384, 1024), (1024,)), (False, False)   |   1990  |   2060
      ((16384, 1024), (4096,)), (True, True)     |   3405  |   3155
      ((16384, 1024), (4096,)), (True, False)    |   3244  |   3154
      ((16384, 1024), (4096,)), (False, True)    |   3282  |   3219
      ((16384, 1024), (4096,)), (False, False)   |   3278  |   3220
      ((16384, 1024), (16384,)), (True, True)    |   4626  |   4672
      ((16384, 1024), (16384,)), (True, False)   |   4629  |   4671
      ((16384, 1024), (16384,)), (False, True)   |   4662  |   4829
      ((16384, 1024), (16384,)), (False, False)  |   4665  |   4824
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88348
Approved by: https://github.com/ngimel
",['torch/_inductor/decomposition.py'],Decomposition table in inductor lacks 'bucketize' function leading to discrepancies in the benchmark results when compared with an eager approach.
a20b36b03dc7a027b7e7710745b1b552f46c65e2,1615356015,"[JIT] Fix backward compatibility test broken by #53410 (#53683)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53683

**Summary**
This commit fixes the BC test broken by #53410. There are no promises
about operator-level BC with the operators added and modified by that
PR, so this test failure does not represent a real backward
compatibility issue.

**Test Plan**
Ran the BC test locally by runniing `dump_all_schemas.py` and then
`check_backward_compatibility.py`.

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D26936505

Pulled By: SplitInfinity

fbshipit-source-id: 829d5d78e4cba44feea382d0fbd66e77dee7eed2
",['test/backward_compatibility/check_backward_compatibility.py'],"Backward compatibility test has broken due to recent changes (#53410), potentially creating an issue with operator-level backward compatibility."
8c6d352bcfa3a8d2f7322d3577117b2d432cd002,1664994233,"Log a new ""timer expired"" event to Scuba in file_based_local_timer (#85861)

Summary: The ""kill worker process"" event was logged to Scuba only when the worker process was really reaped. We want to add a new event ""timer expired"", no matter the worker process will be reaped or not. This will help collect data before we enable the JustKnob to kill the worker process on timeout.

Test Plan:
### Unit Test
```
buck test mode/dev-nosan //caffe2/test/distributed/elastic/agent/server/test:local_agent_test
```
```
Test Session: https://www.internalfb.com/intern/testinfra/testrun/7318349508929624
RE: reSessionID-ea464c43-54e7-44f2-942b-14ea8aa98c74  Up: 10.5 KiB  Down: 1.1 MiB
Jobs completed: 100. Time elapsed: 3206.9s. Cache hits: 91%. Commands: 11 (cached: 10, remote: 1, local: 0)
Tests finished: Pass 55. Fail 0. Fatal 0. Skip 0. 0 builds failed
```
--------
```
buck test mode/dev-nosan //caffe2/test/distributed/elastic/agent/server/test/fb:local_agent_fb_internal_test
```
```
Test Session: https://www.internalfb.com/intern/testinfra/testrun/6473924579130483
RE: reSessionID-231a47b7-a43d-4c0f-9f73-64713ffcbbd3  Up: 5.7 MiB  Down: 1.9 GiB
Jobs completed: 182156. Time elapsed: 282.4s. Cache hits: 99%. Commands: 72112 (cached: 72107, remote: 1, local: 4)
Tests finished: Pass 2. Fail 0. Fatal 0. Skip 0. 0 builds failed
```

Differential Revision: D39903376

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85861
Approved by: https://github.com/d4l3k
",['torch/distributed/elastic/timer/file_based_local_timer.py'],"""Kill worker process"" event only records when process is reaped, lacks data logging on timer expiration events which need to be tracked regardless of worker process reaping."
3edff6b6ec3e0523fbcbc5e6c103ea67f920bb3a,1682956725,"Improve detection of workspace/non-output allocations in cudagraphs (#99985)

When we run cudagraph trees we are not allowed to have permanent workspace allocations like in cublas because we might need to reclaim that memory for a previous cudagraph recording, and it is memory that is not accounted for in output weakrefs so it does not work with checkpointing. Previously, I would check that we didn't have any additional allocations through snapshotting. This was extremely slow so I had to turn it off.

This PR first does the quick checking to see if we are in an error state, then if we are does the slow logic of creating snapshot. Also turns on history recording so we get a stacktrace of where the bad allocation came from.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99985
Approved by: https://github.com/zdevito
","['c10/cuda/CUDACachingAllocator.cpp', 'c10/cuda/CUDACachingAllocator.h', 'test/inductor/test_cudagraph_trees.py', 'test/test_cuda.py', 'torch/_inductor/config.py', 'torch/_inductor/cudagraph_trees.py', 'torch/csrc/cuda/Module.cpp']","Cudagraph runs may contain unaccounted permanent workspace allocations in cublas, causing memory reclaiming and checkpointing issues. Existing check mechanism through snapshotting is extremely slow."
8287c1d96494805728b06e3c7b80d07b55897352,1667267058,"[xnnpack][lite-int][3/n] flatbuffer serializer class (#87907)

Creating a serializer class that allows us to serialize the xnnpack graph creation arguments. This essentially abstracts away the flatbuffer api manipulation and serialization that we deal with.

As a result we can call
```
XNNSerializer::serializeAddNode()
XNNSerializer::serializeTensorValue()
XNNSerializer::finishAndSerialize
```
to serialize the graph

Differential Revision: [D39196312](https://our.internmc.facebook.com/intern/diff/D39196312/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39196312/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87907
Approved by: https://github.com/digantdesai
","['torch/csrc/jit/backends/xnnpack/serialization/serializer.cpp', 'torch/csrc/jit/backends/xnnpack/serialization/serializer.h']","The XNNPACK graph creation arguments lack a straightforward method for serialization, requiring direct manipulation of the FlatBuffer API."
2ce6150d23928773c35274aec369eb0a5ecd6fa4,1665684104,"[ONNX] Fix scalar_type_analysis metadata for copied constant (#86716)

Fix the source of metadata for copied constant. Since the constant is being implicitly casted,
it makes more sense to assign code location and etc with the user node.
This issue was discovered in https://github.com/pytorch/pytorch/issues/86627. This PR also adds unit test coverage for scope
information of nodes when they are altered by CSE and related passes.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86716
Approved by: https://github.com/thiagocrepaldi, https://github.com/malfet
","['test/onnx/test_utility_funs.py', 'torch/csrc/jit/ir/ir.h', 'torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp']","The source of metadata for copied constants is faulty, causing inconsistencies when a constant is implicitly casted. Issue also affects scope info of nodes modified by CSE and related passes."
720cb5023a5ac4a9fa4ee3372a1b3c8a46389e01,1654299544,"[Static Runtime] Implement prim::Fork and aten::wait (#78780)

Summary:
basic implementation of prim::fork and aten::wait

- current implementation uses interpreter to call the forked subgraph
- interpreter call to be replaced in future
- Added custom test cases for fork/wait procedures in the graph

Test Plan:
custom tests are created in test_static_runtime.py file for verification of static_runtime output compared to reference pytorch output.

test command
- buck run caffe2/test:static_runtime
- buck run caffe2/benchmarks/static_runtime:static_runtime_cpptest
- buck test caffe2/benchmarks/static_runtime/fb:test_fb_operators

Differential Revision: D36881214

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78780
Approved by: https://github.com/tenpercent
","['test/test_static_runtime.py', 'torch/csrc/jit/runtime/static/native_ops.cpp']","There is no existing implementation of prim::fork and aten::wait in the static runtime, preventing certain fork/wait procedures from executing properly."
c0ce4b0de916c5916821ae597dc1c36f6e0b1eaa,1655837595,"make refs executor handle kwargs (#79858)

Mostly fixes #78923
I had to disable function patching in fx for functions with kwonly args, see https://github.com/pytorch/pytorch/compare/ngimel/make_fx_fix?expand=1#diff-090b22122be0779cd14afd2ebaf20d1e7c0bfe837e9eefa1d84e7521bb1defc6R446, cc @jamesr66a
But it looks like it was doing weird things anyway - it was patching signature of wrapped function with arbitrary local vars from wrapper, that can't be right, but I don't know what the intent there is.
A lot of functions now fail with nvfuser executor, and some still fail with aten, although with the different errors than before.
Edit: undid the change to _symbolic_script.py, turns out inspect.unwrapping function is not needed, and fx never sees kwargs.
cc @IvanYashchuk, @Chillee

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79858
Approved by: https://github.com/IvanYashchuk, https://github.com/mruberry
","['test/test_prims.py', 'torch/_prims/executor.py', 'torch/fx/node.py', 'torch/testing/_internal/common_methods_invocations.py']","The refs executor is not handling kwargs properly, disabled function patching for functions with kwonly args leads to unusual results, causing some functions to fail with nvfuser and aten executors."
2dc3dc23248005fb3efa3a5aa5ca2817e1d370b1,1619655693,"Enhance error message for Future.setErrorIfNeeded. (#56631)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56631

`setErrorIfNeeded` did not mention whether the future was already
completed or there was some other exception. This particular change ensures
that we also print out the original exception as part of the error message.

This would help in debugging issues where this codepath is triggered.
ghstack-source-id: 127248844

Test Plan: waitforbuildbot

Reviewed By: rohan-varma

Differential Revision: D27919974

fbshipit-source-id: 2273a93f3475929b14f721c976f194f33a5aa746
",['aten/src/ATen/core/ivalue_inl.h'],"Error message by `setErrorIfNeeded` in Future is not sufficiently informative, failing to detail whether a future is already completed or showcasing a different exception."
d2d03f0f4473dc7891385f7a60fdb8efea0148bc,1686207070,"Make index_add_ error if input source shape is wrong (#100321)

Fixes #92576 , checking the following as described in the documentation:

""source.shape[dim] == len(index) and source.shape[i] == self.shape[i] for i != dim""

Would be happy to iterate on this if there are any issues, and would be happy to implement the checking for the CUDA and MPS implementations of index_add_.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/100321
Approved by: https://github.com/lezcano
","['aten/src/ATen/native/TensorAdvancedIndexing.cpp', 'test/onnx/test_pytorch_onnx_onnxruntime.py', 'test/test_legacy_vmap.py', 'test/test_torch.py', 'torch/testing/_internal/common_methods_invocations.py']","The method index_add_ is not providing an error when passed an input with an incorrect source shape, causing a discrepancy with the documented behavior."
48ec939d397b643dd557012b5ccf65476f987ffe,1615240242,"[iOS GPU][BE][2/n] - Use dispatcher in MPSCNNTests.mm (#53429)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53429

Call the testing ops through dispatcher instead of calling them through `at::native`. Some metal ops can't be called through dispatcher yet. For example, `at::t` will call `at::as_strided` which hasn't been implemented on metal yet. For those ops, we'll skip and call `mpscnn::`directly. We'll convert those ops once we have implemented the missing ops.
ghstack-source-id: 123038068

Test Plan:
- Sandcastle CI
- Circle CI
- AIBench/Mobilelab

Reviewed By: SS-JIA, AshkanAliabadi

Differential Revision: D26683366

fbshipit-source-id: bf130b191046f5d9ac9b544d512bc6cb94f08c09
","['aten/src/ATen/native/metal/MetalAten.mm', 'aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm', 'aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm']","The MPSCNNTests.mm is directly calling testing ops through `at::native` rather than the dispatcher, causing issues with ops not implemented on metal yet."
50ab2b702f149f51d60da9f83beca849cb2b050f,1671472145,"move inputs to device on root module only (#91078)

1. No need to move inputs/activations to devices for every nested FSDP instance
2. it also breaks the case when some nested FSDP instances have newly added inputs/activations in the signatures of submodules wrapped by nested FSDP instances, args_tuple[0] and kargs_tuple[0] are not correct to get the inputs/activations for these nested instances

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91078
Approved by: https://github.com/mrshenli, https://github.com/rohan-varma
","['torch/distributed/fsdp/_runtime_utils.py', 'torch/distributed/fsdp/fully_sharded_data_parallel.py']",Moving inputs/activations to devices for every nested FSDP instance is unnecessary and leads to errors when new inputs/activations are added in signatures of submodules wrapped by these nested instances.
3663436db31bd3cebcb76efe05d8355553a05c57,1695430690,"[inductor] fix a max-autotune rng state related bug (#109828)

Fix https://github.com/pytorch/pytorch/issues/109736 .

HF pin move causes regression on accuracy check for HF models on the dashboard. Manually reverting the HF PR ( https://github.com/huggingface/transformers/pull/24696/files ) could recover, but this may hide some real issue. I happen to found that using a warm matmul max-autotune cache can work around the issue. Or putting it in another way:
- make all calls to check_cache cache miss repro the issue
- make all cals to check_cache cache hit works around the issue

I did some sort of 'bisect' to force halving the amount of cache miss each time while still make sure we can repro. Luckily reducing to a single cache miss still repro the issue. With more debugging, it turns out that it's the call to `torch.randn` on cuda device causing the problem.

The fix is to make sure  we restore the rng state when we generate random inputs for max-autotune benchmarking.

TBH, I can not fully explain the root cause although I know it's caused by rng state change.  AOTAutograd already has some logic to preserve rng state. And I can not repro the issue in unit tests. I have a few guess why the RNG state is not restored in the first place after we generate random inputs for max-autotune:
- maybe AOTAutograd misses some corner case to preserve the rng state
- maybe for the failed models, there are some eager fallback that's not handled by inductor. And if those fallback calles random number related APIs, we will see the issue. But again I don't find a good way to simulate this.

Repro:

```
TORCHINDUCTOR_BENCHMARK_KERNEL=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM=1 CUDA_VISIBLE_DEVICES=3 time python benchmarks/dynamo/huggingface.py --backend inductor --amp --accuracy --only PLBartForCausalLM --training --cold-start-latency
```

We always repro the issue without the PR but pass the accuracy check with the PR.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109828
Approved by: https://github.com/eellison
","['benchmarks/dynamo/common.py', 'test/inductor/test_max_autotune.py', 'torch/_dynamo/testing.py', 'torch/_inductor/select_algorithm.py']",A change in RNG state while generating random inputs for max-autotune benchmarking results in accuracy check regression for HF models on the dashboard.
aab67c6dff3819d3c05b328e964f1642227adbe6,1639121385,"Add native masked_softmax (#69268)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69268

This diff enabled native masked softmax on CUDA, also expanded our current warp_softmax to accept masking.
The mask in this masked softmax has to be the same shape as input, and has to be contiguous.

In a following diff I will submit later, I will have encoder mask layout included, where input is BHDD and mask is BD.

Test Plan: buck build mode/opt -c fbcode.enable_gpu_sections=true caffe2/test:nn && buck-out/gen/caffe2/test/nn\#binary.par -r test_masked_softmax

Reviewed By: ngimel

Differential Revision: D32338419

fbshipit-source-id: 48c3fde793ad4535725d9dae712db42e2bdb8a49
","['aten/src/ATen/native/SoftMax.cpp', 'aten/src/ATen/native/cuda/PersistentSoftmax.cuh', 'aten/src/ATen/native/cuda/SoftMax.cu', 'test/test_nn.py']","The current implementation of softmax in CUDA doesn't support masking, and the warp softmax function doesn't accept masks. The mask shape also needs to match the input shape and be contiguous."
be364c0cda53f052dc7d8f2979615ef44c39b0f6,1675415593,"[Inductor] Fix OpenMP discovery on MacOS (#93895)

It's not available as system dependency, so assume that it is installed
using Anaconda

Also, clang on MacOS does not recognize `-fopenmp` flag, but according
to https://mac.r-project.org/openmp/ and local experiments `-Xclang
-fopenmp` always works

Test plan:
Following should run and return true
```python
import torch

def foo(x: torch.Tensor) -> torch.Tensor:
   return torch.sin(x) + torch.cos(x)

if __name__==""__main__"":
    x = torch.rand(3, 3)
    x_eager = foo(x)
    x_pt2 = torch.compile(foo)(x)
    print(torch.allclose(x_eager, x_pt2))
```

Skip number of tests that fail on x86 MacOS (for example rsqrt for bool type and   `test_pixel_shuffle_channels_last_cpu` on machines that do not support AVX2)
Tweak few tests to use double precision when running on CPU, as type promotion for accumulator types is broken.

TODO: Fix PyTorch for M1 compilation with OpenMP, bundle `omp.h` into the package and use it instead.
Fixes https://github.com/pytorch/pytorch/issues/90362

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93895
Approved by: https://github.com/jansel, https://github.com/jgong5
","['test/inductor/test_torchinductor.py', 'test/inductor/test_torchinductor_opinfo.py', 'torch/_inductor/codecache.py']","OpenMP discovery on MacOS fails as it's not available as a system dependency and `clang` doesn't recognize `-fopenmp` flag, and certain tests like `rsqrt` for bool type and `test_pixel_shuffle_channels_last_cpu` fall short on x86 MacOS devices."
00d4ec840ed70372fb00c50c882acaab7367c49b,1611888010,"clone pytorch.github.io with depth 1 (#48115)

Summary:
Speeds up clone of pytorch.github.io in CI/CD - currently takes ~7 minutes each run.

Locally these are the results: 3.73 seconds vs 611.87 seconds.

With depth 1:

```
$ time git clone https://github.com/pytorch/pytorch.github.io -b site --depth 1
...
3.73s user 2.97s system 23% cpu 28.679 total
```

Without:

```
$ time git clone https://github.com/pytorch/pytorch.github.io -b site
...
611.87s user 66.16s system 96% cpu 11:41.99 total
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48115

Reviewed By: mrshenli

Differential Revision: D25107867

Pulled By: ranman

fbshipit-source-id: b6131b51df53b7f71d9b4905181182699c0c6c09
",['.circleci/scripts/python_doc_push_script.sh'],"Cloning of pytorch.github.io in CI/CD is taking excessively long time, approximately 7 minutes per run."
55544cb13a6a192b66e2733ddfb4f977745bb51f,1617238130,"[quant][graphmode][fx] Add support for one value being quantized with different qconfigs (#53586)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53586

Previously one value can only be quantized to one dtype, this PR adds the support for quantizing one value
in the fx graph with multiple dtypes, e.g. first quantize to int8 and then float16

might do some followup PRs to clean up the hacks and refactor the code.

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_multiple_qconfigs_single_value

Imported from OSS

Reviewed By: vkuzo

Differential Revision: D26912676

fbshipit-source-id: ae3653fd67f05870a3a9e808f491871826c555d5
","['test/quantization/test_quantize_fx.py', 'torch/quantization/fx/graph_module.py', 'torch/quantization/fx/quantization_patterns.py', 'torch/quantization/fx/quantize.py', 'torch/quantization/fx/utils.py']","Single value in the fx graph can only be quantized to one dtype, preventing quantization to multiple dtypes such as int8 then float16."
1c79003b3c13c7bc47e5796e4451d6565121f3a0,1687377580,"Enable addmm + GELU epilogue fusion via cuBLASLt (#103811)

Summary:

Previously, addmm + GELU epilogue fusion was unconditionally disabled in `ATen/native/cuda/Blas.cpp` due to compilation and numerical issues in CUDA <= 11.4. This PR:

1. Enables addmm + GELU epilogue fusion for CUDA >= 11.8.

2. Restricts the usage of fused addmm epilogue to contiguous output (bugfix).

3. Extends unit tests with addmm epilogue fusion and GELU activation paths.

Test Plan:

$ python test/test_linalg.py -k test_addmm_relu -v

test_addmm_relu_cpu_bfloat16 (__main__.TestLinalgCPU.test_addmm_relu_cpu_bfloat16) ... ok
test_addmm_relu_cpu_float32 (__main__.TestLinalgCPU.test_addmm_relu_cpu_float32) ... ok
test_addmm_relu_cpu_float64 (__main__.TestLinalgCPU.test_addmm_relu_cpu_float64) ... ok
test_addmm_relu_cuda_bfloat16 (__main__.TestLinalgCUDA.test_addmm_relu_cuda_bfloat16) ... ok
test_addmm_relu_cuda_float32 (__main__.TestLinalgCUDA.test_addmm_relu_cuda_float32) ... ok
test_addmm_relu_cuda_float64 (__main__.TestLinalgCUDA.test_addmm_relu_cuda_float64) ... ok

$ python test/test_linalg.py -k test_addmm_gelu -v

test_addmm_gelu_cpu_bfloat16 (__main__.TestLinalgCPU.test_addmm_gelu_cpu_bfloat16) ... ok
test_addmm_gelu_cpu_float32 (__main__.TestLinalgCPU.test_addmm_gelu_cpu_float32) ... ok
test_addmm_gelu_cpu_float64 (__main__.TestLinalgCPU.test_addmm_gelu_cpu_float64) ... ok
test_addmm_gelu_cuda_bfloat16 (__main__.TestLinalgCUDA.test_addmm_gelu_cuda_bfloat16) ... ok
test_addmm_gelu_cuda_float32 (__main__.TestLinalgCUDA.test_addmm_gelu_cuda_float32) ... ok
test_addmm_gelu_cuda_float64 (__main__.TestLinalgCUDA.test_addmm_gelu_cuda_float64) ... ok

Reviewers: @eellison

Differential Revision: [D46829884](https://our.internmc.facebook.com/intern/diff/D46829884)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/103811
Approved by: https://github.com/IvanYashchuk, https://github.com/eellison
","['aten/src/ATen/native/cuda/Blas.cpp', 'test/test_linalg.py']","Addmm + GELU epilogue fusion is unconditionally disabled due to compilation and numerical issues in CUDA <= 11.4, and fused addmm epilogue usage doesn't support non-contiguous output."
8a6d1289d822f08e9c849b9f1c61f01537746b40,1658247106,"[ao] Revised ModelReport API to take in model at initialization (#81588)

Summary: Currently, the ModelReport API only takes in detectors at the
beginning and for each of its methods, you have to pass in the model
each time, which doesn't really make sense because:

1. you will always want to be working on the same model
2. passing in a different model could break things, so more
fault-tolerant if we keep the model internally and make calls on it

Therefore, now the model will be passed in in intialization, and will
just be used for the rest of the operations with the local link.

All the ModelReport tests have been adjusted to account for this, and
this change must pass all the tests to ensure a successful API
transition.

If you wish to see how the updated API looks, the Expected Usage in the
ModelReport clas description has been updated to reflect the changes.

The README has also been updated with these changes as well.

Test Plan: python test/test_quantization.py TestFxModelReportClass

Reviewers:

Subscribers:

Tasks:

Tags:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81588
Approved by: https://github.com/jerryzh168
","['test/quantization/fx/test_model_report_fx.py', 'torch/ao/quantization/fx/_model_report/model_report.py']","The current ModelReport API requires passing the model for each method, risking potential errors by passing different models and redundancy as the same model is mostly used continuously."
7a192cc51c7172860efd35413acf9a8b9aafd2c9,1678111457,"dynamo: wrap graph break inst in try except block - with context manager setup/teardown (#94758)

Replacement to https://github.com/pytorch/pytorch/pull/94672.

Follow up to https://github.com/pytorch/pytorch/pull/94137.

We simply replace the set grad mode try except blocks with one for a more generic contextmanager (using `__enter__` and `__exit__`), storing the context manager into a `symbolic_local` for the duration of the try block.

(see https://github.com/pytorch/torchdynamo/issues/207 for the original motivation)

This allows us to handle calling inner functions with graph breaks for any arbitrarily deep nesting of live context managers subclassing `AbstractContextManager`. (see tests)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94758
Approved by: https://github.com/yanboliang
","['test/dynamo/test_misc.py', 'test/dynamo/test_repros.py', 'torch/_dynamo/codegen.py', 'torch/_dynamo/output_graph.py', 'torch/_dynamo/resume_execution.py', 'torch/_dynamo/symbolic_convert.py', 'torch/_dynamo/variables/misc.py']",The current handling of graph breaks in inner functions fails with any arbitrarily deep nesting of live context managers subclassing `AbstractContextManager`.
dceae41c29782399c84304812696a8382e9b4292,1689226250,"[PyTorch TB] Write raw tensor as tensor_proto (#104908)

This is the first diff to support logging of raw tensors for [TensorBoard Intermediate Logging](https://www.internalfb.com/intern/wiki/TensorBoard/Intermediate_Logging/)

Ultimately, we aim to support the feature where store full tensor is stored as a tensor protobuf to TB. Protobuf contains shape, dtype, and elements of the given tensor.

1. add `tensor_proto()` to `summary.py` which takes a tensor and convert to protobuf
2. add `add_tensor()` to `writer.py`
3. formatting changes introduced by `arc lint`
-------------

Differential Revision: [D47302017](https://our.internmc.facebook.com/intern/diff/D47302017/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/104908
Approved by: https://github.com/kunalb
","['test/test_tensorboard.py', 'torch/utils/tensorboard/summary.py', 'torch/utils/tensorboard/writer.py']","TensorBoard intermediate logging does not support storing of raw tensors as tensor protobufs, impeding the recording of shape, dtype, and elements of the tensors."
9e2e345af702377c5af57dbe07fa7ec2b5f47bb1,1680052066,"[inductor] avoid kernel cache miss because of different arg name (#97755)

We previously use buffer name for the variable containing randomly generated kernel input in the kernel benchmark. This has a big drawback. The same kernel may be used for different buffers. However if we use buffer name as argument name, the kernel source code for different invocation of the kernel will be different. This cause the following downsides:
- compile time will be longer since we can not reused compiled kernel due to cache miss
- this cause inconsistent behavior with TORCHINDUCTOR_BENCHMARK_KERNEL enabled or disabled. We may see more kernels (some are essentially duplicated) in the compiled module if TORCHINDUCTOR_BENCHMARK_KERNEL is enabled.
- this obscure some optimization opportunities. E.g., a kernel spend 6% time is worth looking at. But if the kernel is called 20 times and now it show up as 20 different kernels each spend 0.3% of time, it would be less obvious that we should optimize this kernel.

In this PR, we just use canonical name like `arg_{i}` rather than the buffer name to avoid all the issues above.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97755
Approved by: https://github.com/jansel
",['torch/_inductor/codegen/triton.py'],"Using buffer name for kernel input in the kernel benchmark causes cache misses, longer compile times, inconsistent behavior with TORCHINDUCTOR_BENCHMARK_KERNEL, and obscures optimization opportunities by duplicating kernels."
eaab653376da76cd3038b7f2bed37b03e2048522,1662071939,"Read via FileAdapter when loading files in torch if not flatbuffer - Part 2 (#84296)

Summary: D38998858 (https://github.com/pytorch/pytorch/commit/3fae89d4a468a02be501357eb123ce2bf7086d2f) used the wrong version of `_load_for_mobile` that kept the ""load everything in memory then parse"" technique.  This fixes it to call the `_load_for_mobile_impl` version which for non-flatbuffer models will stream parse.  See D38998858 (https://github.com/pytorch/pytorch/commit/3fae89d4a468a02be501357eb123ce2bf7086d2f) for the expected memory optimization gains.

Test Plan: CI Signals.

Reviewed By: qihqi

Differential Revision: D39138280

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84296
Approved by: https://github.com/qihqi
",['torch/csrc/jit/mobile/import.cpp'],"While loading non-flatbuffer models in PyTorch, the entirety of the file is first loaded into memory and then parsed, leading to inefficient use of memory."
36d87465fb9b34914e6db50638c0f5bf04e3d7d9,1668451430,"Fix long comment error on dashboard (#89002)

Fix dashboard comment failure due to the following trace:
```
Traceback (most recent call last):
  File ""/scratch/anijain/dashboard/work/pytorch/benchmarks/dynamo/runner.py"", line 1180, in <module>
    DashboardUpdater(args).update()
  File ""/scratch/anijain/dashboard/work/pytorch/benchmarks/dynamo/runner.py"", line 1119, in update
    self.comment_on_gh(comment)
  File ""/scratch/anijain/dashboard/work/pytorch/benchmarks/dynamo/runner.py"", line 1096, in comment_on_gh
    subprocess.check_call(
  File ""/scratch/anijain/dashboard/env/lib/python3.9/subprocess.py"", line 368, in check_call
    retcode = call(*popenargs, **kwargs)
  File ""/scratch/anijain/dashboard/env/lib/python3.9/subprocess.py"", line 349, in call
    with Popen(*popenargs, **kwargs) as p:
  File ""/scratch/anijain/dashboard/env/lib/python3.9/subprocess.py"", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/scratch/anijain/dashboard/env/lib/python3.9/subprocess.py"", line 1821, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 7] Argument list too long: '/data/home/anijain/miniconda/bin/gh'
srun: error: a100-st-p4d24xlarge-27: task 0: Exited with exit code 1
```
That is, we were trying to execute a gh command in the OS that was too long.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89002
Approved by: https://github.com/davidberard98
",['benchmarks/dynamo/runner.py'],"Execution of 'gh' command failing on the dashboard with OSError; the argument list is too long, thus triggering a traceback error."
c9ae705a22d9b92e28d655ed3960d488aef04c0e,1685643304,"[Pytorch] Add Vulkan support for aten::unsqueeze, 1d->2d, 3d->4d (#102042)

Summary:
Add 1d->2d, 3d->4d unsqueeze

Unsqueeze operator: https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze

Test Plan:
Unsqueeze tests:
```
lfq@lfq-mbp xplat % buck run --target-platforms ovr_config//platform/macos:arm64-fbsource //xplat/caffe2:pt_vulkan_api_test_binAppleMac\#macosx-arm64 -c pt.vulkan_full_precision=1 -- --gtest_filter=""*unsqueeze*""
Downloaded 0/44 artifacts, 0.00 bytes, 100.0% cache miss (for updated rules)
Building: finished in 38.6 sec (100%) 523/523 jobs, 8/523 updated
  Total time: 38.6 sec
BUILD SUCCEEDED
Running main() from xplat/third-party/gmock/googletest-1.12.1/googletest/src/gtest_main.cc
Note: Google Test filter = *unsqueeze*
[==========] Running 9 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 9 tests from VulkanAPITest
[ RUN      ] VulkanAPITest.unsqueeze_1dto2d_dim0
[       OK ] VulkanAPITest.unsqueeze_1dto2d_dim0 (76 ms)
[ RUN      ] VulkanAPITest.unsqueeze_1dto2d_dim1
[       OK ] VulkanAPITest.unsqueeze_1dto2d_dim1 (2 ms)
[ RUN      ] VulkanAPITest.unsqueeze_2dto3d_dim0
[       OK ] VulkanAPITest.unsqueeze_2dto3d_dim0 (9 ms)
[ RUN      ] VulkanAPITest.unsqueeze_2dto3d_dim1
[       OK ] VulkanAPITest.unsqueeze_2dto3d_dim1 (1 ms)
[ RUN      ] VulkanAPITest.unsqueeze_2dto3d_dim2
[       OK ] VulkanAPITest.unsqueeze_2dto3d_dim2 (1 ms)
[ RUN      ] VulkanAPITest.unsqueeze_3dto4d_dim0
[       OK ] VulkanAPITest.unsqueeze_3dto4d_dim0 (2 ms)
[ RUN      ] VulkanAPITest.unsqueeze_3dto4d_dim1
[       OK ] VulkanAPITest.unsqueeze_3dto4d_dim1 (1 ms)
[ RUN      ] VulkanAPITest.unsqueeze_3dto4d_dim2
[       OK ] VulkanAPITest.unsqueeze_3dto4d_dim2 (1 ms)
[ RUN      ] VulkanAPITest.unsqueeze_3dto4d_dim3
[       OK ] VulkanAPITest.unsqueeze_3dto4d_dim3 (1 ms)
[----------] 9 tests from VulkanAPITest (98 ms total)

[----------] Global test environment tear-down
[==========] 9 tests from 1 test suite ran. (98 ms total)
[  PASSED  ] 9 tests.
```

clang-format on the glsl files

Differential Revision: D46057585

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102042
Approved by: https://github.com/SS-JIA
","['aten/src/ATen/native/vulkan/glsl/unsqueeze.glsl', 'aten/src/ATen/native/vulkan/ops/Unsqueeze.cpp', 'aten/src/ATen/test/vulkan_api_test.cpp']","The Vulkan backend for Pytorch lacks support for aten::unsqueeze operations, limiting the transformation of 1D to 2D and 3D to 4D tensors."
52fd7e491b24d4cf910b98bbe06c460f7d8e5577,1663718814,"Update torch.ops.aten.all ref to be symbolic-trace friendly (#85352)

- previous impl compared the summed bool values of the tensor to its nelem, which in a symbolic world is a symint and can't be coerced back into a bool for the purpose of shoving into a result tensor

- new impl adds one extra negation op but avoids the need to compare to the symbolic nelem

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85352
Approved by: https://github.com/ezyang, https://github.com/mruberry
","['test/test_proxy_tensor.py', 'torch/_refs/__init__.py']",The previous implementation of torch.ops.aten.all is not compatible with symbolic-tracing as it requires coercion of symbolic integers to booleans to be stored in a tensor.
bfe1abd3b579aef5c885456f262901e529660de4,1642023311,"torch/monitor: add pybind (#69567)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69567

This exposes torch.monitor events and stats via pybind11 to the underlying C++ implementation.

* The registration interface is a tad different since it takes a lambda function in Python where as in C++ it's a full class.
* This has a small amount of changes to the counter interfaces since there's no way to create an initializer list at runtime so they now also take a vector.
* Only double based stats are provided in Python since it's intended more for high level stats where float imprecision shouldn't be an issue. This can be changed down the line if need arises.

```
events = []

def handler(event):
    events.append(event)

handle = register_event_handler(handler)

log_event(Event(type=""torch.monitor.TestEvent"", timestamp=datetime.now(), metadata={""foo"": 1.0}))
```

D32969391 is now included in this diff.
This cleans up the naming for events. type is now name, message is gone, and metadata is renamed data.

Test Plan: buck test //caffe2/test:monitor //caffe2/test/cpp/monitor:monitor

Reviewed By: kiukchung

Differential Revision: D32924141

fbshipit-source-id: 563304c2e3261a4754e40cca39fc64c5a04b43e8
","['test/cpp/monitor/test_counters.cpp', 'test/cpp/monitor/test_events.cpp', 'test/test_monitor.py', 'tools/build_variables.bzl', 'torch/csrc/Module.cpp', 'torch/csrc/monitor/counters.cpp', 'torch/csrc/monitor/counters.h', 'torch/csrc/monitor/events.h', 'torch/csrc/monitor/python_init.cpp', 'torch/csrc/monitor/python_init.h', 'torch/monitor/__init__.py']","Pybind11 exposure of torch.monitor events and stats to underlying C++ implementation is missing, creating inconsistencies with registration interfaces and counter interfaces. Only double-based stats are provided in Python, possibly limiting precision needs."
cf735d899a7a7effb520587e2a13638c278bef95,1649992845,"[quant][core][bug fix][gpu] Added kReluFused to quantized cudnn conv operator's caching

Summary:
Previous implementation of CacheKey neglected kReluFused, but we
need to be able to cache cases based on whether relu is activated or not,
otherwise we will run into situations in which uid is defined in VariantPack
but not in the operator graph.

Test plan:
```
python test/test_quantization.py -k test_qconv2d_cudnn
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75711

Approved by: https://github.com/jerryzh168
",['aten/src/ATen/native/quantized/cudnn/Conv.cpp'],"The CacheKey implementation for the quantized cuDNN conv operator does not consider kReluFused, leading to mismatches between uid in VariantPack and the operator graph."
33e70e34a39200136e7ff410572c6d0030f7790f,1691427142,"More readable Z3 expressions printer. (#106643)

This PR makes Z3 expressions easier to read and understand by creating a custom printer
for them.

Z3 expressions can be printed in 2 forms:

1. Using the builtin `str(e)` function
2. Using the `e.sexpr()` method

Problem is that (1) is a bit hard to read because its line breaks are not so
intuitive. (2) is a bit nicer, but the `to_int` and `to_real` functions clutter things up.

The custom printer is an improved `sexpr()` function:

- Leaves everything in one line
- Gets rid of `to_int` and `to_real` functions
- Reconstruct the floor division operations
- Merge commutative operation chains

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106643
Approved by: https://github.com/ezyang
","['test/dynamo/test_misc.py', 'test/test_fx_experimental.py', 'torch/fx/experimental/symbolic_shapes.py', 'torch/fx/experimental/validator.py']",The default Z3 expressions printer is hard to read and understand due to non-intuitive line breaks and unnecessary clutter from `to_int` and `to_real` functions.
1518d5eec4425b74b6114d18dd8998df20d0fb0a,1689365065,"Update Documentation for TripletMarginLoss (#105115)

This PR updates the documentation for `TripletMarginLoss` in `torch.nn`. The previous version of the documentation didn't mention the parameter `eps` used for numerical stability.

This PR does the following:
1. Describes the purpose and use of the `eps` parameter in the `TripletMarginLoss` class documentation.
2. Includes `eps` in the example usage of `TripletMarginLoss`.

Please review this update for the completeness with respect to the `TripletMarginLoss` functionality. If there are any issues or further changes needed, please let me know.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105115
Approved by: https://github.com/mikaylagawarecki
",['torch/nn/modules/loss.py'],The documentation for `TripletMarginLoss` in `torch.nn` lacks mention and detailed explanation of the `eps` parameter which affects numerical stability.
579ae64d81b7c14313804ca82716dc52f07ce18a,1675748757,"[mobile] List all missing ops at once (#94205)

List all missing ops rather than early termination

Test on device
Logcat lists all operators:
```
12-06 00:23:36.523  8299  8299 F DEBUG   : Abort message: 'terminating with uncaught exception of type c10::Error: Following ops cannot be found: [aten::max_pool2d, aten::conv2d]. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/c/mobile/ ()
12-06 00:23:36.523  8299  8299 F DEBUG   : Exception raised from initialize_operators at xplat/caffe2/torch/csrc/jit/mobile/function.cpp:89 (most recent call first):
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94205
Approved by: https://github.com/JacobSzwejbka
",['torch/csrc/jit/mobile/function.cpp'],The application is terminating early instead of listing all missing operators when there is a c10::Error. It only shows the first operator which cannot be found.
f7b384cc464ccbf481a9e89a383951d821ce5f95,1671243353,"[reland][quant][pt2e] Add early prototype top level quantize_pt2e APIs (#91035)

Summary:
This PR introduces the top level APIs for quantization support in PyTorch 2.0 Export stack
* torch.ao.quantization.quantize_pt2e.prepare_pt2e
Takes a model that is captured by the PyTorch 2.0 export (torchdynamo full graph mode) and prepares the model for calibration
for post training quantization

* torch.ao.quantization.quantize_pt2e.convert_pt2e
Takes a calibrated model and converts that to a reference quantized model that can be lowered later to quantized operator libraries or delegation modules

Also added a backend config for the qnnpack_pt2e backend:
* torch.ao.quantization.backend_config.get_qnnpack_pt2e_backend_config

Note: everything related to quantize_pt2e are experimental (prototype), and we don't have any bc guarantees

Test Plan:
python test/test_quantization.py TestQuantizePT2EModels

Reviewers:

Subscribers:

Tasks:

Tags:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91035
Approved by: https://github.com/HDCharles
","['test/quantization/fx/test_quantize_pt2e.py', 'test/test_quantization.py', 'torch/ao/quantization/_pt2e/utils.py', 'torch/ao/quantization/_quantize_pt2e.py', 'torch/ao/quantization/backend_config/_qnnpack_pt2e.py', 'torch/ao/quantization/fx/prepare.py']","Top level APIs for quantization support in PyTorch 2.0 Export stack are missing, preventing the preparation and conversion of models for post-training quantization. Also, a backend config for the qnnpack_pt2e backend is needed."
e311bed2a8e014f0ccf6fdc3fce11884982ac930,1688088741,"Turn translation validation on for tests and accuracy runs by default. (#103611)

This PR turns translation validation on by default for tests and accuracy benchmark
runs. It also installs Z3 on CI.

The main changes are:

- Add `--no-translation-validation` as an option in _test/run_tests.py_
    - Set `PYTORCH_TEST_WITH_TV` environment variable
- Add `TEST_WITH_TV` variable in _torch/testing/_internal/common_utils.py_
- Turn translation validation on for accuracy benchmarks in _benchmarks/dynamo/common.py_
- Add Z3 installation on CI scripts

Pull Request resolved: https://github.com/pytorch/pytorch/pull/103611
Approved by: https://github.com/ezyang
","['.ci/docker/common/install_inductor_benchmark_deps.sh', '.ci/pytorch/common_utils.sh', '.ci/pytorch/win-test.sh', 'benchmarks/dynamo/common.py', 'test/dynamo/test_dynamic_shapes.py', 'test/dynamo/test_functions.py', 'test/dynamo/test_misc.py', 'test/dynamo/test_repros.py', 'test/run_test.py', 'test/test_proxy_tensor.py', 'torch/testing/_internal/common_utils.py']",Tests and accuracy benchmarks are currently running with translation validation disabled by default. There is also no Z3 installed on the CI.
f1f57e1e54593b279dcb856cc9fd8d3cfcd4e44b,1685977831,"trigger tracing for MTIA events (#102288)

Summary: trigger tracing for MTIA events on python side when ProfilerActivity.MTIA is specified

Test Plan:
Test diff: D45437426

```
hg graft D45437426
```
- in one terminal

```
cd ~/fbsource/fbcode
buck2 run -j 8 \
    //infra_asic_fpga/firmware/tools/mad/service:mad_service
```
- in another terminal

Pytorch profiler
```
buck run mode/dev-nosan -j 8 //caffe2/torch/fb/acc_runtime/afg/tests:test_afg  -- -m kernel_add
```

Differential Revision: D46122853

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102288
Approved by: https://github.com/aaronenyeshi
","['torch/autograd/profiler.py', 'torch/profiler/profiler.py']",MTIA events are not triggering tracing in the Python side when ProfilerActivity.MTIA is specified.
711ded688de0e351eddfbe5751260fa23bc697c7,1625862652,"Add a script to codemod max_tokens_total pragmas to C/C++ files (#61369)

Summary:
This PR adds a new script: `max_tokens_pragmas.py`

This is a utility script that can add/remove `max_tokens_total` pragmas from the codebase.

- [x] Implement script and test manually
- [x] Write test script

Examples:
First, change directories
```bash
cd tools/linter/clang_tidy
```

Then run the following:
```bash
cat << EOF > test/test1.cpp
// File without any prior pragmas

int main() {
    for (int i = 0; i < 10; i++);
    return 0;
}
EOF

cat << EOF > test/test2.cpp
// File with prior pragmas

#pragma clang max_tokens_total 1

int main() {
    for (int i = 0; i < 10; i++);
    return 0;
}
EOF

cat << EOF > test/test3.cpp
// File with multiple prior pragmas

#pragma clang max_tokens_total 1

// Different pragma; script should ignore this
#pragma clang max_tokens_here 20

int main() {
    for (int i = 0; i < 10; i++);
    return 0;
}

#pragma clang max_tokens_total 1
EOF

# Add pragmas to some files
python3 max_tokens_pragma.py --num-max-tokens 42 test/*.cpp
grep ""#pragma clang max_tokens_total 42"" test/*.cpp

# Remove pragmas from files
python3 max_tokens_pragma.py --strip test/*.cpp
grep ""#pragma clang max_tokens_total 42"" test/*.cpp # should fail

# Ignore files
python3 max_tokens_pragma.py --num-max-tokens 42 test/*.cpp --ignores test/test2.cpp
grep ""#pragma clang max_tokens_total 42"" test/*.cpp # should not list `test/test2.cpp`
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61369

Test Plan: `tools/linter/clang_tidy/test/test_max_tokens_pragma.py`

Reviewed By: malfet

Differential Revision: D29604291

Pulled By: 1ntEgr8

fbshipit-source-id: 3efe52573583769041a07e6776161d4d5bbf16a7
","['tools/linter/clang_tidy/__main__.py', 'tools/linter/clang_tidy/max_tokens_pragma.py', 'tools/test/test_max_tokens_pragma.py']","No existing means of programmatically adding or removing `max_tokens_total` pragmas directly in C/C++ files, across the codebase."
ca499567d289a9eca26bb23e01e3b73e6160b702,1637450148,"barebones numeric suite for quantization with dynamic tracing (#67776)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67776

This adds a barebones `add_loggers` and `extract_logger_info` API
to analyze intermediate activations of models using quantization
with dynamic tracing.  The API generally matches the NS for FX tool,
with some omissions.  For now, this is moving fast to help us
debug real models, and the API will be 100% aligned before this is marketed to users,
in future PRs.

Note: the current approach couples Numeric Suite with the quantization
logic. This is not the best for composability, and may be changed
at a future time.

Test Plan:
```
python test/test_quantization.py TestAutoTracing.test_numeric_suite
```

```
python test/test_quantization.py TestAutoTracing.test_numeric_suite
```

Differential Revision:
D32231332
D32231332

Reviewed By: jerryzh168

Pulled By: vkuzo

fbshipit-source-id: 8adfb50cd8b7836c391669afe2e2ff6acae6d40a
","['test/quantization/dbr/test_quantize_dbr.py', 'torch/ao/ns/_numeric_suite_dbr.py', 'torch/ao/quantization/_dbr/auto_trace.py', 'torch/ao/quantization/_dbr/quantization_state.py', 'torch/ao/quantization/_dbr/utils.py']","The Numeric Suite for quantization utilizing dynamic tracing is currently barebones, causing issues for analyzing intermediate activations of models. Current coupling of Numeric Suite with quantization logic lacks composability purity."
f807229fd448b431b8cfd2bd33d8142eee7c8859,1628637676,"[ONNX] add support for prim::Unitialized in lower_tuples pass (#56912)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/56911

Code from issue generates this Torchscript:
```
graph(%self : __torch__.MyModule,
      %t.1 : Tensor):
  %12 : None = prim::Constant()
  %7 : str = prim::Constant[value=""Negative input""]() # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:11:28
  %3 : int = prim::Constant[value=0]() # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:10:15
  %9 : int = prim::Constant[value=5]() # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:13:31
  %33 : (Tensor, Tensor) = prim::Uninitialized()
  %4 : Tensor = aten::lt(%t.1, %3) # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:10:11
  %6 : bool = aten::Bool(%4) # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:10:11
  %34 : (Tensor, Tensor) = prim::If(%6) # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:10:8
    block0():
       = prim::RaiseException(%7) # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:11:12
      -> (%33)
    block1():
      %11 : int[] = prim::ListConstruct(%9)
      %16 : Tensor = aten::zeros(%11, %12, %12, %12, %12) # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:13:19
      %18 : int[] = prim::ListConstruct(%9)
      %23 : Tensor = aten::zeros(%18, %12, %12, %12, %12) # /mnt/nvdl/usr/msladek/notes/python_code/unitialized.py:13:35
      %24 : (Tensor, Tensor) = prim::TupleConstruct(%16, %23)
      -> (%24)
  return (%34)
```

Problem is that onnx exporter during lower_tuples pass doesn't support forwarding of tuples in prim::Unitialized.
Solution is:
1. add prim::Unitialized to supported_op in lower_tuples pass
1. As prim::Unitialized has now multiple outputs, we should call giveFreshAlias for every output

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56912

Reviewed By: nikithamalgifb

Differential Revision: D29837200

Pulled By: SplitInfinity

fbshipit-source-id: 321fae6fe52b1523df5653dbb9ea73b998ef1cda
","['test/onnx/test_pytorch_onnx_onnxruntime.py', 'torch/csrc/jit/ir/alias_analysis.cpp', 'torch/csrc/jit/passes/lower_tuples.cpp']","The ONNX exporter doesn't support forwarding of tuples in prim::Uninitialized during the lower_tuples pass, causing issues in the generated Torchscript."
b42c50da260034c6b64b7fec4eb410913daa903c,1657158929,"[Profiler] Eliminate transitive include of profiler implementation headers. (#80564)

This PR hides `torch/csrc/profiler/collection.h` and `torch/csrc/profiler/kineto_shim.h` from the public headers by forward declaring the symbols in `profiler_kineto.h` and hiding them in a unique_ptr.

I want to directly use Kineto symbols (specifically, the enums) in profiler; however without this change they leak into the PyTorch public headers and that's no good. Thus, we have to break that dependency.

Differential Revision: [D37365997](https://our.internmc.facebook.com/intern/diff/D37365997/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80564
Approved by: https://github.com/aaronenyeshi
","['torch/csrc/autograd/init.cpp', 'torch/csrc/autograd/profiler_kineto.cpp', 'torch/csrc/autograd/profiler_kineto.h', 'torch/csrc/profiler/kineto_shim.cpp', 'torch/csrc/profiler/kineto_shim.h']","Kineto symbols that are being directly used in the profiler are leaking into the PyTorch public headers, creating an undesirable dependency and exposure."
5bb5bfccf74ec03a9a974da05b81ca45bbbcd9ee,1636393405,"[lint] add lintrunner support for circleci_linter (#67872)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67872

As title. This demonstrates some of the nice features of lintrunner:
- Uniform error reporting means you get a nice diff of the changes for
free
- Can run with -a to just accept the changes (don't need to tell people
to run a special regenerate command since the linter adaper already knows how.)

Differential Revision:
D32187386
D32187386

Test Plan: Imported from OSS

Reviewed By: driazati

Pulled By: suo

fbshipit-source-id: 71de6b042730be80ff6794652039e9bc655a72b1
","['tools/linter/adapters/circleci_linter.py', 'tools/linter/adapters/clangformat_linter.py', 'tools/linter/adapters/clangtidy_linter.py', 'tools/linter/adapters/flake8_linter.py', 'tools/linter/adapters/grep_linter.py', 'tools/linter/adapters/mypy_linter.py']","The existing linter lacks support for CircleCI, leading to non-uniform error reporting and an inability to run with -a for accepting changes automatically."
d726ce6668307cb2198b43693201b266857f5452,1615517013,"Support loading a non-DP/DDP model from a DP/DDP state_dict (#53224)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53224

Loading a DP/DDP dict just needs to strip the module prefix from all items in the state dict and the metadata.

One existing example is here: https://github.com/facebookresearch/fvcore/blob/master/fvcore/common/checkpoint.py#L239.

#Closes: https://github.com/pytorch/pytorch/issues/41048/
ghstack-source-id: 123722976

Test Plan:
buck test mode/dev-nosan caffe2/test:nn -- test_load_state_dict
buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_save_load_checkpoint

Reviewed By: rohan-varma, mrshenli

Differential Revision: D26798495

fbshipit-source-id: 035c7d0907d7ae8f0d7ca21ec71f7f96ef8df6c8
","['test/distributed/test_c10d.py', 'test/test_nn.py', 'torch/nn/modules/utils.py', 'torch/nn/parallel/distributed.py']",Unable to load a non-DP/DDP model from a DP/DDP state_dict due to module prefix discrepancies.
93be0e2053b191a9c2553ad9a565a36d16e8595e,1647990358,"[SR] Avoid boxing inputs in DictConstruct/ListUnpack (#74250)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74250

The `DictConstruct`/`ListUnpack` implementations currently put all of their inputs onto a stack before calling the JIT implementation in `vararg_functions.cpp`. This was done to avoid code duplication, but it's quite wasteful since it causes extra heap allocations and, potentially, refcount bumps.

Given that these two ops are quite common and the code duplication is only a few lines, it seems reasonable to avoid this cost.
ghstack-source-id: 151897634

Test Plan: Existing unit tests

Reviewed By: navahgar

Differential Revision: D34901245

fbshipit-source-id: ece0618a6134a35720f214e79c64f12045f074d0
(cherry picked from commit 1f8e223c1887ed205c84a7ac4587813f94b11bad)
",['torch/csrc/jit/runtime/static/native_ops.cpp'],Current implementations of `DictConstruct` and `ListUnpack` introduce unnecessary heap allocations and potentially extra refcounts due to boxing inputs onto a stack prior to calling the JIT implementation.
4eec865f5802f2c34eb34f9f89165f0aa4b5502f,1652894494,"[nvFuser] Improving bitwise ops support (#77158)

- Some renaming to better match PyTorch API:
  - `lshift` -> `bitwise_left_shift`
  - `rshift` -> `bitwise_right_shift`
  - `andOp` -> `bitwise_and`
  - `orOp` -> `bitwise_or`
  - `xorOp` -> `bitwise_xor`
  - `notOp` -> `bitwise_not`
- Fix type inferences and type checking of these ops
- Add `bitwise_*` to parser and python frontend
- Improve test coverage
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77158
Approved by: https://github.com/kevinstephano, https://github.com/jjsjann123
","['test/test_jit_cuda_fuser.py', 'torch/csrc/jit/codegen/cuda/arith.cpp', 'torch/csrc/jit/codegen/cuda/arith.h', 'torch/csrc/jit/codegen/cuda/parser.cpp', 'torch/csrc/jit/codegen/cuda/python_frontend/python_bindings.cpp', 'torch/csrc/jit/codegen/cuda/type_inference.cpp']","There's a discrepancy between the naming convention used in the bitwise operations in `nvFuser` and the PyTorch API, plus inadequate type inferences and type checkings of these operations."
ec144b9412ab2e592de95377236393bc5605622c,1683570885,"handle new param from torch.compile (Inductor pattern matcher), enable_log (#100814)

This PR puts a placeholder param handler for a new param being passed in from Inductor, enable log.
Fixes this error below, where I've been unable to run torch.compile on NanoGPT due to this error:

~~~
File ""/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_inductor/fx_passes/fuse_attention.py"", line 219, in _sfdp_init
    register_replacement(
  File ""/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_inductor/pattern_matcher.py"", line 658, in register_replacement
    search_gm = trace_fn(search_fn, example_inputs)
  File ""/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_inductor/pattern_matcher.py"", line 828, in training_graph
    aot_function(
torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:
TypeError: patched_aot_function() got an unexpected keyword argument 'enable_log'
~~~

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100814
Approved by: https://github.com/fegin
",['torch/distributed/_spmd/aot_function_patch.py'],Running torch.compile on NanoGPT fails due to TypeError: patched_aot_function() receiving an unexpected keyword argument 'enable_log'.
88ed93c2ca0853d7841b3526fb42ca55376438b5,1633996091,"Fix type checking errors in torch/quantization/fx/qconfig_utils.py (#66428)

Summary:
- [x] Fix the Pyre type checking errors in `torch/quantization/fx/qconfig_utils.py`
```
torch/quantization/fx/qconfig_utils.py:241:46 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.
torch/quantization/fx/qconfig_utils.py:267:46 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.
torch/quantization/fx/qconfig_utils.py:284:43 Incompatible variable type [9]: fuse_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.
```
Fixes the issue: [MLH-Fellowship/pyre-check/issues/73](https://github.com/MLH-Fellowship/pyre-check/issues/73)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/66428

Reviewed By: grievejia

Differential Revision: D31545215

Pulled By: 0xedward

fbshipit-source-id: 767ae7888854c2eec2ecf14855a5b011110b9271
",['torch/ao/quantization/fx/qconfig_utils.py'],"The variables `prepare_custom_config_dict`, `convert_custom_config_dict`, `fuse_custom_config_dict` in `torch/quantization/fx/qconfig_utils.py` are declared as `Dict[str, typing.Any]` but are being used as `None` resulting in incompatible variable type errors."
bd4c4537dc477b9a4df4cb44c2042a10d31341ab,1666738372,"aten cpu and xnnpack to be compatible with arvr mode build (#87125)

Summary:
When building 3d photo sdk generator package in arvr/mode/mac and arvr/mode/mac-arm modes, we got several issues with aten cpu and xnnpack libraries.

The reason is that those packages are using platform-* properties (platform-deps, platform-srcs...) which are not compatible with arvr modes.

This diff fixes those issues by using `select` for non-platform properties when is_arvr_mode() is true, while keeping those platform ones for non-arvr modes.

Test Plan:
```
buck build //arvr/projects/compphoto/photo3d_sdk/unity/plugin:generator_plugin_shared arvr/mode/mac-arm/dev
buck build //arvr/projects/compphoto/photo3d_sdk/unity/plugin:generator_plugin_shared arvr/mode/mac-arm/opt

buck build //arvr/projects/compphoto/photo3d_sdk/unity/plugin:generator_plugin_shared arvr/mode/mac/dev
buck build //arvr/projects/compphoto/photo3d_sdk/unity/plugin:generator_plugin_shared arvr/mode/mac/opt
```

and sandcastle builds

Differential Revision: D40028669

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87125
Approved by: https://github.com/kimishpatel
","['c2_defs.bzl', 'third_party/xnnpack.buck.bzl']",Building 3D photo sdk generator in arvr mode for both mac and mac-arm leads to compatibility issues with aten cpu and xnnpack libraries due to the use of platform-* properties.
7c3a30fd79a868e5e3e4cd67e520938f1a8483b4,1620171536,"fx quant: remove matching hack for binary qhandler (#57470)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57470

Removes the earlier hack of matching patterns originally matched
to BinaryOpQuantizeHandler to switch to CopyHandler. After this PR,
each pattern can only be matched to one type of QuantizeHandler or
to nothing.

Test Plan:
```
python test/test_quantization.py TestQuantizeFx
python test/test_quantization.py TestQuantizeFxOps
```

Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D28152909

fbshipit-source-id: afc285e770bd7eb0518c90e3ee4874c421e78bbc
","['torch/quantization/fx/quantization_patterns.py', 'torch/quantization/fx/quantize.py']",Patterns in BinaryOpQuantizeHandler are being improperly matched to CopyHandler due to a hack in the current setup.
e9a51a6a07b73d9ef097dc6baa5b5ac562d8f0cd,1697595589,"[BE] Revive test_typing (#111428)

`test_typing.py` was written to use `pytest` in https://github.com/pytorch/pytorch/pull/54234 which unfortunately rendered it incompatible with run_test.py, and therefore it was not running in CI all this time.

In this PR, same functionality is re-written using unittest framework, and `parametrize` from `torch.testing._internal._common_utils`.

Valid `test_typing.py` with ufmt

Disable `fail/bitwise_ops.py` and `pass/jit.py` as it regressed at some point as well as one of examples in `namedtuple.py` as `torch.linalg.qr` type is no longer revealed correctly.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111428
Approved by: https://github.com/clee2000
","['test/run_test.py', 'test/test_typing.py', 'test/typing/fail/disabled_bitwise_ops.py', 'test/typing/pass/disabled_jit.py', 'test/typing/reveal/namedtuple.py']","The `test_typing.py` file is not compatible with run_test.py due to use of `pytest`, resulting in its tests not being executed during CI runs. Certain tests, including `fail/bitwise_ops.py` and `pass/jit.py`, along with an example in `namedtuple.py` have regressed."
782e4f5c02abaf5b9cdba4eaa827bc70a310bca8,1675993212,"[quant] Add quantize and dequantize operators to decomposition table (#93312)

Summary:
This PR tries to decompose the operators in torch.ops.quantized_decomposed namespace to more
primitive aten operators, this would free us from maintaining the semantics of the quantize/dequantize
operators, which can be expressed more precises in terms of underlying aten operators

Note: this PR just adds them to the decomposition table, we haven't enable this by default yet

Test Plan:
python test/test_quantization.py TestQuantizePT2E.test_q_dq_decomposition

Reviewers:

Subscribers:

Tasks:

Tags:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93312
Approved by: https://github.com/vkuzo, https://github.com/SherlockNoMad
","['test/quantization/fx/test_quantize_pt2e.py', 'torch/_meta_registrations.py', 'torch/ao/quantization/fx/_decomposed.py']","Lack of decomposition for operators in `torch.ops.quantized_decomposed` namespace to primitive aten operators, causing a higher maintenance overhead for the semantics of quantize/dequantize operators."
95ae9a20e495dfd8d2f30e728b751fe40ee8a450,1611677358,"Enable ROCM Skipped tests in test_ops.py (#50500)

Summary:
Removed skipCUDAIfRocm to re-enable tests for
ROCM platform.

Initially, Only 4799 cases were being run.
Out of those, 882 cases were being skipped.
After removing skipCUDAIfRocm from two places
in test_ops.py, now more than 8000 cases are
being executed, out of which only 282 cases
are bing skipped, which are FFT related tests.

Signed-off-by: Arindam Roy <rarindam@gmail.com>

Fixes #{issue number}

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50500

Reviewed By: albanD

Differential Revision: D25920303

Pulled By: mrshenli

fbshipit-source-id: b2d17b7e2d1de4f9fdd6f1660fb4cad5841edaa0
",['test/test_ops.py'],"Large number of test cases in test_ops.py for ROCM platform are being skipped due to skipCUDAIfRocm, significantly reducing the coverage."
15884418e25ada69c175f9803f3b2c4757cd7661,1651114869,"[quant][core][gpu][improvement] Made exception throwing message clearer in Quantized Cudnn Conv2d, Linear, and Add ops (#76102)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76102

Summary plan:
Previous exception throwing messages for these 3 ops did not
differentiate amongst each other. This PR addresses that issue by
including the name of the op in the string thrown by the exception for
debugging purposes.

Test Plan:
N/A as this only changes what's written to stdout when an exception is
thrown in one of the 3 ops

Reviewed By: jerryzh168

Differential Revision: D35779082

Pulled By: dzdang

fbshipit-source-id: 4e9098995ca2b1dcd18fe294281f335740a63ec9
(cherry picked from commit 503524f3ae569f59a3b1149f68454b3b51658e3d)
","['aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp', 'aten/src/ATen/native/quantized/cudnn/Conv.cpp', 'aten/src/ATen/native/quantized/cudnn/Linear.cpp']","Exception messages in Quantized Cudnn Conv2d, Linear, and Add ops are unclear and do not distinguish between each operation, making debugging difficult."
c1343ff706b9ca55d58650906eb827e2f1137257,1633733982,"[Pytorch Edge] Support profiling kineto events from external source (#64397)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64397

This diff exposes a way to add events to kineto profiler from external
source.
This can be a backend that executes a subgraph and wants to record this
execution in kineto profiler.
This diff also adds ""backend"" metadata to identify the backend an event
would have executed on.

Test Plan:
test_lite_interpreter

Imported from OSS

Reviewed By: raziel

Differential Revision: D30710710

fbshipit-source-id: 51399f9b0b647bc2d0076074ad4ea9286d0ef3e2
","['test/cpp/jit/test_backend_compiler_lib.cpp', 'test/cpp/lite_interpreter_runtime/test_mobile_profiler.cpp', 'torch/csrc/autograd/profiler_kineto.cpp', 'torch/csrc/autograd/profiler_kineto.h', 'torch/csrc/jit/mobile/profiler_edge.cpp', 'torch/csrc/jit/mobile/profiler_edge.h']",External backend execution of a subgraph cannot be recorded in the kineto profiler. There is also no way to identify which backend an event executed on.
ecf7e96969dec08f5e0091f1584557f13c290c18,1635209045,"[Light] Remove ambiguity from compile_spec names, use actual output type (#67209)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67209

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67198

Fixing a couple instances where parameters were named method_compile_spec when they were actually compile_specs that could have multiple method_compile_specs inside.
Also use output dtype from buffer.

Test Plan:
Mobilenetv3 compiles and runs fine
```
(pytorch)  ~/fbsource/fbcode/caffe2/fb/nnc
└─ $ PYTORCH_JIT_LOG_LEVEL=""aot_compiler"" buck run //caffe2/binaries:aot_model_compiler -- --model mobilenetv3.pt --model_name=pytorch_dev_mobilenetv3 --model_version=v1 --input_dims=""1,3,224,224
""
Downloaded 4501/6195 artifacts, 433.89 Mbytes, 14.3% cache miss (for updated rules)
Building: finished in 06:34.6 min (100%) 20233/20233 jobs, 5467/20233 updated
  Total time: 06:35.0 min
BUILD SUCCEEDED
The compiled llvm assembly code was saved to mobilenetv3.compiled.ll
The compiled model was saved to mobilenetv3.compiled.pt

└─ $ ./compile_model.sh -m pytorch_dev_mobilenetv3 -p /data/users/priyaramani/fbsource/fbcode/caffe2/fb/nnc/mobilenetv3.pt -v v1 -i ""1,3,224,224""
+ VERSION=v1
+ getopts m:p:v:i:h opt
+ case $opt in
+ MODEL=pytorch_dev_mobilenetv3
.
.
Columns 961 to 9701e-11 *
-4.2304 -3.9674  2.4473 -0.8664 -0.7513  1.2140  0.0010  3.8675  1.2714  2.2989

Columns 971 to 9801e-11 *
-2.7203  1.6772 -0.7460 -0.6936  4.4421 -0.9865 -0.5186 -1.4441  1.3047 -1.6112

Columns 981 to 9901e-11 *
 0.1275 -1.8815  2.5105 -0.4871 -2.2342  0.8520  0.8658  1.6180  3.8901 -0.2454

Columns 991 to 10001e-11 *
-1.4896  4.1337 -2.6640  0.8226  0.2441 -1.4830 -1.7430  1.8758  0.5481  0.5093
[ CPUFloatType{1,1000} ]
Starting benchmark.
Running warmup runs.
Main runs.
Main run finished. Milliseconds per iter: 276.255. Iters per second: 3.61984
Memory usage before main runs: 104366080 bytes
Memory usage after main runs: 343441408 bytes
Average memory increase per iter: 2.39075e+07 bytes
0 value means ""not available"" in above
```

Reviewed By: ljk53

Differential Revision: D31698338

fbshipit-source-id: da6c74c1321ec02e0652f3afe6f97bf789d3361b
","['binaries/aot_model_compiler.cc', 'torch/csrc/jit/mobile/nnc/aot_compiler.cpp']","Parameters in compile_specs incorrectly labeled as method_compile_spec, leading to confusion. Also, output dtype not effectively utilized from buffer."
8316affc45c820d298ac46224cf5a08f7ddeb256,1692622932,"Add frame/recompile counter to all log messages in tracing context (#107530)

All log messages that occur while running Dynamo compilation now have `[X/Y]` added to the beginning of their message. X represents the frame being compiled, while Y says which compilation of the frame. For example, if you are debugging a frame that is repeatedly recompiling, you can look for N/0, N/1, N/2, etc. for the same N.  Here is what the logs look like as you transition from one frame to another:

<img width=""1372"" alt=""image"" src=""https://github.com/pytorch/pytorch/assets/13564/4897e368-1e50-4807-b342-54e911bcf087"">

To accurately get this prefix added to all messages, I had to expand the scope of the `tracing` context manager. Its scope now coincides with `log_compilation_event`. To do this, I had to populate fake mode lazily in the TracingContext, since it isn't created until later, inside the OutputGraph.

This subsumes the previous X.Y logging that was solely for dynamic shapes.

Unfortunately I had to reindent some stuff. Review the diff with whitespace off.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107530
Approved by: https://github.com/anijain2305
ghstack dependencies: #107505, #107516
","['test/dynamo/test_export.py', 'torch/_dynamo/convert_frame.py', 'torch/_dynamo/output_graph.py', 'torch/_guards.py', 'torch/_logging/_internal.py', 'torch/fx/experimental/symbolic_shapes.py']","Log messages during Dynamo compilation are not clearly associated with specific frames or compilation runs, leading to confusion during debugging repeated recompilations."
c5f63f859efa9123b6b1f5eda8cbfeb04381f2e5,1637794847,"Add slow path to `getCustomClassTypeImpl` (#68717)

Summary:
This fixes custom class registration issue when `typeid` is not guaranteed to be unique across multiple libraries, which is the case for libc++ runtime on MacOS 11 in particular for M1
From [libcxx/include/typeinfo](https://github.com/llvm-mirror/libcxx/blob/78d6a7767ed57b50122a161b91f59f19c9bd0d19/include/typeinfo#L139):
```
// -------------------------------------------------------------------------- //
//                          NonUniqueARMRTTIBit
// -------------------------------------------------------------------------- //
// This implementation of type_info does not assume always a unique copy of
// the RTTI for a given type inside a program. It packs the pointer to the
// type name into a uintptr_t and reserves the high bit of that pointer (which
// is assumed to be free for use under the ABI in use) to represent whether
// that specific copy of the RTTI can be assumed unique inside the program.
// To implement equality-comparison of type_infos, we check whether BOTH
// type_infos are guaranteed unique, and if so, we simply compare the addresses
// of their type names instead of doing a deep string comparison, which is
// faster. If at least one of the type_infos can't guarantee uniqueness, we
// have no choice but to fall back to a deep string comparison.
```

But `std::type_index` hash is computed always assuming that implementation is unique
By adding a slow path this problem can be fixed in those scenarios.

Fixes https://github.com/pytorch/pytorch/issues/68039

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68717

Reviewed By: seemethere

Differential Revision: D32605187

Pulled By: malfet

fbshipit-source-id: 8d50e56885b8c97dad3bc34a69c47ef879456dd1
","['.github/scripts/generate_ci_workflows.py', 'aten/src/ATen/core/ivalue.h']","The `typeid` is not unique across different libraries in case of libc++ runtime on MacOS 11, particularly for M1, causing issues in custom class registration with `getCustomClassTypeImpl`."
7629477ff713d9f8f763830519da0080eec1b030,1618597661,"Filter out more expected errors from sccache log (#56281)

Summary:
This PR extends `.jenkins/pytorch/print_sccache_log.py` to filter out a distracting ""error"" message that walterddr came across while debugging failures in https://github.com/pytorch/pytorch/issues/55176:

```
=================== sccache compilation log ===================
ERROR 2021-04-05T15:44:18Z: sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: """", stderr: ""/var/lib/jenkins/.cache/torch_extensions/test_compilation_error_formatting/main.cpp: In function ‘int main()’:\n/var/lib/jenkins/.cache/torch_extensions/test_compilation_error_formatting/main.cpp:2:23: error: expected ‘;’ before ‘}’ token\n int main() { return 0 }\n                       ^\n"" }
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56281

Test Plan: TODO (reviewers: is there an easy way to test this?)

Reviewed By: walterddr

Differential Revision: D27826064

Pulled By: samestep

fbshipit-source-id: 7322a830c1246820a5b2b7bbeaa4697ebd13b617
",['.jenkins/pytorch/print_sccache_log.py'],"The sccache log displays error messages expected during debugging failures, making the log cluttered and distracting for developers."
898482f1bfd93f94d0be32d9201cbb6a093ec272,1697169146,"[logging] log exceptions when provided (#111164)

This PR will cause logging.exception() to also dump the exception and stacktrace. Copied from https://github.com/python/cpython/blob/74723e11109a320e628898817ab449b3dad9ee96/Lib/logging/__init__.py#L707-L711

repro:

<details>

```python
import torch
import torch._inductor.config

torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = ""runtime_error""

def fn(x, y):
    return (x @ y).relu()

x, y = [torch.rand((16, 16), device='cuda') for _ in range (2)]
torch.compile(fn)(x, y)
```
run with TORCHDYNAMO_REPRO_AFTER=aot TORCHDYNAMO_REPRO_LEVEL=4

</details>

before:
```
...
[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.
```

now:
```
...
[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.
Traceback (most recent call last):
  File ""/data/users/dberard/scripts/relu_accuracy_issue.py"", line 10, in <module>
    torch.compile(fn)(x, y)
...
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111164
Approved by: https://github.com/eellison
",['torch/_logging/_internal.py'],"The logging.exception() method does not currently dump the exception and stacktrace, making it difficult to troubleshoot runtime errors."
4a033be4482441d3f61a887502d75356a90e6a6a,1660850268,"[functorch] reclassify svd as an allowed failure; add test (#83612)

svd when done on a batch of inputs vs the input in a for-loop may return
different results because svd isn't unique. So, instead of checking that
the output of vmap and the output of a for-loop are the same, we check
that matrix-multiplying the decomposed tensors results in the same
tensor when doing it under vmap vs under a for-loop.

Test Plan:
- new test
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83612
Approved by: https://github.com/samdow
",['functorch/test/test_vmap.py'],"Using svd on a batch of inputs versus a single input in a for-loop leads to inconsistent results, as svd is not designed to return unique outcomes."
70810a3691ee3199cb4c18979a76654ded42fc3b,1655152154,"turn on -Wall with a few exceptions in Bazel build

Summary:
We add the following exceptions:
 * sign-compare: this is heavily violated in our codebase
 * unknown-pragmas: we use this intentionally for some loop unrolling
   in CUDA

Because they are included in -Wall by default, we remove the following
warnings from our explicit list:
 * unused-function
 * unused-variable

Test Plan: Rely on CI.

Reviewers: alband, seemethere

Subscribers:

Tasks:

Tags:

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79306

Approved by: https://github.com/malfet
",['tools/rules/cu.bzl'],"Bazel build generates numerous 'sign-compare' and 'unknown-pragmas' warnings due to existing codebase practices and specific CUDA loop unrolling mechanism. Also, the explicit warning list contains redundant items that are default in -Wall, like 'unused-function' and 'unused-variable'."
b4395b046a722a6920a326957d869415e8041f06,1614891085,"Edit SiLU documentation (#53239)

Summary:
I edited the documentation for `nn.SiLU` and `F.silu` to:
- Explain that SiLU is also known as swish and that it stands for ""Sigmoid Linear Unit.""
- Ensure that ""SiLU"" is correctly capitalized.

I believe these changes will help users find the function they're looking for by adding relevant keywords to the docs.

Fixes: N/A

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53239

Reviewed By: jbschlosser

Differential Revision: D26816998

Pulled By: albanD

fbshipit-source-id: b4e9976e6b7e88686e3fa7061c0e9b693bd6d198
","['torch/nn/functional.py', 'torch/nn/modules/activation.py']","The documentation for `nn.SiLU` and `F.silu` does not explain that SiLU stands for ""Sigmoid Linear Unit"", and is also known as ""swish"", which may be confusing to some users."
d4b09dbab3cc9842b917ed5e74d8b901406502c3,1631227289,"[doc][hackathon] To add Adagrad Optimizer to the documentation (#63254)

Summary:
It has been discussed before that adding description of Optimization algorithms to PyTorch Core documentation may result in a nice Optimization research tutorial. In the following tracking issue we mentioned about all the necessary algorithms and links to the originally published paper  https://github.com/pytorch/pytorch/issues/63236.

In this PR we are adding description of Adagrad to the documentation.  For more details, we refer to the paper
http://jmlr.org/papers/v12/duchi11a.html

<img width=""658"" alt=""AdaGradAlgo"" src=""https://user-images.githubusercontent.com/73658284/132743276-a52ea3fb-70a5-4788-94b7-f99367907a26.png"">

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63254

Reviewed By: albanD

Differential Revision: D30852139

Pulled By: iramazanli

fbshipit-source-id: 9e496560a97e92be8386585b01d9bd3bba4b0c66
",['torch/optim/adagrad.py'],"PyTorch Core documentation is missing a description of the Adagrad optimizer, hindering users from fully understanding or applying optimization algorithms."
374278f4319375f835938ee776b0bc1ededc3e40,1625171179,"Improved sparse CSR tensor sampling method (#60283)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/59379

The improved sparse CSR tensor sampling method is described in https://pearu.github.io/csr_sampling.html that features:
- for specified `nnz`, one gets a CSR sample with the same `nnz`
- variability of the number of specified columns per row is maximized
- `crow_indices` content is randomized
- a given row specific `col_indices` content is sorted and filled with unique values (see also https://github.com/pytorch/pytorch/issues/60277)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60283

Reviewed By: bhosmer

Differential Revision: D29492605

Pulled By: cpuhrsch

fbshipit-source-id: 8d875b7c2b0573a9ab37047c6d8fe8b540295ce1
","['test/test_sparse_csr.py', 'torch/testing/_internal/common_utils.py']","Sparse CSR tensor sampling method does not deliver consistent 'nnz' values and has inadequate variation in the number of specified columns per row. Additionally, 'crow_indices' content isn't randomized, and row specific 'col_indices' are not sorted or filled with unique values."
3abbcf079d38d468a45073b13cb13627c9c0f367,1630105572,".github: Add cpp_docs job to current gcc5 workflow (#64044)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64044

Adds the cpp_docs job to the current workflow, also modifies the scripts
surrounding building docs so that they can be powered through
environment variables with sane defaults rather than having to have
passed arguments.

Ideally should not break current jobs running in circleci but those
should eventually be turned off anyways.

Coincides with work from:
* https://github.com/seemethere/upload-artifact-s3/pull/1
* https://github.com/seemethere/upload-artifact-s3/pull/2

Signed-off-by: Eli Uriegas <eliuriegas@fb.com>

cc ezyang seemethere malfet walterddr lg20987 pytorch/pytorch-dev-infra

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D30610010

Pulled By: seemethere

fbshipit-source-id: f67adeb1bd422bb9e24e0f1ec0098cf9c648f283
","['.circleci/scripts/cpp_doc_push_script.sh', '.circleci/scripts/python_doc_push_script.sh']","The current workflow for the gcc5 job is missing the cpp_docs job, and the scripts for building docs require explicit arguments instead of utilizing environment variables with default settings."
87b71e570eabaccae6f8273c239844d2a896db34,1682137318,"[Profiler] Support HTML plot output for profiler export_memory_timeline API (#99751)

Summary:
Support the file extension .html, which will include a PNG image of the plot embedded into an HTML file.

This allows users to avoid processing the timeline manually in their own frontend UI.

Test Plan:
CI Tests

Ran on resnet50 model and generated this html file w/ plot:
See attached html file: {F954232276}
Screenshot: {F954232469}

Differential Revision: D45152735

Pulled By: aaronenyeshi

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99751
Approved by: https://github.com/davidberard98
","['torch/profiler/_memory_profiler.py', 'torch/profiler/profiler.py']","Profiler's export_memory_timeline API lacks an option to output as an HTML file with embedded PNG image, forcing users to process the timeline manually in their frontend UI."
32d0c3e8eeeb2abd49d667d3fad6aa493b9385c0,1627061438,"Support for reference convert_fx working on gpu

Summary:
This PR enables gpu only quantization, best used with is_reference since
there are not many gpu kernels for ops as of now.

This PR mainly changes how qconfigs and their obs constructors operate once they
on modules qconfig. The function add_module_to_qconfig_obs_ctr takes the obs constructors on the original
qconfig, and configures them so that when invoked, the created obs will
be on whatever device the module occupies. (Once observers are created,
module.to(device) is already setup so that it moves any observers). To do this,
a new method and a few small chanegs were added to the _PartialWrapper class that
our observers already use to create constructors (without changing the
existing functionality). These changes work in
concert with changes to the prepare flow such that when the qconfigs are
propagated to the moduels (in quantize.py and qconfig_utils.py) they are configured using add_module_to_qconfig_obs_ctr.

Ideally this would work on other models but the is_reference support for
a lot of modules isn't there yet, those tests should be added in a
future PR

Test Plan:
python test/test_quantization.py TestQuantizeFxModels.test_static_gpu_convert_basic

python test/test_quantization.py TestQuantizeFxModels.test_switch_device_prepare_convert

python test/test_quantization.py TestQuantizeFxModels.test_prepare_serialize_switch_device_convert

python test/test_quantization.py TestQuantizeFx.test_qconfig_precedence

Reviewed By: vkuzo

Differential Revision: D29684114

fbshipit-source-id: 19fefb8e1998eaf212723e836276ccf39467f2e7
","['test/quantization/fx/test_quantize_fx.py', 'torch/quantization/fx/convert.py', 'torch/quantization/fx/match_utils.py', 'torch/quantization/fx/pattern_utils.py', 'torch/quantization/fx/prepare.py', 'torch/quantization/fx/qconfig_utils.py', 'torch/quantization/fx/quantization_patterns.py', 'torch/quantization/fx/utils.py', 'torch/quantization/observer.py', 'torch/quantization/qconfig.py', 'torch/quantization/quantize.py']","There's no support for GPU-only quantization and issues encountered when observer constructors are invoked - created observers do not occupy the same device as the module, hindering operation on GPU."
1d9c1cc00ab9fe024b2453853e684172897c9d67,1622904607,"[4/n] [c10d] Introduce the multi-tenancy feature in TCPStore (#58331)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58331

This PR is the final part of a stack that addresses the GitHub issue #41614; it introduces the multi-tenancy feature to the `TCPStore` class allowing two server stores to be instantiated with the same host:port pair.
ghstack-source-id: 130676394

Test Plan:
- Run the existing and newly-introduced tests.
- Run several smoke tests including the short code snippet referred in GitHub issue #41614.

Reviewed By: H-Huang

Differential Revision: D28453850

fbshipit-source-id: f9066b164305de0f8c257e9d5736e93fd7e21ec6
","['test/distributed/test_store.py', 'torch/lib/c10d/TCPStore.cpp', 'torch/lib/c10d/test/TCPStoreTest.cpp']","TCPStore currently does not support multi-tenancy, making it impossible to instantiate two server stores with the same host:port pair."
769df7430d113f5a5729042712bf058492ca970a,1657040886,"[lint] create a workflow consistency linter (#80200)

In order to maintain consistency between jobs, introduce a linter that
checks whether jobs sharing the same `sync-tag` are indeed the same.

`sync-tag` is just a dummy input on the reusable workflow. I chose to
use a dummy input over the following alternatives:
- The job's id isn't great, because we are likely to change a job's id
  (say, when upgrading CUDA or linux versions)
- The job's name doesn't work as we have build/test jobs that share the
  same name
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80200
Approved by: https://github.com/janeyx99
",['tools/linter/adapters/workflow_consistency_linter.py'],Jobs sharing the same `sync-tag` in reusable workflows are not maintaining consistency leading to potential workflow inconsistencies.
c468e35d834a1d233a2905cde93dc8f3b6f027dd,1641368696,"[caffe2] don't use __FUNCSIG__ when building for Windows with clang (#70561)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70561

When building with strict(er) compiler warnings on Windows, clang complains that `__FUNCSIC__` is a proprietary language extension. When using clang, it seems we can use `__PRETTY_FUNCTION__` instead, like we do on other platforms. This is also in line with the logic on L100:127.

Test Plan: CI

Reviewed By: kalman5

Differential Revision: D33386400

fbshipit-source-id: d45afa92448042ddcd1f68adc7a9ef4643276b31
",['c10/util/TypeIndex.h'],"Clang compiler warns about the use of proprietary language extension `__FUNCSIG__` when building on Windows, suggesting the use of `__PRETTY_FUNCTION__` instead."
bb8baea9329032d385b830b9a1bc2b93aaf756a0,1652095495,"[primTorch] flatten, squeeze, unsqueeze... (#77043)

This PR ...

Makes the following testing changes:

- Updates stride testing in test_python_reference_consistency to only check strides of dimensions with length > 1
- Creates reference inputs for reshape
- Creates reference inputs for chunk
- Extends the sample inputs for unsqueeze
- Extends the sample inputs for stack -- test_conj_view and test_neg_view are now xfailed
  - https://github.com/pytorch/pytorch/issues/77046

Makes the following architecture changes:
- Adds the refs.special (sub)module
- Adds the refs.nn.functional (sub)module

Adds the following prims:
- expand_dims
- view_of
- rev
- clone

Adds the following references:
  -  flatten
  - squeeze
  - unsqueeze
  - special.i0e
  - special.i1e
  - logical_or
  - logical_and
  - isclose
  - flip
  - stack
  - nn.functional.elu
  - chunk
  - clone
  - narrow

Identifies the following bugs in PyTorch today:
- https://github.com/pytorch/pytorch/issues/77054
- https://github.com/pytorch/pytorch/issues/77055

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77043
Approved by: https://github.com/ngimel
","['test/test_jit.py', 'test/test_jit_fuser_te.py', 'test/test_ops.py', 'torch/_decomp/decompositions.py', 'torch/_prims/__init__.py', 'torch/_prims/utils.py', 'torch/_refs/__init__.py', 'torch/_refs/nn/functional/__init__.py', 'torch/_refs/special/__init__.py', 'torch/testing/_internal/common_methods_invocations.py']","Inconsistencies and lack of testing for certain operations in PyTorch, such as expand_dims, view_of, and clone, along with inadequate reference inputs for reshape and chunk functions, potentially causing incorrect functionality."
d0c0e13b699d66b5217c557bb8664b0a166e7b67,1686267744,"[Specialized Kernel] Translate Kernel Assignment Logic from function.yaml to native_functions.yaml (#102576)

Updating `gen_executorch.translate_native_yaml()` to translate kernel assignments when converting `functions.yaml` to `native_functions.yaml`
---
Functions.yaml format:
```
- func: add.out
	type_alias:
		T0: [<Type>, <Type>]
		T1: [<Type>]
	dim_order_alias:
		D0: [0, 1, 2, 3]
		D1: [0, 3, 2, 1]
	kernels:
		- arg_meta: null
		  kernel_name: default_impl
		- arg_meta:
			self: [T0, D0]
			other:[T0, D0]
			out: [T0, D0]
		  kernel_name: test_impl
```

native_functions.yaml format
```
func: add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
  kernel:
    default: default_impl
    v<Version>/<TYPE Enum>;<DIM Order>|<TYPE Enum>;<DIM Order>|<TYPE Enum>;<DIM Order>: test_impl
```
Example: **'v1/6;0,1,2,3|3;0,1,2,3|6;0,1,2,3' : 'test_impl'**

## Note:
- If a ""kernels"" field is not present in functions.yaml (as it currently is), the output is unaffected
---
Design Doc: https://docs.google.com/document/d/1gq4Wz2R6verKJ2EFseLyPdAF0wqomnCrVDDJpRkYsRw/edit?kh_source=GDOCS#

Differential Revision: [D45971107](https://our.internmc.facebook.com/intern/diff/D45971107/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/102576
Approved by: https://github.com/larryliu0820
","['tools/test/test_executorch_gen.py', 'torchgen/executorch/model.py', 'torchgen/executorch/parse.py', 'torchgen/gen.py', 'torchgen/gen_executorch.py']","Kernel assignment translation from functions.yaml to native_functions.yaml format in `gen_executorch.translate_native_yaml()` is not supported, resulting in verification difficulties for these assignments."
f76d1c022e4d9f144ce5976f40b6433a8834ffba,1648138768,"[Dynamic RPC] Allow for optional world_size argument in init_rpc (#73372)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73372

This PR which allows for optional `world_size` argument in init_rpc. This makes changes in rendezvous to allow for `NoneType` for world_size and creates a new code path when initializing TensorPipe agent for init_rpc. The TensorPipe agent is protected by a critical section enforced using the store, so that only one node can create a TPAgent at a time.
This PR does not yet enable RPC commands between ranks.

Previously:
```python
os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '29500'
init_rpc(""worker0"", world_size=1, rank=0)
```

Now (only rank is needed):
```python
os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '29500'
init_rpc(""worker0"", rank=0)
```

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D34621651

Pulled By: H-Huang

fbshipit-source-id: 09dbb511d5a00c219a6ce0a35501ff2e388998b0
(cherry picked from commit 834aedc3256167399c323169ef2f0c9b3cf98dff)
","['torch/csrc/distributed/rpc/init.cpp', 'torch/csrc/distributed/rpc/tensorpipe_agent.cpp', 'torch/csrc/distributed/rpc/tensorpipe_agent.h', 'torch/distributed/__init__.py', 'torch/distributed/rendezvous.py', 'torch/distributed/rpc/__init__.py', 'torch/distributed/rpc/backend_registry.py', 'torch/distributed/rpc/options.py', 'torch/testing/_internal/distributed/rpc/rpc_test.py']","The `init_rpc` function requires a mandatory `world_size` argument, which is inconvenient when only rank is needed."
88616349d776ecd6b6a34b07b3564d5debb7ba5a,1696536255,"[state_dict][1/N] Implement the basic functions of distributed.checkpoint._state_dict (#105902)

This PR implements the basic functions of distributed.checkpoint._state_dict. This PR currently contains the flattening of optimizer state_dict which makes the PR too large. A later version may split it into 2 for a better code review.

Differential Revision: [D47647719](https://our.internmc.facebook.com/intern/diff/D47647719/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D47647719/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/105902
Approved by: https://github.com/wz337
","['test/distributed/checkpoint/test_state_dict.py', 'torch/distributed/checkpoint/state_dict.py', 'torch/distributed/fsdp/_shard_utils.py']","The basic functions of distributed.checkpoint._state_dict are not implemented, leading to issues with the flattening of the optimizer state_dict."
0e0f8fd03e0967059456d4755c47e8fd4407796d,1660276806,"Implement QAT for APoT (#83282)

### Summary:
This PR implements QAT for APoT FakeQuant. It runs QAT with FX graph mode quantized models (Resnet-18 pre-trained model, full ImageNet dataset) to compare accuracy metrics for different qconfig settings of uniform vs. APoT quantized activation and weight. It also refactors the APoT PTQ module `apot_fx_graph_mode_ptq.py` (previously `fx_graph_mode_apot.py`) such that shared helper functions between PTQ and QAT are in a separate file `quantization_util.py`.

Model #2 (uniformly quantized activation, APoT quantized weight) shows comparable accuracy compared to model #1 (uniformly quantized activation, APoT quantized weight) for 8-bit and significant accuracy improvement for 4-bit (see ""Accuracy Stats"" section below).

### Test Plan:
Run QAT models with: `python test/quantization/core/experimental/apot_qat.py`
Run PTQ models with: `python test/quantization/core/experimental/apot_ptq.py`

### Accuracy Stats
8-bit (Uniform int8, APoT b = 8 k = 2)

Model #1: Uniform activation, uniform weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 69.67% (Top-1), 89.04% (Top-5)

Model #2: Uniform activation, APoT weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 69.72% (Top-1), 89.06% (Top-5)

4-bit (Uniform int4, APoT b = 4 k = 2)

Model #1: Uniform activation, uniform weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 46.85% (Top-1), 72.85% (Top-5)

Model #2: Uniform activation, APoT weight (FX Graph Mode quantized)
Evaluation accuracy on test dataset: 66.45% (Top-1), 86.23% (Top-5)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83282
Approved by: https://github.com/jerryzh168
","['test/quantization/core/experimental/apot_fx_graph_mode_ptq.py', 'test/quantization/core/experimental/apot_fx_graph_mode_qat.py', 'test/quantization/core/experimental/quantization_util.py']","QAT and PTQ for APoT FakeQuant needs optimization, especially for M#2: Uniform activation, APoT weight. The accuracy of 4-bit QAT, as compared to 8-bit, is not satisfactory."
feefc94573730712b314d6af2d76db000a242a31,1632294151,"[fx2trt] Use itensor_to_tensor_meta to track the TensorMeta info for ITensor node (#65427)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65427

Previously we added a input_tensor_meta for dequantize function, this is a bit hacky since this creates a dependency between
the arguments of dequantize and if there are passes that changes the input then we would need to update tensor meta as well

Test Plan:
python torch/fx/experimental/fx2trt/example/quantized_resnet_test.py

Imported from OSS

Reviewed By: soulitzer

Differential Revision: D31094274

fbshipit-source-id: 5e40648d3081e2363f3a70bcc9745df4a8190ad3
","['torch/fx/experimental/fx2trt/converters/acc_ops_converters.py', 'torch/fx/experimental/fx2trt/converters/add.py', 'torch/fx/experimental/fx2trt/fx2trt.py', 'torch/fx/experimental/fx_acc/acc_ops.py']","The dependency between the arguments of the dequantize function and the input_tensor_meta is being mishandled, causing potential problems when passes change the input."
dcdc7b2ffc172afe102109d12bfa4f029a494411,1651598428,"[RF][scuba] add pytorch_operator_stats column for Static Runtime out variant (#76566)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76566

Add static_runtime_out_variant field to pytorch_operator_stats scuba.

Add field for static_runtime_out_variant to RecordFunction.

Test Plan:
`ptail perfpipe_pytorch_operator_stats_dev | grep devbig371`
No out variant, SR on: P498206546
Out variant: P498206634

Check column shows up in scuba: https://fburl.com/scuba/pytorch_operator_stats_dev/tfgmth1t

CMF 4M test https://www.internalfb.com/intern/servicelab/802987274/
ICVR 4M https://www.internalfb.com/intern/servicelab/802987272/

AF prod canary
https://our.intern.facebook.com/intern/ads/canary/443234131523314631

Reviewed By: mikeiovine

Differential Revision: D36016857

fbshipit-source-id: f3af315d1d2b0d8b147be76df63daa8ab872bf8e
(cherry picked from commit 208db7cd15fb3e1be66db2d1834eebaf0964d017)
","['aten/src/ATen/record_function.cpp', 'aten/src/ATen/record_function.h', 'torch/csrc/jit/runtime/static/impl.cpp']","The `pytorch_operator_stats` scuba lacks a static_runtime_out_variant field, making it impossible to capture stats for the Static Runtime 'out' variant."
7c3fa5c70dbbdc45bc51b29773753da966583483,1682013430,"Revert ""Build Windows binaries with Visual Studio 2022 Build Tools (#90855) (#99591)

This reverts commit a88c15a849152291b1ebdab13860726dd8be1d81.  Once we have the AMI ready, we can revert this and use VS2022 again.  This is to mitigate flaky Windows build in trunk https://github.com/pytorch/builder/issues/1387.

Note that as VS2019 is already available in the current AMI, it won't be installed again per logic in https://github.com/pytorch/builder/blob/main/windows/internal/vs2019_install.ps1#L25-L29. Thus, this helps avoid the flaky installation issue.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99591
Approved by: https://github.com/kit1980, https://github.com/Blackhex, https://github.com/malfet
","['.circleci/scripts/binary_windows_build.sh', '.circleci/scripts/binary_windows_test.sh']",Using Visual Studio 2022 Build Tools for building Windows binaries causes unstable and unreliable builds.
f1f3c8b0fad9d647454a4d0507a2db4381563c8e,1617212425,"Adding PyTorch + DNNL + AMD BLIS path (#54953)

Summary:
These changes provide the user with an additional option to choose the DNNL+BLIS path for PyTorch.

This assumes BLIS is already downloaded or built from source and the necessary library file is available at the location: $BLIS_HOME/lib/libblis.so and include files are available at: $BLIS_HOME/include/blis/blis.h and $BLIS_HOME/include/blis/cblas.h

Export the below variables to build PyTorch with MKLDNN+BLIS and proceed with the regular installation procedure as below:
$export BLIS_HOME=path-to-BLIS
$export PATH=$BLIS_HOME/include/blis:$PATH LD_LIBRARY_PATH=$BLIS_HOME/lib:$LD_LIBRARY_PATH
$export BLAS=BLIS USE_MKLDNN_CBLAS=ON WITH_BLAS=blis
$python setup.py install

CPU only Dockerfile to build PyTorch with AMD BLIS is available at : docker/cpu-blis/Dockerfile
Example command line to build using the Dockerfile:
sudo DOCKER_BUILDKIT=1 docker build . -t docker-image-repo-name
Example command line to run the built docker container:
sudo docker run --name container-name -it docker-image-repo-name

Fixes #{issue number}

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54953

Reviewed By: glaringlee

Differential Revision: D27466799

Pulled By: malfet

fbshipit-source-id: e03bae9561be3a67429df3b1be95a79005c63050
","['cmake/Dependencies.cmake', 'cmake/Modules/FindBLAS.cmake', 'cmake/Modules/FindBLIS.cmake']","PyTorch lacks a user option to follow the DNNL+BLIS path, assuming that the BLIS library and include files are downloaded or built from source and set at the specified location."
035310c5744c496a225a301ba60d4fdefe6b4e82,1634157571,"Handle shared memory cases in MathBithFallback (#63602)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63602

This PR fixes the case when a read and write is performed on a memory shared between mutable and (or) non-mutable arguments. Example:
```
a=torch.tensor([1+1j])
b=a.conj()
b.add_(a) # should return tensor([2]) but returns tensor ([2-2j])
```

The issue here is that in the conjugate fallback, we resolve the conjugation in-place for mutable arguments which can be a problem as shown above in the case when other input arguments share memory with the mutable argument(s).
This PR fixes this issue by:
1. first scanning through the operator input arguments and creating a vector of mutable arguments that have the conj bit set to `True` (and accordingly setting the flag `check_for_alias_with_mut_arg ` to `True` or `False`).
2. Iterating through all the arguments. At this time we only look at the non-mutable arguments. If `check_for_alias_with_mut_arg` is set to `True`, then we iterate through `mutable_inputs` to check if the current arg tensor in question doesn't alias any of the entries in `mutable_inputs`. If yes, then we clone the non-mutable tensor arg, else we resolve the conjugation as before.
3. Now we look through the mutable_inputs vector (which contains only mutable input tensors with conj bit set to `True`). We in-place conjugate each of the entries in the vector.
4. Do the computation.
5. Re-conjugate the mutable argument tensors.

NOTE: `TensorLists` are not fully handled in ConjugateFallback. Please see the in-line comment for more details.

Fixes https://github.com/pytorch/pytorch/issues/59943

Test Plan: Imported from OSS

Reviewed By: gmagogsfm

Differential Revision: D30466905

Pulled By: anjali411

fbshipit-source-id: 58058e5e6481da04a12d03f743c1491942a6cc9b
","['aten/src/ATen/ConjugateFallback.cpp', 'aten/src/ATen/native/MathBitFallThroughLists.h', 'aten/src/ATen/native/MathBitsFallback.h', 'aten/src/ATen/native/NegateFallback.cpp', 'test/test_view_ops.py']","In-place resolution of conjugates in MathBithFallback fails when dealing with shared memory cases involving mutable and non-mutable arguments, leading to incorrect results. TensorLists are not properly handled in ConjugateFallback."
6ca141fe6c72707efb7105884e65c806a3968426,1623191489,"Make detach return an alias even under inference mode (#59633)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59633

Fixes #59614

This fix isn't 100% correct but it appears to stem the bleeding.
A better fix would be understand how to detect when function
implementations don't uphold required invariants, leading to
refcount disaster.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: gchanan

Differential Revision: D28962183

Pulled By: ezyang

fbshipit-source-id: 6ec71994666289dadef47bac363e6902df90b094
","['aten/src/ATen/native/TensorProperties.cpp', 'aten/src/ATen/native/TensorShape.cpp']","Inference mode is causing the detach function not to return an alias, possibly leading to a flaw in function implementations and a refcount disaster."
a6e716cfedc4995f8bc563b6fdd63552f6695e14,1658204962,"[JIT] Add may_contains_alias function in SchemaInfo class (#81444)

- Created may_contain_alias method in SchemaInfo which is a wrapper around FunctionSchema may_contain_alias that also accounts for argument values. This is done using similar logic to AliasDB using an internal understanding of wildcard sets and container object
- Added a multitude of tests for various graph edge cases (inputs aliasing, outputs aliasing, multiple input wildcards, multiple container objects, etc...).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81444
Approved by: https://github.com/davidberard98
","['test/cpp/jit/test_schema_info.cpp', 'torch/csrc/utils/schema_info.cpp', 'torch/csrc/utils/schema_info.h']","Lack of method in SchemaInfo class to check for aliasing in argument values, taking into account various edge cases like multiple container objects, multiple input wildcards, etc."
37b468ac777ba548a2808010fd2f1b146b779fe0,1668116128,"[xnnpack][lite-int][on-device] rebuild serialized modules at runtime (#88780)

This is the on-device runtime work. We modify the compile and execute from our hacky solution from before to what will actually be running at runtime.

First we rebuild our graph from the serialized flatbuffer string. We also introduce a runtime wrapper that inherits CustomClassHolder that allows us to forward along the built xnngraph runtime to our execute function

Once the subgraph object has been rebuilt by our we pass it along to the runtime wrapper for us to forward along to execute

At execute we prep the input/outputs and invoke the runtime using our runtime wrapper. Finally we forward those results to our execution

Differential Revision: [D39413031](https://our.internmc.facebook.com/intern/diff/D39413031/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39413031/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88780
Approved by: https://github.com/digantdesai
","['test/jit/xnnpack/test_xnnpack_delegate.py', 'torch/csrc/jit/backends/xnnpack/xnnpack_backend_lib.cpp']","During on-device runtime, the serialized modules fail to rebuild, causing potential issues with graph recreation, input/output preparation and execution process."
dc63948dc9dbe4c224a78a7c14f406893f6fd381,1664425446,"[ONNX] Update behavior for `register_custom_op_symbolic` (#85636)

Update `register_custom_op_symbolic`'s behavior to _only register the symbolic function at a single version_. This is more aligned with the semantics of the API signature.

As a result of this change, opset 7 and opset 8 implementations are now seen as fallback when the opset_version >= 9. Previously any ops internally registered to opset < 9 are not discoverable by an export version target >= 9. Updated the test to reflect this change.

The implication of this change is that users will need to register a symbolic function to the exact version when they want to override an existing symbolic. They are not impacted if (1) an implementation does not existing for the op, or (2) they are already registering to the exact version for export.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85636
Approved by: https://github.com/BowenBao
","['test/onnx/internal/test_registraion.py', 'test/onnx/test_pytorch_onnx_no_runtime.py', 'test/onnx/test_utility_funs.py', 'torch/onnx/_internal/registration.py', 'torch/onnx/utils.py']","Custom operator symbolic function registered with `register_custom_op_symbolic` is not discoverable if the opset version is targeted >=9, requiring users to register to the exact version for overriding existing symbolics."
2edfcafd4b7db1cbe1a0a79a8625e54bd2527789,1680949034,"[inductor] remove RBLOCK from persistent reduction kernel's parameter list (#98653)

This PR resolves comments https://github.com/pytorch/pytorch/pull/97203#discussion_r1160491318 . Send a separate PR since it's easier to test and make sure there is no perf impact.

Tests:
1. python test/inductor/test_torchinductor.py
2. run `python benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --dashboard --only hf_Bert --disable-cudagraphs --training` before and after the change to make sure the perf change is neutral.

Now a persistent reduction kernel in hf_Bert looks like:
```
@persistent_reduction(
    size_hints=[4096, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    meta={'signature': {0: '*fp32', 1: '*i64', 2: '*fp16', 3: '*i64', 4: '*fp16', 5: '*i64', 6: '*fp16', 7: '*fp16', 8: '*fp16', 9: '*fp16', 10: 'i32', 11: 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': ['in_out_ptr0'], 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), equal_to_1=())]}
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 4096
    rnumel = 768
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98653
Approved by: https://github.com/jansel
","['torch/_inductor/codegen/triton.py', 'torch/_inductor/triton_heuristics.py']","The persistent reduction kernel's parameter list in the inductor package includes RBLOCK, which may not be necessary or could cause performance issues."
b524a1101ad1608582b064863282e12e0b05df4f,1628180770,"ns for fx: add ref_node_target_type (#62685)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62685

Adds a `ref_node_target_type` field to hold the string type
of the base node. This is needed because in some cases
the previous node does not match ref_node (if we have observers,
or if we are logging inputs), and it is useful to know the type
of ref_node.

Test Plan:
```
python test/test_quantization.py TestFXNumericSuiteCoreAPIs
```

Imported from OSS

Reviewed By: hx89

Differential Revision: D30082947

fbshipit-source-id: 98ded7b25a5d8d5ea820e0ef62c3799b65c3fc77
","['torch/quantization/_numeric_suite_fx.py', 'torch/quantization/ns/graph_passes.py', 'torch/quantization/ns/weight_utils.py']","Differences between previous node and ref_node in some cases (observers or logging inputs) are not properly accommodated, causing difficulty to identify the true type of ref_node."
c599cf24ad01a4befa58642f9e703792f53b79fa,1669760981,"[FSDP] Another fix for `DTensor`, `use_orig_params=True` (#89845)

The issue for `test_2d_parallel.py` is that `DTensor` does not support the idiom `param.data = view` where `view` is a `DTensor`. To work around this, we do not preserve the parameter variable `param` and instead create a new parameter variable altogether via `nn.Parameter(view)`. Preserving the parameter variable when unsharded was not a strict requirement -- it just made sense to do that if we are already doing that when _sharded_, where it _is_ a strict requirement to support the optimizer step. The sharded case is not an issue for 2D because sharded implies local tensor, not `DTensor`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89845
Approved by: https://github.com/zhaojuanmao
",['torch/distributed/fsdp/flat_param.py'],"`DTensor` does not support the idiom `param.data = view` causing a failure in `test_2d_parallel.py`. Also, the preservation of the parameter variable when unsharded is not executed correctly."
96c745dfdcc0a55b187eaed3c17277b94c791eaa,1678858421,"Fix int() casting in torch.nn.RNN to have correctly traced JIT and ONNX graph. (#92970)

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Fixes #91351

As for unit tests - in this PR I only fixed LSTM unit test to properly use dynamic axes and expose export issue by running test with same ONNX for additional inputs.
If the changes approved, we should also fix the rest of the tests (RNN/GRU and beyond).

I have verified the following updated tests are working with new code and failing with the old code:
test/onnx/test_pytorch_onnx_onnxruntime.py::TestONNXRuntime_opset_version_14_is_script_False_keep_initializers_as_inputs_True::test_rnn_name_lstm_nonlinearity_None_unilayer_bidirectional_no_initial_state_with_variable_length_sequences_with_dropout
test/onnx/test_pytorch_onnx_onnxruntime.py::TestONNXRuntime_opset_version_14_is_script_False_keep_initializers_as_inputs_True::test_rnn_name_lstm_nonlinearity_None_unilayer_bidirectional_with_initial_state_with_variable_length_sequences_with_dropout

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92970
Approved by: https://github.com/titaiwangms, https://github.com/kit1980
","['test/onnx/test_pytorch_onnx_onnxruntime.py', 'torch/nn/modules/rnn.py', 'torch/nn/utils/rnn.py']","In torch.nn.RNN, int() casting issue is hindering correct tracing of JIT and ONNX graph, leading to export problems."
57f72b8433d8b84a6e58cd4bca0313da77a688bc,1619995310,"[DDP] Uneven inputs: option to throw early (#56755)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56755

Rehash of https://github.com/pytorch/pytorch/pull/47488

Adds a flag to ddp join() context manager that enables throwing a
StopIteration across all ranks when this flag is specified.

To do this, we implement the design in #47250. When running with this flag, we schedule an additional allreduce in the case that a joined rank needs to throw a StopIteration. In non-joined ranks forward pass, we match this allreduce and if at least one rank tells us to throw, we raise a StopIteration.

Tested by modifying existing tests, as well as adding additional tests validating that this works with SyncBatchNorm models and a model with custom collectives in the forward pass.

Currently running perf benchmarks, will post when those are available, but we expect a small (~2%) perf reduction when enabling this feature due to the blocking allreduce. Hence we will only recommend it for models with collective comm.
ghstack-source-id: 127883115

Test Plan: Ci

Reviewed By: SciPioneer

Differential Revision: D27958369

fbshipit-source-id: c26f7d315d95f17bbdc28b4a0561916fcbafb7ca
","['torch/nn/parallel/distributed.py', 'torch/testing/_internal/distributed/distributed_test.py']","Uneven inputs in distributed data parallel (DDP) results in uneven iterations across different ranks, causing early termination of the program."
dbeda994db52a57052d21d0c24b2d1a8e05671e9,1613750650,"Update FindvecLib.cmake for macOS 10.14, 10.15 and Big Sur (#51288)

Summary:
When compiling libtorch on macOS there is the option to use the `vecLib` BLAS library from Apple's (Accelerate)[https://developer.apple.com/documentation/accelerate] framework. Recent versions of macOS have changed the location of veclib.h, this change adds the new locations to `FindvecLib.cmake`

To test run the following command:
```
BLAS=vecLib python setup.py install --cmake --cmake-only
```

The choice of BLAS library is confirmed in the output:
```
-- Trying to find preferred BLAS backend of choice: vecLib
-- Found vecLib: /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Accelerate.framework/Versions/Current/Frameworks/vecLib.framework/Versions/Current/Headers
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51288

Reviewed By: jbschlosser

Differential Revision: D26531136

Pulled By: malfet

fbshipit-source-id: ce86807ccbf66973f33b3acb99b7f40cfd182b9b
",['cmake/Modules/FindvecLib.cmake'],libTorch compilation on recent versions of macOS fails due to changed location of veclib.h in vecLib BLAS library.
cb4aeff7d8e4c70bb638cf159878c5204d0cc2da,1646967193,"[easy][PyTorchEdge] Add magic number to flatbuffer schema (#74048)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74048

ghstack-source-id: 151064703

Test Plan:
```
Executing in directory: /data/users/pavithran/fbsource
buck build //xplat/caffe2:mobile_bytecode --config client.id=nuclide

DEBUG: /data/users/pavithran/fbsource/tools/build_defs/fbcode_macros/build_defs/lib/cpp_common.bzl:287:14: Using disallowed linker flag 'ANativeActivity_onCreate' in library rule 'fbsource//third-party/toolchains/android-ndk:r18b_native_app_glue'
Parsing buck files: finished in 1.2 sec
Building: finished in 0.4 sec (100%) 1/1 jobs, 1/1 updated
  Total time: 1.7 sec
More details at https://www.internalfb.com/intern/buck/build/ad0db098-e3c1-465c-b69a-3cda4ab9c2ee
BUILD SUCCEEDED
```

Reviewed By: dbort

Differential Revision: D34797167

fbshipit-source-id: f3c115f80951bb11e17163283603aa7877c7c472
(cherry picked from commit 2ded6963c5d57b6c1e5ff15b8fa3b7d81e66bb33)
",['torch/csrc/jit/serialization/mobile_bytecode_generated.h'],The current flatbuffer schema in PyTorchEdge doesn't have a magic number for identifying or verifying the data.
c4af6ba173fcc318503227699a4f31055385fe43,1644360746,"Show friendly error message when forgetting `init` in `torch.cuda` (#72404)

Summary:
# Problem
The error message `RuntimeError: Invalid device argument` is not friendly when users just forget calling `torch.cuda.init()`.
This error message is shown for example by calling  `torch.cuda.reset_accumulated_memory_stats`, or other methods which internally calls [assertValidDevice](https://github.com/pytorch/pytorch/blob/6297aa114f8278f5cf5e661cba9bcf5c83f2ac1e/c10/cuda/CUDACachingAllocator.cpp#L1561-L1566).

# Reproduce
```python
$ python
Python 3.8.6 (default, Apr  1 2021, 08:23:31)
[GCC 7.5.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch.cuda
>>> torch.cuda.reset_accumulated_memory_stats(0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py"", line 219, in reset_accumulated_memory_stats
    return torch._C._cuda_resetAccumulatedMemoryStats(device)
RuntimeError: Invalid device argument.
>>> torch.cuda.current_device()
0
```

# This PR
Shows better error message like `RuntimeError: Invalid device argument 0: did you call init?`. I cited the error message from https://github.com/pytorch/pytorch/blob/6297aa114f8278f5cf5e661cba9bcf5c83f2ac1e/c10/cuda/CUDACachingAllocator.cpp#L1392-L1396.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/72404

Reviewed By: mruberry

Differential Revision: D34063268

Pulled By: ngimel

fbshipit-source-id: 0775d9c83a4a0eb0eb41bf6efecca94a00692141
(cherry picked from commit 07a1a3d0b41e1898d0c293ca77ca712de91df51e)
",['c10/cuda/CUDACachingAllocator.cpp'],Calling `torch.cuda.reset_accumulated_memory_stats` or similar methods without initiating `torch.cuda.init()` gives an unclear error message `RuntimeError: Invalid device argument`.
d49f6d556be98ac53135f649b1433d906b657a71,1620767698,"[DataLoader] Fix tempfile binding and removing for torch_shm_manager (#57566)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57566

Fix the problem that `tempfile` has never been deleted even after `torch_shm_manager` is destroyed.
- The previous implementation has wrong path length for the Linux Socket. It leads to we lose the last character of the name of `tempfile` when bind the pathname to socket. At the end, we can not delete this file due to unexpected file name.
- After we solve the racing problem by introducing a temporary directory, it becomes more dangerous since it prevents `torch_shm_manager` to delete directory as the tempfile persists in the temporary directory.

Test Plan: Imported from OSS

Reviewed By: VitalyFedyunin

Differential Revision: D28202866

Pulled By: ejguan

fbshipit-source-id: 912cfd8fec0cc309d47df223b2b0faa599c60799
","['c10/util/tempfile.h', 'torch/lib/libshm/core.cpp', 'torch/lib/libshm/manager.cpp', 'torch/lib/libshm/socket.h']","Tempfile persisting even after torch_shm_manager is destroyed due to an incorrect path length for Linux Socket, preventing deletion of the temporary directory."
0e67b2f7dd13db1fea421d860ede65a653738dfe,1667859884,"Dynamo Dashboard Improvements (#88516)

Implement various features in https://github.com/pytorch/torchdynamo/issues/1644:
- Upload nightly run logs to /fsx before parsing - for backing up parsing failures.
- Flag models with (1) < 0.95x speedup, (2) > 2min compile time, (3) < 0.9x compression ratio
- Flag models that were passing yesterday but failed today.
- Other small bug fixes.

See https://github.com/pytorch/torchdynamo/issues/1831 for sample outputs.
Also tested by running run.sh:
```bash
# Setup the output directory
rm -rf ../test-dynamo-runner-logs-3/
mkdir ../test-dynamo-runner-logs-3/

# Commands for torchbench for device=cuda, dtype=float32 for training and for performance testing
python benchmarks/dynamo/torchbench.py --performance --float32 -dcuda --output=../test-dynamo-runner-logs-3//inductor_torchbench_float32_training_cuda_performance.csv --training --inductor   --no-skip --dashboard --only mobilenet_v2 --cold_start_latency

# Commands for torchbench for device=cuda, dtype=float32 for training and for accuracy testing
python benchmarks/dynamo/torchbench.py --accuracy --float32 -dcuda --output=../test-dynamo-runner-logs-3//inductor_torchbench_float32_training_cuda_accuracy.csv --training --inductor   --no-skip --dashboard --only mobilenet_v2
```

with the command
`python benchmarks/dynamo/runner.py --output-dir ../test-dynamo-runner-logs-3/ --dashboard-archive-path /data/home/williamwen/dynamo-runner-logs-copy --training --run --compilers inductor --flag-compilers inductor --suites torchbench --update-dashboard` (need to comment out the `generate_commands` line and change the github issue ID from 681 to something else).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88516
Approved by: https://github.com/anijain2305
",['benchmarks/dynamo/runner.py'],"There is a lack of features for flagging certain conditions in Dynamo Dashboard, including speedup less than 0.95x, compile time taking more than 2 minutes, compression ratio less than 0.9x, and models that were functional yesterday but failed today. No backup system for run logs before parsing."
2a6a159c0ce96bb00d0973ab1c5db020c03a1815,1682963583,"Modify repeat_interleave docs to highlight potential overloading (#99650)

Fixes #99259 , drawing to attention that input is optional by putting a variation of the method signature at the top of the file and by modifying the input arguments.

Note that I'm not certain how to get the additional signature at the same level of indentation as the first one, but I think this change does a good job of highlighting the change is optional.

Would be happy to iterate on this if there are any issues.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99650
Approved by: https://github.com/mikaylagawarecki
",['torch/_torch_docs.py'],"Issue in repeat_interleave documentation - lack of clarity regarding optional input argument, causing confusion about method signature."
106d3f0115e2849d364c6d88eec175a1540ce217,1687358255,"[AOTAutograd] make _unsafe_view() logic happen during the runtime epilogue (#103919)

Fixes https://github.com/pytorch/pytorch/issues/103153

AOTAutograd has some logic for handling the case when we have:
* a graph output that is a view of an intermediate
* None of the other aliases of that output escape the graph, so from the perspective of the user + the autograd engine, we can pretend that the output is not a view

However, that logic would inject an `_unsafe_view()` call into the graph at trace time. This isn't wrong, but inductor will just immediately decompose `_unsafe_view()` into `view()`, and so the output tensor will continue to show up as having view metadata w.r.t. autograd.

This PR changes the `unsafe_view()` call to be in the runtime epilogue, instead of being part of the graph (where the compiler might do bad things to it - the compiler also shouldn't have to concern itself with autograd metadata).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/103919
Approved by: https://github.com/ezyang
","['test/functorch/test_aotdispatch.py', 'torch/_functorch/aot_autograd.py']","AOTAutograd's handling of graph outputs as views of intermediates, when no other aliases escape the graph, results in an unnecesary `_unsafe_view()` call being injected at trace time, causing issues with autograd metadata."
7b2d375148d229f809689093f6b673041c3586cf,1624469894,"Fix convolution_depthwise3x3_winograd for multichannel output (#60460)

Summary:
Before this change it was implemented with the assumption, that number of groups, input  and output channels are the same, which is not always the case
Extend the implementation to support any number of output channels as long as number of groups equals to the number of input channels (i.e. kernel.size(1) == 1)

Fixes https://github.com/pytorch/pytorch/issues/60176

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60460

Reviewed By: albanD

Differential Revision: D29299693

Pulled By: malfet

fbshipit-source-id: 31130c71ce86535ccfba2f4929eee3e2e287b2f0
","['aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp', 'test/test_nn.py']","The convolution_depthwise3x3_winograd assumes that the number of groups, input, and output channels are the same, leading to issues when this is not the case."
27a86204fc6ddfe1edfdc4e1c4f23301265cc748,1655927593,"[ci] remove old USER_LAND code from ci scripts

We used to have a parser in jenkins that looked for ""ENTERED USER LAND""
and ""EXITED USER LAND"" as a coarse way to classify failures in to infra
vs. user failures.

This is a good idea and I think we should implement it again for GHA.
But at the moment we don't have it, so remove the unused code.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80045

Approved by: https://github.com/janeyx99
","['.jenkins/pytorch/common.sh', '.jenkins/pytorch/common_utils.sh']","Unused code concerning an old feature for classifying failures in Jenkins (""ENTERED USER LAND"" and ""EXITED USER LAND"") still exists in the CI scripts."
c6cdca5c681fa007da6f2276df84f1e3aecde5ee,1660002223,"[ONNX] Reland #81953 Type utility for converting among JIT, torch and ONNX data types  (#82995)

Re-land #81953

Add `_type_utils` for handling data type conversion among JIT, torch and ONNX.

- Replace dictionary / list indexing with methods in ScalarType
- Breaking: **Remove ScalarType from `symbolic_helper`** and move it to `_type_utils`
- Deprecated: ""cast_pytorch_to_onnx"", ""pytorch_name_to_type"", ""scalar_name_to_pytorch"", ""scalar_type_to_onnx"", ""scalar_type_to_pytorch_type"" in `symbolic_helper`
- Deprecate the type mappings and lists. Remove all internal references
- Move _cast_func_template to opset 9 and remove its reference elsewhere (clean up). Added documentation for easy discovery

Why: List / dictionary indexing and lookup are error-prone and convoluted.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82995
Approved by: https://github.com/kit1980
","['test/onnx/test_pytorch_onnx_no_runtime.py', 'torch/onnx/__init__.py', 'torch/onnx/_type_utils.py', 'torch/onnx/symbolic_helper.py', 'torch/onnx/symbolic_opset10.py', 'torch/onnx/symbolic_opset11.py', 'torch/onnx/symbolic_opset12.py', 'torch/onnx/symbolic_opset13.py', 'torch/onnx/symbolic_opset16.py', 'torch/onnx/symbolic_opset8.py', 'torch/onnx/symbolic_opset9.py']","Conversion among JIT, torch, and ONNX data types is prone to error and complexity due to reliance on dictionary/list indexing, resulting in convoluted code and insufficient deprecation of type mappings and lists."
08fab7ae13587a671d36e83dfbe8809fd779849f,1633980646,"Wextra fix for Integration.cpp (#66321)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66321

Fixes
```
stderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]
    if (curr_shape.size() >= target_n_dim)
        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~
stderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]
    if (curr_shape.size() >= target_n_dim)
        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~
```

Test Plan: Sandcastle

Reviewed By: ngimel

Differential Revision: D31505347

fbshipit-source-id: 100b76215f78c3ce75bf4a993715a6767189747d
",['aten/src/ATen/native/Integration.cpp'],"Comparing integers of different signs in Integration.cpp, specifically 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long'), is resulting in an error."
c97275acf6518746e9fd06d5e005685d4e43c126,1677124256,"Fix OOMing periodic shards (#95246)

Tests have been consistently failing with the error on the following shards with the error `RuntimeError: CUDA error: out of memory`
- `periodic / linux-bionic-cuda11.7-py3-gcc7-slow-gradcheck / test (default, 1, 2, linux.4xlarge.nvidia.gpu)`
- `periodic / linux-bionic-cuda11.7-py3-gcc7-slow-gradcheck / test (default, 2, 2, linux.4xlarge.nvidia.gpu)`

Seeing if serializing those test files makes the periodic jobs succeed again.  This feels a bit off since there are so many different test files that have failed and need to be serialized, indicating a potential perf regression somewhere

Failures on hud: https://hud.pytorch.org/hud/pytorch/pytorch/master/1?per_page=100&name_filter=periodic%20%2F%20linux-bionic-cuda11.7-py3-gcc7-slow-gradcheck%20%2F%20test%20(default%2C%20
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95246
Approved by: https://github.com/Skylion007, https://github.com/huydhn
",['test/run_test.py'],"Tests on certain shards are consistently failing due to CUDA out of memory error, hinting a potential performance regression."
b96acb75918773106a66c63059f5fb518bcaa3a3,1628179540,"Allow disabled tests to be re-enabled with IGNORE_DISABLED_ISSUES (#62686)

Summary:
Part 1 of fixing https://github.com/pytorch/pytorch/issues/62359

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62686

Test Plan:
1. Check out this PR and run `python setup.py install`.
2. The test we will be running requires CUDA. If you don't have CUDA, you can try this on another device or simply comment out the skipIf statement before the `test_jit_cuda_extension` test in `test_cpp_extensions_jit.py`
3. Run: `IN_CI=1 python test/run_test.py -i test_cpp_extensions_jit -- -k test_jit_cuda_extension` and notice that it should skip. If it doesn't skip, edit test/.pytorch-disabled-tests.json: modify the platforms list of the first issue (61655) to include whatever platform you are on (macos or linux), and just run `python test/test_cpp_extensions_jit.py -v -k test_jit_cuda_extension --import-disabled-tests` to make sure it skips.
4. Now `export PYTORCH_IGNORE_DISABLED_ISSUES=61655` or `export PYTORCH_IGNORE_DISABLED_ISSUES=34952,61655`.
5. `rm test/.pytorch-*` to clear the cached files.
6. Run the same command as in step 5 and note that it SHOULDN'T skip. It should run.

Reviewed By: walterddr, samestep

Differential Revision: D30108773

Pulled By: janeyx99

fbshipit-source-id: dbf015a266f57577dc9283b0cdff720083b5c0cb
","['tools/stats/import_test_stats.py', 'torch/testing/_internal/common_utils.py']","Even when explicitly re-enabled by setting the PYTORCH_IGNORE_DISABLED_ISSUES, the disabled tests in the PyTorch system wouldn't execute."
80091cb0f70b67943209a1dbe17f2f367d788906,1628206507,"[DDP] Allow tuning of first bucket (#62748)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62748

Previously after buckets were rebuilt the first bucket size was always
defaulted to 1MB, this diff allows first bucket to be tuned like the rest of
the bucket sizes can.

Setting `dist._DEFAULT_FIRST_BUCKET_BYTES = 1` results in the following logs as
expected:
I0804 12:31:47.592272 246736 reducer.cpp:1694] 3 buckets rebuilt with size
limits: 1, 1048, 1048 bytes.
ghstack-source-id: 135074696

Test Plan: CI

Reviewed By: SciPioneer, wanchaol

Differential Revision: D30110041

fbshipit-source-id: 96f76bec012de129d1645e7f50e266d4b255ec66
","['torch/csrc/distributed/c10d/init.cpp', 'torch/csrc/distributed/c10d/reducer.cpp', 'torch/csrc/distributed/c10d/reducer.hpp', 'torch/nn/parallel/distributed.py']","First bucket size defaults to 1MB after buckets are rebuilt, regardless of any tuning applied to the bucket sizes in the Distributed Data Parallel (DDP) system."
24b9b304d9aac3ab132471adc4c1d8b677d8a4ea,1634027114,"[TensorExpr] Nuke TE shape inference. (#65554)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65554

We're relying on JIT based shape inference and not using the TE
implementation.

Question to the audience: we set `hasBroadcasts_` in that function, but
this function was almost never invoked. Do we behave correctly in the
presence of rand-calls and broadcasts?

Test Plan: Imported from OSS

Reviewed By: bertmaher

Differential Revision: D31148925

Pulled By: ZolotukhinM

fbshipit-source-id: 2898a57e389ea0950163122089d0fec3d92701c4
","['torch/csrc/jit/tensorexpr/kernel.cpp', 'torch/csrc/jit/tensorexpr/kernel.h', 'torch/csrc/jit/tensorexpr/operators/misc.h']","Tensor Expression (TE) shape inference is not being utilized despite being in place, raising concerns about correct behaviour in scenarios involving rand-calls and broadcasts."
f04e6594ed7d7657a059ef63e82e136aa2bbc0fd,1630596629,"Fix broken caffe2 test: PlanExecutorTest.BlockingErrorPlan (#64401)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64401

PlanExecutorTest.BlockingErrorPlan uses `ASSERT_DEATH` which internally performs a `fork()`. This can cause problems under certain configurations that use threads. This change updates this test to use the ""threadsafe"" style for GTest death tests in order to improve its quality in multithreaded environments.

Test Plan:
I confirmed that this change fixes the issue on my devvm with the following command:
```
buck test mode/dev //caffe2/caffe2:caffe2_test_cpu -- PlanExecutorTest.BlockingErrorPlan
```

Reviewed By: praihan

Differential Revision: D30709447

fbshipit-source-id: 12ffd9ad0371e2e5b43a9873c80568e5ab02d246
",['caffe2/core/plan_executor_test.cc'],"The test PlanExecutorTest.BlockingErrorPlan is failing due to issues with `ASSERT_DEATH` in multithreaded environments, relating particularly to the use of `fork()`."
730fef25c71dfb7c7d3f403df48541cc0aac154d,1644539849,"Convert type comments to annotations in caffe2/test/onnx/ (#72632)

Summary:
Convert type comments in caffe2/test/onnx/

Produced by running:
```
python -m  libcst.tool codemod convert_type_comments.ConvertTypeComment caffe2/test/onnx/
```
from the parent directory.

One question is whether we actually want to scrap type comment here. There are some jit tests where we're explicitly aiming to validate py2-style type comments; I don't think this test is one of those cases but if I'm misreading it I can close the PR.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/72632

Reviewed By: msaroufim

Differential Revision: D34112196

Pulled By: stroxler

fbshipit-source-id: a3d18cb36e7eeb4af9be781e98776bf24b96b854
(cherry picked from commit 9301019e510e0891d48f9f2d536c251c1c5d8c02)
",['test/onnx/test_pytorch_onnx_onnxruntime.py'],Type comments in caffe2/test/onnx/ could be potentially unnecessary and may not accurately validate py2-style type comments.
b57e6fdb50e13f37a222a095d9ef3533942d7dc1,1676310984,"[MPS] Enable Memory Leak Detection for test_mps.py (#94646)

- To check for Memory Leaks in `test_mps.py`, set the env-variable `PYTORCH_TEST_MPS_MEM_LEAK_CHECK=1` when running test_mps.py (used CUDA code as reference).
- Added support for the following new python interfaces in MPS module:
`torch.mps.[empty_cache(), set_per_process_memory_fraction(), current_allocated_memory(), driver_allocated_memory()]`
- Renamed `_is_mps_on_macos_13_or_newer()` to `_mps_is_on_macos_13_or_newer()`, and `_is_mps_available()` to `_mps_is_available()` to be consistent in naming with prefix `_mps`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94646
Approved by: https://github.com/malfet
","['aten/src/ATen/detail/MPSHooksInterface.h', 'aten/src/ATen/mps/MPSHooks.cpp', 'aten/src/ATen/mps/MPSHooks.h', 'test/test_mps.py', 'torch/backends/mps/__init__.py', 'torch/csrc/mps/Module.cpp', 'torch/mps/__init__.py']",There's no method available to detect potential memory leaks in 'test_mps.py' and some inconsistencies in the naming of MPS functions are observed.
6302cdb9bc8bbd2f77d002db296f272ef882d2fc,1646170387,"[Reland] Add BUILD_LAZY_CUDA_LINALG option (#73447)

Summary:
When enabled, it will generate `torch_cuda_linalg` library, which would depend on cusolve and magma and registers dynamic bindings to it from LinearAlgebraStubs

Avoid symbol clashes that can result in infinite recursion by moving all symbols in the library to its own namespace.

Add checks that should prevent calling self in recursion to `LinearAlgebraStubs.cpp`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/73447

Reviewed By: albanD

Differential Revision: D34538827

Pulled By: malfet

fbshipit-source-id: f2535b471d3524768a84b2e169b6aa24c26c03bf
(cherry picked from commit 4ec24b079c861c1122f0fa86e280b977c3c2f7ac)
","['aten/src/ATen/DynamicLibrary.cpp', 'aten/src/ATen/DynamicLibrary.h', 'aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp', 'aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp', 'aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h', 'test/cpp_extensions/setup.py', 'tools/build_variables.bzl']",Symbol clashes in `torch_cuda_linalg` library lead to infinite recursion; the library doesn't have its own namespace for symbols and lacks checks preventing recursion in `LinearAlgebraStubs.cpp`.
da520a43f228d4b2f5fda6ec0412080504822fc7,1660926539,"[Vulkan] Fix issues in GRU and LSTM (#83722)

Summary:
This diffs fixes several issues in GRU and LSTM vulkan ops:
- Add create_gru_context and create_lstm_context to vulkanFoldPrePackingOps
- Add filter to insertPrePackedGruOp and insertPrePackedLstmOp to avoid matching gru.data and lstm.data usages
- Fixed output dimension of GRU and LSTM
- Allowed batch_first to be false when batch=1 and seq=1

Test Plan:
Check that optimize_for_mobile runs and correctly folds the create context ops
```
buck run :export_for_mobile ~/ferraris/ferraris.ptl ~/ferraris
```

Check that vulkan api tests are still passing
```
buck run //xplat/caffe2:pt_vulkan_api_test_binAppleMac\#macosx-arm64
```

Reviewed By: SS-JIA

Differential Revision: D38811967

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83722
Approved by: https://github.com/SS-JIA
","['aten/src/ATen/native/vulkan/ops/Gru.cpp', 'aten/src/ATen/native/vulkan/ops/Lstm.cpp', 'aten/src/ATen/test/vulkan_api_test.cpp', 'torch/csrc/jit/passes/vulkan_rewrite.cpp']","Issues in Vulkan operations for GRU and LSTM: incorrect output dimensions, lack of context creation, unfiltered pre-packaged ops, and inability to handle batch_first=false when batch=1 and seq=1."
7709382b5010fbcc15bb0ae26240ae06aa4e973d,1697846565,"Fix regression in `torch.equal` behavior for NaNs (#111699)

`torch.equal(x, x)` should return false if one of `x` is a tenor of floats one of which is NaN.
So, it renders some of the optimization proposed in https://github.com/pytorch/pytorch/pull/100024 invalid, though as result `torch.equal` will become much slower for identical floating point tensors.

Add regression test that calls torch.equal for tensor containing NaN

Fixes https://github.com/pytorch/pytorch/issues/111251

Pull Request resolved: https://github.com/pytorch/pytorch/pull/111699
Approved by: https://github.com/Skylion007, https://github.com/albanD
","['aten/src/ATen/native/ReduceOps.cpp', 'test/test_torch.py']","`torch.equal(x, x)` is returning true when tensor `x` contains NaN values, contradicting expected behavior. This is breaking some optimizations."
eb94df28c748bff6a55c06e9b3440a525ea1f867,1663777825,"Use pip install cu117 (#85097)

Creates new wheel workflow specific to CUDA 11.7 that does not bundle the cudnn and cublas.

Workflow:
https://github.com/pytorch/pytorch/actions/runs/3094622781

New Package:
manywheel-py3_10-cuda11_7-with-pypi-cudnn | 843 MB

Old Package:
manywheel-py3_10-cuda11_7 | 1.65 GB

Testing workflow:

[manywheel-py3_7-cuda11_7-with-pypi-cudnn-build / build](https://github.com/pytorch/pytorch/actions/runs/3091145546/jobs/5000867662#logs):
```
Bundling without cudnn and cublas.
+ DEPS_LIST=(""/usr/local/cuda/lib64/libcudart.so.11.0"" ""/usr/local/cuda/lib64/libnvToolsExt.so.1"" ""/usr/local/cuda/lib64/libnvrtc.so.11.2"" ""/usr/local/cuda/lib64/libnvrtc-builtins.so.11.7"" ""$LIBGOMP_PATH"")
+ DEPS_SONAME=(""libcudart.so.11.0"" ""libnvToolsExt.so.1"" ""libnvrtc.so.11.2"" ""libnvrtc-builtins.so.11.7"" ""libgomp.so.1"")
.....
pytorch_extra_install_requirements: nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11, nvidia-cublas-cu11
```

[manywheel-py3_7-cuda11_7-build / build](https://github.com/pytorch/pytorch/actions/runs/3091145546/jobs/5000863250#logs)

```
Bundling with cudnn and cublas.
+ DEPS_LIST=(""/usr/local/cuda/lib64/libcudart.so.11.0"" ""/usr/local/cuda/lib64/libnvToolsExt.so.1"" ""/usr/local/cuda/lib64/libnvrtc.so.11.2"" ""/usr/local/cuda/lib64/libnvrtc-builtins.so.11.7"" ""/usr/local/cuda/lib64/libcudnn_adv_infer.so.8"" ""/usr/local/cuda/lib64/libcudnn_adv_train.so.8"" ""/usr/local/cuda/lib64/libcudnn_cnn_infer.so.8"" ""/usr/local/cuda/lib64/libcudnn_cnn_train.so.8"" ""/usr/local/cuda/lib64/libcudnn_ops_infer.so.8"" ""/usr/local/cuda/lib64/libcudnn_ops_train.so.8"" ""/usr/local/cuda/lib64/libcudnn.so.8"" ""/usr/local/cuda/lib64/libcublas.so.11"" ""/usr/local/cuda/lib64/libcublasLt.so.11"" ""$LIBGOMP_PATH"")
+ DEPS_SONAME=(""libcudart.so.11.0"" ""libnvToolsExt.so.1"" ""libnvrtc.so.11.2"" ""libnvrtc-builtins.so.11.7"" ""libcudnn_adv_infer.so.8"" ""libcudnn_adv_train.so.8"" ""libcudnn_cnn_infer.so.8"" ""libcudnn_cnn_train.so.8"" ""libcudnn_ops_infer.so.8"" ""libcudnn_ops_train.so.8"" ""libcudnn.so.8"" ""libcublas.so.11"" ""libcublasLt.so.11"" ""libgomp.so.1"")
```

cc: @malfet @ptrblck
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85097
Approved by: https://github.com/malfet
","['.circleci/scripts/binary_populate_env.sh', '.github/scripts/generate_binary_build_matrix.py', 'setup.py']","CUDA 11.7 specific wheel workflow includes unnecessary bundling of cudnn and cublas, leading to increased package size."
8595b6eeed9a0e96aa6b1779000aeb96c13e6202,1633096515,"Avoid UB when indexing into size-0 tensors (#65878)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65878

If we attempt to compute an offset into an empty tensor we trigger UB, since
we'd be adding an offset to a nullptr, which is UB
(https://reviews.llvm.org/D67122) even if we never use the pointer.

Since indexing into an empty tensor yields an empty tensor anyways, let's just
return the underlying (null) data ptr in this case.

ghstack-source-id: 139448496

Test Plan:
r-barnes originally pointed this out to me in a failing TE fuser test:
https://www.internalfb.com/intern/testinfra/diagnostics/5910974579561425.281475022329152.1632898053/
```
buck test mode/dev //caffe2/test:jit -- --exact 'caffe2/test:jit - test_unsupported_nn_functional_pad_circular_cpu_float32 (test_jit_fuser_te.TestNNCOpInfoCPU)'
```

But it turns out it's easily triggered by anything that tries to operate on a
slice of a size-0 tensor:
```
def test_pad(self):
    F.pad(torch.ones(0, 3, 3), (1, 2), 'circular')

def test_index(self):
    input = torch.zeros(0, 3, 3)
    out = torch.zeros(0, 3, 6)
    out[..., 1:4] = input[..., 0:3]

def test_add(self):
    torch.ones(0, 2)[:, 1] + torch.ones(0, 1)
```

What's the right place for these sort of operator corner-case tests?  Should
they be/are they part of OpInfo?

Reviewed By: jamesr66a

Differential Revision: D31296914

fbshipit-source-id: 0ef52ad311dceeed985498f8d9390bc6fbaefbfc
",['c10/core/TensorImpl.h'],"Indexing into or operating on empty tensors results in undefined behavior due to computing an offset with a nullptr, as observed in various operator tests."
965b9f483ef99f98af8a5be0e751d41e5ef0efdc,1643164335,"[cuDNN] Add a new optimized cuDNN RNN algorithm for small RNN hidden_size (#62143)

Summary:
This PR enables a new cuDNN RNN/LSTM algorithm `CUDNN_RNN_ALGO_PERSIST_STATIC_SMALL_H` when the hidden_size is small. Operator benchmark observes 10x performance improvement in some shapes.

- [X] forward https://github.com/xwang233/code-snippet/tree/master/cudnn-rnn-bench-62143/forward
- [X] backward https://github.com/xwang233/code-snippet/tree/master/cudnn-rnn-bench-62143/backward
- [X] end-to-end model: benchmark looks good

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62143

Reviewed By: anjali411

Differential Revision: D33771442

Pulled By: ngimel

fbshipit-source-id: 0640abc6b90ebd2428c3182ce03bf0b9c30a2ec9
(cherry picked from commit 73b153a528fb9b64b994c1174882bc2f64b1ed47)
",['aten/src/ATen/native/cudnn/RNN.cpp'],"Performance bottleneck observed when hidden_size is small for cuDNN RNN/LSTM algorithm, causing significant delays in forward and backward operations."
11b9a81e02551261a172e674e873de89a62610ef,1653935509,"[NNC] channels last propagation within NNC fusion group (#76948)

Decide the memory layout propagation policy and propagate it within the NNC fusion group. The memory layout propagation policy could be `Contiguous` and `Channels-last contiguous`.
 - `Contiguous`: Convert the non-contiguous including channels-last contiguous input tensors to contiguous and generate the contiguous output `Buf` for lowering function.
 - `Channels-last contiguous`: Convert the input tensors to channels-last contiguous and generate the channels-last contiguous output `Buf` for lowering function.

Currently, the rule is simple. If all the input and out tensors of the NNC fusion group are channels-last contiguous, then the propagated memory layout is `Channels-last contiguous`. Otherwise, it is always `Contiguous` which is as same as current situation. It means that this PR provides a fast path to channels-last and the optimization is conservative since its trigger conditions are strict.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76948
Approved by: https://github.com/ZolotukhinM
","['test/test_tensorexpr.py', 'torch/csrc/jit/passes/symbolic_shape_runtime_fusion.cpp', 'torch/csrc/jit/tensorexpr/kernel.cpp', 'torch/csrc/jit/tensorexpr/kernel.h']","Non-contiguous including channels-last contiguous input tensors in NNC fusion group are always converted to contiguous, regardless of the original layout, potentially affecting performance and optimization."
f56cb41c2e71334322233d518816827a60aab6db,1678765663,"Fix calls to sizes to enable dynamic shapes with sdpa (#96674)

Fixes part of #96414

Replaces any calls to sizes, with sym_sizes. Still seeing an error with the repro script:
``` Bash
Exception raised from sizes_default at /scratch/drisspg/work/pytorch/c10/core/TensorImpl.h:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x7d (0x7f697f4a141d in /scratch/drisspg/work/pytorch/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0xdd (0x7f697f49fbcd in /scratch/drisspg/work/pytorch/torch/lib/libc10.so)
frame #2: c10::TensorImpl::sizes_custom() const + 0x95 (0x7f697f4824c5 in /scratch/drisspg/work/pytorch/torch/lib/libc10.so)
frame #3: at::native::empty_like(at::Tensor const&, c10::optional<c10::ScalarType>, c10::optional<c10::Layout>, c10::optional<c10::Device>, c10::optional<bool>, c10::optional<c10::MemoryFormat>) + 0x92c (0x7f69809d18ac in /scratch/drisspg/work/pytorch/torch/lib/libtorch_cpu.so)
frame #4: <unknown function> + 0x23f5ce7 (0x7f698193bce7 in /scratch/drisspg/work/pytorch/torch/lib/libtorch_cpu.so)
```

still trying to track down this empty call

from the looks of it, might be coming from at::layer_norm?
the BT from lldb is 221 frames however, so lots of noise

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96674
Approved by: https://github.com/ezyang
","['aten/src/ATen/native/transformers/cuda/sdp_utils.h', 'test/dynamo/test_dynamic_shapes.py']","The 'sizes' function in sdpa fails to work with dynamic shapes, resulting in an error in the at::layer_norm routine with large backtrace. The issue appears to involve a problematic empty call in the 'empty_like' function."
fb2693a6329b5db68e99bfa0bb46484ea6f69d0b,1613060994,"Use bool/float instead of np.bool/np.float (#52103)

Summary:
This is causing type hint test errors on the latest numpy:

```
torch/testing/_internal/common_quantized.py:38: error: Module has no attribute ""float""; maybe ""float_"", ""cfloat"", or ""float64""?  [attr-defined]
torch/testing/_internal/common_methods_invocations.py:758: error: Module has no attribute ""bool""; maybe ""bool_"" or ""bool8""?  [attr-defined]
```

Runtime-wise, there's also a deprecation warning:

```
__main__:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
```

Fixes #{issue number}

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52103

Reviewed By: suo

Differential Revision: D26401210

Pulled By: albanD

fbshipit-source-id: a7cc12ca402c6645473c98cfc82caccf161160c9
","['torch/testing/_internal/common_methods_invocations.py', 'torch/testing/_internal/common_quantized.py']",Using deprecated numpy scalar types 'np.bool' and 'np.float' in torch/testing files result in type hint test errors and runtime deprecation warnings.
5c7e801c50e4478e8f96ab287a77ee4b08051f75,1661795591,"[pytorch][on device quant] Finalize method for ondevice quant (#83571)

Summary:
After inserting quant dequant nodes in the graph, we need
1. Insert packed param creation and quantized op
2. Create packed_params attribute in the top module. For this we need
graph that inlined except for calculate_qparams method calls. But they
can be inlined too. So perhaps we need to make sure no other callmethods
exist.
3. Insert SetAttr for the packed param
4. Insert GetAttr for the packed param
5. Use GetAttr output for quantized op where applicable, e.g.
linear_dynamic

The above is added to quantize_<method-name> method created inprevious
step. Once the above steps are done clone the method into
quantized_<method-name>

Modify quantize_<method-name>:
1. Remove all outputs from the method.
2. Run dce
3. Remove all inputs from the method except self.

Modify quantized_<method-name>:
1. Remove all packed_param setAttr nodes.
2. Run dce.

This should result in removal of all nodes that generate packed param.

Test Plan: To be written

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D38771416](https://our.internmc.facebook.com/intern/diff/D38771416)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83571
Approved by: https://github.com/jerryzh168
","['build_variables.bzl', 'test/quantization/jit/test_ondevice_quantization.py', 'torch/ao/quantization/observer.py', 'torch/ao/quantization/quantize_jit.py', 'torch/csrc/jit/passes/quantization/finalize.cpp', 'torch/csrc/jit/passes/quantization/finalize.h', 'torch/csrc/jit/passes/quantization/quantization_patterns.h', 'torch/csrc/jit/passes/quantization/register_packed_params.cpp', 'torch/csrc/jit/passes/quantization/register_packed_params.h', 'torch/csrc/jit/python/init.cpp']","After inserting quant dequant nodes in PyTorch graph, packed param creation, SetAttr and GetAttr are not properly inserted resulting in calculation and usage issues, specifically with methods such as linear_dynamic."
501320ed8198bcf5e05787e77e8c8cfd6a122780,1623119857,"[pytorch] deprecate default_op_deps.yaml (#59573)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59573

To do mobile selective build, we have several options:
1. static dispatch;
2. dynamic dispatch + static analysis (to create the dependency graph);
3. dynamic dispatch + tracing;

We are developing 3. For open source, we used to only support 1, and
currently we support both 1 and 2.

This file is only used for 2. It was introduced when we deprecated
the static dispatch (1). The motivation was to make sure we have a
low-friction selective build workflow for dynamic dispatch (2).
As the name indicates, it is the *default* dependency graph that users
can try if they don't bother to run the static analyzer themselves.
We have a CI to run the full workflow of 2 on every PR, which creates
the dependency graph on-the-fly instead of using the committed file.

Since the workflow to automatically update the file has been broken
for a while, it started to confuse other pytorch developers as people
are already manually editing it, and it might be broken for some models
already.

We reintroduced the static dispatch recently, so we decide to deprecate
this file now and automatically turn on static dispatch if users run
selective build without providing the static analysis graph.

The tracing-based selective build will be the ultimate solution we'd
like to provide for OSS, but it will take some more effort to polish
and release.

Differential Revision:
D28941020
D28941020

Test Plan: Imported from OSS

Reviewed By: dhruvbird

Pulled By: ljk53

fbshipit-source-id: 9977ab8568e2cc1bdcdecd3d22e29547ef63889e
","['cmake/Codegen.cmake', 'test/mobile/custom_build/build.sh']","The workflow to automatically update default_op_deps.yaml for PyTorch's mobile selective build has been broken and it is now confusing developers, potentially already causing broken models."
b03b45afd93b9686418d5d65997d013af2db4038,1627036325,"[DDP Comm Hook] Use a single tensor instead of a tensor list as the comm hook result (#62074)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62074

Since SPMD mode is retired, the comm hook result will always be a single tensor.

This can improve comm hook developer experience, as no need to add an extra `[0]` to the precursor future result.

#Closes: https://github.com/pytorch/pytorch/issues/61914
ghstack-source-id: 134164593

Test Plan:
buck test mode/dev-nosan caffe2/test/distributed:c10d
buck test mode/dev-nosan caffe2/test/distributed:distributed_nccl_fork

Reviewed By: rohan-varma

Differential Revision: D29864732

fbshipit-source-id: 59fe6dd78b66214b1788514ad4d236039d9bda31
","['test/distributed/test_c10d_common.py', 'test/distributed/test_c10d_gloo.py', 'test/distributed/test_c10d_nccl.py', 'torch/csrc/distributed/c10d/comm.hpp', 'torch/csrc/distributed/c10d/python_comm_hook.cpp', 'torch/csrc/distributed/c10d/python_comm_hook.h', 'torch/csrc/distributed/c10d/reducer.cpp', 'torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py', 'torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py', 'torch/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py', 'torch/testing/_internal/distributed/distributed_test.py']","Comm hook results in SPMD mode returning a list of tensors instead of a single tensor, creating an extra, unnecessary step for the developer."
3e307c2562a5e5042599dbc6852c17be9e5a9623,1648486452,"[quant][gpu][core] Implemented quantized add operator using cudnn (#74463)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74463

This PR implements the quantized add operator using cudnn operations.
Also added a corresponding test function in test_quantized_op.py. Ideally,
we should merge this function with the cpu variant, but for now, we will
keep it separate until cudnn v8 is in the default build. Other factors also
complicate the merge as cudnn quantized add is currently only supported for
int8 symmetrically quantized tensors.

Test Plan:
In pytorch main dir, execute
```
python test/test_quantization.py TestQuantizedOps.test_qadd_relu_cudnn
```

TBA

Differential Revision:
D35009111
D35009111

Reviewed By: jerryzh168

Pulled By: dzdang

fbshipit-source-id: 13afa7f0192ffaf1f36334b1af827202c7dd0f74
(cherry picked from commit 2b5759523e2fec5c849941552b904e412d67a138)
","['aten/src/ATen/native/quantized/cudnn/BinaryOps.cpp', 'test/quantization/core/test_quantized_op.py']","Quantized add operator for GPUs is not currently supported, causing issues for operations involving int8 symmetrically quantized tensors."
43b56b3814749997689904537cb3f354b3ac442f,1649268053,"Add Parsing of tensor constants (#75119)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75119

Add support for parsing Tensor constants like Double(4, 4) ... by initializing random tensors. This makes saving IR and then parsing it lossy, so I have it toggled as default not on, but is useful in cases like repro-ing Fusions with tensor constants post-freezing.

cc Krovatkin

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D35373999

Pulled By: eellison

fbshipit-source-id: a5c8d9f93f23a7442258fc745ed6b6def330dca8
(cherry picked from commit 32dd6567522973563bd452bf486ed27b02e4e35c)
","['test/test_jit.py', 'torch/csrc/jit/ir/irparser.cpp', 'torch/csrc/jit/ir/irparser.h', 'torch/csrc/jit/python/init.cpp']","There is no support for parsing Tensor constants like Double(4, 4), causing saving and parsing IR to be lossy, especially in scenarios such as reproducing Fusions with tensor constants post-freezing."
7c1f3cc89e5ab335c638a4879748043698c5eff4,1648146220,"[quant] Populate FakeQuantize quant_min/quant_max to observer (#74581)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74581

As title, currently the quant_min/quant_max of the FakeQuantize are not populated to the observer. We plan to populate when they are both not None.

To do this we need to do
1. Remove the current default quant_min/quant_max value (0/255) as it's not universal for various dtype.
2. Move the upper bound/lower bound check before creating the observer.

Test Plan:
```
[jiaxuzhu@devvm3400.frc0 /data/users/jiaxuzhu/fbsource/fbcode] buck test mode/dev //caffe2/test:quantization -- --exact 'caffe2/test:quantization - test_quant_min_max_override (quantization.core.test_workflow_module.TestFakeQuantize)'
Parsing buck files: finished in 0.8 sec
Downloaded 0/2 artifacts, 0.00 bytes, 100.0% cache miss (for updated rules)
Building: finished in 9.5 sec (100%) 18535/84579 jobs, 2/84579 updated
  Total time: 10.3 sec
More details at https://www.internalfb.com/intern/buck/build/1cab97ef-0788-4d06-92ed-a828995e3bde
BUILD SUCCEEDED
Tpx test run coordinator for Facebook. See https://fburl.com/tpx for details.
Running with tpx session id: 24be645e-eebc-45d6-8111-052ef1225fa0
Trace available for this run at /tmp/tpx-20220323-094106.724238-24be645e-eebc-45d6-8111-052ef1225fa0/trace.log
RemoteExecution session id: reSessionID-24be645e-eebc-45d6-8111-052ef1225fa0-tpx
Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/5066549674998735
    ✓ ListingSuccess: caffe2/test:quantization : 483 tests discovered (20.179)
    ✓ Pass: caffe2/test:quantization - test_quant_min_max_override (quantization.core.test_workflow_module.TestFakeQuantize) (18.896)
Summary
  Pass: 1
  ListingSuccess: 1
If you need help understanding your runs, please follow the wiki: https://fburl.com/posting_in_tpx_users
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/5066549674998735
```

Reviewed By: jerryzh168

Differential Revision: D34971236

fbshipit-source-id: 4407fd03116a296053256b333f7ce6d28dcc9c42
(cherry picked from commit f6980bccea802f220cc5b6dfe1bf3a3a3eef0a34)
","['test/quantization/core/test_workflow_module.py', 'torch/ao/quantization/fake_quantize.py', 'torch/ao/quantization/utils.py']","FakeQuantize's quant_min/quant_max values are not passed to the observer and the default values are not applicable for all data types, causing inaccuracies in quantization."
76f29d53bfc469ff8a1e612e543168e39d326346,1620221430,"ns for fx: change matching to only match known types (#57186)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57186

Before this PR, we matched any pair of nodes with equal or related
types.

This PR changes the behavior to only match nodes whose type is in
the allowlist (the relatedness mappings). This will prevent matching
user defined modules, unless users add them to the mappings.

This is motivated by a couple of things:
1. if user defined types are matched, it can break scriptability of the
   model with loggers attached. This happens whenever the user module
   has a return type of anything other than a Tensor or a tuple of
   Tensors.
2. we tried the past behavior on a couple of models, and it hasn't been
   useful.

Test Plan:
```
python test/test_quantization.py TestFXGraphMatcher
python test/test_quantization.py TestFXGraphMatcherModels
python test/test_quantization.py TestFXNumericSuiteCoreAPIs
python test/test_quantization.py TestFXNumericSuiteCoreAPIsModels
```

Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D28077981

fbshipit-source-id: 0a698e52b807cda47e6923310448a985b26eb362
","['test/quantization/test_numeric_suite_fx.py', 'torch/quantization/ns/graph_matcher.py', 'torch/quantization/ns/pattern_utils.py']","Current node matching process in fx: matches all equal or related types which can potentially disrupt scriptability with loggers attached on user-defined models. An additional issue, it provides no significant use in several tested models."
33e9a0b5f6674bd429bda689534e1c987b38cf6e,1637047969,"[Reland] Python tracer. (#68325)

Summary:
There were two issues with the original PR:
1) My assumption that bound C functions could be trusted to stay alive was not valid. I'm still not entirely sure what was dying, but I've just added a cache so that the first time I see a function I collect the repr just like I was already doing with Python functions.

2) `std::regex` is known to be badly broken and prone to segfaults. Because I'm just doing a very simple prefix prune it's fine to do it manually; see `trimPrefix`. Long term we should move all of PyTorch to `re2` as the internal lint suggests, but CMake is hard and I couldn't get it to work.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68325

Reviewed By: chaekit

Differential Revision: D32432596

Pulled By: robieta

fbshipit-source-id: 06fb4bcdc6933a3e76f6021ca69dc77a467e4b2e
","['tools/build_variables.bzl', 'torch/csrc/autograd/init.cpp', 'torch/csrc/autograd/profiler_kineto.cpp', 'torch/csrc/autograd/profiler_kineto.h', 'torch/csrc/autograd/profiler_python.cpp', 'torch/csrc/autograd/profiler_python.h', 'torch/profiler/python_tracer.py']","PyTorch Python tracer encountering segmentation faults due to usage of `std::regex`. Additionally, the assumption of bound C functions' lifespan can lead to unexpected termination."
8c66f97c9bf6ab4f800777961f4a254aa4fdc5d4,1692834095,"[profiler] move _enable_dynamo_cache_lookup_profiler (#107720)

_enable_dynamo_cache_lookup_profiler used to get turned on when running `__enter__` or `__exit__` with the profiler. But it's possible to turn the profiler on and off without the context manager (e.g. with a schedule and calling `.step()`). Instead, we should put these calls (which are supposed to be executed when the profiler turns on/off) where `_enable_profiler()` and `_disable_profiler()` are called.

This puts `_enable_dynamo_cache_lookup_profiler` and `_set_is_profiler_enabled` into `_run_on_profiler_(start|stop)` and calls that on the 3 places where `_(enable|disable)_profiler` get called.

Differential Revision: [D48619818](https://our.internmc.facebook.com/intern/diff/D48619818)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/107720
Approved by: https://github.com/wconstab
","['test/dynamo/test_profiler.py', 'torch/autograd/profiler.py', 'torch/profiler/profiler.py']","Dynamo cache lookup profiler might not be enabled when the profiler is turned on/off without a context manager, for example with a schedule and calling `.step()`.
"
12c9a932ca956c6b355b57f1fb4bab35dd69bfad,1676644091,"Assert more invariants on ValueRanges (#94906)

The main new invariant is lower/upper must be a Sympy expression of some sort (filtered through `simple_sympify`). There are some simpler sanity checks (mostly making sure the range is well formed). There is a type confusion problem (it's not immediately obvious if a range is for float/int/bool) but we aren't going to solve this for now as it is more complicated.

Billing of changes:

* ValueRanges.wrap() now accepts sympy expressions
* ValueRanges now accepts non-sympy expressions and will sympyify them appropriately. Rewrite calls to ValueRanges to not sympify manually as it is unnecessary
* Don't attempt to test sqrt(-1)
* Add ValuesRanges.unknown() which gives -oo, oo bounds, and rewrite direct calls to -math.inf, math.inf to use it
* Make multiply work between ValueRanges.unknown() and ValueRanges.wrap(0)
* Consistently use sympy.oo instead of math.inf

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94906
Approved by: https://github.com/eellison
","['test/test_value_ranges.py', 'torch/utils/_sympy/value_ranges.py']","ValueRanges lower/upper isn't required to be Sympy expressions, leading to type confusion and poorly formed ranges. Sanity checks are lacking and manual sympify calls are prevalent."
372a19d2c673a20fe50955b20ea4e3685266d630,1661813620,"Update start_index and end_index for adaptive pooling (#84010)

### Description
The PR fixes the issue #81409. To fix the issue the procedure of determining start and end indices for adaptive max pooling and average pooling is modified towards integer-only arithmetic.

### Testing
The testing of the new functions is straightforward:

```
#include <iostream>
#include <cassert>
#include <cmath>

int64_t start_index(int64_t a, int64_t b, int64_t c) {
  return (int64_t)std::floor((float)(a * c) / b);
}

int64_t end_index(int64_t a, int64_t b, int64_t c) {
  return (int64_t)std::ceil((float)((a + 1) * c) / b);
}

int64_t start_index_new(int64_t a, int64_t b, int64_t c) {
  return (a / b) * c + ((a % b) * c) / b;
}

int64_t end_index_new(int64_t a, int64_t b, int64_t c) {
  return 1 + ((a + 1) * c - 1) / b;
}

int main() {
    size_t N = 2<<24;
    std::cout<<N<<'\n';
    int64_t c = 1;

    for(int64_t i=1; i<N; i++) {
        for(int64_t j=1; j<N; j++) {
            int64_t s_id0 = start_index(i, j, c);
            int64_t s_id1 = start_index_new(i, j, c);
            assert(s_id0 == s_id1);
        }
    }

    for(int64_t i=1; i<N; i++) {
        for(int64_t j=1; j<N; j++) {
            int64_t e_id0 = end_index(i, j, c);
            int64_t e_id1 = end_index_new(i, j, c);
            assert(e_id0 == e_id1);
        }
    }
}
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84010
Approved by: https://github.com/ezyang
","['aten/src/ATen/native/AdaptiveAveragePooling3d.cpp', 'aten/src/ATen/native/AdaptiveMaxPooling3d.cpp', 'aten/src/ATen/native/AdaptivePooling.h', 'aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu', 'aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu', 'aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu', 'aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu', 'test/nn/test_pooling.py']","Issue with start and end indices determination in adaptive max pooling and average pooling involves non-integer calculations, leading to potential errors for large inputs."
5332d8705b71be9394594899c8f14cca5af0d8e3,1646422541,"[FX lowering] Modify replace_all_uses_with to allowing filtering of nodes to update; use it to (#73763)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73763

The test that is enabled generates a graph as such:

```
linear_25 --> sigmoid_14 --> output_1
         \--> output_2
```
Before this diff, (unpadding) layout_transform nodes would be added as follows:

```
linear_25 --> layout_xform1 --> sigmoid_14 --> layout_xform2--> output_1
                           \--> output_2
```
This causes an assertion to fail for the sigmoid node where the input and output types
don't match due to padding differences.

This diff modifies the replacement algorithm to not affect users of an output's parent node
when the user requires padded inputs. This yields the following graph instead:

```
linear_25 --> sigmoid_14 --> layout_xform2--> output_1
         \--> layout_xform1 --> output_2
```

Test Plan: Manually and CI

Reviewed By: jfix71, dborkovic

Differential Revision: D34623590

fbshipit-source-id: 3834b06c95fc5626eccc282216cbe039ac5a3242
(cherry picked from commit af012372ae1a6bb654b0ed9b765993960d5251e4)
","['test/test_fx.py', 'torch/fx/node.py']","Layout transform nodes addition is causing a mismatch error between input and output types due to differing padding, specifically when output's parent node user requires padded inputs."
c9cae1446f9a9509406238a946b1735fe36a73e3,1611432874,"fix unflatten_dense_tensor when there is empty tensor inside (#50321)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50321

Quantization team reported that when there are two empty tensors are replicated among ranks, the two empty tensors start to share storage after resizing.

The root cause is unflatten_dense_tensor unflattened the empty tensor as view of flat tensor and thus share storage with other tensors.

This PR is trying to avoid unflatten the empty tensor as view of flat tensor so that empty tensor will not share storage with other tensors.

Test Plan: unit test

Reviewed By: pritamdamania87

Differential Revision: D25859503

fbshipit-source-id: 5b760b31af6ed2b66bb22954cba8d1514f389cca
","['test/cpp/api/tensor_flatten.cpp', 'torch/csrc/utils/tensor_flatten.h']","When two empty tensors are replicated among ranks, the tensors share the same storage after resizing due to issues in unflatten_dense_tensor function."
b05dd931ee808f5e4c0ce1512af0c3aa9d9fde54,1614724752,"[Gradient Compression] Add is_the_last_bucket_to_allreduce method to GradBucket class (#53010)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53010

To determine the boundary between different iterations in a DDP communication hook, currently the user code needs `bucket.get_index() == 0`, which involves internal bucketization implementation details and undermines the usability of DDP communication hook.

Create an API to hide the details and improve the usability before publishing GradBucket APIs.
ghstack-source-id: 122723081

Test Plan: buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_powerSGD_ddp_comm_hook_nccl

Reviewed By: rohan-varma

Differential Revision: D26720813

fbshipit-source-id: f4a3147382c1f970534d7f0dee0cd599156c8b8c
","['torch/csrc/distributed/c10d/init.cpp', 'torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py', 'torch/lib/c10d/comm.hpp']","The boundary between different iterations in a DDP communication hook is determined by 'bucket.get_index() == 0', heavily reliant on internal bucketization implementation, hindering the usability of DDP communication hook."
375e21b2c662dc3b8094b540411d8207a8c67cb6,1652729594,"check that flip doesn't accept repeating dimensions (#77500)

Per title.
Before this PR `flip` throws errors on invalid inputs from ATen implementation itself, and not from error checks happening in prims/refs.
We should make sure that prims/refs do all the necessary error checking (@mruberry is going to test that by moving reference error inputs testing to call meta implementations instead of real ones).
In general, most error checking should live in refs, prims meta functions should propagate the necessary properties, but they should assume that they are getting valid inputs. The checks on the inputs should happen in refs, where they can be traced to the necessary guards, or lead to RuntimeErrors during tracing.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77500
Approved by: https://github.com/mruberry
","['torch/_prims/utils.py', 'torch/_refs/__init__.py']","`flip` function in PyTorch does not properly handle invalid or repeating dimensions, and throws errors from the ATen implementation rather than from comprehensive error checks supposed in prims/refs."
c3570fd945a8b7c0918b3ab6774e1b30e64ed0e8,1643033743,"fx quant: preserve node stack trace throughout prepare and convert (#70757)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70757

This is an initial PR on a way to preserve stack traces throughout FX
graph mode quantization.  It preserves the stack traces for ops
for all of the quantize handlers. A future PR will add stack traces
for dtype transitions.

Test Plan:
```
python test/test_quantization.py
TestQuantizeFx.test_stack_trace_preserved
```

Note: the above only tests a single case. In a future PR, once we
expand coverage, we can expand the utility functions to check for stack
traces on all tests.

```
python test/test_quantization.py
TestQuantizeFx.test_stack_trace_preserved
```

Imported from OSS

Differential Revision:
D33432485
D33432485

Reviewed By: jerryzh168

Pulled By: vkuzo

fbshipit-source-id: 56c56850393132487430a850fa1def826a9c39c0
(cherry picked from commit c11155b31eb9d228380501860f522a8c89eb2040)
","['test/quantization/fx/test_quantize_fx.py', 'torch/ao/quantization/fx/quantization_patterns.py', 'torch/ao/quantization/fx/utils.py', 'torch/ao/quantization/quantize_fx.py']","During FX graph mode quantization, stack traces for operations are not being preserved in all quantize handlers, which may result in a loss of debugging information."
9a2db6f091695a0ddd7955c107df25a10e9ad3ac,1636559635,"Factor backend routing logic out of convolution forward (#67790)

Summary:
This PR introduces a new function `_select_conv_backend` that returns a `ConvBackend` enum representing the selected backend for a given set of convolution inputs and params.

The function and enum are exposed to python for testing purposes through `torch/csrc/Module.cpp` (please let me know if there's a better place to do this).

A new set of tests validates that the correct backend is selected for several sets of inputs + params. Some backends aren't tested yet:
* nnpack (for mobile)
* xnnpack (for mobile)
* winograd 3x3 (for mobile)

Some flowcharts for reference:
![conv_routing_graph md](https://user-images.githubusercontent.com/75754324/140828957-1135b400-38c0-4c9f-87ef-4f33ceebeeae.png)
![conv_nogroup_routing_graph md](https://user-images.githubusercontent.com/75754324/140828977-ed223a4e-aa86-49f1-9925-c0f6b9ab36af.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67790

Reviewed By: zou3519

Differential Revision: D32280878

Pulled By: jbschlosser

fbshipit-source-id: 0ce55174f470f65c9b5345b9980cf12251f3abbb
","['aten/src/ATen/native/ConvUtils.h', 'aten/src/ATen/native/Convolution.cpp', 'test/test_nn.py', 'torch/csrc/Module.cpp', 'torch/testing/_internal/common_device_type.py']","The backend selection logic for convolution forwards is mixed in with the procedure itself, complicating both understanding and testing of the correct backend choice based on different convolution inputs and parameters."
40de6b80eef78db2de831fcce1bc796f7b4719cc,1645597378,"[ONNX] Add infra for quantized model export and support quantized mobilenet v3 (#72215)

* Add infrastructure and helper functions to enable future work for other quantized operators and models.
* Add export for quantized operators needed by torchvision mobilenet v3 large.
    * ATen namespace: hardsigmoid, flatten, adaptive_avg_pool, quantize_per_tensor, dequantize.
    * Quantized namespace: conv2d, conv2d_relu, hardswish, add, mul.
* Numerous bug fixes, in unpack_quantized_weight.cpp, symbolic functions, and unit test.

Co-authored-by: BowenBao <bowbaomicrosoft.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/73102
","['test/onnx/test_pytorch_onnx_onnxruntime.py', 'test/onnx/test_utility_funs.py', 'torch/csrc/jit/passes/onnx/peephole.cpp', 'torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp', 'torch/onnx/symbolic_helper.py', 'torch/onnx/symbolic_opset10.py', 'torch/onnx/symbolic_opset11.py', 'torch/onnx/symbolic_opset14.py', 'torch/onnx/symbolic_opset9.py', 'torch/onnx/symbolic_registry.py']","Quantized model export lacking support for torchvision's mobilenet v3 large, with missing quantized operators causing unit test failures and bugs in symbolic functions and unpack_quantized_weight.cpp."
6c8e516a8034d7ab615025ac57b5bc2e7680efb1,1646062676,"Add pickling support for WorkerInfo (#73371)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73371

This PRs allows for the pybinded class `WorkerInfo` to be pickled. The class is pickled into a tuple of worker_name and rank in format `(NAME, ID)`. This allows WorkerInfo to be passed as an argument for RPC calls.

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D34458153

Pulled By: H-Huang

fbshipit-source-id: 7b8f99960bdc0e24021e252d8c8138bcb53f698c
(cherry picked from commit 8fb119bf760eef9f313a44e9287c9253cbb09cae)
","['torch/csrc/distributed/rpc/init.cpp', 'torch/testing/_internal/distributed/rpc/rpc_test.py']","`WorkerInfo` class in PyTorch cannot be pickled, this prevents it from being passed as an argument for RPC calls."
7f2592195d349406e0f64c8bfba003985ab5a156,1624389367,"Adds stream recording for cross-stream uses of gradients in streaming backward (#60230)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/33909.

I _think_ the two recordDataPtrOnStreams i added are necessary and sufficient. They're the ones that worked for dmitrivainbrand's intricate multistream pipelining in https://github.com/pytorch/pytorch/issues/33909 and I can more or less convince myself they're enough, but it's hard to be sure (and hard to test).

PRing without a test now for visibility. I'll try to come up with something.

input_buffer.cpp needs to compile in cuda or cpu-only builds, so I can't call `c10::cuda::CUDACachingAllocator::recordStream` directly. I planned to work around by adding a binding in VirtualGuardImpl but https://github.com/pytorch/pytorch/pull/57047 spared me the trouble, thanks lw .

Recording a usage stream on a generic tensor was uglier than I expected, see https://github.com/pytorch/pytorch/issues/60306. Up to you guys if adding a unified way to record streams on a tensor backed by any TensorImpl should block this PR (and if so, whether it should happen in a separate PR or as part of this PR).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60230

Reviewed By: mrshenli

Differential Revision: D29289392

Pulled By: albanD

fbshipit-source-id: 1339d382b7d238a461b082597b3962847b5201fe
",['torch/csrc/autograd/input_buffer.cpp'],"Cross-stream usage of gradients in streaming backward doesn't correctly record streams, possibly leading to incorrect operations in complex multi-stream scenarios."
1f1e2dab6bc444140fb1695b5db5306476d1f5f2,1620348805,"Remove optional type for ord parameter in vector_norm (#57662)

Summary:
As per discussion here https://github.com/pytorch/pytorch/pull/57127#discussion_r624948215

Note that we cannot remove the optional type from the `dim` parameter because the default is to flatten the input tensor which cannot be easily captured by a value other than `None`

### BC Breaking Note
This PR changes the `ord` parameter of `torch.linalg.vector_norm` so that it no longer accepts `None` arguments. The default behavior of `2` is equivalent to the previous default of `None`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57662

Reviewed By: albanD, mruberry

Differential Revision: D28228870

Pulled By: heitorschueroff

fbshipit-source-id: 040fd8055bbe013f64d3c8409bbb4b2c87c99d13
","['aten/src/ATen/native/LinearAlgebra.cpp', 'test/backward_compatibility/check_backward_compatibility.py', 'test/test_linalg.py', 'torch/csrc/api/include/torch/linalg.h', 'torch/csrc/autograd/FunctionsManual.cpp', 'torch/csrc/autograd/FunctionsManual.h', 'torch/linalg/__init__.py', 'torch/overrides.py', 'torch/testing/_internal/common_methods_invocations.py']","`torch.linalg.vector_norm` function in PyTorch accepts `None` as an input for the `ord` parameter, leading to non-uniformity and unnecessary complexity in handling default values."
9539e6216bc122b604e8fb55c075d1d525c2522b,1633787166,"Quantization docs: add pages for Numeric Suite (Eager and FX) (#66222)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66222

Description:
1. creates doc pages for Eager and FX numeric suites
2. adds a link from main quantization doc to (1)
3. formats docblocks in Eager NS to render well
4. adds example code and docblocks to FX numeric suite

Test Plan:
```
cd docs
make html
python -m http.server
// renders well
```

Reviewed By: jerryzh168

Differential Revision: D31447610

Pulled By: vkuzo

fbshipit-source-id: 441170c4a6c3ddea1e7c7c5cc2f1e1cd5aa65f2f
","['torch/ao/ns/_numeric_suite.py', 'torch/ao/ns/_numeric_suite_fx.py', 'torch/ao/ns/fx/utils.py']","The Quantization documentation lacks pages for Eager and FX Numeric Suites, including example code and well-rendered docblocks, making it difficult for users to understand these features."
849c6a526e86e66e5112407a53c6b052fd5342cd,1645532972,"Extrapolated on equiv between linalg @ and solve (#71769)

Summary:
Potentially fixes https://github.com/pytorch/pytorch/issues/71385 similar docstring could also fix  https://github.com/pytorch/pytorch/issues/71384

Updated the doc to `torch.linalg.inv` to include nuance around equivalence to `torch.linalg.solve`:

Update is below:
```
.. note::
    Consider using :func:`torch.linalg.solve` if possible for multiplying a matrix on the left by
    the inverse, as::

        linalg.solve(A, B) == linalg.inv(A) @ B  # When B is a matrix

    It is always prefered to use :func:`~solve` when possible, as it is faster and more
    numerically stable than computing the inverse explicitly.
```

IvanYashchuk please inform if this the right direction or over-extrapolation. I can apply the same changes to the `tensorinv` doc to fix https://github.com/pytorch/pytorch/issues/71384. Also in https://github.com/pytorch/pytorch/issues/71384 there was a mention of updating `torch.matmul` error message to indicate the proper tensor shapes, I could also potentially do that in this PR if needed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/71769

Reviewed By: H-Huang

Differential Revision: D34242541

Pulled By: mruberry

fbshipit-source-id: 40e98dad4d821928d1dea72d4512ee579b690a32
(cherry picked from commit a0321a5de9ddc5bbcd2203bba20cb6751e98f126)
",['torch/linalg/__init__.py'],"The documentation of `torch.linalg.inv` lacks clarification about its equivalence to `torch.linalg.solve` for multiplying a matrix on the left by the inverse, causing confusion and possible performance inefficiencies."
ef8d461b09d89b6495b7df822f7f69c333867000,1695315187,"Fix torchbench --multiprocess (#109657)

`python benchmarks/dynamo/torchbench.py --multiprocess` currently fails due to initializing distributed multiple times:

```
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:6789 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:6789
 (errno: 98 - Address already in use).
```

Because torchbench calls itself via mp.spawn, there is the parent run (with `--multiprocess`) and child runs (with `--multiprocess --only <model>`).

This PR addresses this by fixing two issues:
1) distributed is initialized once in parent run and once in child runs, it should be initialized only in child runs where we have accurate rank and world size info
2) torchbench overrides CUDA_VISIBLE_DEVICES/world_size sometimes, but it shouldn't for distributed use cases where we want to use all available gpus

I am also adding a CI test to cover this type of issue in #109311

### Test plan
parent run test: `python benchmarks/dynamo/torchbench.py --ci --accuracy --timing --explain --inductor --device cuda --inference --bfloat16 --output /home/xmfan/local/pytorch/test/test-reports/inference_torchbench.csv --multiprocess`
child run test: `python benchmarks/dynamo/torchbench.py --ci --accuracy --timing --explain --inductor --device cuda --inference --bfloat16 --output /home/xmfan/local/pytorch/test/test-reports/inference_torchbench.csv --multiprocess --only simple_gpt`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109657
Approved by: https://github.com/H-Huang
",['benchmarks/dynamo/common.py'],"Executing `python benchmarks/dynamo/torchbench.py --multiprocess` fails due to multiple initializations of distributed, causing server socket to not listen on any local network address. Additionally, torchbench sometimes overrides CUDA_VISIBLE_DEVICES/world_size incorrectly."
c7ef620a14b1cadf0e36d637dba9226b919ac3ff,1632967725,"[quant] Add imports to the torch/ao/quantization/__init__.py (#64911)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64911

The import statements that involve the `quantize.py` were not added to the module level __init__ file. Those imports are necessary to mimic the behavior of the old import locations. Otherwise, the user would need to change their import statements to `from torch.ao.quantization.quantize import quantize` (instead of `from torch.ao.quantization import quantize`.

Another change in this diff is that we don't use `__all__` anymore. The all dunder was never used in quantization anyway, and just creates a potential bug when using `from ... import *`.
ghstack-source-id: 139342483

Test Plan: `buck test mode/dev //caffe2/test:quantization`

Reviewed By: vkuzo

Differential Revision: D30897663

fbshipit-source-id: a7b4919a191755e3ba690a79ce3362889f416689
",['torch/ao/quantization/__init__.py'],"Missing import statements in `torch/ao/quantization/__init__.py` are causing incorrect import locations for `quantize.py`, requiring users to change their import statements."
c58709b7bb3e7818ee41de2f2d1e58451705ff9c,1621880912,"Helper function for skipping module parameter / buffer initialization (#57555)

Summary:
This PR introduces a helper function named `torch.nn.utils.skip_init()` that accepts a module class object + `args` / `kwargs` and instantiates the module while skipping initialization of parameter / buffer values. See discussion at https://github.com/pytorch/pytorch/issues/29523 for more context. Example usage:

```python
import torch

m = torch.nn.utils.skip_init(torch.nn.Linear, 5, 1)
print(m.weight)

m2 = torch.nn.utils.skip_init(torch.nn.Linear, 5, 1, device='cuda')
print(m2.weight)

m3 = torch.nn.utils.skip_init(torch.nn.Linear, in_features=5, out_features=1)
print(m3.weight)
```
```
Parameter containing:
tensor([[-3.3011e+28,  4.5915e-41, -3.3009e+28,  4.5915e-41,  0.0000e+00]],
       requires_grad=True)
Parameter containing:
tensor([[-2.5339e+27,  4.5915e-41, -2.5367e+27,  4.5915e-41,  0.0000e+00]],
       device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],
       requires_grad=True)
```

Bikeshedding on the name / namespace is welcome, as well as comments on the design itself - just wanted to get something out there for discussion.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57555

Reviewed By: zou3519

Differential Revision: D28640613

Pulled By: jbschlosser

fbshipit-source-id: 5654f2e5af5530425ab7a9e357b6ba0d807e967f
","['test/test_nn.py', 'torch/nn/utils/__init__.py', 'torch/nn/utils/init.py']",There is no function available to instantiate a module in PyTorch while skipping the initialization of parameter or buffer values.
ff96f6d04f062e660224e7ba1f00da867d25d0de,1696343704,"[core IR][reland] Add `split.Tensor` and `unbind` decompositions to core ATen decomp table (#110323)

Summary:
This is a reland of [github PR #110102]( https://github.com/pytorch/pytorch/pull/110102).

The original PR had to be unlanded due to internal CI failures. This diff applies some small fixes to the failing tests to adjust to the new decompositions.

Note that `lift_fresh` will not be decomposed for now, since it was found that [constant propogation looks specifically for `lift_fresh`](https://github.com/pytorch/pytorch/blob/13af952f94e611f67ac306233898b753a9cf73d2/torch/fx/experimental/proxy_tensor.py#L381-L386). Therefore decomposing `lift_fresh` will interfere with constant propogation during export.

Test Plan: Github CI and internal CI

Differential Revision: D49761321

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110323
Approved by: https://github.com/jansel
","['test/export/test_serialize.py', 'torch/_decomp/__init__.py', 'torch/_inductor/decomposition.py']","The core ATen decomposition table is lacking `split.Tensor` and `unbind` decompositions, potentially leading to issues with internal CI testing and with the constant propagation phase during resource export."
88a160dc21c022a8cba26dd3e5be8e1418a66f68,1614201982,"[TensorExpr] LoopNest: Cleanup LoopNest constructors. (#52726)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52726

This change removes `input_bufs_` and `intermediate_bufs_` from
`LoopNest` class as they can be deduced from the root stmt and the list
of output bufs. As a result, the constuctor of the LoopNest also becomes
simpler as we now need to pass just one list of bufs.

Note: we might consider passing list of input bufs for verification
purposes (only inputs buffers are allowed to not have a definition), but
since we don't really have an IR verifier yet, there is no need in it
now. Once we add IR verifier, we could reconsider it.

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D26629596

Pulled By: ZolotukhinM

fbshipit-source-id: 81f544e9602b6855b7968d540b9ae06bd7c7e6d8
","['test/cpp/tensorexpr/test_loopnest.cpp', 'torch/csrc/jit/tensorexpr/loopnest.cpp', 'torch/csrc/jit/tensorexpr/loopnest.h']","LoopNest class has unnecessary attributes `input_bufs_` and `intermediate_bufs_`. These can be deduced from existing data, causing redundancy and complexity in the LoopNest constructor."
7513455c743d3d644b45a804902c1a0d14b69f45,1618831221,"Make tensordot resize output tensor's size if out= argument is specified & make it safely cast & copy output (#56286)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/56022.
Fixes https://github.com/pytorch/pytorch/issues/56316

For `torch.tensordot`,
1. `tensordot`'s out variant now resizes the output tensor provided as the `out` argument if necessary.
2. Added a check to verify if the output tensor provided as the argument for `out` is on the same device as the input tensors.
3. Added a check to verify if the dtype of the result is castable to the dtype of the output tensor provided as an argument for `out`.
4. Because of (2) & (3), `tensordot`'s out variant now [safely casts & copies output](https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch).
5. `test_tensordot` in `test_linalg.py` had a bug - the output tensor wasn't being defined to be on the same device as the input tensors. It was fixed by simply using a `device` argument in its definition.
6. Added an `OpInfo` for `tensordot` and modified the `OpInfo` for `inner`.

cc heitorschueroff mruberry

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56286

Reviewed By: ngimel

Differential Revision: D27845980

Pulled By: mruberry

fbshipit-source-id: 134ab163f05c31a6900dd65aefc745803019e037
","['aten/src/ATen/native/Linear.cpp', 'test/test_linalg.py', 'torch/testing/_internal/common_methods_invocations.py']","Tensordot's out variant in PyTorch fails to resize output tensor when necessary, does not check if output tensor is on the same device as input tensors or if result dtype is castable to output tensor's dtype, leading to unsafe casting and output copying."
03a1bb61fcaaede66749c5b25aa7eb05484be9d1,1655136591,"[ci] improvements to test stats uploading

Two improvements that are useful for `testsuite` uploading:
- When there are multiple tags of the same name, coalesce them into a
  list. This allows us to capture, e.g. when a `testsuite` contains
  multiple `testcase`s.
- Be able to skip a tag. We don't really want the inner `testcase` tags,
  since we upload those separately.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79364

Approved by: https://github.com/janeyx99
",['tools/stats/upload_test_stats.py'],The test suite uploader is currently unable to properly handle multiple tags with the same name and includes unwanted inner 'testcase' tags.
7ff1990caff094dc83babc0518a6b5bd9697e8b7,1619197616,"[c10d] Increment sequence numbers on collectives. (#55718)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55718

Increments sequence numbers when ProcessGroupGloo::enqueue or
ProcessGroupNCCL::collective is run, which is a common call all collectives
make. The next step will be to log these along with other collective info in
debug mode as well as integrating them with the process group wrapper.
ghstack-source-id: 127215077

Test Plan: CI

Reviewed By: SciPioneer

Differential Revision: D27690690

fbshipit-source-id: cb284b7c760763b7c0f814a41f06656fabf806d6
","['test/distributed/test_c10d_common.py', 'test/distributed/test_c10d_gloo.py', 'test/distributed/test_c10d_nccl.py', 'torch/lib/c10d/ProcessGroupGloo.cpp', 'torch/lib/c10d/ProcessGroupNCCL.cpp']","Sequence numbers are not incrementing when running ProcessGroupGloo::enqueue or ProcessGroupNCCL::collective, which are common calls all collectives make."
cd07214a41d9e160082d4f8070926f5b77a3d34e,1693353752,"Fix various issues on build-triton-wheel workflow (#108187)

There are more issues that I expect at the beginning:

* Triton was uploaded on `main` instead of `nightly` and release branch
* The environment `conda-aws-upload` wasn't used correctly in both wheel and conda upload
* Conda update wasn't run in a separate ephemeral runner
* Duplicated upload logic, should have just use `bash .circleci/scripts/binary_upload.sh` instead
* Handle `CONDA_PYTORCHBOT_TOKEN` and `CONDA_PYTORCHBOT_TOKEN_TEST` tokens in a similar way as https://github.com/pytorch/test-infra/pull/4530

Part of https://github.com/pytorch/pytorch/issues/108154
Pull Request resolved: https://github.com/pytorch/pytorch/pull/108187
Approved by: https://github.com/atalman
",['.circleci/scripts/binary_upload.sh'],"The 'build-triton-wheel' workflow is facing multiple issues including Triton being uploaded on incorrect branch, improper usage of 'conda-aws-upload' environment, non-segregated conda update running, redundant upload logic and disparate handling of 'CONDA_PYTORCHBOT_TOKEN' and 'CONDA_PYTORCHBOT_TOKEN_TEST' tokens."
122f8648ab8f872bd8ed970728c5ac894dce5300,1646936975,"[PyTorch Distributed] Add debug hint for NCCL async system error (#73897)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73897

add a debug hint that async system error can be caused by unexpected exit of
a remote process if not an actual network issue. For example, the exit of the remote process
can cause a closed network connection error at a local process. The hint helps to direct
the debug focus to the remote process.

Test Plan: unit tests

Reviewed By: pritamdamania87, rohan-varma

Differential Revision: D34702348

fbshipit-source-id: d19f9116e9efe5f6d76c0158a7a447616437ca69
(cherry picked from commit 005e74b7b6764ecd832b3410063285bff2411b56)
",['torch/csrc/distributed/c10d/NCCLUtils.hpp'],"Unexpected exit of a remote process can cause a closed network connection error at a local process, which is interpreted as an async system error in NCCL."
0810961d5f119e3c1b33b97e5d513cbd3be9eb64,1659666214,"Remove flatbuffer types/headers from flatbuffer_serializer[_jit].h (#82619)

Hide the flatbuffers types and headers from the serialize APIs, and stop using the DEPRECATED functions from flatbuffer_loader.h.

This required creating the new `DetachedBuffer` type to replace/hide `flatbuffers::DetachedBuffer`, a class that owns a span of custom-allocated memory.

This is another step towards hiding the flatbuffers types and headers from the load/serialize APIs.

Differential Revision: [D38292798](https://our.internmc.facebook.com/intern/diff/D38292798/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D38292798/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82619
Approved by: https://github.com/qihqi
","['test/cpp/jit/test_flatbuffer.cpp', 'torch/csrc/init_flatbuffer_module.cpp', 'torch/csrc/jit/mobile/flatbuffer_loader.h', 'torch/csrc/jit/serialization/flatbuffer_serializer.cpp', 'torch/csrc/jit/serialization/flatbuffer_serializer.h', 'torch/csrc/jit/serialization/flatbuffer_serializer_jit.cpp', 'torch/csrc/jit/serialization/flatbuffer_serializer_jit.h']","The serialize APIs expose flatbuffers types and headers and are using DEPRECATED functions from flatbuffer_loader.h, consequently leading to possible long-term maintenance issues."
208d06ca8c9e8a94652a8a5f9f1f974b6eb5aa5f,1626747252,"Port other comparison ops: `ne`, `lt`, `gt`, `le`, `ge` to structured kernels. (#60942)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/60942

Tracking Issue: #55070

This PR applies the same transformation of `eq` to the other comparison ops: `ne`, `lt`,
`gt`, `le`, and `ge`. Macros for crating meta and impl functions are used (since the
checks they have are the same).

Test Plan: Imported from OSS

Reviewed By: soulitzer

Differential Revision: D29509868

Pulled By: ezyang

fbshipit-source-id: 6a1ed1d93d08884c9e09d3f419037533a235d68c
","['aten/src/ATen/native/BinaryOps.cpp', 'aten/src/ATen/native/BinaryOps.h', 'aten/src/ATen/native/cpu/BinaryOpsKernel.cpp', 'aten/src/ATen/native/cuda/CompareGEKernel.cu', 'aten/src/ATen/native/cuda/CompareGTKernel.cu', 'aten/src/ATen/native/cuda/CompareLEKernel.cu', 'aten/src/ATen/native/cuda/CompareLTKernel.cu', 'aten/src/ATen/native/cuda/CompareNEKernel.cu']","The comparison operations `ne`, `lt`, `gt`, `le`, `ge` are not properly structured in kernels leading to consistency issues."
0606057af36053579dbac6160a1576ea873df2e5,1615330283,"[PyTorch] Add c10::MaybeOwned and Tensor::expect_contiguous (#53317)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53317

This seems like it might help in cases where we have to call
`Tensor::contiguous`, but we expect that the tensor in question will
be contiguous a good portion of the time.
ghstack-source-id: 123203771

Test Plan:
Profiled AdIndexer on inline_cvr; time spent in
clip_ranges_gather_sigrid_hash_each_feature<int> was cut in half from
1.37% to 0.66%

Reviewed By: smessmer

Differential Revision: D26738036

fbshipit-source-id: b5db10783ccd103dae0ab3e79338a83b5e507ebb
","['aten/src/ATen/templates/TensorBody.h', 'c10/test/util/MaybeOwned_test.cpp', 'c10/util/MaybeOwned.h']",Issue: Frequent usage of `Tensor::contiguous` is impacting performance. Predictions of a tensor being contiguous majority of the times are not optimized.
342139589c27e59ee99a897a7e4bc49c80dc738e,1669951712,"[quant][fx] Add support for matching multiple arguments in patterns (#89986)

Summary:
This PR adds support for matching patterns that has multiple arguments, it's needed for quantization in PyTorch 2.0 early prototype

Before this PR, we only support patterns like:
```
x -> conv -> bn -> relu
(relu, (bn, conv))
```
where each operator has a single node, the code breaks when we want to match a pattern that has an op that has multiple arguments, such as:
```
                           shape \
        transpose -> reshape -> output ->
```
where `reshape` has two arguments

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_match_pattern_with_multiple_args

Reviewers:

Subscribers:

Tasks:

Tags:

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89986
Approved by: https://github.com/vkuzo
","['test/quantization/fx/test_quantize_fx.py', 'torch/ao/quantization/fx/match_utils.py', 'torch/ao/quantization/fx/prepare.py']","Pattern matching in quantization breaks when attempting to match patterns with operators that have multiple arguments, such as during a reshape operation with two arguments."
36d91b5513d69629d65ece96b83496a7c7363289,1683637137,"Add differentiable mkldnn_rnn_layer_backward to support double backward of LSTM (#100627)

### Description

This PR is to fix #99413, which shows the limitation of double backward using oneDNN in LSTM.

This PR does not implement double backward function itself, because that is pretty hard to spell out. Instead, it implements mkldnn_rnn_layer_backward using differentiable operations, so that double backward can be done automatically.

During backward process, it needs to use gates and hidden states between cells during one layer. However, these middle variables are stored in the `workspace`, and it is hard to figure them out. Therefore, in backward, we need re-calculate them first.

Corresponding UT has been added based on the failing case in # 99413. The UT with gradcheck and gradgradcheck which is added in https://github.com/pytorch/pytorch/pull/26660 cannot test LSTM using oneDNN, because UT only supports `double` datatype, while oneDNN does not support it.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100627
Approved by: https://github.com/jgong5, https://github.com/soulitzer
","['test/test_nn.py', 'torch/csrc/autograd/FunctionsManual.cpp', 'torch/csrc/autograd/FunctionsManual.h']","Limited double backward capability in LSTM using oneDNN, with difficulty in getting intermediate variables from the 'workspace' during the backward process. There's also an inability to test LSTM via a unit test that only supports 'double' datatype, which oneDNN does not.
"
08ef4ae0bcedd41dc93cb7687a242d87c998e4e7,1641443614,"Remove unnecessary sync in linalg.det (#67014)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67014

LAPACK functions return negative infos when there was an unexpected
input. This happens (for example) when the user does not specify
matrices of the correct size. We already check all this things on the
PyTorch end, so this check that induces a synchronisation is
unnecessary.

I also took this chance to avoid some code repetition in the computation
of the determinant of `P`. I also changed the use of `ExclusivelyOwned<Tensor>`
by regular `Tensors` + moving into the tuple, which should be as efficient or more.

cc jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D32684851

Pulled By: mruberry

fbshipit-source-id: dc046d1cce4c07071d16c4e2eda36412bd734e0f
","['aten/src/ATen/native/LinearAlgebra.cpp', 'torch/linalg/__init__.py']",An unnecessary synchronization is induced in linalg.det due to checks for negative infos returned by LAPACK functions despite initial validations from the PyTorch end.
ac5a40e068b0a1ce3c873ef1f681564376dbae52,1626713105,"Fix benchmark's import module and remove its usage of tools.stats.scribe (#61808)

Summary:
There're a few convoluted logic here to fix the `benchmarks`'s import module for pytest.

- On one hand, if we want to use `tools.stats.scribe` from `benchmarks`, we will need to add `benchmarks/__init__.py`
- On the other hand, if we add `benchmarks/__init__.py`, it breaks how `pytest` is working on searching what is the system built `torch` instead of the local source module `../torch`
  - That's why we are seeing errors like

```
ImportError while loading conftest '/var/lib/jenkins/workspace/benchmarks/fastrnns/conftest.py'.
benchmarks/fastrnns/__init__.py:1: in <module>
    from .cells import *  # noqa: F403
benchmarks/fastrnns/cells.py:1: in <module>
    import torch
torch/__init__.py:29: in <module>
    from .torch_version import __version__ as __version__
torch/torch_version.py:9: in <module>
    from .version import __version__ as internal_version
E   ModuleNotFoundError: No module named 'torch.version'
```

Instead, this PR changed the usage of `upload_scribe.py` back to its original form using HTTP request, and only circleci for now will continue the this path using the `python benchmarks/upload_scribe.py`, which is gated by `if [[ -z ""${GITHUB_ACTIONS}"" ]];`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61808

Reviewed By: seemethere

Differential Revision: D29750188

Pulled By: zhouzhuojie

fbshipit-source-id: 3b842b21978f2159001e9c6c1cdc96c5a0515f2e
","['.jenkins/pytorch/test.sh', 'benchmarks/upload_scribe.py']","Use of tools.stats.scribe from benchmarks breaks pytest functionality. It imports system built torch instead of local source, leading to a ModuleNotFoundError."
b28cb43f5c00b10ab0dfd3f4c6d629e1f998497e,1697295084,"Intra-graph reordering pass on Inductor scheduler IR (based on #100762) (#108091)

This PR implements intra-graph communication reordering pass on Inductor scheduler IR, based on Horace's previous PR #100762.

Main algorithm:
1. Greedily moves waits as late as possible (i.e. until we reach a use)
2. Greedily moves comms as early as possible (i.e. until we reach an input)
3. Move computes following simple heuristics to improve overlap.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108091
Approved by: https://github.com/Chillee, https://github.com/wanchaol
","['test/distributed/test_compute_comm_reordering.py', 'torch/_inductor/comm_analysis.py', 'torch/_inductor/comms.py', 'torch/_inductor/config.py', 'torch/_inductor/fx_passes/post_grad.py', 'torch/_inductor/scheduler.py', 'torch/_inductor/utils.py']",Inefficient ordering of communication and wait operations in Inductor scheduler IR hinders optimal operation overlap.
b96ea9f361f2ed872c4a7d662427cadec345b702,1697004607,"[export] Get export APIs ready for PTC (#110410)

Summary:
https://docs.google.com/document/d/1QJJEGnj2nHGPODlw38BEG3KLLCOTfdOVjPrNQbz_LM8/edit#bookmark=id.lp80wfshq130
Changes:
* `torch.export` will return a functional ATen graph w/o decompositions
* `exported_program.run_decompositions(decomposition_table)` will optionally take a decomposition table, and run decompositions on the exported program, returning a new exported program. By default we will run the Core ATen decomposition table.

Calling convention for Executorch stays the same:
```
pre_autograd_graph = capture_pre_autograd_graph(f, args, ...)
aten_graph_no_decomps = torch.export.export(pre_autograd_graph, args, ...)
# Within to_edge we decompose to core aten and then convert to edge
edge_graph = exir.to_edge(aten_graph_no_decomps)
```

Test Plan: CI

Differential Revision: D49742989

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110410
Approved by: https://github.com/ydwu4
","['test/export/test_serialize.py', 'torch/_export/__init__.py']",Current torch.export does not allow for function decomposition on an exported ATen graph; lack of optional decomposition table while running decompositions leading to limitation in customization.
71933d381b7c021dfa1818e05539a1910fe95296,1666817714,"[ao] Fixing tests for block pruning shapes (#87326)

The current unittests were only checking the tensors whose shapes were already multiples of the block size. That caused some hidden bugs to creep in. Specifically, for the shapes that would require padding for the mask/data, the sparsifier would try to apply shape-mismatching tensors onto each other. This caused segfaults as well as silent failures.

This makes minor adjustments to the code to make sure the masks and data shapes are aligned, as well as fixing the tests to catch this.

Test Plan:

```python
python test/test_ao_sparsity.py
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87326
Approved by: https://github.com/jcaip
","['test/ao/sparsity/test_sparsifier.py', 'torch/ao/pruning/sparsifier/weight_norm_sparsifier.py']","Unit tests for block pruning are failing to catch instances where tensor shapes aren't multiples of the block size, causing shape-mismatching tensors which results in segfaults and silent failures."
e9c1ccee2247a7746fde202067a7d47b72809968,1646177294,"Bug fix: allow std 0 in the meta definition of `normal_` (#70085)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70085

All other `normal` variants allow 0.  Looks like a mistake made while
copying the check.  Even the `normal_` implementation disagrees:

```
>>> t = torch.rand(2, 3, device='meta')
>>> t.normal_(mean=4, std=0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: normal_ expects std > 0.0, but found std=0
>>>
>>>
>>> t = torch.rand(2, 3)
>>> t.normal_(mean=4, std=0)
tensor([[4., 4., 4.],
        [4., 4., 4.]])
```

Fixes #69523.

Test Plan: Imported from OSS

Reviewed By: davidberard98

Differential Revision: D34089967

Pulled By: bdhirsh

fbshipit-source-id: c57963e55f06c9513c4f0839f8f7a21eca86b584
(cherry picked from commit d6ffe43ddddd24daa5d9eb8befc852dd2108fc89)
",['aten/src/ATen/native/Distributions.cpp'],"The `normal_` function in PyTorch expects std > 0.0 but other `normal` variants and the implementation allow std=0, leading to inconsistency and errors."
baefe47161dea9b4b4c9ec52982a064084c9342c,1694560799,"Fix std_mean f16 opinfo test by using reference_in_float (#109081)

It seems that the compiled f16 op is more accurate than the eager f16
op:

**Compiled float16 vs Eager float64**

    Mismatched elements: 25 / 25 (100.0%)
    Greatest absolute difference: 3.718038455710615e-05 at index (1, 0) (up to 1e-07 allowed)
    Greatest relative difference: 0.0018021699903143316 at index (0, 4) (up to 1e-07 allowed)

**Eager float16 vs Eager float64**

    Mismatched elements: 25 / 25 (100.0%)
    Greatest absolute difference: 7.280254198286512e-05 at index (3, 3) (up to 1e-07 allowed)
    Greatest relative difference: 0.004104326045245938 at index (0, 4) (up to 1e-07 allowed)

**Compiled float16 vs Eager float16**

    Mismatched elements: 7 / 25 (28.0%)
    Greatest absolute difference: 7.62939453125e-05 at index (3, 3) (up to 1e-05 allowed)
    Greatest relative difference: 0.00588226318359375 at index (0, 4) (up to 0.001 allowed)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109081
Approved by: https://github.com/eellison
",['test/inductor/test_torchinductor_opinfo.py'],"The compiled f16 op and eager f16 op are displaying discrepancies in accuracy, with mismatched elements and differences beyond acceptable limits, causing test failures in std_mean f16 opinfo."
d44e610efa2098462db696c76fc74f618edc8805,1637788822,"[CUDA Pinned Memory] Event recording with non-blocking copies should track the storage context, not the tensor data pointer (#68749)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68749

The logic for asynchronous copies (either HtoD or DtoH) using cudaMemcpyAsync relies on recording an event with the caching host allocator to notify it that a given allocation has been used on a stream - and thus it should wait for that stream to proceed before reusing the host memory.

This tracking is based on the allocator maintaining a map from storage allocation pointers to some state.

If we try to record an event for a pointer we don't understand, we will silently drop the event and ignore it (https://github.com/pytorch/pytorch/blob/9554ebe44e6e73dc75105d4935d41e626e03299b/aten/src/ATen/cuda/CachingHostAllocator.cpp#L171-L175).

Thus, if we use the data_ptr of a Tensor instead of the storage allocation, then reasonable code can lead to incorrectness due to missed events.

One way this can occur is simply by slicing a tensor into sub-tensors - which have different values of `data_ptr()` but share the same storage, for example:

```
image_batch = torch.randn(M, B, C, H, W).pin_memory()
for m in range(M):
  sub_batch = image_batch[m].cuda(non_blocking=True)
  # sub_batch.data_ptr() != image_batch.data_ptr() except for m == 0.
  # however, sub_batch.storage().data_ptr() == image_batch.storage().data_ptr() always.
```

Therefore, we instead use the storage context pointer when recording events, as this is the same state that is tracked by the caching allocator itself. This is a correctness fix, although it's hard to determine how widespread this issue is.

Using the storage context also allows us to use a more efficient structure internally to the caching allocator, which will be sent in future diffs.

Test Plan: Test added which demonstrates the issue, although it's hard to demonstrate the race explicitly.

Reviewed By: ngimel

Differential Revision: D32588785

fbshipit-source-id: d87cc5e49ff8cbf59052c3c97da5b48dd1fe75cc
","['aten/src/ATen/native/cuda/Copy.cu', 'test/test_cuda.py']","Tensor slicing creates sub-tensors sharing the same storage but with different `data_ptr()` values, causing event tracking errors in async copy operations using CUDA pinned memory."
4636fe701c2906ca176f3ee727b759376c96f255,1673560276,"Limit the memory and CPU of Bazel build to avoid crashing the runner (#92056)

I'm seeing quite a number of runner errors ""i-NUMBER lost communication with the server. Verify the machine is running and has a healthy network connection. Anything in your workflow that terminates the runner process, starves it for CPU/Memory, or blocks its network access can cause this error"" with Bazel build and test job, i.e. https://hud.pytorch.org/hud/pytorch/pytorch/master/1?per_page=50&name_filter=bazel

The job runs on normal `linux.2xlarge` runner.  As the error doesn't occur with any other jobs running on the same type of runner with the exception of XLA.  I suspect that this is due to a resource constraint crashing the runner.  So this PR sets a limit to the amount of memory and CPU and bazel can use.  Even if bazel crashes, i.e. with OOM error, it's still better than crashing the whole runner and losing all the logs.

Example failures:

* https://hud.pytorch.org/pytorch/pytorch/commit/33e3c9ac679d95f28b2486ceea14188caa191d5c
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92056
Approved by: https://github.com/ZainRizvi
",['.jenkins/pytorch/build.sh'],"Bazel build and test jobs are causing runner errors, possibly due to resource constraints, causing loss of communication with the server and crash of the runner."
0765dbc25ed9368f41225e7de231ee3dd6b188a3,1677204655,"[Functional Collectives] Migrate DeviceMesh::all_reduce to use functional all_reduce. (#95009)

BC: This changes the signature and semantics of DeviceMesh::all_reduce.

DeviceMesh::all_reduce now uses a functional collective under the hood which makes it more easily traceable.
You no longer need to use CommTensor to get a trace.

all_reduce now is async only and uses AsyncCollectiveTensor to ensure proper stream synchronization.

Signature changed: removed `async_op` param and changes return type from `Optional[Work]` to `torch.Tensor`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95009
Approved by: https://github.com/wanchaol
","['test/distributed/_spmd/test_tracing.py', 'test/distributed/_tensor/test_device_mesh.py', 'torch/distributed/_functional_collectives.py', 'torch/distributed/_spmd/distribute.py', 'torch/distributed/_tensor/device_mesh.py', 'torch/distributed/_tensor/placement_types.py']",The DeviceMesh::all_reduce function is difficult to trace and lacks proper stream synchronization.
e07ae941f0e0f3b624c799c03bf3e941863d0814,1658534228,"[Vulkan] Patch Linear op to support higher dimensional input. (#81773)

Summary: Previously, the Vulkan Linear op only supported 2d input. This diffs adds support for higher dimensional input.

Test Plan:
Added test cases to `/xplat/caffe2/aten/src/ATen/test/vulkan_api_test.cpp`

On Mac:
```
buck run //xplat/caffe2:pt_vulkan_api_test_binAppleMac
```
On Android:
```
buck build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 //xplat/caffe2:pt_vulkan_api_test_binAndroid\#android-arm64 --show-output
adb push buck-out/gen/xplat/caffe2/pt_vulkan_api_test_binAndroid\#android-arm64 /data/local/tmp/vulkan_api_test
adb shell ""/data/local/tmp/vulkan_api_test""
```

Differential Revision: D37938388

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81773
Approved by: https://github.com/SS-JIA
","['aten/src/ATen/native/vulkan/ops/Mm.cpp', 'aten/src/ATen/test/vulkan_api_test.cpp']",The Vulkan Linear operation currently only supports 2D input and does not handle higher dimensional input effectively.
aceceb3d5c4f8b0632c01b75d6c51c08609cb131,1618345409,"Reland #50999 (Added pow() on CPU for float16 & bfloat16) (#55280)

Summary:
#### Reason for relanding
Line 1607 of `torch/testing/_internal/common_methods_invocations.py` of https://github.com/pytorch/pytorch/issues/50999  had `dtype` instead of `dtype=torch.bool`, so 4 of the 9 sample inputs for `bool` had incorrect dtype. This bug was caught by https://github.com/pytorch/pytorch/issues/54949.

1. Added support for pow() on CPU for `float16` (`Half`) and `bfloat16` types.
Both `pow(Tensor, Scalar)` and `pow(Tensor, Tensor)` are now supported for the aforementioned types.
However autograd isn't supported for `Float16` on CPU yet, as `log_vml_cpu` can't be enabled for it.
2. heitorschueroff added `pow_tensor_scalar_optimized_kernel` to refactor & simplify `PowKernel.cpp`.
It provides a common path for all the complex types & floating point types (except Float16, due to lack of complete AVX2 vectorization support for it).  It replaced code that had previously been duplicated for (float, double) and complex types,
so PowKernel.cpp looks a lot cleaner now.
3. Enabled (unskipped) some tests for `erf`, `erfc`,`erfinv`, `tan` and `linalg.vector.norm` which were being skipped earlier due to `pow()` not having been implemented for `float16` & `bfloat16`.
4. Added an OpInfo for `pow()` & enabled some test cases for `pow()`.
5. Extended the coverage of existing tests for `pow` in `test_binary_ufuncs.py` in order to enable comparison with `numpy`, even with discontiguous tensors, and added a test to ensure that a runtime error is raised for `pow`'s inplace variant if resizing the base tensor is required during its invocation.
6. Added `float16` & `bfloat16` to `square`'s dtype lists in its `UnaryUfuncInfo`.
7. Removed redundant `dtypesIfCPU` and `dtypesIfCUDA` from `OpInfo`s where they are equal to `dtypes`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55280

Reviewed By: jbschlosser

Differential Revision: D27591772

Pulled By: heitorschueroff

fbshipit-source-id: c7420811b32595bb3353149a61e54a73f2eb352b
","['aten/src/ATen/native/cpu/PowKernel.cpp', 'test/test_binary_ufuncs.py', 'test/test_torch.py', 'torch/testing/_internal/common_methods_invocations.py']",'pow()' function not supported for 'float16' and 'bfloat16' on CPU. Incorrect Dtype in sample inputs for bool. Insufficient test coverage for 'pow' function.
096ff0ecca3f76dcdeebe4e515c4fc3843078753,1651677985,"introduce new --gen-dir flag to generate_code and use it in fbcode (#75800)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75800

This leads to more similarities between OSS CMake and eventually OSS
Bazel. We will be able to generate files with the same names and not
have different file lists between the builds.
ghstack-source-id: 155300043

Test Plan: Verified locally and in CI.

Reviewed By: dreiss

Differential Revision: D35648586

fbshipit-source-id: 9f1638b5665ebcc64466883f65ef24a2bfd05228
(cherry picked from commit 7f2acff1baa8dfafddefdc720714f8d39feda436)
","['build.bzl', 'tools/build_variables.bzl', 'tools/setup_helpers/generate_code.py']","The `generate_code` utility is currently without a flag to specify generation directory, leading to discrepancies in file names and lists between different build systems."
be3ad8c6377d569fce2c78dac7ce261ff4a50157,1650652548,"[PyTorch][2/4] Support static dispatch with multiple backends (#75605)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75605

Usecase: Milan models have multiple backends and need to use static dispatch to save on static initialization time and to hit native functions directly from the unboxed APIs.

This change passes in List[BackendIndex] and adds ability to generate code for multiple static backends with 1 or 0 kernels
ghstack-source-id: 154525738

(Note: this ignores all push blocking failures!)

Test Plan:
Builds lite_predictor_flatbuffer with multiple backends

```
buck build --config pt.enable_lightweight_dispatch=1 --config pt.static_dispatch_backend=CPU,QuantizedCPU,CompositeExplicitAutograd //xplat/caffe2/fb/lite_predictor:lite_predictor_flatbuffer
```

Reviewed By: larryliu0820

Differential Revision: D35510644

fbshipit-source-id: f985718ad066f8578b006b4759c4a3bd6caac176
(cherry picked from commit a6999729c8cc26c54b8d5684f6585d6c50d8d913)
","['cmake/Codegen.cmake', 'tools/codegen/gen.py']",Multiple backends in Milan models cause an increase in static initialization time and hinder direct hit of native functions due to lack of support for static dispatch.
87484d67e33764dd46ce781faffeb00739c96ed4,1641843045,".github: Enable linux binary builds (#68388)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68388

Updates the gpu architectures as well as adding a trigger for
on_pull_request for the binary build workflows so that we can iterate on
this later

TODO:
* Create follow up PR to enable nightly linux GHA builds / disable CircleCI nighlty linux builds

Signed-off-by: Eli Uriegas <eliuriegas@fb.com>

Test Plan: Imported from OSS

Reviewed By: janeyx99

Differential Revision: D33462294

Pulled By: seemethere

fbshipit-source-id: 5fa30517550d36f504b491cf6c1e5c9da56d8191
","['.circleci/scripts/binary_linux_test.sh', '.circleci/scripts/binary_populate_env.sh', '.circleci/scripts/binary_upload.sh', '.github/scripts/generate_binary_build_matrix.py', '.github/scripts/generate_ci_workflows.py']","Linux binary builds are not currently enabled, which may limit the project's testing processes and interoperability with Linux-based systems."
68eec90cfd3ba09dd41cfa18a17b9c892d204a79,1677532029,"Support elementwise add / mul for [B, *] nested, [B, 1] dense (CUDA only) (#95620)

Small hack to reuse the 3D custom kernel from #88289 for [B, *] nested, [B, 1] dense elementwise add / mul. Simply treat the inputs as [B, *, 1], [B, 1, 1]. This is added to satisfy an internal ask.

Future work: full general broadcasting support between mixed nested / dense.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95620
Approved by: https://github.com/cpuhrsch, https://github.com/drisspg
","['aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp', 'test/test_nestedtensor.py']","Unable to perform elementwise add/mul operations for [B, *] nested and [B, 1] dense tensors in CUDA."
f0f49a1153e54e81a8939659649deefb86ebd1e7,1644270576,"[torch.package] add test case for repackaging parent module (#72367)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72367

Pull Request resolved: https://github.com/pytorch/pytorch/pull/72299

Test Plan:
Before https://github.com/pytorch/pytorch/pull/71520:
```
Summary
  Pass: 106
  Fail: 1
    ✗ caffe2/test:package - test_repackage_import_indirectly_via_parent_module (package.package_d.test_repackage.TestRepackage)
  Skip: 22
  ...
  ListingSuccess: 1
```

After https://github.com/pytorch/pytorch/pull/71520:

```
BUILD SUCCEEDED
    ✓ ListingSuccess: caffe2/test:package : 129 tests discovered (28.595)
    ✓ Pass: caffe2/test:package - test_repackage_import_indirectly_via_parent_module (package.package_d.test_repackage.TestRepackage) (18.635)
Summary
  Pass: 1
  ListingSuccess: 1
```

Reviewed By: PaliC

Differential Revision: D34015540

fbshipit-source-id: b45af5872ae4a5f52afbc0008494569d1080fa38
(cherry picked from commit 432d728e6627437f9685750a511396ce593ae3d0)
","['test/package/package_d/imports_directly.py', 'test/package/package_d/imports_indirectly.py', 'test/package/package_d/subpackage_0/__init__.py', 'test/package/package_d/subpackage_0/subsubpackage_0/__init__.py', 'test/package/test_repackage.py']",Absence of a test case for properly repackaging parent modules in torch packages causing potential unnoticed failures.
c55cb29bb205e94bb94cc3073e8b7da02af86430,1694128895,"enforce equalities (#108429)

Sometimes one might want to impose equalities that are not required by guards, e.g. say that you only want square images when rectangular images would suffice.

Curiously we never checked that the concrete values passed in example shapes actually satisfy such equality constraints. So, e.g., you could multiply two tensors of shapes MxK and KxN, specify that M and N must be equal, and then pass examples where they are not equal.

Relatedly, the symbolic shape dimensions for inputs in the exported graph were not forced to be equal.

However, runtime assertions still fire because they take into account all equality constraints. This would result in the strange situation where export would succeed but the exported program with the same example inputs would fail.

This PR fixes these issues.

Differential Revision: [D48910918](https://our.internmc.facebook.com/intern/diff/D48910918/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108429
Approved by: https://github.com/zhxchen17
","['test/dynamo/test_export.py', 'torch/fx/experimental/symbolic_shapes.py']",Equality constraints imposed on tensor shapes are not being enforced during export leading to inconsistencies when running the exported program with the same example inputs.
9b74267eb6de5076e7eb2e92bc34eef771384c1e,1665152078,"[autocast] Make it easier to register rules (#86402)

On the way to resolving https://github.com/pytorch/pytorch/issues/86294

Previously, there were three macros used to register autocast rules:
- KERNEL
- KERNEL_DIFFERENT_REDISPATCH_SIGNATURE
- KERNEL_CPU

This PR makes the KERNEL and KERNEL_CPU macros less redundant for users.
KERNEL_DIFFERENT_REDISPATCH_SIGNATURE is weird and only used three
times, so I didn't change them.

Concretely, KERNEL(OP, OP_NAME, SIGNATURE, POLICY) is redundant:
- op/op_name are similar, and the signature can be decltype'd.
PR changes it so that instead, one uses either:
- KERNEL(OP, POLICY)
- KERNEL2(OP, OVERLOAD, POLICY)
depending on whether the operator name has an overload.

This PR also gives the same treatment to the KERNEL_CPU macro, which is
used for registering autocast cpu rules: it splits KERNEL_CPU into
KERNEL_CPU(OP, POLICY) AND KERNEL_CPU2(OP, OVERLOAD, POLICY).

I will do some more cleanup of things that are implemented via
`m.impl(...)` in a follow-up PR so that I don't get confused when I need
to rebase.

Test Plan:
- wait for tests (how good are our autocast tests?)
- code reading
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86402
Approved by: https://github.com/ezyang
",['aten/src/ATen/autocast_mode.cpp'],"The process of registering autocast rules in Pytorch is overly complicated, involving multiple redundant macros making it harder for users to understand and use."
295fd20eb57305efb888c4f6fbbeeb7f7dbfd488,1675196643,"[CI] Add Python-3.11 Linux conda builds (#93186)

This PR almost a no-op, as most of the logic resides in the builder repo, namely:
https://github.com/pytorch/builder/commit/6342242c50dab6abd2178f33ba4f3c5a51c9427d
https://github.com/pytorch/builder/commit/8f361d91e15c6a815daf916abea2741cf092a462

Remove `conda-forge` channel dependency for test job, but add `malfet` channel for 3.11 testing (as numpy is not in default channel yet)
Build and upload following dependencies to `pytorch-nightly` channel:
```
anaconda copy --to-owner pytorch-nightly malfet/numpy/1.23.5
anaconda copy --to-owner pytorch-nightly malfet/numpy-base/1.23.5
anaconda copy --to-owner pytorch-nightly malfet/mkl-service/2.4.0
anaconda copy --to-owner pytorch-nightly malfet/mkl_random/1.2.2
anaconda copy --to-owner pytorch-nightly malfet/mkl_fft/1.3.1

anaconda copy --to-owner pytorch-nightly malfet/sympy/1.11.1
anaconda copy --to-owner pytorch-nightly malfet/mpmath/1.2.1
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93186
Approved by: https://github.com/atalman, https://github.com/ZainRizvi
","['.circleci/scripts/binary_linux_test.sh', '.github/scripts/generate_binary_build_matrix.py']","Python 3.11 Linux conda build dependencies aren't automatically managed and need to be manually built and uploaded to the 'pytorch-nightly' channel, creating dependency management issues."
399169fc925efc00ff4b51c5c4170117105abb8b,1652465190,"add BFloat16 operators on CPU: diag, fmod, cumsum, cumprod (#61897)

Added BFloat16 support for searchsorted, diag, fmod, cumsum, and cumprod on CPU, and collected the benchmark data of these OPs (searchsorted, diag, fmod, cumsum, cumprod) for BFloat16 and Float32 data type by using the operator_benchmark tool of PyTorch on the platform of Intel(R) Xeon(R) Platinum 8180 CPU @ 2.50GHz

Number of cores: 1 core, 28 cores(1 socket)
[cumsum_cumprod_benchmark.txt](https://github.com/pytorch/pytorch/files/6980232/cumsum_cumprod_benchmark.txt)
[diag_benchmark.txt](https://github.com/pytorch/pytorch/files/6980233/diag_benchmark.txt)
[fmod_benchmark.txt](https://github.com/pytorch/pytorch/files/6980234/fmod_benchmark.txt)
[searchsorted_benchmark.txt](https://github.com/pytorch/pytorch/files/6980328/searchsorted_benchmark.txt)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61897
Approved by: https://github.com/VitalyFedyunin
","['aten/src/ATen/native/TensorShape.cpp', 'aten/src/ATen/native/cpu/BinaryOpsKernel.cpp', 'aten/src/ATen/native/cpu/ReduceOpsKernel.cpp', 'torch/testing/_internal/common_methods_invocations.py']","Lack of BFloat16 support for operators (searchsorted, diag, fmod, cumsum, cumprod) on CPU, causing limitations in PyTorch computational capabilities."
9fd14fcd09629c680c5b2ba1054da74204838298,1685542472,"Improve repeat_interleave with scalar repeat value (#102570)

`repeat_interleave_symint` is currently implemented by guarding on the `SymInt`
and converting it to a tensor to pass to the Tensor overload. This instead
implements it as a copy of an expanded tensor, which can be done without guards
and is also much more efficient in eager mode to boot.

For example, these are timings for `x.repeat_interleave(100, dim=-1)` with `x.shape == (1000, 100)`

| Device | Time (Master) | Time (This PR)  | Speedup |
|--------|---------------|-----------------|---------|
| cpu    | 18.8 ms       | 3.5 ms          | 5.4     |
| cuda   | 271 us        | 134 us          | 2.0     |

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102570
Approved by: https://github.com/lezcano
","['aten/src/ATen/native/Repeat.cpp', 'test/inductor/test_torchinductor.py']","The current implementation of `repeat_interleave_symint` converts `SymInt` to a tensor and results in inefficient performance, especially in eager mode."
c61778355c3b237a25be92f80b74fd87e83d76e3,1618930702,"Upgrade ShellCheck to v0.7.2 (#56445)

Summary:
[First ShellCheck release in over a year!](https://github.com/koalaman/shellcheck/releases/tag/v0.7.2) I'm thankful for doing https://github.com/pytorch/pytorch/issues/55109 at the beginning of this month, because otherwise `master` would have just suddenly started failing a few hours ago.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56445

Test Plan:
CI. You can also run `shellcheck` locally; for instance, if you're on Mac and [installed it with Homebrew](https://github.com/koalaman/shellcheck/tree/v0.7.2#installing):
```sh
brew upgrade shellcheck
rm -r .extracted_scripts ; tools/extract_scripts.py --out=.extracted_scripts
tools/run_shellcheck.sh .jenkins/pytorch .extracted_scripts
```

Reviewed By: janeyx99

Differential Revision: D27874084

Pulled By: samestep

fbshipit-source-id: 3bd871a368fe03aecd559e2f55bce36af49cfa27
",['.jenkins/pytorch/macos-common.sh'],"Old version of ShellCheck causing compatibility issues, leading to potential sudden failures on `master` branch."
eea752f853942564945fe642009dd5bc464acd55,1675237433,"[Quant][ONEDNN] Fix weight reorder issue for grouped convolution (#91934)

**Summary**
For onednn quant backend only.
QConv weight may be reordered to another blocked format if input shape is changed at runtime. It's a bug that group info is not retained for such reordering. This may lead to wrong shape of weight after reordering. This PR fixes this bug.

**Test plan**
python test/test_quantization.py -k test_conv_reorder_issue_onednn

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91934
Approved by: https://github.com/jgong5, https://github.com/jerryzh168
","['aten/src/ATen/native/quantized/cpu/qconv.cpp', 'test/quantization/core/test_quantized_op.py']","Grouped convolution weight reordering in the ONEDNN quant backend does not retain group info, resulting in incorrect weight shapes when input shapes are altered at runtime."
5f9939f65eea8b5ea017fdd10668f48364d6c0b1,1659154110,"Introduce discontinuity to nested tensor (#80981)

Nested tensor used to assume the buffer memory to be contiguous. However, some operations can break that assumption:
* reshape
* transpose
* slice

To be able to access underlying tensors from discontinuous buffer, we need 3 metadata:
* sizes of each tensor (`nested_size_tensor_`)
* strides of each tensor (`nested_stride_tensor_`)
* offset of each tensor (`offsets_`)

so we access each tensor by `buffer.as_strided(size, stride, offset)`

This pull request introduces the offsets metadata, then added reshape and transpose so that we can create discontinuous cases for testing. Unbind, select, dropout, softmax, bmm are refactored to provide tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80981
Approved by: https://github.com/jbschlosser
","['aten/src/ATen/NestedTensorImpl.cpp', 'aten/src/ATen/NestedTensorImpl.h', 'aten/src/ATen/native/TensorShape.cpp', 'aten/src/ATen/native/nested/NestedTensorMath.cpp', 'aten/src/ATen/native/nested/NestedTensorMath.h', 'aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp', 'aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp', 'c10/core/DispatchKeySet.cpp', 'test/test_nestedtensor.py']","Nested tensors assume contiguous buffer memory leading to performance issues with operations like reshape, transpose and slice that can introduce discontinuity."
87564a1bd7ab2843e51e1502705c56b860138724,1646881071,"[Static Runtime] Add native op support for `aten::len` (#73899)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73899

This change adds native op wrappers to Static Runtime as appears in JIT (https://www.internalfb.com/code/fbsource/[429d233b9beb5e6f60df7304b792e2ff332f6ecd]/fbcode/caffe2/torch/csrc/jit/runtime/register_prim_ops.cpp?lines=613 , search for ""aten::len"" in that file).

Test Plan: Added unittests, ""StaticRuntime.LenWith*"", and confirmed they are passing with `V0307 17:39:39.817956 3516654 impl.cpp:1792] Switch to native impl for node: %2 : int = aten::len(%input.1)` per added unittest: P485159811

Reviewed By: mikeiovine

Differential Revision: D34705231

fbshipit-source-id: 916b1f8bdbc92def07bc3f98ce1db22f0f5ce206
(cherry picked from commit 66d2bb9a0a294b55e1bc87ae33f5553b1460e74b)
","['benchmarks/static_runtime/test_static_runtime.cc', 'torch/csrc/jit/runtime/static/native_ops.cpp']","Static Runtime lacks native operation support for `aten::len`, causing discrepancies with JIT operation handling."
35fb0077495247bcda218a136c3a70f3022de7d2,1665725928,"[Profiler][Minor] Separate standalone profilers from the main PyTorch profiler. (#85511)

There are a number of instrumentation utils which have been added to the profiler toolkit. They are generally small and self contained, often wrapping vendor APIs. (NVTX, ITT)

They don't really interact with the much more expansive machinery of the PyTorch profiler beyond registration / unregistration, minor util sharing, and reusing the profiler base class. Just as in the case of stubs, it makes sense to group them in a dedicated subfolder.

Differential Revision: [D39108649](https://our.internmc.facebook.com/intern/diff/D39108649/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39108649/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85511
Approved by: https://github.com/albanD
","['build_variables.bzl', 'torch/csrc/autograd/init.cpp', 'torch/csrc/autograd/profiler_kineto.cpp', 'torch/csrc/profiler/python/init.cpp', 'torch/csrc/profiler/standalone/execution_graph_observer.cpp', 'torch/csrc/profiler/standalone/execution_graph_observer.h', 'torch/csrc/profiler/standalone/itt_observer.cpp', 'torch/csrc/profiler/standalone/itt_observer.h', 'torch/csrc/profiler/standalone/nvtx_observer.cpp', 'torch/csrc/profiler/standalone/nvtx_observer.h', 'torch/profiler/profiler.py']","Instrumentation utilities in the Pytorch profiler toolkit, such as NVTX and ITT, are not properly integrated with the more extensive machinery of the PyTorch profiler and require better organization and structuring."
a935118c90db94879b41d6dc359520e99e6cdca2,1613891520,"Fix caffee2 to use MaybeAlign when using LLVM trunk

Summary: Trunk at 13 uses a different type for `CreateAlignedStore` and `CreateAlignedLoad` so updating usage here to reflect this.

Test Plan:
buck build mode/opt-clang-thinlto sigrid/predictor/v2:sigrid_remote_predictor -c cxx.extra_cxxflags=""-Wforce-no-error -fbracket-depth=300"" -c cxx.profile=""fbcode//fdo/autofdo-bolt-compatible/sigrid/predictor/v2/sigrid_remote_predictor:autofdo-bolt-compatible"" -c cxx.modules=False

Previously:
caffe2/torch/csrc/jit/tensorexpr/llvm_codegen.cpp:1079:21: error: no matching member function for call to 'CreateAlignedLoad'
      value_ = irb_.CreateAlignedLoad(vaddr, 4);
               ~~~~~^~~~~~~~~~~~~~~~~
third-party-buck/platform009/build/llvm-fb/include/llvm/IR/IRBuilder.h:1681:13: note: candidate function not viable: no known conversion from 'int' to 'llvm::MaybeAlign' for 2nd argument
  LoadInst *CreateAlignedLoad(Value *Ptr, MaybeAlign Align,

Now:
Passes

Differential Revision: D26562330

fbshipit-source-id: dbf9ca5247ccd4351861995c2c5480a7cc55c202
",['torch/csrc/jit/tensorexpr/llvm_codegen.cpp'],"The `CreateAlignedStore` and `CreateAlignedLoad` functions in caffee2 aren't compatible with LLVM trunk version 13, causing build errors due to type mismatch."
d200e9de26c39e19dd082d49b26f7daa9b9afff3,1624503365,"[Static Runtime] Test for dynamic shapes in SR unit tests (#60579)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/60579

- Modify testStaticRuntime to take two sets of inputs so if the second set of inputs have bigger shapes, it would trigger memory allocations in resize_ calls.
- Modify test scripts so that the output of the test op is managed by the memory planner, as explained in comments.

Reviewed By: ajyu

Differential Revision: D29221452

fbshipit-source-id: 09f0f7eb384dc8ca67594f1fa76e1e31392ee6ca
","['benchmarks/static_runtime/test_scripts.h', 'benchmarks/static_runtime/test_static_runtime.cc']","Static Runtime unit tests do not account for dynamic shapes, failing to trigger memory allocations in resize_ calls.
"
8cc9ec2f6b270e84bcdda9b898ec8d74caf3e671,1638250789,"Add option to get input dtype from user (#68751)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68751

Add option to get input dtype from user for AOT compilation

Test Plan:
BI model compiles and runs fine
```
(pytorch)  ~/fbsource/fbcode/caffe2/fb/nnc
└─ $ buck run //caffe2/binaries:aot_model_compiler -- --model=bi.pt --model_name=pytorch_dev_bytedoc --model_version=v1 '--input_dims=1,115;1' --input_types='int64;int64'
Building... 8.3 sec (99%) 7673/7674 jobs, 0/7674 updated
WARNING: Logging before InitGoogleLogging() is written to STDERR
W1116 14:32:44.632536 1332111 TensorImpl.h:1418] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())
E1116 14:32:44.673710 1332111 huge_pages_allocator.cc:287] Not using huge pages because not linked with jemalloc
The compiled llvm assembly code was saved to bi.compiled.ll
The compiled model was saved to bi.compiled.pt
```

> Error thrown when input dims and input types sizes don't match

```
(pytorch)  ~/fbsource/fbcode/caffe2/fb/nnc
└─ $ buck run //caffe2/binaries:aot_model_compiler -- --model=bi.pt --model_name=pytorch_dev_bytedoc --model_version=v1 '--input_dims=1,115;1' --input_types='int64;int64;int64'
.
.
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at aot_model_compiler.cc:208] split(';', FLAGS_input_dims).size() == split(';', FLAGS_input_types).size(). Number of input_dims and input_types should be the same
.
.
.
```

Reviewed By: ljk53

Differential Revision: D32477001

fbshipit-source-id: 8977b0b59cf78b3a2fec0c8428f83a16ad8685c5
","['binaries/aot_model_compiler.cc', 'torch/csrc/jit/mobile/nnc/aot_compiler.cpp', 'torch/csrc/jit/mobile/nnc/aot_compiler.h']","AOT compilation does not provide an option to obtain input dtype from the user, causing an error when input dimension sizes and input type sizes don't match."
4d82e5bf449929c2732250f8620976e5be96a7f3,1648176771,"[PyTorch] Avoid registering ops into dispatcher in lightweight dispatch (#74664)

Summary:
This change adds the following logic:

If lightweight dispatch is enabled, do not generate `TORCH_LIBARAY` API calls for operator schema and implementations, since these operators will be registered into JIT op registry.

`skip_dispatcher_op_registration` is an existing argument to `gen.py`. With that set, `RegisterDispatchKey.cpp` will not generate `m.def` and `m.impl` for each native function. This logic will be removed once we find a better way to skip op registration into dispatcher.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74664

Test Plan: Rely on unit tests for lightweight dispatch.

Reviewed By: priyaramani

Differential Revision: D34634300

Pulled By: larryliu0820

fbshipit-source-id: d87828f2c6c62f15024ce9e98823b09ee5a81336
(cherry picked from commit 3eb1c27547dea6accd9fa95496189f3699d91201)
","['tools/codegen/dest/register_dispatch_key.py', 'tools/codegen/gen.py', 'tools/codegen/gen_backend_stubs.py']","Lightweight dispatch is registering unnecessary operators into the JIT op registry and dispatcher, increasing overhead and possibly affecting performance."
70be6f84703c047ee22ca1c8eca39267df2cb6d8,1655932769,"[ao] Added generate report capability to ModelReport class

Summary: The ModelReport class in model_report.py combines the
functionality of the detectors and the ModelReportObserver. It creates
an end-to-end system where a user can pass in a prepared Graph Model to
insert the ModelReportObservers, then after the user callibrates their
model, the callibrated model can then be used by the ModelReport class
to generate reports based on what the user wished to gather information
on.

This contains the implementation and the tests for the generate_report
method which is used on a callibrated fx model to generate reports based
on data collected by the inserted observers during the callibration
phase and also potentially remove those observers if desired.

This also addresses and fixes a revert issue that has been fixed.

Test Plan: python test/test_quantization.py TestFxModelReportClass

Reviewers:

Subscribers:

Tasks:

Tags:

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80054

Approved by: https://github.com/HDCharles
","['test/quantization/fx/test_model_report_fx.py', 'torch/ao/quantization/fx/_model_report/detector.py', 'torch/ao/quantization/fx/_model_report/model_report.py']",ModelReport class lacks a method to generate reports based on data collected by inserted observers during the calibration phase of a prepared Graph Model.
929f1d5317755ef1161e27bc9925fa8c546c345c,1652915933,"[RELAND] Adds torch.cuda.is_current_stream_capturing (#77789)

Resubmit of https://github.com/pytorch/pytorch/pull/77673, which was reverted due to Windows test failures: https://github.com/pytorch/pytorch/pull/77673#issuecomment-1130425845.

I suspect these failures happened because I don't explicitly set a side stream for graph capture in the new test.
Not setting a side stream explicitly is alright on Linux because cuda tests implicitly use a side stream.
I think Windows cuda tests implicitly use the default stream, breaking capture and leaving the backend in a bad state.
Other graphs tests explicitly set side streams and don't error in Windows builds, so i'm 95% sure doing the same for the new test will work.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77789
Approved by: https://github.com/ezyang
","['aten/src/ATen/cuda/detail/CUDAHooks.h', 'test/test_cuda.py', 'torch/csrc/cuda/Module.cpp', 'torch/cuda/__init__.py', 'torch/cuda/graphs.py']","On Windows, not explicitly setting a side stream for graph capture in tests leads to capture failure and leaves the backend in a bad state. This issue does not occur in Linux builds."
3cc5c42a23d93ba84cb331173714518d8a8e004d,1692896292,"Fix aot sequence_nr to reset bwd flag (#107210)

The way the aot autograd sequence_nr tracking works is that we run the aot export logic, the dynamo captured forward graph is run under an fx.Interpreter, which iterates through the nodes of the forward graph while setting the `current_metadata`.
Since during backward what is run doesn't correspond to any node during forward, we fallback to the global `current_metadata`. And since this global metadata is ends up being shared between runs, that leads to weirdness if we forget to reset things, e.g., depending whether this is the first test run, the printed results will be different.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107210
Approved by: https://github.com/bdhirsh
","['test/dynamo/test_aot_autograd.py', 'torch/_functorch/aot_autograd.py', 'torch/fx/proxy.py', 'torch/fx/traceback.py']","The autograd sequence_nr tracking in aot export logic shares global metadata between runs, causing inconsistent results if not reset, especially during the first test run."
1d03a6a90181d903df5bb7f6c2edf3801f45a5ca,1674726934,"[Quant][Fx] Fix issue: qconfig_mappings of onednn backend are not correctly set for fused modules (#91297)

**Summary**
For onednn quantization backend only.
Currently, FX fusion requires that all separate ops in a fused module/op have the same `qconfig`. To support `linear - leaky_relu` and `linear - tanh` fusion with onednn backend, we previously explicitly set the same `qconfig` to `linear`, `leaky_relu` and `tanh`. However, this brings two problems:
- It breaks fusion of `linear - relu` since `relu` does not have the same `qconfig` as `linear` does. And it does not look good if we set `qconfig` to all these ops. They should use a global `qconfig` by default.
- `Tanh` requires `fixed_qparams_qconfig` otherwise it is not quantized. So, we cannot set another `qconfig` to `tanh`.

Looks like there is not a straightforward way to solve the problems. This PR fixes them by the following:
- Do not set `qconfig` to these ops so that these ops use a global `qconfig` and `linear - relu` and `linear - leaky_relu` can be fused correctly.
- Set the same `qconfig` to `linear` and `tanh` manually by users when they want to fuse `linear - tanh` with onednn backend.

A known issue still exists: users cannot fuse `linear - tanh` and quantize standalone `tanh` at the same time.

**Test plan**
python test/test_quantization.py -k test_qconfig_dict_with_fused_modules

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91297
Approved by: https://github.com/jgong5, https://github.com/jerryzh168
","['test/quantization/fx/test_quantize_fx.py', 'torch/ao/quantization/qconfig_mapping.py']","""Onednn backend's qconfig_mappings are not correctly set for fused modules causing issues with different operations like linear-relu, linear-leaky_relu and linear-tanh fusions."""
8740c68c41c563de42b011cef42b3de7690e9446,1657561378,"[primTorch] Adds contiguous and expand references (#79820)

I also filed  while creating this PR.

This PR...

**Filed issues**

- https://github.com/pytorch/pytorch/issues/79818
- https://github.com/pytorch/pytorch/issues/80154

**prims**

- Fixes prims.squeeze when called with an unsorted list of dimensions
- Removes the clone prim

**refs**
- adds contiguous
- adds expand
- updates clone to call empty_like and copy_to
- updates empty to accept a memory format
- updates empty_like to accept a memory_format

**utils**
- adds helper functions for working with memory formats and channels last tensors, in particular

**tests**

- removes unused clamp sample input functions (mooted by clamp's new reference inputs)
- extends the reference inputs for clone to include different memory formats
- creates reference inputs for contiguous
- xfails operators that depend on clone (including clone) on `test_python_ref` (see issues)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79820
Approved by: https://github.com/ngimel
","['test/test_ops.py', 'test/test_prims.py', 'torch/_prims/__init__.py', 'torch/_prims/utils.py', 'torch/_refs/__init__.py', 'torch/testing/_internal/common_methods_invocations.py']","Issues with unsorted list of dimensions when calling prims.squeeze, and existence of unused clamp sample input functions. Furthermore, inadequate support for different memory formats in clone, contiguous, and expand functions."
e6bc8f415b5bd5b576123ef004021130751b3894,1666231991,"[BE] Move conda cmake installation to Docker (#87309)

This is parts of the effort to consolidate pip and conda installation in the CI to improve our CI reliability.  This moves conda cmake installation to Docker in those use cases that require it:

* Ubuntu bionic and focal

On the other hand:
* XLA doesn't seem to need conda cmake anymore (Build and test successfully)
* Centos is not in used anywhere in the CI
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87309
Approved by: https://github.com/ZainRizvi, https://github.com/malfet
","['.circleci/docker/build.sh', '.circleci/docker/common/install_conda.sh', '.jenkins/pytorch/common.sh']","Issues with CI reliability due to split pip and conda installations, with a particular focus on Ubuntu bionic and focal use cases that require conda cmake installation."
f10aab03c469b6d5446cee5385000b0c6cd222c5,1696907251,"[sparse] Fix semi-structured sparse shape mismatch bug (#110420)

Summary:

Currently, PyTorch incorrectly calculates the size of the returned
matrix when we pass a non-contiguous batched (>2d) input to the
semi-structured sparse subclass.

This is most common in MLP layers, where we have 2 linear layers back to back.

This will lead to an error like the following:
```
RuntimeError: shape '[20, 64, 64, 3072]' is invalid for input of size
62914560

```
Where the size of the sparse matmul result is off because we infer the
output shape with the wrong tensor shape.

This happens because of a bug where we did not update the subclass
tensor shape when doing transpose.
For semi-structured sparsity, transposing is a no-op where we just set
the boolean flag, but we forgot to also update the tensor shape.

Note that this error goes away in inference mode, since we avoid
decomposing the aten.linear op and handle shape folding ourselves,
which changes the execution path.

An alternative way to fix this issue is to set
TORCH_FLATTEN_LINEAR_3D=True, which will also fix this error.

Test Plan:
```
python test/test_sparse_semi_structured.py -k test_mlp

```

Reviewers:

Subscribers:

Tasks:

Tags:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110420
Approved by: https://github.com/alexsamardzic, https://github.com/cpuhrsch
","['test/test_sparse_semi_structured.py', 'torch/sparse/semi_structured.py']","Non-contiguous batched (>2d) input to semi-structured sparse subclass results in incorrect size calculation of returned matrix, leading to errors especially in MLP layers with 2 linear layers back to back."
d82333e92a408513c0fc52262d9d58afb6d93aba,1620296655,"[pytorch][nnc] protocol classes to persist the context for compiled functions (#56851)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56851

This is part of the changes to enable NNC AOT compilation for mobile.
At the end of the ahead-of-time compilation the compiler produces two sets of artifacts:
1. ""compiled assembly code"" - kernel functions in assembly format optimized for target platforms;
2. ""compiled model"" - regular TorchScript model that contains serialized parameters (weights/bias/etc) and invokes kernel functions via ""handles"" (name/version id/input & output specs/etc of the kernel functions).

This PR introduces a set of classes to represent kernel functions (a.k.a ""handles""), which can be serialized/deserialized into/from the ""compiled model"" as an IValue.
Also introduces APIs to register/look-up ""compiled assembly code"".
ghstack-source-id: 128285802

Test Plan:
- unit tests
- for FB build environment:
buck test //caffe2/test/mobile/nnc:mobile_nnc

Reviewed By: kimishpatel, raziel

Differential Revision: D27921866

fbshipit-source-id: 4c2a4d8a4d072fc259416ae674b3b494f0ca56f3
","['.jenkins/pytorch/macos-lite-interpreter-build-test.sh', 'test/mobile/nnc/test_context.cpp', 'test/mobile/nnc/test_registry.cpp', 'tools/build_variables.bzl', 'torch/csrc/jit/mobile/nnc/context.cpp', 'torch/csrc/jit/mobile/nnc/context.h', 'torch/csrc/jit/mobile/nnc/registry.cpp', 'torch/csrc/jit/mobile/nnc/registry.h']","Lack of a proper system for representing kernel functions (""handles"") in NNC AOT compilation for mobile, as well as issues in registration and lookup of ""compiled assembly code""."
2cb385dd6e880b6339dd337e1b987adb547c862f,1638982456,"OpInfo for `nn.functional.dropout2d`, revise sample inputs for `dropout` (#67891)

Summary:
Earlier, we were only testing for inputs with the shape of `(5,)` for `nn.functional.dropout`, but since it's used a lot - I feel it's a good idea to test for a few more shapes including scalars. This PR:

1. Revises sample inputs for `nn.functional.dropout`
2. Adds an OpInfo for `nn.functional.dropout2d`.

A note regarding the documentation:

Looks like `nn.functional.dropout2d` also supports inputs of shape `(H, W)` apart from `(N, C, H, W) / (C, H, W)` but the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d) doesn't mention that (`H, W` case). Should that be revised or am I missing anything here? (Filed an issue here: https://github.com/pytorch/pytorch/issues/67892)

```python
# A 2D tensor is a valid input for Dropout2d
In [11]: tensor = torch.randn((3, 4), device='cpu', dtype=torch.float32)
In [12]: dropout2d = torch.nn.Dropout2d(p=0.5)

In [13]: dropout2d(tensor)
Out[13]:
tensor([[-0.1026, -0.0000, -0.0000, -0.0000],
        [-1.5647,  0.0000, -0.0000, -0.5820],
        [-0.0000, -3.2080,  0.1164, -3.6780]])
```

Issue Tracker: https://github.com/pytorch/pytorch/issues/54261

cc: mruberry zou3519

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67891

Reviewed By: mrshenli

Differential Revision: D32628527

Pulled By: mruberry

fbshipit-source-id: 4c9b89550f1d49526e294378ce107eba9f29cabb
","['test/test_fx_experimental.py', 'torch/testing/_internal/common_methods_invocations.py']","Testing for `nn.functional.dropout` is limited to inputs with the shape of `(5,)`, and `nn.functional.dropout2d` documentation does not mention support for inputs of shape `(H, W)`."
e54c1f6c9010b7e68ebefb6de0a7fbf57698b6d9,1650054545,"[torch][elastic] Make final agent barrier to shutdown properly

Summary:
When workers finish their work TE agent will start `synchronize_barrier` procedure. The barrier will wait for other agents at the end of the execution.

There is a race condition may happen: The barrier uses TCPStore which is located on Rank0. When Rank0 finishes the work, other ranks may still be in a process of executing `get_all` method. This means that some of them will fail because the TCPStore will be destroyed.

The fix adds additional check on Rank0 process: Rank0 process now waits for all other ranks to finish before terminating the process.

Test Plan: unit tests

Differential Revision: D35227180

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74931
Approved by: https://github.com/kiukchung
","['test/distributed/elastic/agent/server/test/api_test.py', 'test/distributed/elastic/utils/util_test.py', 'torch/distributed/elastic/utils/store.py']","A race condition exists in the TE agent's `synchronize_barrier` procedure, causing some ranks to fail when executing `get_all` method due to premature destruction of TCPStore on Rank0."
9b6ccde0e6408cbef9029bd593dfba7ab6b39ca6,1684148604,"fix precision error in constraint solver (#101307)

When adding guards to the constraint solver, we check that they are consistent, i.e., they do not simplify to false when their free symbols are substituted with the corresponding concrete values.

However this check may ""spuriously"" fail because it doesn't take into account precision errors when comparing floats. Since the symbols involved are all positive integers, we try to approximate floats in the guards with rationals, providing concrete values as hints: `sympy.nsimplify` does the job.

As an alternative approach, we considered using `sympy.evalf` to compare with reduced precision. But we did not pursue it because
* the choice of what is a good reduced precision feels arbitrary (`sympy` uses `1e15` by default);
* more importantly, there is no guarantee that we will not encounter the same problem when solving downstream.

Differential Revision: [D45826951](https://our.internmc.facebook.com/intern/diff/D45826951/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/101307
Approved by: https://github.com/ezyang
","['test/test_dynamic_shapes.py', 'torch/fx/experimental/symbolic_shapes.py']","Constraint solver fails to account for float precision errors when checking consistency of guards, leading to potential false inconsistencies due to float approximations."
b9acfcddeb3b320f142f07d8051e5e344480f939,1612812107,"Support mypy ignore annotation with particular rule specified (#51675)

Summary:
Previously TorchScript allows a ignore-all type check suppression rule that looks like
```
code code code  # type: ignore
```

But a more common use case is
```
code code code  # type: ignore[specific-rule]
```
This PR allows the more common use case

Fixes https://github.com/pytorch/pytorch/issues/48643

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51675

Reviewed By: ansley

Differential Revision: D26304870

Pulled By: gmagogsfm

fbshipit-source-id: 0ac9ee34f0219c86e428318a69484d5aa3ec433f
","['test/test_jit.py', 'torch/jit/annotations.py']","TorchScript currently only supports ignore-all type check suppression rule (e.g., # type: ignore), and does not support specific rule annotation (e.g., # type: ignore[specific-rule])."
8e8d170674f75bff5fd2155f832a47c99334cb73,1644425532,"Optim foreach cleanup for Adadelta (#69980)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69980

- Merged `torch/optim/adadelta.py` and `torch/optim/_multitensor/adadelta.py` into `torch/optim/adadelta.py`
- Moved adadelta functional forms from `torch/optim/_functional.py` and `torch/optim/_multi_tensor/_functional.py` to `torch/optim/adadelta.py`
- `torch/optim/_functional.py` just imports from `torch/optim/adadelta.py`
- Added a test `test_optimizers_foreach_flag` which replicates `test_multi_tensor_optimizers` in `test/test_optim.py`
- Add a test `test_adadelta_new` that replicates the behavior of `test_adadelta` but with `foreach` flag instead of using the multitensor adadleta class. If we delete `_multitensor/` we could replace `test_adadelta` with this

Remaining TODO:

- [ ] single_tensor adadelta supports complex but multitensor does not, need to integrate the singletensor logic in multitensor and switch the `test_adadelta_complex` to test for foreach in [True, False]

Test Plan: Imported from OSS

Reviewed By: VitalyFedyunin, albanD

Differential Revision: D33413059

Pulled By: mikaylagawarecki

fbshipit-source-id: 92a9fa98705762bb6bd464261671e49aef40070e
(cherry picked from commit a008227d227749d79367d7d592bcefcf51c22df5)
","['test/test_optim.py', 'torch/distributed/optim/functional_adadelta.py', 'torch/optim/_functional.py', 'torch/optim/_multi_tensor/__init__.py', 'torch/optim/_multi_tensor/_functional.py', 'torch/optim/_multi_tensor/adadelta.py', 'torch/optim/adadelta.py']","Optimizers in Adadelta use multiple file locations and repetitions, single_tensor Adadelta supports complex types but multi_tensor does not, making it difficult in functionality integration and consistent testing."
07bd053a7ef92263db8d612f4fc7c28e06ade45c,1666224144,"[rpc] Wrap exception creation with try/catch (#87224)

Sometimes, we cannot recreate the exception with only string (for example if it is a custom exception type). Ideal situation would be to carry over all details on how to recreate the remote end's exception and throw that on client, but for now, we raise a RuntimeError with the original error msg when we cannot reconstruct.

Created from CodeHub with https://fburl.com/edit-in-codehub

Differential Revision: [D40353274](https://our.internmc.facebook.com/intern/diff/D40353274/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87224
Approved by: https://github.com/fduwjj
","['torch/distributed/rpc/internal.py', 'torch/testing/_internal/distributed/rpc/rpc_test.py']","Inability to recreate certain exceptions (e.g., custom types) from only strings, causing improper RuntimeError handling with the original error message."
d3c4ec767b1bf414de6d5b5aed6a280c6367aafb,1692220605,"[quant][pt2e] Fix handling for SharedQuantizationSpec (#106922)

Summary:
Previously if we have:
```
conv1 -> cat
conv2  /
```
and configure output of conv1/conv2 to be int8 quantized, and cat also int8 quantized and with shared inputs,
it will not produce expected results (input of cat will not be shared)

The problem is that there is some missing checks when inserting observers for input for cat

This PR fixes the problem.

Fixes: https://github.com/pytorch/pytorch/issues/106760
Test Plan:
python tes/test_quantization.py TestQuantzePT2E.test_shared_qspec

Reviewers:

Subscribers:

Tasks:

Tags:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/106922
Approved by: https://github.com/kimishpatel
","['test/quantization/pt2e/test_quantize_pt2e.py', 'torch/ao/quantization/fx/prepare.py', 'torch/ao/quantization/pt2e/prepare.py']","Configuration of output for conv1/conv2 to be int8 quantized, along with cat also int8 quantized and with shared inputs, does not produce the expected results - the input of cat is not shared."
9157a2889f9e17dda1be6b0b137ab1801ba253a9,1631874656,"Pass GITHUB_TOKEN to linux CI jobs and avoid skipping torchhub tests (#64807)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/64760

This should hopefully put the torchhub tests back.

This also avoids skipping the torchhub tests: currently the tests are skipped if they fail, which pretty much defeats the purpose of having a test in the first place since we're never notified when they do fail.

cc ezyang seemethere malfet lg20987 pytorch/pytorch-dev-infra nairbv NicolasHug

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64807

Reviewed By: seemethere

Differential Revision: D30994585

Pulled By: NicolasHug

fbshipit-source-id: 561782c22462b5cfec99cca153eb59623db5660a
",['test/test_utils.py'],"Torchhub tests are currently being skipped on failure, defeating their purpose as no notification is sent upon failure. Also, the linux CI jobs are not receiving the GITHUB_TOKEN."
def33d4d7a0c74fea0c845eb972af244a69be0d8,1693944155,"Fix inductor <> ddp_optimizer issue (#108081)

@wconstab pointed out that inductor found a graph with 6 input mutations and only 1 output, and seemed to be (incorrectly) chopping off the first ""6"" outputs from the graph (even though there is only 1). It looks like this is because:

(1) AOTAutograd has special handling for input mutations in inference vs. training graphs. In a training graph, whenever AOTAutograd sees an input mutation, it will add an **extra** output to the graph, corresponding to the updated input (and then at runtime, it will grab the updated input, and perform the actual mutation outside of the graph).

In inference, AOTAutograd is smarter and can leave the input mutations directly in the graph for inductor to optimize (doing this in training is harder). In inference, AOTAutograd will **not** add any extra graph outputs for input mutations.

It looks like inductor was unconditionally assuming that input mutations counted as extra outputs in the graph, which is wrong for the inference case.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108081
Approved by: https://github.com/wconstab
","['test/inductor/test_torchinductor.py', 'torch/_inductor/compile_fx.py']","Inductor incorrectly chops off outputs when handling input mutations in inference graphs, assuming extra graph outputs where there should be none."
cb87983cb8f4a26928f9852d96de63da6d4f363c,1664827952,"Decay integer-only (Optional)SymIntArrayRef to IntList in IValue (#86094)

We have logic that says if you ask for a SymIntList from an IValue, but the IValue is actually an IntList, we will still give it to you in that case (check ivalue_to_arg in aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h). However, we also need the *inverse* version of this logic, which says that if you construct an IValue from a SymIntArrayRef, and it is actually integer only, we need to store it as an IntList, so that toIntList on the IValue will work.

The way this works is a bit twisty, but our basic strategy is to disable construction of IValue from list container types that contain SymInt directly, and then directly implement variants of these constructors by hand, which iterate over the elements of the list and test if there are any SymInts or not to decide what type to construct the underlying List. These variants have to be templated, otherwise we will run afoul ambiguous overloads. I only did the overloads that actually occurred in practice; you may need to add more if you SymIntify more stuff.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86094
Approved by: https://github.com/anjali411, https://github.com/albanD
","['aten/src/ATen/core/ivalue.h', 'aten/src/ATen/core/ivalue_inl.h', 'torch/csrc/jit/python/pybind_utils.cpp']","Constructing an IValue from a SymIntArrayRef, when it is actually integer only, does not store it as an IntList, causing toIntList on the IValue to fail."
9a12aa6cad7f5ed7f720352085241883df2f4d92,1657040445,"Add cached nvFuser's fusion creation for torch._prims.executor (#80525)

In the current setup for each call of the `execute` function, a `Fusion` object was constructed using `GraphModule` and args, that's expensive.

This PR makes use of `functools.lru_cache` to pay the `Fusion` creation cost once per `GraphModule` and set of args. Currently, the shape, strides, and dtype of tensors are static it can be changed later to make better use of the nvFuser's internal caching mechanism (by specifying only ndim, contiguity, dtype).

On master:
```py
In [2]: a = torch.randn(3, 3, device='cuda')

In [3]: with TorchRefsMode.push():
   ...:     gm = make_fx(lambda x: torch.sigmoid(x))(a)
   ...:

In [4]: %%timeit
   ...: execute(gm, a, executor=""nvfuser"")
   ...: torch.cuda.synchronize()
175 ms ± 1.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```
This PR:
```py
In [2]: a = torch.randn(3, 3, device='cuda')

In [3]: with TorchRefsMode.push():
   ...:     gm = make_fx(lambda x: torch.sigmoid(x))(a)
   ...:

In [4]: %%timeit
   ...: execute(gm, a, executor=""nvfuser"")
   ...: torch.cuda.synchronize()
62.6 µs ± 9.99 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
```

In addition, this PR adds support for pytree inputs and extends the test for this.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80525
Approved by: https://github.com/kevinstephano, https://github.com/jjsjann123, https://github.com/SherlockNoMad
","['test/test_prims.py', 'torch/_prims/executor.py', 'torch/_prims/nvfuser_executor.py', 'torch/_prims/utils.py']","The existing torch._prims.executor's `execute` function unnecessarily constructs a new `Fusion` object with every call, resulting in slow execution times even when the GraphModule and arguments remain unchanged between calls."
f05710dd400142e22bb98a9e16291820767b68d4,1651708083,"[LT] Add a trie data structure for caching IR nodes

Summary: TrieCache provides a way to look up an IR node before we
actually create it. If the lookup hits in TrieCache, we reuse the
existing node and move the current pointer in TrieCache to point to that
node; if the lookup misses, we create a new node and insert it into TrieCache.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76542

Approved by: https://github.com/wconstab, https://github.com/JackCaoG
","['test/cpp/lazy/test_trie_cache.cpp', 'tools/build_variables.bzl', 'torch/csrc/lazy/core/ir_builder.h', 'torch/csrc/lazy/core/trie.cpp', 'torch/csrc/lazy/core/trie.h']","There is no mechanism to check if an IR node already exists before creating a new one, leading to duplication and inefficient use of memory resources."
a7054869150bdb152cfdbd645b4b9630214e27bb,1647487822,"[Quant][fx] Refactor lowering code (part 1) (#74128)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74128

**Summary:** This commit is the first step towards refactoring the
lowering code in _lower_to_native_backend.py. The main changes
included in this commit are:

(1) Remove the use of the subgraph rewriter in lowering
(2) Replace the use of `is_match` with manual pattern matching

The motivation behind (2) is it simplifies the lowering code
significantly; previously we had many different but similar
patterns for slightly different models. There should be no
change in behavior with this PR.

Note that this is only part 1 of the refactoring. Part 2
will merge the static and dynamic lowering code paths
and refactor the currently duplicate pattern matching /
cleanup code into common helper functions.

**Test Plan:**
python test/test_quantization.py TestQuantizeFx
python test/test_quantization.py TestQuantizeFxOps

**Reviewers:** jerryzh168

**Subscribers:** jerryzh168

Test Plan: Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D34910597

Pulled By: andrewor14

fbshipit-source-id: c6fea0c538ce5efc5afaf53e072922528988dda7
(cherry picked from commit fa05cb9fc0909fe6e199a6b50ea2001c9e9ac0ee)
","['torch/ao/quantization/fx/_lower_to_native_backend.py', 'torch/ao/quantization/fx/quantized_fusion_patterns_and_replacements.py', 'torch/ao/quantization/fx/subgraph_rewriter_FORKED_DO_NOT_USE.py']","Lowering code in _lower_to_native_backend.py presents complicated patterns due to the use of `is_match` and subgraph rewriter, causing difficulty in handling different models. Duplication exists in static and dynamic lowering and there's no common pattern matching/cleanup code."
e9d9151eecd3871aa15c48e3f1fa25c6faee9a3a,1678898044,"[aot autograd] avoid cloning some inputs unnecessarily when they dont require grad (#96342)

When constructing the joint graph, we normally have to clone any inputs that are mutated, so that we can pass in the original, pre-mutation inputs as leaves to autograd.

Previously, we were doing this for all mutated inputs - but we only need to do it for inputs that require gradients and participate in autograd.

Hopefully this should speed up code like batch norm - I think before this we were unnecessarily cloning the running stats during training.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96342
Approved by: https://github.com/albanD, https://github.com/ezyang
","['test/functorch/test_aotdispatch.py', 'torch/_functorch/aot_autograd.py']","Unnecessary cloning of all mutated inputs when constructing the joint graph is slowing down the operations, especially in scenarios such as batch norm training where inputs may not require gradients."
3ac90132351b3dab51d6ecede52db62e101257e1,1614740407,"Implements `torch.linalg.lstsq` (#49093)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/44378 by providing a wider range of drivers similar to what SciPy is doing.

The supported CPU drivers are `gels, gelsy, gelsd, gelss`.
The CUDA interface has only `gels` implemented but only for overdetermined systems.

The current state of this PR:
- [x] CPU interface
- [x] CUDA interface
- [x] CPU tests
- [x] CUDA tests
- [x] Memory-efficient batch-wise iteration with broadcasting which fixes https://github.com/pytorch/pytorch/issues/49252
- [x] docs

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49093

Reviewed By: H-Huang

Differential Revision: D26723384

Pulled By: mruberry

fbshipit-source-id: c9866a95f14091955cf42de22f4ac9e2da009713
","['aten/src/ATen/native/BatchLinearAlgebra.cpp', 'aten/src/ATen/native/LinearAlgebraUtils.h', 'aten/src/ATen/native/cuda/BatchLinearAlgebra.cu', 'test/test_linalg.py', 'test/test_namedtuple_return_api.py', 'torch/csrc/api/include/torch/linalg.h', 'torch/csrc/autograd/utils/wrap_outputs.h', 'torch/linalg/__init__.py', 'torch/overrides.py', 'torch/testing/_internal/common_methods_invocations.py', 'torch/testing/_internal/common_utils.py']","PyTorch is missing a wider range of drivers for `torch.linalg.lstsq` similar to what SciPy provides, limiting the functionality for handling systems of linear equations. Also, there is an inefficient memory usage issue when performing batch-wise iterations with broadcasting."
d0dcebe0a703551047fed9a0e4cdad6f06ff355c,1651888475,"[xplat] Move BatchLinearAlgebraKernel.cpp to aten_native_source_non_codegen_list

Summary:
For some reason `aten/src/ATen/native/BatchLinearAlgebraKernel.cpp` were part of `aten_cpu_source_non_codegen_list` rather than `aten_native_source_non_codegen_list`
Fixes linking issues after https://github.com/pytorch/pytorch/pull/67833
```
stderr: ld.lld: error: undefined symbol: at::TensorIteratorBase::for_each(c10::function_ref<void (char**, long long const*, long long, long long)>, long long)
>>> referenced by TensorIterator.h:352 (buck-out/gen/fe3a39b8/xplat/caffe2/aten_headerAndroid#header-mode-symlink-tree-with-header-map,headers/ATen/TensorIterator.h:352)
>>>               buck-out/gen/fe3a39b8/xplat/caffe2/aten_cpuAndroid#android-x86,compile-pic-BatchLinearAlgebraKernel.cpp.o93aa6b34/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp.o:(at::native::(anonymous namespace)::unpack_pivots_cpu_kernel(at::TensorIterator&, long long))
clang: error: linker command failed with exit code 1 (use -v to see invocation)

    When running <c++ link>.
    When building rule //xplat/caffe2:aten_cpuAndroid#android-x86,shared (ovr_config//platform/android:fbsource-base).""
```

Test Plan: CI

Reviewed By: dreiss, cccclai

Differential Revision: D36215453

fbshipit-source-id: 5f9c7cab742bb87a70b5acda46ef85817e50575c
(cherry picked from commit a1691c34f6bae484f710ac9321bfd8a8c999189e)
",['tools/build_variables.bzl'],BatchLinearAlgebraKernel.cpp being part of aten_cpu_source_non_codegen_list is causing linker errors due to an undefined TensorIteratorBase::for_each symbol.
16f30b494c43d9c4742caf62a18d282a59e0f2e7,1655752254,"Make l1_loss composite

Fixing the forward AD for `sgn` in the next PR of this stack uncovered a
number of issues with the derivatives of `l1_loss`. Upon inspection,
`l1_loss` was just implemented as a composite function, but it was not
differentiable. This PR makes it a fully differentiable function.

As a side note, `l1_loss_out` was incorrect in a number of ways. Even
more, it is not exposed to the public as `F.l1_loss` does not accept an
`out=` parameter. As such it is not even tested. I wonder how useful is
to have `out=` variants for loss functions if we don't expose them at
all. Even more, I wonder how useful is to have `_out` variants  for loss
functions, given that their most normal use case is to return just a
real number cc jbschlosser

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79804

Approved by: https://github.com/zou3519, https://github.com/malfet
","['aten/src/ATen/native/Loss.cpp', 'test/forward_backward_compatibility/check_forward_backward_compatibility.py', 'tools/autograd/gen_variable_type.py', 'torch/_decomp/decompositions.py', 'torch/csrc/autograd/FunctionsManual.cpp', 'torch/csrc/autograd/FunctionsManual.h', 'torch/csrc/jit/runtime/static/generated_ops.cpp', 'torch/csrc/lazy/core/shape_inference.cpp', 'torch/csrc/lazy/core/shape_inference.h', 'torch/testing/_internal/common_methods_invocations.py']","`l1_loss` function is not differentiable, and the implementation of `l1_loss_out` has a number of inaccuracies and is not exposed or tested for availability."
9ad0de3c6f4459ded04004e78e335f633cf05e92,1623692251,"Rework requires_grad on DifferentiableGraphOp (#57575)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57575

This PR does two things:

1. reverts ""Manual revert of D27369251 (https://github.com/pytorch/pytorch/commit/f88a3fff65b35cb6d4968fc54a9a0a1314a9a3b7) (#56080)"" in commit
   92a09fb87a567100122b872613344d3a422abc9f.

2. fixing DifferentiableGraph output with wrong requires_grad flag

Fixing requires_grad on outputs from DifferentiableGraph, the proper flag is
retrieved from profiling information. We previously only retrieves the profiling
information on the first profile node in all its uses. However, in case where
control flows are present, we need to iteratively search for profile node with
profiling information available, in case the first use is in an inactive code
path.

e.g.
```
  graph(%0 : Tensor,
        %1 : Bool):
  ..., %2 : Tensor = prim::DifferentiableGraph_0(%0)
  %3 : Tensor = prim::If(%1)
    block0():
      %4 : Tensor = prim::DifferentiableGraph_1(%2)
      -> (%4)
    block1():
      %5 : Tensor = prim::DifferentiableGraph_2(%2)
      -> (%5)
  -> (%3)
with prim::DifferentiableGraph_0 = graph(%0 : Tensor):
  ...
  %out : Tensor = aten::operation(...)
  ...
  return (..., %out)
with prim::DifferentiableGraph_1 = graph(%0 : Tensor):
  %temp : Tensor = prim::profile[profiled_type=Tensor](%0)
  ...
with prim::DifferentiableGraph_2 = graph(%0 : Tensor):
  %temp : Tensor = prim::profile[profiled_type=Float(...)](%0)
  ...
```

Test Plan: Imported from OSS

Reviewed By: bdhirsh

Differential Revision: D29038773

Pulled By: Krovatkin

fbshipit-source-id: 6c0a851119f6b8f2f1afae5c74532407aae238fe
","['test/jit/test_autodiff_subgraph_slicing.py', 'torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp']","The requires_grad flag on outputs from DifferentiableGraph is incorrect, with potential issues when control flows are present and the first use is in an inactive code path."
7e2becb70f54b6e3f7e97c268d8c9d9692dcbc1f,1613606865,"[PyTorch] Reduce copy/move in c10::ivalue::from (#52324)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52324

`c10::ivalue::from` took its parameter by value. `List` has
an expensive move ctor (it has to copy the type shared_ptr) and dtor
(it has to decref the type, which isn't null), so it's better to avoid
intermediate List objects in function parameters.
ghstack-source-id: 121807292

Test Plan:
Profiled AdIndexer benchmark; time spent in push_outputs is
down from 0.5% to 0.23%.
Comparing assembly for
`c10::impl::push_outputs<c10::List<at::Tensor>, false>::call`, we went
from 4 List move ctor calls and 5 ~intrusive_ptr calls to 2 move ctor
calls and 3 dtor calls, respectively.

Reviewed By: bhosmer

Differential Revision: D26471093

fbshipit-source-id: 7b2c5e8d391a428f2b4d895717a43123c8d7a054
",['aten/src/ATen/core/ivalue_inl.h'],"Intermediate List objects in c10::ivalue::from function parameters causing unnecessary copy/move operations, making `List` move constructor and destructor expensive."
aa562f94b3440e258fec745b2e8355d1ce1eb37f,1672885819,"[vulkan] Remove dependencies from op/ in vTensor and move it to higher level namespace (#91023)

Small refactor to remove any code used by vTensor under the `op/` folder to appropriate locations in the `api/` folder. Also remove vTensor from the `ops` namespace, it now resides in the higher level `at::native::vulkan` namespace which will also be used for the Graph data structures in the future.

This is the last step required for vTensor to be able to moved to the api folder.

Differential Revision: [D42052680](https://our.internmc.facebook.com/intern/diff/D42052680/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91023
Approved by: https://github.com/salilsdesai
","['aten/src/ATen/native/vulkan/api/Utils.h', 'aten/src/ATen/native/vulkan/ops/Common.cpp', 'aten/src/ATen/native/vulkan/ops/Common.h', 'aten/src/ATen/native/vulkan/ops/Copy.cpp', 'aten/src/ATen/native/vulkan/ops/Copy.h', 'aten/src/ATen/native/vulkan/ops/Tensor.cpp', 'aten/src/ATen/native/vulkan/ops/Tensor.h', 'aten/src/ATen/test/vulkan_quantized_api_test.cpp']","vTensor utilization in `op/` folder and under `ops` namespace causing issues for future Graph data structure implementation, and preventing vTensor's transition to `api` folder.
"
a37fbf9b45a52f141c15fc230c7168079c9557ac,1617229026,"[Futures] Bump log verbosity when ignoring cb errors in python future. (#54476)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54476

Per title. For `add_done_callback`, we log but swallow exceptions in order to keep consistent with what concurrent.futures python library does, see discussion in https://github.com/pytorch/pytorch/pull/45675.

Although, it would be good to improve the verbosity here as this can be a source of confusion if users are setting a different future via `add_done_callback`, and an error is hit resulting in an unexpected hang (see https://github.com/pytorch/pytorch/issues/52132 for more details on how this can happen).
ghstack-source-id: 125300389

Test Plan: CI

Reviewed By: lw

Differential Revision: D27253004

fbshipit-source-id: 72ed21c8fb6d27de5797c17fc46b762f893e6fea
",['torch/csrc/jit/python/pybind_utils.h'],"Errors in 'add_done_callback' resulting in unexpected hangs can be a source of confusion especially when users are setting a different future, due to the low verbosity of error logging."
1896d801913fe156f46b0b65f4b1e38f314210b3,1662394309,"[PyTorch][Profiler] Increase max number of elements to record in execution graph (#84285)

Summary: Noticed some jobs are exceeding the max num of elements in an array. 100 was too conservative (observed 128 sizes in CMF model), but we also don't want have unbounded container size. Setting to a large number 4096 that probably will catch extreme cases.

Test Plan:
```
buck build mode/opt-split-dwarf //hpc/models/ads:ads_10x_launcher --show-output

buck-out/gen/hpc/models/ads/ads_10x_launcher.par +checkpoint=model_store +launcher=mast +data_loader=dist +mode=mast launcher.data_project=ads_model_platform launcher.fbl_entitlement=ads_global_qps checkpoint.model_type=ctr_mobile_feed_model data_loader.table_ds=[""2022-08-15""] data_loader.num_batches=5000 profiling_trace=true
```

Differential Revision: D39137530

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84285
Approved by: https://github.com/robieta
",['torch/csrc/profiler/execution_graph_observer.cpp'],The current maximum limit for elements in the execution graph in PyTorch Profiler is too small and causing jobs with large datasets to fail.
ac3effd150e829da8bac7525adadec2fad31ece1,1647399233,"[PyTorch GPU Allocator] Better use of blocks with rounding of allocation sizes (#74213)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74213

In the current CUDACachingAllocator, the sizes are rounded up in multiple of blocks size of 512, so this works for smaller sizes. However for large sizes, we can have lots of different size blocks in the larger pool. This is problematic when we have variable batch sizes 1001, 1021, 1023 -> all will go to different block size and will create different size of blocks. This will create lots of unused blocks and will waste GPU memory capacity.

This diff adds a rounding approach to allocation size. It rounds up the size to nearest power-of-2 divisions and the power2-division can be changed with env variable setting.

   For example, if we need to round-up  size of1200 and if number of divisions is 4,
   the size 1200 lies between 1024 and 2048 and if we do 4 divisions between
   them, the values are 1024, 1280, 1536, and 1792. So the function will
   return 1280 as the nearest ceiling of power-2 division.

env setting:
   export PYTORCH_CUDA_ALLOC_CONF=roundup_power2_divisions:4
ghstack-source-id: 151446017

Reviewed By: ezyang

Differential Revision: D34868036

fbshipit-source-id: 494785add16e6b37c920dcb5a2b81d4c637b554a
(cherry picked from commit 548454ccacbd8700e7ffd2d762e40b4ba37abbae)
",['c10/cuda/CUDACachingAllocator.cpp'],"The existing CUDACachingAllocator in PyTorch GPU Allocator inefficiently handles variable batch sizes, leading different batch sizes to go to different block sizes. This results in a lot of unused blocks and wasted GPU memory capacity."
3b977a0d2834d300c0301a0c6af98c8e939019ce,1619536442,"[DataLoader] Add `generate_state` for NumPy seeding (#56797)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56797

After adding default seeding strategy for NumPy random module within each worker of DataLoader #56488, two concerns are raised:
- We dropped the support for NumPy < 1.17 due to `SeedSequence`
- In order to support seeding for NumPy < 1.17, how can we provide seed for `numpy.random`?
  - First option is set the same seed as `random`. But, the problem is a same algorithm is shared between `numpy.random` and `random`. With the same seed, they will have exact same state sequence. Thanks to rkern, we noticed this so-called [bad things](https://github.com/PyTorchLightning/pytorch-lightning/pull/6960#issuecomment-818393659).
  - Considering most of users do not aware this problem, we can provide a better seed by default for `numpy.random` using same `SeedSequence` algorithm as numpy. This is just a workaround with hard-coded function to generate an array of four int32 as the seed.

To better coping with this problem since there are amount of 3rd party libraries not just `NumPy` having random module. We may at the end need to implement a `SeedSequence` within `torch.random` module, then users can `spawn` a new `SeedSequence` for each library.

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D28000619

Pulled By: ejguan

fbshipit-source-id: 5701c8124a38ea5ded69eb8eee70f9680877ffa6
","['test/test_dataloader.py', 'torch/utils/data/_utils/worker.py']","The current numpy random seed generation strategy in DataLoader is creating identical state sequences with numpy.random and random, and lacks support for numpy versions under 1.17."
14c28caed9b3b8109afb4622c8772e489887429a,1657672542,"[JIT] Add determinism checks for ops in SchemaInfo subclass (#81000)

- Added is_non_deterministic which returns whether a given op is non-deterministic. Currently this is implemented with a hard-coded list of non-deterministic functions copied from ir.cpp in AliasDB, but this will eventually be implemented by returning with a given schema has the non_deterministic tag.
- Tested is_non_deterministic method with a deterministic op and a non deterministic op in test_schema_info.cpp

**Note that the case for op ""aten::dropout(Tensor input, float p, bool train) -> Tensor"" which is deterministic whenever ""train=false"" is not accounted for in this pr and will be fixed in a later pr. Currently ""aten::dropout(Tensor input, float p, bool train) -> Tensor"" is always considered nondeterministic.**
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81000
Approved by: https://github.com/davidberard98
","['test/cpp/jit/test_schema_info.cpp', 'torch/csrc/utils/schema_info.cpp', 'torch/csrc/utils/schema_info.h']","Deterministic checks for ops in SchemaInfo subclass are missing, leading to inability to verify whether a given operation is deterministic or not. Furthermore, the specific case for op ""aten::dropout"" is not correctly accounted for based on the ""train"" parameter."
bd0f0f40a11bf3ca4ddf55191c767e3a29aba4bb,1688188470,"[PT2][Quant] Enable symbolic shape in linear quantization (#104473)

When tracing with symbolic shapes, arbitrary sym_size nodes can appear in the
graph. Earlier changes did not account for this and quantizer fails to annotate
the right nodes. This diff fixes that by not annotating sym_size nodes, which
should really not be relevant for quantization.

As next steps, we should validate in quant workflow that a) sym_int nodes are not
being quantized and b) add similar support, as this diff, for generic
annotations

Differential Revision: [D47132050](https://our.internmc.facebook.com/intern/diff/D47132050/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/104473
Approved by: https://github.com/jerryzh168
","['test/quantization/pt2e/test_quantize_pt2e.py', 'torch/ao/quantization/_pt2e/quantizer/qnnpack_quantizer.py', 'torch/ao/quantization/_pt2e/quantizer/utils.py']","When tracing with symbolic shapes, quantizer fails to properly annotate nodes due to unaccounted sym_size nodes, affecting the overall quantization process."
ce0786add26c1e117b16b58e8ae12dbe776133e1,1657111450,"fx quant: fix warning in util function when cloning tensors (#80883)

Summary:

Some of the util functions in FX graph mode quantization throw warnings
such as:

```
/Users/vasiliy/pytorch/torch/ao/quantization/fx/utils.py:410: UserWarning: To copy construct from
a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().
requires_grad_(True), rather than torch.tensor(sourceTensor).
```

This PR fixes the warnings by moving the code to the recommended syntax if the
value is a tensor.

Test plan:

```
python test/test_quantization.py -k test_conv_linear_reference
// warning appeared before this PR and disappeared after this PR
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80883
Approved by: https://github.com/jerryzh168
","['torch/ao/quantization/fx/utils.py', 'torch/nn/quantized/_reference/modules/rnn.py', 'torch/nn/quantized/_reference/modules/utils.py']",Util functions in FX graph mode quantization are generating user warnings when attempting to copy construct from a tensor with the current syntax.
d79aec91f759acb00d9af4bf92f57ec752dd65b7,1645030723,"[easy][PTE] Reduce unnecessary ref count bumps in callstack debug (#72547)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72547

toTuple() returns a  new intrusive pointer that bumps its underlying ref count. Whereas, toTupeRef returns a reference. We can save an unnecessary ref count bump.

Based on https://fb.workplace.com/groups/pytorch.edge.team/permalink/1021780808376658/

similar to D34047666 (https://github.com/pytorch/pytorch/commit/85d7e73a8aa2dd74970017d11c7411b36b89dfc4)
ghstack-source-id: 148665193

Test Plan:
```
> Executing task: buck: buck test //xplat/caffe2:test_lite_interpreter  --config client.id=nuclide <

Executing in directory: /data/users/pavithran/fbsource
buck test //xplat/caffe2:test_lite_interpreter  --config client.id=nuclide

clang-9: warning: argument unused during compilation: '-pthread' [-Wunused-command-line-argument]

Parsing buck files: finished in 2.1 sec
Creating action graph: finished in 0.5 sec
[RE] Metadata: Session ID=[reSessionID-66858379-0761-4966-a933-bc7f0d0add95]
[RE] Waiting on 0 remote actions. Completed 523 actions remotely, action cache hit rate: 0.00%.
Downloaded 3947/5089 artifacts, 20.92 Mbytes, 12.5% cache miss (for updated rules)
Building: finished in 01:04.0 min (100%) 5438/5438 jobs, 5192/5438 updated
  Total time: 01:06.6 min
Testing: finished in 06:53.7 min (71 PASS/0 FAIL)
BUILD SUCCEEDED
RESULTS FOR //xplat/caffe2:test_lite_interpreter
PASS    406.0s 71 Passed   0 Skipped   0 Failed   //xplat/caffe2:test_lite_interpreter
TESTS PASSED

Terminal will be reused by tasks, press any key to close it.
```

Reviewed By: kimishpatel

Differential Revision: D34082609

fbshipit-source-id: 4bcbdb2d11dd4c3bc392010487dccd2270278222
(cherry picked from commit dd64eb386d02335e566fb6496f2ff00a8879ccc3)
",['torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp'],Unnecessary ref count bumps occurring in callstack debug due to toTuple() returning a new intrusive pointer each time.
d96ef8c1b1860185f0bd91699f71a087cf9e9efe,1629769541,"[Static Runtime] SR clones graph input (#63704)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63704

Previously SR did not clone the graph. This was leading to subtle bugs in `testStaticRuntime`; static runtime would modify its graph, and the graph used by the JIT interpreter would change as well. The JIT interpreter would then crash if SR-only ops were added!

Cloning the graph is more consistent with the behavior of the `Module` ctor.

Test Plan: `buck test caffe2/benchmarks/static_runtime/...`

Reviewed By: hlu1

Differential Revision: D30463294

fbshipit-source-id: b771551a1f55f95fde79373b23babcf3e5ddf726
",['torch/csrc/jit/runtime/static/impl.cpp'],Static Runtime's previous approach of not cloning the graph leads to subtle bugs and crashes when SR-specific operations are added due to changes in the JIT interpreter's graph.
35090b869d38d123dc520b65b54cd9d0747e6e30,1680202694,"set num_warps to at least 4 (#97950)

To avoid IMAs in https://gist.github.com/ngimel/25e81c996d9c8c652d97e33cc9c7d5f4
This is not a general fix (e.g. if inputs were a bit larger, num_warps would naturally be 4, and we could still have spills and hit ptxas bugs), but will do for now.
Longer term, we should check spills in kernels we generate and recompile with more warps if there are spills.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97950
Approved by: https://github.com/bertmaher
",['torch/_inductor/triton_heuristics.py'],Small inputs cause num_warps to be less than 4 leading to potential register spills and ptxas bugs in the generated kernels.
6f655d4195132901ae4c37d232296114a597b322,1686864490,"Add symbolic tracing support to torch._dynamo.export (fake input + weights) (#100017)

Fixes #95900
Using the following repro as guide:

```python
import torch
import torch._dynamo
from torch._subclasses import fake_tensor
from torch.fx.experimental.symbolic_shapes import ShapeEnv
from torch._dynamo.output_graph import config
class Model(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.linear = torch.nn.Linear(2, 2)
        self.linear2 = torch.nn.Linear(2, 2)

    def forward(self, x):
        out = self.linear(x)
        out = self.linear2(out)
        return out

fake_mode = fake_tensor.FakeTensorMode(allow_non_fake_inputs=False,
                                       allow_fallback_kernels=True,
                                       shape_env=ShapeEnv(
                                            allow_scalar_outputs=config.capture_scalar_outputs,
                                            allow_dynamic_output_shape_ops=config.capture_dynamic_output_shape_ops,
                                            frame_id=0
                                        ),
)
# Fakefying input/model before calling torch._dynamo.export
with fake_mode:
    fake_x = torch.rand(5, 2, 2)
    model = Model()

# Calling torch._dynamo.export without active fake mode
graph_module, guards = torch._dynamo.export(
    model,
    fake_x,
    aten_graph=True,
    fake_mode=fake_mode
)
graph_module.print_readable()
graph_module.graph.print_tabular()
```

Summary of changes:

    * Plumb fake_mode through torch.export API. When specified, it
    replaces the creation of a new FaketendorMode at InstructionTranslator on behalf of OutputGraph
     Hacks FakeTensor.__new__ to prevent a
    torch.tensor._make_subclass call for inputs that are already fakefied by
    user. This probably need to be fixed in a nicer way. Any idea?
    * Removed a few asserts that didn't want faked tensors coming
    from user script
    * Added torch._subclasses.fake_tensor.FakeTensor to type list on a few
    asserts check to allow fake inputs

The changes above allowed symbolic tracing with both static and dynamic shapes.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100017
Approved by: https://github.com/ezyang
","['test/dynamo/test_export.py', 'torch/_dynamo/eval_frame.py', 'torch/_dynamo/output_graph.py', 'torch/_dynamo/utils.py', 'torch/_dynamo/variables/builder.py', 'torch/_functorch/aot_autograd.py', 'torch/_guards.py', 'torch/_subclasses/fake_tensor.py', 'torch/fx/experimental/proxy_tensor.py']","Symbolic tracing in torch._dynamo.export does not support fake inputs and weights, leading to failed asserts when a user attempts to pass in faked tensors."
e1551f16786e9898a8d7381dc43272dc21ccbfed,1621367400,"Clarify .github/scripts/generate_ci_workflows.py (#58498)

Summary:
Followup to https://github.com/pytorch/pytorch/issues/58491:

- use f-string to remove the literal `generated` string from the generator script, so Phabricator no longer thinks it is a generated file
- remove the special logic for `test_runner_type` and instead explicitly specify for every workflow

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58498

Test Plan:
```
make generate-gha-workflows
```
Also, check that Phabricator doesn't classify `.github/scripts/generate_ci_workflows.py` as ""Generated changes"" in this diff.

Reviewed By: seemethere

Differential Revision: D28516291

Pulled By: samestep

fbshipit-source-id: 8736eaad5d28082490be0a9b2e271c9493c2ba9d
",['.github/scripts/generate_ci_workflows.py'],"Phabricator erroneously classifies `.github/scripts/generate_ci_workflows.py` as a generated file, and the handling of `test_runner_type` is not explicitly specified for every workflow."
afa1ff8e04dad60dee9f979451a36713903f521d,1615584355,"Implements `torch.linalg.lstsq` (#49093)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/44378 by providing a wider range of drivers similar to what SciPy is doing.

The supported CPU drivers are `gels, gelsy, gelsd, gelss`.
The CUDA interface has only `gels` implemented but only for overdetermined systems.

The current state of this PR:
- [x] CPU interface
- [x] CUDA interface
- [x] CPU tests
- [x] CUDA tests
- [x] Memory-efficient batch-wise iteration with broadcasting which fixes https://github.com/pytorch/pytorch/issues/49252
- [x] docs

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49093

Reviewed By: albanD

Differential Revision: D26991788

Pulled By: mruberry

fbshipit-source-id: 8af9ada979240b255402f55210c0af1cba6a0a3c
","['aten/src/ATen/native/BatchLinearAlgebra.cpp', 'aten/src/ATen/native/LinearAlgebraUtils.h', 'aten/src/ATen/native/cuda/BatchLinearAlgebra.cu', 'test/test_linalg.py', 'test/test_namedtuple_return_api.py', 'torch/csrc/api/include/torch/linalg.h', 'torch/csrc/autograd/utils/wrap_outputs.h', 'torch/linalg/__init__.py', 'torch/overrides.py', 'torch/testing/_internal/common_methods_invocations.py', 'torch/testing/_internal/common_utils.py']","Limited range of drivers in Pytorch's `torch.linalg.lstsq`, preventing it from paralleling SciPy's functionality, and inefficient memory use in batch-wise iteration with broadcasting."
bc93454e4a51f85f6c8603309bed7bd6de84385a,1670456313,"correctly set strides for expanded/unsqueezed dimensions (#90341)

Fixes https://github.com/pytorch/torchdynamo/issues/1959, #90260
However, I wasn't able to make existing stride tests fail before the fix, even though I'm comparing all, not just significant strides.
Separately running refs on meta tensors produces wrong strides as shown in #90260, however, it looks like in meta tests some other way of computing meta info is used (I've been running
```
pytest -s -v test/test_meta.py -k test_meta_outplace_expand_cuda_float64
```
and verified that it has sample input that should fail, and that it indeed compares all the strides, but the produced `meta_rs` results somehow still had correct strides).

Edit: @SherlockNoMad helped me figure out how to fail the tests, and now I've set the correct ops for checking. `expand` fails for some test inputs because it special-cases 0-dim input case, correctly modeling it in prims would require a lot of changes, so skipping that for now.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90341
Approved by: https://github.com/SherlockNoMad
","['test/test_meta.py', 'torch/_prims/__init__.py', 'torch/_prims_common/__init__.py']","Strides for expanded/unsqueezed dimensions are not being set correctly, causing failures in certain test inputs and incorrect strides in meta tensor outputs."
d74bb42f7ad347181f5ac60d27889739c5a79bf4,1642197312,"Add a missing precondition to `DistributedSampler` docstring (#70104)

Summary:
Distributed sampler sets different indices for different processes. By doing this, it assumes that the data is the same across the board and in the same order. This may seem trivial, however, there are times that users don't guarantee the order items are gonna have, because they rely on something such as the order the filesystem lists a directory (which is not guaranteed and may vary on different computers), or the order a `set` is iterated.

I think it's better to make it clearer.

cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan-varma gqchen aazzolini osalpekar jiayisuse SciPioneer H-Huang

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70104

Reviewed By: bdhirsh

Differential Revision: D33569539

Pulled By: rohan-varma

fbshipit-source-id: 68ff028cb360cadaee8c441256c1b027a57c7089
",['torch/utils/data/distributed.py'],"`DistributedSampler` assumes data is consistent and in same order across all processes, however, variable order from filesystem listing or set iteration is not accounted for, leading to potential indexing issues."
53c0d91db9fe1bebb6d8fadff9dc7f26f16b6c7c,1633098477,"Make autograd codegen for differentiable outputs safer to use (#65823)

Summary:
This PR adds raising an error when `len(output_differentiability) != len(outputs)`

Notes in derivatives.yml tell that
> 'output_differentiability' and value a list of the same length as the number of outputs from the forward function.

but it was not enforced in codegen leading to confusion and unexpected bugs https://github.com/pytorch/pytorch/issues/65061#issuecomment-930271126.

cc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65823

Reviewed By: mrshenli

Differential Revision: D31307312

Pulled By: albanD

fbshipit-source-id: caeb949e9249310dffd237e77871e6d0d784e298
",['tools/codegen/api/autograd.py'],"Autograd codegen doesn't enforce equal lengths between 'output_differentiability' and 'outputs', leading to potential confusion and unexpected bugs."
f0e972a481b51ddf9d21d3ba8e8ef0ec2e15cd8a,1624838441,"To add Nesterov Adam algorithm for multi-tensor optimizers API (#59165)

Summary:
Previously in the PR: https://github.com/pytorch/pytorch/issues/59009 we added NAdam to Optimizers.  Here in this PR we are proposing multi-tensor version of NAdam for PyTorch.

Nadam has been proposed in the paper   https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ and report  and report : http://cs229.stanford.edu/proj2015/054_report.pdf by Timothy Dozat.

It has been one of the most used algorithm in Deep Learning community.

It worth to noting that the implementation of NAdam is inspired by the implementation for Keras :
https://github.com/tensorflow/tensorflow/blob/f9d386849581d15d72f6f1f96f12aac230a8edbe/tensorflow/python/keras/optimizer_v2/nadam.py

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59165

Reviewed By: vincentqb

Differential Revision: D29360577

Pulled By: iramazanli

fbshipit-source-id: 0fe14016303b2df2cb8cc31912a2674acf63d1e5
","['test/test_optim.py', 'torch/optim/_multi_tensor/__init__.py', 'torch/optim/_multi_tensor/_functional.py', 'torch/optim/_multi_tensor/nadam.py']","The current optimizer algorithms in PyTorch do not include a multi-tensor version of the widely used NAdam algorithm, limiting performance efficiency in deep learning applications."
620dbc43d8e3c836ad8d934987ee2f87fefbad7a,1666378980,"Slowly introduce ops to be tested by test_numpy_ref on MPS backend (#87342)

Enable a test that would have caught https://github.com/pytorch/pytorch/issues/86239

Prior to the fix for that bug, this test fails with

```
_____________________________ TestCommonMPS.test_numpy_ref_mps_where_mps_float32 _____________________________
Traceback (most recent call last):
  File ""/Users/alex/git/pytorch/test/test_ops.py"", line 197, in test_numpy_ref_mps
    self.compare_with_reference(
  File ""/Users/alex/git/pytorch/torch/testing/_internal/common_utils.py"", line 2366, in compare_with_reference
    actual = torch_fn(t_inp, *t_args, **t_kwargs)
  File ""/Users/alex/git/pytorch/torch/testing/_internal/opinfo/core.py"", line 1068, in __call__
    return self.op(*args, **kwargs)
  File ""/Users/alex/git/pytorch/torch/testing/_internal/common_methods_invocations.py"", line 15167, in <lambda>
    op=lambda self, condition, other: torch.where(condition, self, other),
RuntimeError: 0'th index 3 of x tensor does not match the other tensors
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87342
Approved by: https://github.com/albanD
","['test/test_mps.py', 'torch/testing/_internal/common_device_type.py', 'torch/testing/_internal/common_methods_invocations.py', 'torch/testing/_internal/opinfo/definitions/linalg.py', 'torch/testing/_internal/opinfo/definitions/signal.py']","Issue with the MPS backend executing the 'test_numpy_ref_mps_where_mps_float32' test. The 0th index of x tensor isn't matching, causing a RuntimeError."
767a104698166090713d4f1c5bec9bdd4544fec1,1632413290,"[quant] change observer FQNs generated in prepare step (#65420)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65420

Context: In some FB use cases we have a need to map observer stats from train model checkpoint to inference model. We observerd that some buffer names are different becuase the intermediate activation tensors
are generated differently across train and inference model. More details in https://fb.quip.com/PtGcAR0S5CQP

Currently, for each observer (activation_post_process), the FQN of the module inserted is determined based on the FQN of the input tensor it is observing.

In this change we change the observer FQN to include the FQN of the op/module it is observing rather than tensor/intermediate op names along with the “input”/“output” detail.

Before
```
def forward(self, x):
    x_activation_post_process_0 = self.x_activation_post_process_0(x);  x = None
    mods1_w = self.mods1.w
    mods1_w_activation_post_process_0 = self.mods1_w_activation_post_process_0(mods1_w);  mods1_w = None
    mods1_b = self.mods1.b
    linear = torch.nn.functional.linear(x_activation_post_process_0, mods1_w_activation_post_process_0, bias = mods1_b);  x_activation_post_process_0 = mods1_w_activation_post_process_0 = mods1_b = None
    linear_activation_post_process_0 = self.linear_activation_post_process_0(linear);  linear = None
    return linear_activation_post_process_0
```

After
```
def forward(self, x):
    mods1_input_activation_post_process_0 = self.mods1_input_activation_post_process_0(x);  x = None
    mods1_w = self.mods1.w
    mods1_w_activation_post_process_0 = self.mods1_w_activation_post_process_0(mods1_w);  mods1_w = None
    mods1_b = self.mods1.b
    linear = torch.nn.functional.linear(mods1_input_activation_post_process_0, mods1_w_activation_post_process_0, bias = mods1_b);  x_activation_post_process_0 = mods1_w_activation_post_process_0 = mods1_b = None
    mods1_output_activation_post_process_0 = self.mods1_output_activation_post_process_0(linear);  linear = None
    return mods1_output_activation_post_process_0
```

Test Plan:
python test/test_quantization.py test_observer_fqn

Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D31088652

fbshipit-source-id: 2f1526f578a13000b34cfd30d11f16f402fd3447
","['test/quantization/fx/test_equalize_fx.py', 'test/quantization/fx/test_quantize_fx.py', 'torch/ao/quantization/fx/prepare.py']","Discrepancy in buffer names across train and inference model, as the Fully Qualified Name (FQN) for an observer is based on the input tensor its observing rather than the operation or module it is observing."
f012d0ea5b86d2511fd6095973a029452d215863,1672261227,"[autograd.Function] enable the extended Function feature flag by default (#91441)

The autograd.Function <> functorch interaction is in a mostly completed
state now. There are some minor action items remaining
(https://github.com/pytorch/pytorch/issues/90224), but I want to enable
the feature by default so that PyTorch CI / other parties / etc can
begin testing to see if there is any impact on the original
autograd.Function API (there shouldn't be).

The longer-term plan for the feature flag is:
- keep it around until at least the next release (so that people can
turn off the feature if it breaks something in existing code)
- delete the flag then (either before or after the release, I haven't
decided yet)

Test Plan:
- new test
- wait for CI
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91441
Approved by: https://github.com/albanD, https://github.com/soulitzer
","['test/test_autograd.py', 'torch/csrc/autograd/function.cpp']","The interaction between autograd.Function and functorch may potentially impact the original autograd.Function API, needs widespread testing to verify no breakages in existing code."
9477211e7d609ce382c0e22d7721c14c36d083de,1629329370,"Hoisting common expressions out of If blocks (#59492)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59492

Adding code to find common expressions from the two subblocks of an if
operation and hoist them before the if block.
This also allows Dead Code Elimination to
then eliminate some if blocks.

Also eliminated some dead code in the codebase.

Test Plan:
python test_jit.py TestIfHoisting

Imported from OSS

Reviewed By: ngimel

Differential Revision: D29399533

fbshipit-source-id: 9336b9dc48c02c38862f98f98cd72fc1767a1802
","['test/jit/test_if_hoisting.py', 'test/quantization/jit/test_quantize_jit.py', 'test/test_jit.py', 'tools/build_variables.bzl', 'torch/csrc/jit/ir/node_hashing.cpp', 'torch/csrc/jit/passes/common_expression_hoisting.cpp', 'torch/csrc/jit/passes/common_expression_hoisting.h', 'torch/csrc/jit/passes/symbolic_shape_analysis.cpp', 'torch/csrc/jit/python/init.cpp', 'torch/csrc/jit/runtime/graph_executor.cpp', 'torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp']","Common expressions within the subblocks of an 'If' operation aren't identified and hoisted, hindering Dead Code Elimination and resulting in unnecessary if blocks."
3fe2b73416d13dd746486bb3c8fba089b852d24f,1689210839,"Update use_mkldnn in LSTM op to avoid input and parameter not in the same device (#102050)

This PR is to fix https://github.com/pytorch/pytorch/issues/101935.

Only when input, parameters and hidden states are all in CPU device, LSTM will go into oneDNN fast path implementation. Otherwise, it will fallback to the original implmentation.

Note here, if input and parameters are indeed not in the same device, it will encounter Error `Input and parameter tensors are not at the same device, found input tensor......` in `check_attributes`. Therefore, the proper usage of LSTM is `input.to(device)` and `model.to(device)` together.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102050
Approved by: https://github.com/XiaobingSuper, https://github.com/albanD
","['aten/src/ATen/native/RNN.cpp', 'test/test_nn.py']","When input and parameters aren't in the same device, LSTM operation in PyTorch generates an error ""Input and parameter tensors are not at the same device""."
36f52cc099e925a72612c5df23d72ee782658ed5,1681433220,"[BuildSpeed] Limit `Logcumsumexp` complex to OSS builds only (#98957)

As it takes ridiculous amount of time to build with complex times on CUDA-11.4.

Build speeds for a single gpu architecture (`sm_80`) on 3Ghz 8275CL Intel Xeon:
- 143 sec to compile for all dtypes using CUDA-11.6
- 351 sec to compile for all dtypes using CUDA-11.4
- 24 sec to compile for only floating dtypes using CUDA-11.6
- 52 sec to compile for only floating dtypes using CUDA-11.4

Tweak code a bit to make it compilable with MSVC, which is having trouble with nested preprocessor directives.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98957
Approved by: https://github.com/r-barnes, https://github.com/ngimel
",['aten/src/ATen/native/cuda/LogcumsumexpKernel.cu'],Building with complex times on CUDA-11.4 significantly increases compile time; MSVC also has compatibility issues with nested preprocessor directives.
e69a1398cbe534874060460faf36af21d24ce6e7,1629859161,"compute reduction intermediate buffer size in elements (#63885)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/63869
`iter` strides are in bytes, and we are additionally multiplying size computed using those strides by `sizeof(arg_t)`. Computing `output_memory_size` in elements should be enough.
This doesn't fix the still real problem of allocating large intermediate tensor, but it makes this tensor smaller by typically a factor of 4.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63885

Reviewed By: mruberry

Differential Revision: D30526034

Pulled By: ngimel

fbshipit-source-id: 0aca7f887974b7776e380463bbd82d32a5786ee8
",['aten/src/ATen/native/cuda/Reduce.cuh'],Reduction intermediate buffer size is being computed incorrectly causing unnecessary allocation of larger tensors.
a8c0b362ce2583030b5d3f92455dd823537a771b,1633626326,"[pytorch][PR] Add hash and int128 utils for Lazy Tensor Core"" (#66181)

Summary:
These utils are prerequisites for Lazy Node base class.
- set up new torch/csrc/lazy, test/cpp/lazy dirs
- add source files to build_variables.bzl in new lazy_core_sources var
- create new test_lazy binary

Fixes https://github.com/pytorch/pytorch/issues/65636

Pull Request resolved: https://github.com/pytorch/pytorch/pull/66181

Original commit changeset: 3d0d5377d71e

Test Plan:
Run PyTorch XLA corresponding PR in XLA CI:
https://github.com/pytorch/xla/pull/3148/files

Reviewed By: suo

Differential Revision: D31416438

fbshipit-source-id: 58a6a49c5bc30134bc6bae2e42778f359b9a8f40
","['c10/util/int128.cpp', 'c10/util/int128.h', 'test/cpp/lazy/test_misc.cpp', 'tools/build_variables.bzl', 'torch/csrc/lazy/core/hash.cpp', 'torch/csrc/lazy/core/hash.h']",Lack of hash and int128 utilities in Lazy Tensor Core inhibiting the setup of Lazy Node base class.
fa1aa836ce9b240f532cbe4ef609e3d54485c596,1654645319,"Copy rollbear/strong_type to `c10/util`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78162

There are a lot of cases in the profiler (and I suspect elsewhere in the codebase) where we just sling around ints and hope for the best. I've hit a bunch of bugs while refactoring the profiler that amount to ""I fat fingered it, but it still compiles because an int is an int"".

Options considered:
 1) BOOST_STRONG_TYPEDEF
    This was my initial plan, but requires tweaking to remove boost deps and prevent runtime overhead. (https://godbolt.org/z/oKs18Y7a8)

 2) https://github.com/foonathan/type_safe
    Seems cool and well regarded, but is also large and would be a pain to pull in.

 3) https://github.com/rollbear/strong_type
    Single header and very easy to configure.

(3) Seems to be the best fit, and I've found it really pleasant to work with when refactoring the python profiler.

Differential Revision: [D36364595](https://our.internmc.facebook.com/intern/diff/D36364595/)

Approved by: https://github.com/aaronenyeshi
",['c10/util/strong_type.h'],"Issues in the PyTorch profiler and possibly other parts of the codebase due to loose handling of integers leading to bugs, needing a method to ensure type safety."
93d75568c7070942a59337dd83194c2fd5221adb,1681426023,"[ONNX] Refactor ShapeInferenceWithFakeTensor to fill  metavalue into the original gm (#98760)

From https://github.com/pytorch/pytorch/pull/97494#discussion_r1160068456, the passes should modify gm inplace, but before this PR, `ShapeInferenceWithFakeTensor` utilizes Transform.transform() to make a copy of the gm, and rely on the assumption that the topological order of two graphs should be the same. This PR addresses the issue by saving another metavalue `static_shape` into gm for op_level_debug, instead of overwriting `val`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98760
Approved by: https://github.com/BowenBao
","['torch/onnx/_internal/fx/fx_exporter.py', 'torch/onnx/_internal/fx/op_validation.py', 'torch/onnx/_internal/fx/passes/fx_to_onnxscript.py', 'torch/onnx/_internal/fx/passes/shape_inference.py']","`ShapeInferenceWithFakeTensor` is not modifying gm in-place as expected, instead makes a copy of the gm, potentially causing inconsistency due to reliance on the assumption that the topological order of two graphs should be the same."
8a1dab3d26638ddfe20acf516010ec658197f1de,1620882615,"[tsm] add support for jetter to Role (base_image) for mast launches

Summary:
1. Adds `ml_image` buck macro
2. Adds `--run_path` option to `torch.distributed.run`
3. Adds `tsm/driver/fb/test/patched/foo` (for unittesting)
4. Changes to `distributed_sum` to use `ml_image` (see Test plan for how this was tested in local and mast)

NOTE: need to enable jetter for flow and local schedulers (will do this on a separate diff since this diff is already really big)

Test Plan:
## Local Testing
```
# build the two fbpkgs (base and main)
buck run //pytorch/elastic/examples/distributed_sum/fb:torchx.examples.dist_sum.base
buck run //pytorch/elastic/examples/distributed_sum/fb:torchx.examples.dist_sum

# fetch the fbpkgs
cd ~/tmp

fbpkg fetch --symlink-tags  -o -d . jetter:prod
fbpkg fetch --symlink-tags  -o -d . torchx.examples.dist_sum.base
fbpkg fetch --symlink-tags  -o -d . torchx.examples.dist_sum

jetter/LAST/jetter apply-and-run \
  torchx.examples.dist_sum.base/LAST/torchrun \
  torchx.examples.dist_sum/LAST \
  -- \
  --as_function \
  --rdzv_id foobar \
  --nnodes 1 \
  --nproc_per_node 2 \
  --max_restarts 0 \
  --role worker \
  --no_python \
~/torchx.examples.dist_sum/LAST/pytorch/elastic/examples/distributed_sum/fb/main.py
```

## Mast Testing
```
buck-out/gen/pytorch/elastic/torchelastic/tsm/fb/cli/tsm.par run_ddp \
  --scheduler mast
  --base_fbpkg torchx.examples.dist_sum.base:78f01b5 \
  --fbpkg torchx.examples.dist_sum:f38ab46 \
  --run_cfg hpcClusterUuid=MastNaoTestCluster,hpcIdentity=pytorch_r2p,hpcJobOncall=pytorch_r2p \
  --nnodes 2 \
  --resource T1 \
  --nproc_per_node 4 \
  --name kiuk_jetter_test \
 pytorch/elastic/examples/distributed_sum/fb/main.py
```
Runs successfully: https://www.internalfb.com/mast/job/tsm_kiuk-kiuk_jetter_test_34c9f0fa?

Reviewed By: tierex, yifuwang

Differential Revision: D28177553

fbshipit-source-id: 29daada4bc26e5ef0949bf75215f35e557bd35b8
",['torch/distributed/run.py'],"The torch.distributed.run mechanism currently lacks support for specifying a run path, and the Role (base_image) doesn't support jetter for mast launches. Also, there's no macro available for ml_image."
b066931106e4e15b90fa56e33e151eaf48856c66,1643240008,"fixing of usage of rel_tol for test adadelta (#71880)

Summary:
Recently I made a PR to change some test tolerances: https://github.com/pytorch/pytorch/pull/69919
It turns out that the previous decorator does not work with the test optim unit test framework. I have summarized the issue in the following doc:
https://docs.google.com/document/d/1BOrp29r31A2WXwM0O6ydsCs43wi01sAgdduKd7is_ec/edit?usp=sharing

Pull Request resolved: https://github.com/pytorch/pytorch/pull/71880

Reviewed By: cpuhrsch

Differential Revision: D33801967

Pulled By: jbschlosser

fbshipit-source-id: 094feba10e2ee2a94e3ab754e4140e16b634ea09
(cherry picked from commit d504ddd950f69a6784b93a2e7630d24d5c7051fe)
",['test/test_optim.py'],"Test optim unit test framework is not compatible with the current decorator used, leading to problems in adjusting test tolerances."
337605054359a63083edcc7dcd8d887ce32947ed,1665670462,"fix type promotion for group_norm composite C++ kernel (#86607)

python decomp for `native_group_norm` is correct in more cases than the C++ composite. Updating the tests to fail properly in this case was more annoying than just fixing the C++ decomp, so I fixed it here.

When the input tensor had a dtype with less precision than float32, the C++ decomp would unconditionally set the mean/variance to float32, which was wrong.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86607
Approved by: https://github.com/albanD
","['aten/src/ATen/native/group_norm.cpp', 'test/test_meta.py']",C++ composite for `native_group_norm` incorrectly sets the mean/variance to float32 for input tensors with less precision than float32.
21a9a93eb4bf8660e24721bb24f69ebb0340f2fb,1616527818,"gdb special command to print tensors (#54339)

Summary:
This is something which I wrote because it was useful during my debugging sessions, but I think it might be generally useful to other people as well so I took the liberty of proposing an official `pytorch-gdb` extension.

`pytorch-gdb` is a gdb script written in python. Currently, it contains only one command: `torch-tensor-repr`, which prints a human-readable repr of an `at::Tensor` object. Example:
```
Breakpoint 1, at::native::neg (self=...) at [...]/pytorch/aten/src/ATen/native/UnaryOps.cpp:520
520     Tensor neg(const Tensor& self) { return unary_op_impl(self, at::neg_out); }
(gdb) # the default repr of 'self' is not very useful
(gdb) p self
$1 = (const at::Tensor &) 0x7ffff72ed780: {impl_ = {target_ = 0x5555559df6e0}}
(gdb) torch-tensor-repr self
Python-level repr of self:
tensor([1., 2., 3., 4.], dtype=torch.float64)
```

The idea is that by having an official place where to put these things, `pytorch-gdb` will slowly grow other useful features and make the pytorch debugging experience nicer and faster.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54339

Reviewed By: bdhirsh

Differential Revision: D27253674

Pulled By: ezyang

fbshipit-source-id: dba219e126cc2fe66b2d26740f3a8e3b886e56f5
","['tools/gdb/pytorch-gdb.py', 'torch/csrc/utils.cpp']","GDB's default representation of `at::Tensor` objects in PyTorch is not human-friendly, making debugging sessions difficult to understand and interpret."
ddd916c21010c69b6015edd419d62f28664373a7,1628010736,"[quant][refactor] Return the models in checkGraphModeFxOp for further checking (#62487)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62487

checkGraphModeFxOp is our utility test function to quantize a given model with FX Graph Mode Quantization
and checks whether the result model contains expected ops, previously it only returns a result on the sample data for the
quantized model, this PR chagnes it to return prepared, quantized, quantized_reference models together with the result
for quantized models.

Test Plan:
python test/test_quantization.py TestQuantizeFx
python test/test_quantization.py TestQuantizeFxOps

Imported from OSS

Reviewed By: iramazanli

Differential Revision: D30053981

fbshipit-source-id: 31fbce48d138261d0b00ba24e1427fd0c6208990
","['test/quantization/fx/test_quantize_fx.py', 'torch/testing/_internal/common_quantization.py']","The utility test function `checkGraphModeFxOp`, used for model quantization in FX Graph Mode, only returns a result for the sample data of quantized model, but does not provide prepared, quantized, and quantized_reference models."
4bbff920148f282555f52677d9325b683ba2d1b2,1611335792,"Refactor build targets for torch::deploy (#50288)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50288

torch::deploy will bundle the objects contained in libtorch-python together with frozenpython into a shared library.  Therefore, the libtorch-python objs can't bring with them a dependency on system python.

Buck TARGETS are added throughout the caffe2 tree to make available objects or headers that will be needed by torch::deploy but would have brought unsuitable dependencies if accessed using existing targets.

CMakeLists are modified to separate a torch-python-objs object library which lets torch::deploy compile these objs with the same compile flags as libttorch_python used, but without some of the link-time dependencies such as python.

CudaIPCTypes is moved from libtorch_python to libtorch_cuda because it is really not a python binding, and it statically registers a cuda_ipc_callback which would be duplicated if included in each copy of torch::deploy.

Test Plan: no new functionality, just ensure existing tests continue to pass

Reviewed By: malfet

Differential Revision: D25850785

fbshipit-source-id: b0b81c050cbee04e9de96888f8a09d29238a9db8
","['cmake/Summary.cmake', 'tools/build_variables.bzl', 'torch/csrc/CudaIPCTypes.cpp', 'torch/csrc/CudaIPCTypes.h']","Dependency on system python in libtorch-python objs conflicts with the bundling process in torch::deploy, creating unsuitable dependencies, and causing duplication of statically registered cuda_ipc_callback in each copy of torch::deploy."
d98b1c400d8d3aea9ca998341b70a1351149b929,1627508704,"[pruner] add cuda tests for pruner (#61993)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61993

Repeating `test_pruner` unit tests for Linear and Conv2d models with device = 'cuda' to confirm pruner will work on GPU
- set device to cuda
- move model to device
- assert that module.weight.device is cuda
ghstack-source-id: 134554382

Test Plan:
`buck test mode/dev-nosan //caffe2/test:ao -- TestBasePruner`

https://pxl.cl/1Md9c

Reviewed By: jerryzh168

Differential Revision: D29829293

fbshipit-source-id: 1f7250e45695d0ad634d0bb7582a34fd1324e765
",['test/ao/sparsity/test_pruner.py'],"Lack of unit tests for the pruner functionality in Linear and Conv2d models when device is set to 'cuda', leaving uncertainty if pruner works correctly on GPU."
017ecb782d22360f2d405ffba5e1b6aaa9e6b8a6,1660168213,"[ONNX] Update legacy code, initialize onnx_shape_inference=True by default (#82767)

Legacy code has onnx_shape_inference=False by default, which is misleading
as every other export api sets it to True unless otherwise overriden by caller.
There is only two tests that need updating according to this change.
* test_utility_funs.py::test_constant_fold_shape. The resulting number of nodes
  in graph is increased by 1, due to that previously the extra constant node was
  added as initializer.
* test_utility_funs.py::test_onnx_function_substitution_pass. Enabling onnx
  shape inference discovered discrepancy in test input shape and supplied dynamic
  axes arguments.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82767
Approved by: https://github.com/justinchuby, https://github.com/abock
","['test/onnx/test_pytorch_jit_onnx.py', 'test/onnx/test_utility_funs.py', 'torch/onnx/_globals.py', 'torch/onnx/symbolic_helper.py', 'torch/onnx/utils.py']","Default setting of onnx_shape_inference to False in legacy code is inconsistent with other export APIs and inspires misleading assumptions. Also, two tests are found inconsistent with this setting."
cb94ea60440920dad11b205fa14d805143f0182b,1683847644,"[BE] Simplify tests, elaborate testnames in test_optim.py (#101004)

- Deletes unused kwargs
- Make test names more descriptive to remove need of comments. Overall it's better to codify over comment
- Added a test for duplicate params across groups
- Greatly simplified test_empty_grad to discover that the crux of the bug was NOT its emptiness, but rather with multi-dim emptiness.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/101004
Approved by: https://github.com/albanD
",['test/test_optim.py'],"In `test_optim.py`, tests are excessively complex and their names are not descriptive enough, causing confusion and mistakes. A particular concern is a misunderstood bug related to 'multi-dim emptiness' in `test_empty_grad`."
e1f44ee3b3470e6d12af78a5110334cf9fbbbf56,1679973053,"[inductor] correctly setup constant in the wrapper (#97571)

V.graph.constants like seed_cuda_0 is not handled properly in the wrapper. Recently we move the code that initializes constants from global scope to a function. That makes assigning to seed_cuda_0 creating a new local variable rather than setup the global variable.

Add 'global var_name' lines to maintain the same behavior as before.

Test:

Run the forward graph for nvidia_deeprecommender's training run. Previous fail and now pass with the fix.

Thanks @ngimel  for report the issue with repro and @Chillee  for pointing out the root cause.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97571
Approved by: https://github.com/ngimel
","['test/inductor/test_kernel_benchmark.py', 'test/inductor/test_triton_wrapper.py', 'torch/_inductor/codegen/wrapper.py']",The initialization of constants like seed_cuda_0 is incorrectly creating new local variables instead of setting up global variables due to a recent code relocation. This breaks forward graph execution in nvidia_deeprecommender's training run.
3367e632b2613bfd26bee8b8d74951b791687346,1656105378,"[c10d] Make reduce as a custom op (#79686)

Summary:
This patch makes reduce as a custom op such that it's dispatcher
passable. It's one part of the effort to route comm ops to the dispatcher
such that tracing mechanisms that relies on the dispatcher can trace them,
e.g., LazyTensor and AOTAutograd.

Test Plan:
python test/distributed/test_c10d_nccl.py -k test_reduce_ops
python test/distributed/test_c10d_gloo.py -k test_reduce_basics
...and other existing distributed tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79686
Approved by: https://github.com/mrshenli
","['torch/csrc/distributed/c10d/Ops.cpp', 'torch/csrc/distributed/c10d/Ops.hpp', 'torch/csrc/distributed/c10d/Types.hpp', 'torch/csrc/distributed/c10d/init.cpp']","'Reduce operation is not being passed through the dispatcher, affecting tracing mechanisms including LazyTensor and AOTAutograd.'"
0b2566456f037e1cc07154142e6b78d2bbb9c0d0,1659137338,"[CUDNN] Update tests and dispatching for CUDNN V8 API behavior for `bfloat16` convs (#81139)

cuDNN via the V8 API supports `bfloat16` on Ampere (`>= (8, 0)` but not older devices) which might be unexpected given current test settings. This PR fixes some dispatching to check the device capability before dispatching `bfloat16` convs and adjusts the expected failure conditions for the autocast test.

CC @xwang233 @ptrblck
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81139
Approved by: https://github.com/ngimel
","['aten/src/ATen/cuda/detail/CUDAHooks.cpp', 'aten/src/ATen/cuda/detail/CUDAHooks.h', 'aten/src/ATen/detail/CUDAHooksInterface.h', 'aten/src/ATen/native/Convolution.cpp', 'test/test_cuda.py']","The current cuDNN V8 API tests and dispatches 'bfloat16' convs even on devices that do not support it, leading to unexpected behavior and incorrect test failure conditions."
df42f15e28306427a3bb7c28945c068301f352a4,1692717364,"Improve `generate_opcheck_tests`, add opcheck utility (#107597)

Summary:
This PR improves `generate_opcheck_tests`:
- We shouldn't run automated testing through operators called in
  torch.jit.trace / torch.jit.script
- I improved the error message and added a guide on what to do if one of the
  tests fail.
- While dogfooding this, I realize I wanted a way to reproduce the failure
  without using the test suite. If you pass `PYTORCH_OPCHECK_PRINT_REPRO`, it
  will now print a minimal repro on failure. This involves serializing some
  tensors to disk.
- The minimal repro includes a call to a new API called `opcheck`.

The opcheck utility runs the same checks as the tests generated
by `generate_opcheck_tests`. It doesn't have a lot of knobs on it for
simplicity. The general workflow is: if an autogenerated test fails, then the
user may find it easier to reproduce the failure without the test suite by
using opcheck

Test Plan: - new tests

Differential Revision: D48485013

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107597
Approved by: https://github.com/ezyang
","['test/test_custom_ops.py', 'torch/testing/_internal/optests/__init__.py', 'torch/testing/_internal/optests/generate_tests.py']","Torch.jit trace/script operators are being utilized in automated tests causing inefficiency, and failed tests are difficult to reproduce without substantial references and proper tooling."
70a545b21e84df32d8dd07c39087290f48b5d624,1632334247,"Add Tensor._make_wrapper_subclass (#65340)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65340

I thought about a few possible ways of doing this.  The main hazard is
that if I create a CPU tensor that doesn't have any real storage, the
moment I actually try to access the data on the tensor I will segfault.
So I don't want to use _make_subclass on a ""cpu meta tensor"" because
the CPU meta tensor (with no subclass) is radioactive: printing it
will immediately cause a segfault.  So instead, I have to create
the CPU meta tensor AND subclass all in one go, and that means I need
another function for it.  One downside to doing it this way is
I need another overload for explicit strides, and in general it is
difficult to get the view relationships to all work out properly;
tracked at https://github.com/pytorch/pytorch/issues/65339

Fixes https://github.com/pytorch/pytorch/issues/62972
Fixes https://github.com/pytorch/pytorch/issues/62730

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D31057231

Pulled By: ezyang

fbshipit-source-id: 73522769e093ae8a1bf0c7f7e594659bfb827b28
","['test/test_python_dispatch.py', 'torch/csrc/DynamicTypes.cpp', 'torch/csrc/autograd/python_variable.cpp', 'torch/overrides.py']","Creating a CPU tensor without any real storage leads to a segfault upon data access. Currently, there is no way to create the CPU meta tensor and subclass simultaneously to prevent this issue."
f418e1f8b63c0c15f52b373a57bfd9d65d02b172,1677773012,"Upload external contribution data to s3 (#95747)

Context: We want to create a metric panel to track external contributions to the PyTorch repo

This PR creates a daily job to track how many external contributions occurred the day before and uploads it to a s3 collection which is accessible by rockset.

`upload_external_contrib_stats.py` is a python script which grabs the neccesary stats from github and sticks them into an s3 bucket. It is used here to do daily uploads, but can generally be used for larger queries as well.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95747
Approved by: https://github.com/huydhn, https://github.com/kit1980
","['tools/stats/check_disabled_tests.py', 'tools/stats/upload_external_contrib_stats.py', 'tools/stats/upload_stats_lib.py', 'tools/stats/upload_test_stats.py']",There's no daily job to monitor and record the count of external contributions to the PyTorch repository.
6daf60be5abe4184121bc41e69e336015a268d6a,1668878219,"[ONNX] Add setType from user into InferredType and Reliable in ConstantValueMap (#88622)

`setType` API is not respected in current exporter because the graph-level shape type inference simply overrides every NOT ONNX Op shape we had from node-level shape type inference. To address this issue, this PR (1) makes custom Op with `setType` **reliable** in ConstantValueMap to secure its shape/type information in pass:  _C._jit_pass_onnx. (2) If an invalid Op with shape/type in pass: _C._jit_pass_onnx_graph_shape_type_inference(graph-level), we recognize it as reliable.

1. In #62856, The refactor in onnx.cpp made regression on custom Op, as that was the step we should update custom Op shape/type information into ConstantValueMap for remaining Ops.

2. Add another condition besides IsValidONNXNode for custom Op setType in shape_type_inference.cpp. If all the node output has shape (not all dynamic), we say it's custom set type.

3. ~However, this PR won't solve the [issue](https://github.com/pytorch/pytorch/issues/87738#issuecomment-1292831219) that in the node-level shape type inference, exporter invokes the warning in terms of the unknow custom Op, since we process its symbolic_fn after this warning, but it would have shape/type if setType is used correctly. And that will be left for another issue to solve. #84661~ Add `no_type_warning` in UpdateReliable() and it only warns if non ONNX node with no given type appears.

Fixes #81693
Fixes #87738

NOTE: not confident of this not breaking anything. Please share your thoughts if there is a robust test on your mind.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88622
Approved by: https://github.com/BowenBao
","['test/onnx/internal/test_diagnostics.py', 'test/onnx/test_pytorch_onnx_shape_inference.py', 'torch/csrc/jit/passes/onnx.cpp', 'torch/csrc/jit/passes/onnx/shape_type_inference.cpp', 'torch/csrc/jit/passes/onnx/shape_type_inference.h']","The current exporter is ignoring the 'setType' API, which results in inaccurate shape type inference at the graph level for NOT ONNX and custom Ops, as it is overriding the shape type inferred at the node level."
5f56c4fb32dbb5dd4e75a3a3a9726ae95931926d,1692625176,"[torch.compile x autograd.Function] More test cases (#107467)

I pulled a bunch of autograd.Function from test_autograd.py and added a
smoke test for them. Ideally we would actually run test_autograd.py as a
part of the Dynamo test suite, but we have excluded it due to there
being too many errors and I don't have time to figure that out at the
moment.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107467
Approved by: https://github.com/ydwu4
ghstack dependencies: #107459, #107461
",['test/dynamo/test_autograd_function.py'],"Certain autograd.Functions from test_autograd.py are not being tested as part of the Dynamo test suite, causing potential undetected errors or issues."
f7365eca901b0ed5c5edc0d2ae92834b7b75c0d2,1670851987,"Add unbacked symints support; item works now (#90624)

The big idea is to add `create_unbacked_symfloat` and `create_unbacked_symint` to ShapeEnv, allowing you to allocate symbolic floats/ints corresponding to data you don't know about at compile time. Then, instead of immediately erroring out when you try to call local_scalar_dense on a FakeTensor, we instead create a fresh symint/symfloat and return that.

There a bunch of odds and ends that need to be handled:

* A number of `numel` calls converted to `sym_numel`
* When we finally return from item(), we need to ensure we actually produce a SymInt/SymFloat when appropriate. The previous binding code assumed that you would have to get a normal Python item. I add a pybind11 binding for Scalar (to PyObject only) and refactor the code to use that. There is some trickiness where you are NOT allowed to go through c10::SymInt if there isn't actually any SymInt involved. See comment.
* One of our unit tests tripped an implicit data dependent access which occurs when you pass a Tensor as an argument to a sizes parameter. This is also converted to support symbolic shapes
* We now support tracking bare SymInt/SymFloat returns in proxy tensor mode (this was already in symbolic-shapes branch)
* Whenever we allocate an unbacked symint, we record the stack trace it was allocated at. These get printed when you attempt data dependent access on the symint (e.g., you try to guard on it)
* Subtlety: unbacked symints are not necessarily > 1. I added a test for this.

These unbacked symints are not very useful right now as you will almost always immediately raise an error later when you try to guard on them. The next logical step is adding an assertion refinement system that lets ShapeEnv learn facts about unbacked symints so it can do a better job eliding guards that are unnecessary.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90624
Approved by: https://github.com/Skylion007, https://github.com/voznesenskym
","['aten/src/ATen/native/Scalar.cpp', 'test/test_proxy_tensor.py', 'tools/autograd/templates/python_variable_methods.cpp', 'torch/_subclasses/fake_tensor.py', 'torch/csrc/lazy/core/tensor_impl.cpp', 'torch/csrc/lazy/core/tensor_impl.h', 'torch/csrc/utils/pybind.cpp', 'torch/csrc/utils/pybind.h', 'torch/csrc/utils/python_arg_parser.h', 'torch/fx/experimental/proxy_tensor.py', 'torch/fx/experimental/symbolic_shapes.py']","When calling local_scalar_dense on a FakeTensor, the system immediately errors out. Furthermore, issues arise with handling Tensor as an argument to a sizes parameter and tracking bare SymInt/SymFloat returns in proxy tensor mode. Unbacked symints are also not necessarily > 1."
6015987dc387b3703c0e148fe67230f258125ed4,1655308156,"Added edge case checking in isGreen (#79565)

Relates to #76700

**Overview**: One edge case not accounted for in the original logic of `isGreen` was for commits with no workflow checks. Similarly, if any of the required checks are not present (ex: if all of the pull checks are skipped), the workflow should not be promoteble. A commit should only be promoteable if there is it least one workflow check from each required group present (i.e. none of them are skipped)

**Test Plan:** Verify that commits on the HUD with no workflow checks are not considered promote-able. Added a test case with no workflows in `test_print_latest_commits.py`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79565
Approved by: https://github.com/seemethere
","['.github/scripts/print_latest_commits.py', '.github/scripts/test_print_latest_commits.py']","The 'isGreen' function doesn't correctly handle the edge case where workflow checks are absent from a commit, or when all the required checks are skipped, leading to incorrectly promotable workflows."
61a5c779bf5e7510400970ca0b6f5b6e2cb429f4,1655337802,"Make l1_loss composite

Fixing the forward AD for `sgn` in the next PR of this stack uncovered a
number of issues with the derivatives of `l1_loss`. Upon inspection,
`l1_loss` was just implemented as a composite function, but it was not
differentiable. This PR makes it a fully differentiable function.

As a side note, `l1_loss_out` was incorrect in a number of ways. Even
more, it is not exposed to the public as `F.l1_loss` does not accept an
`out=` parameter. As such it is not even tested. I wonder how useful is
to have `out=` variants for loss functions if we don't expose them at
all. Even more, I wonder how useful is to have `_out` variants  for loss
functions, given that their most normal use case is to return just a
real number cc jbschlosser

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78257

Approved by: https://github.com/jbschlosser
","['aten/src/ATen/native/Loss.cpp', 'test/forward_backward_compatibility/check_forward_backward_compatibility.py', 'tools/autograd/gen_variable_type.py', 'torch/_decomp/decompositions.py', 'torch/csrc/autograd/FunctionsManual.cpp', 'torch/csrc/autograd/FunctionsManual.h', 'torch/csrc/jit/runtime/static/generated_ops.cpp', 'torch/csrc/lazy/core/shape_inference.cpp', 'torch/csrc/lazy/core/shape_inference.h', 'torch/testing/_internal/common_methods_invocations.py']","The `l1_loss` function is implemented as a composite function but it's not differentiable, leading to issues with the derivatives. Additionally, `l1_loss_out` function has various inaccuracies and is not publicly exposed or tested."
3864207c2a71a3ba8dc13bcf9582a726a10292cd,1685834321,"Replace _dynamo.config with an object instead of module (#96455)

Summary:
    Replace _dynamo.config with an object instead of module

    Current usage patterns of setting and reading fields on config will work
    unchanged.

    Only changes needed going forward:
    1. import torch._dynamo.config will not work. However, just doing
       import torch._dynamo is sufficient to access dynamo config
       as torch._dynamo.config.

    2. Files inside of _dynamo folder need to access config via
       from torch._dynamo.config_util import config instead of
       from torch._dynamo import config. Because _dynamo/__init__.py
       imports some of the files so it would be circular import.

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96455
Approved by: https://github.com/jansel
","['test/inductor/test_config.py', 'torch/_config_utils.py', 'torch/_dynamo/__init__.py', 'torch/_dynamo/config.py', 'torch/_dynamo/config_utils.py', 'torch/_dynamo/test_minifier_common.py', 'torch/_functorch/__init__.py', 'torch/_functorch/config.py', 'torch/_inductor/__init__.py', 'torch/_inductor/config.py']",Currently setting and reading fields on _dynamo.config is problematic due to circular imports and the inability to access the config via the _dynamo folder.
e47e946bbf488890858fe1491df3bffa441d9011,1696262977,"[aotinductor] Use dynamic_shape instead of constraints (#110360)

Summary:
Previously we used export's constraints to specify all batch-size dimensions being dynamic. This is done by creating 1 constraint `dynamic_dim(inp[0][0], lower, upper)`, followed by `dynamic_dim(inp[0][0]) == dynamic_dim(inp[i][0])` for every input `i`.

Through the new `dynamic_shapes` API, we can use `Dims(""batch_size"")` on every dimension to specify which dimensions are dynamic and equal to each other, and `None` otherwise: `{i: [Dims(""batch_size"", lower, upper), None] for every input i}`

Note: `dynamic_shapes` and `constraints` utilize the same ""constraints"" backend so this diff should be idempotent.

Test Plan: `buck2 run @//mode/dev-nosan //caffe2/torch/fb/model_transform/experimental/benchmark/test/aotinductor:test_aot_inductor_benchmark`

Reviewed By: chenyang78, aakhundov

Differential Revision: D49784351

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110360
Approved by: https://github.com/desertfire
",['torch/_export/__init__.py'],All batch-size dimensions being dynamically specified solely through export's constraints can limit flexibility and increase complexity.
cdc9127733c26cf0f01e59e0e3396bd10775d834,1690827619,"[ONNX] Perform Shape inference on added ""Cast"" node (#106093)

This commit fixes a bug where some ""If"" nodes blocked shape inference during the onnx graph building.

In fixup_onnx_controlflow, a ""Cast"" node is added to conditions in ""If"" and ""Loop"" nodes if the condition type is not bool.

This commit performs shape inference on this new ""Cast"" node which allows its output to be marked as ""reliable"" in ConstantValueMap during further shape inference. This would have eventually happened when shape inference is performed on the entire graph, but the inferred shapes are also useful to have during onnx graph building, since it allows some ops (like Squeeze) to export into simpler subgraphs.

Also adds a test for this.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106093
Approved by: https://github.com/thiagocrepaldi
","['test/onnx/test_pytorch_onnx_shape_inference.py', 'torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp']","Certain ""If"" nodes are blocking shape inference during ONNX graph building due to ""Cast"" nodes being added for condition types not being a boolean in ""If"" and ""Loop"" nodes.
"
92eb9d363a7110a43746cc3e704a393837d5047c,1679046355,"Decoder native functions join the dead code society (#96025)

Summary: Decoder native joins the dead code society

With the recent introduction of PT2, we no longer need native decoder operators:
1 - full-function SDPA kernels can be used to implement cross-attention efficiently without the (slower) decoder MHA blob.
2 - torch.compile() generates more efficient code across many platforms from the python implementation of decoders than the decoder layer blob by tailoring code to target

Test Plan: github & sandcastle

Differential Revision: D43811808

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96025
Approved by: https://github.com/ezyang, https://github.com/albanD
","['aten/src/ATen/native/transformers/attention.cpp', 'aten/src/ATen/native/transformers/transformer.cpp', 'test/forward_backward_compatibility/check_forward_backward_compatibility.py', 'test/test_transformers.py']","With the introduction of PT2, native decoder operators become redundant and even slower than efficiently implemented cross-attention through full-function SDPA kernels."
d8cc8ffebcf9d93e728734e879685264aa2d22c0,1680025163,"[DataLoader] Short circuit pin_memory recursion when operating on bytes (#97737)

Slack thread: https://pytorch.slack.com/archives/GEEQ2K4MD/p1679962409906099

I was seeing some massive (~2x) slowdowns on a job after running it on PyTorch 2.0. From some profiling in `py-spy` it looked like the pin_memory thread was doing a lot more work than before. Looking at a trace in `nsys` I saw the thread doing the forward pass having a bunch of `pthread_cond_timedwait` with GIL reacquire calls in it’s call stack, and it seemed like the thread doing the forward pass was getting blocked (waiting for the GIL) by the pin memory thread (which was holding the GIL).

After some debugging I found out the issue. If a `bytes` was passed into `pin_memory`, previously in 1.13 (before https://github.com/pytorch/pytorch/pull/94709) it would short-circuit and return here
https://github.com/pytorch/pytorch/blob/d922c29a22e4bf0fba49526f7536395eb8cd66f4/torch/utils/data/_utils/pin_memory.py#L54-L55
since `bytes` was in `torch._six.string_classes`:
```
>>> from torch._six import string_classes
>>> string_classes
(<class 'str'>, <class 'bytes'>)
>>>
```

However after https://github.com/pytorch/pytorch/pull/94709, if a `bytes` was passed into `pin_memory` it would fall into here instead
https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/utils/data/_utils/pin_memory.py#L68-L73
because the previous check is now doing `isinstance(data, str)` instead of `isinstance(data, (str, bytes))`!
https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/utils/data/_utils/pin_memory.py#L56-L57

As a result, `pin_memory` gets called recursively for each element in the `bytes` leading to a ton of wasted recursion. This also explains the slowdown / GIL contention I was seeing.

This PR simply changes `isinstance(data, str)` to `isinstance(data, (str, bytes))` to match the behavior before https://github.com/pytorch/pytorch/pull/94709

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97737
Approved by: https://github.com/albanD, https://github.com/NivekT
",['torch/utils/data/_utils/pin_memory.py'],"""Bytes"" passed into ""pin_memory"" now triggers unnecessary recursion slowing down the process significantly, possibly because of increased Global Interpreter Lock (GIL) contention."
dcc3ae98b7278d9d85be853bfcd070b2a081003f,1665423439,"[NestedTensor] Add a contiguous checks to get_buffer (#86496)

# Summary
Many NestedTensor ops are implemented using a connivence function named get_buffer. This returns a dense, contiguous tensor that is a view of the underlying storage of the NestedTensor. This function allows NestedTensor ops to piggy back off of the implementations for dense tensor under certain scenarios.  This PR adds a TORCH_CHECK() to get buffer to insure that the calling NT is in fact contiguous. It also adds an ""unsafe"" version for a few ops that are designed to handle contiguity.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86496
Approved by: https://github.com/albanD, https://github.com/cpuhrsch
","['aten/src/ATen/NestedTensorImpl.cpp', 'aten/src/ATen/NestedTensorImpl.h', 'aten/src/ATen/native/nested/NestedTensorMath.cpp', 'aten/src/ATen/native/nested/NestedTensorUtils.cpp']",The NestedTensor operations could potentially operate on non-contiguous tensors due to the lack of contiguity checks in the get_buffer function.
c76c4912bbd6c1f2cb3e04d532643ca19393415e,1644443823,"[SR] Make fused_sigrid_transforms work on graph outputs (#71507)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71507

We previously disabled `FuseListUnpack` if the fused outputs of the op would alias the graph outputs. The concern was that some ops were assuming that `p_node->Output(0).isTensor()` implies `p_node->Output(i).isTensor()` for all `i > 0`. This condition can be violated if there exists both managed and unmanaged tensors in the output list.

Instead of adding this special case and missing out on some fusions, we should implement fused ops correctly.

Reviewed By: d1jang

Differential Revision: D33669034

fbshipit-source-id: 8b291b5fe610ffbe47b88a5a018daa63cb5665b0
(cherry picked from commit c6cba235a69da92b97b1a02d5c33065bb09eb0a9)
",['torch/csrc/jit/runtime/static/passes.cpp'],"'FuseListUnpack' functionality is restricted when fused outputs of the operation alias the graph outputs, possibly due to incorrect assumptions regarding tensor management in the output list. This leads to missed fusion opportunities."
e7c79cb158fb75f164e5a8332afe51b29eb37906,1619489287,"Add type annotations to nnapi (#48142)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/48141

~Mypy is complaining about a missing arg in a function call.~
```bash
torch/backends/_nnapi/serializer.py:806: error: Too few arguments for ""_do_add_binary""  [call-arg]
Found 1 error in 1 file (checked 1140 source files)
```

https://github.com/pytorch/pytorch/blob/9392137dbe4fa6e146c5312636c1fb76b9889ece/torch/backends/_nnapi/serializer.py#L804-L806

~dreiss, would you mind take a look when you have some cycles to spare and see what would be the appropriated value for `fuse_code` here? Thanks :)~

Edit: https://github.com/pytorch/pytorch/issues/48925 got merged a couple of days ago. The blocking part is now unblocked, and I just pushed the changes to make mypy happy again. This PR is ready for review.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48142

Reviewed By: ezyang

Differential Revision: D28006249

Pulled By: walterddr

fbshipit-source-id: 5e43eeba7143512a549efaad31541f86718add7c
","['torch/backends/_nnapi/prepare.py', 'torch/backends/_nnapi/serializer.py', 'torch/types.py']","Mypy reports a missing argument in a call to the function ""_do_add_binary"" in the nnapi serializer, leading to type annotation errors."
93de80203d79615d406e741357172fef11cee29b,1619481799,"ns for fx: move node I/O dtype mapping to be local instead of global (#56296)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56296

To support shadows of custom functions, we need to allow user to
specify I/O type of the custom functions.

This PR is a cleanup in preparation for making the above happen.
We make the I/O dtype mappings be generated by a function instead
of a global variable. In the next PR, we will add a hook so user
can modify these mappings.

Test Plan:
```
python test/test_quantization.py TestFXNumericSuiteCoreAPIs
```

Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D27831996

fbshipit-source-id: 782f5e77de0eef3899b9b7def0fdabd8dcafef12
","['test/quantization/test_numeric_suite_fx.py', 'torch/quantization/ns/mappings.py', 'torch/quantization/ns/utils.py']",The global I/O dtype mappings in shadow custom functions inhibit user specification of custom functions' I/O types.
3a53dbae2a6caad7b3e464ab599eb297b934a4ee,1687289060,"Update viable/strict script to ignore unstable jobs (#103899)

As distributed jobs had been failing in the past few days, viable/strict branch hasn't been updated since June 15th.  The issue was discovered when looking into nightly https://hud.pytorch.org/hud/pytorch/pytorch/nightly which sync with viable/strict.

Despite the fact that the failing job has been marked as unstable by https://github.com/pytorch/pytorch/issues/103612, the script still counted it as a failure https://github.com/pytorch/pytorch/actions/runs/5319411414/jobs/9631875636, and we kind of forget to monitor viable/strict delay to notice this earlier.  An alarm would probably need to be setup for this.

I also update the Rockset query a bit to add a comment on that it's used for.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/103899
Approved by: https://github.com/clee2000, https://github.com/seemethere, https://github.com/malfet
",['.github/scripts/fetch_latest_green_commit.py'],"The viable/strict update script counts unstable jobs as failures, leading to a halt in updates even when these unstable jobs fail."
9a0b2acd766877cf51b076fbdf06d76af4d4637e,1633545754,"[quant] Remove hypothesis from qtopk (#66158)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66158

qtopk used hypothesis which created flaky tests. In addition to that the tests generated were not representative, and would not catch the cases that we are interested in.

This diff removes the hypothesis from the qtopk and merges the qtopk and qtopk_nhwc tests. We now use specific testcases.
ghstack-source-id: 139768865

Test Plan: `buck test mode/dev //caffe2/test:quantization -- test_qtopk`

Reviewed By: jerryzh168

Differential Revision: D31401341

fbshipit-source-id: a8fb37a7221fc43c159f34e28aa4a91ed3506944
",['test/quantization/core/test_quantized_op.py'],"The qtopk tests are flaky and unrepresentative due to their use of hypothesis, failing to capture relevant test cases."
287f74c4fc5bba6864a31910a988b03638fca4da,1683130254,"Revert D45387167: Multisect successfully blamed D45387167 for test or build failures (#100424)

Summary:
This diff is reverting D45387167
D45387167: Basic dynamo support for traceable collectives (#94440) by wconstab has been identified to be causing the following test or build failures (internal)

If you believe this diff has been generated in error you may Commandeer and Abandon it.

Test Plan: NA

Reviewed By: s4ayub

Differential Revision: D45448312

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100424
Approved by: https://github.com/rohan-varma, https://github.com/kumpera
","['test/distributed/test_inductor_collectives.py', 'torch/_dynamo/allowed_functions.py', 'torch/_dynamo/config.py', 'torch/distributed/_functional_collectives.py']","The commit D45387167, which added basic Dynamo support for traceable collectives, is causing test or build failures."
3727baea6f181f35ed83772abb960ba264d50f12,1631646766,"[PyTorch Edge][Model Loading] Operator Call De-dup at TorchScript Serialization Level [2/2] (#64269)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64269

Revert changes in D29826210 (https://github.com/pytorch/pytorch/commit/693d8f2f0767413bb995b895fccad87dfd4f05a7) (we don't need operator lambda caching since there aren't duplicate operators anymore)

This diff stack results in an additional approx 12% speedup in model loading time (from 229ms to 200ms) when run against an 87MB speech model that jiatongzhou provided.
ghstack-source-id: 138014904

Test Plan:
**Speech Transducer v25 model (as in D29826210 (https://github.com/pytorch/pytorch/commit/693d8f2f0767413bb995b895fccad87dfd4f05a7))**

|| Before | After |
|Load Time|[229ms](https://www.internalfb.com/intern/aibench/details/160889436133243)|[200ms](https://www.internalfb.com/intern/aibench/details/837884532607514)|
|Save File Size|[86.23 MB](https://lookaside.facebook.com/intern/diff/file/data/?number=658544950)|[86.1 MB](https://lookaside.facebook.com/intern/diff/file/data/?number=658554403)|

The ""after"" flamegraph shows significantly less time is spent on ```append_operator``` than before.

Steps
- Check out desired commit in devserver (base branch or this diff)
- ```buck build bento/kernels:bento_kernel_pytorch```
- Use N1094068 with pytorch_local kernel to save model for lite interpreter
- Edit ```aibench/specifications/models/pytorch/speech_transducer/v25.json ``` to have new model location and md5
- ```buck run aibench:run_bench -- -b aibench/specifications/models/pytorch/speech_transducer/v25.json --framework pytorch --platform android/arm64 --devices ""S8US"" --force_profile --remote ```

**Test that saving a model with de-dup ops doesn't change its output**
https://www.internalfb.com/intern/anp/view/?id=1137434

Reviewed By: iseeyuan

Differential Revision: D30615710

fbshipit-source-id: bb4052f0f16eccab386585e94411056f94bce43c
","['torch/csrc/jit/mobile/function.cpp', 'torch/csrc/jit/mobile/function.h', 'torch/csrc/jit/mobile/import.cpp']","Duplicate operators in TorchScript serialization lead to unnecessary redundancy, impacting model loading times and file size."
25e84fa4e56c5b5a7b52c2284c7f08e9e52f52c2,1643214686,"Add forward AD formulas for some losses (#71026)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71026

...and fmod

Testing:
- L1Loss: new module tests (linear in the real case only)
- SmoothL1Loss: new module tests
- MSELoss: tested - OpInfo + new module tests
- huberloss: tested - OpInfo + new module tests
- multi-margin-loss: new module tests
- kl-div: OpInfo + new module tests
- fmod: OpInfo

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D33485661

Pulled By: soulitzer

fbshipit-source-id: 542ef5148183b9f574d06b2e2e345d0d889537b7
(cherry picked from commit 60765438e8de82cf9dd2fca71f2ae218c0a38493)
","['torch/csrc/autograd/FunctionsManual.cpp', 'torch/csrc/autograd/FunctionsManual.h', 'torch/testing/_internal/common_methods_invocations.py', 'torch/testing/_internal/common_nn.py']","The project lacks forward AD formulas for certain loss functions such as L1Loss, SmoothL1Loss, MSELoss, huberloss, multi-margin-loss, kl-div and fmod."
493a233c0478a9b26faf0c2e675bd56a0c08a8d4,1617853400,"[torch/elastic] Revise the rendezvous handler registry logic. (#55466)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55466

Improve the implementation and the unit test coverage of `RendezvousHandlerRegistry`.

### Note
See the original diff (D27442325 (https://github.com/pytorch/pytorch/commit/df299dbd7d8241669aaac7ce07ed0c034f219b4f)) that had to be reverted due to an unexpected Python version incompatibility between the internal and external PyTorch CI tests.

Test Plan: Run the existing and newly-introduced unit tests.

Reviewed By: tierex

Differential Revision: D27623215

fbshipit-source-id: 51538d0f154f64e04f685a95d40d805b478c93f9
","['test/distributed/elastic/rendezvous/api_test.py', 'torch/distributed/elastic/rendezvous/__init__.py', 'torch/distributed/elastic/rendezvous/api.py', 'torch/distributed/elastic/rendezvous/registry.py']",Issues with the existing `RendezvousHandlerRegistry` implementation and lack of sufficient unit test coverage are causing Python version incompatibility between internal and external PyTorch CI tests.
5970fb402e61936c76b11edea6965bbe5a3557e8,1683657700,"C++ CustomClass in Python: indicate which methods are not implemented (#100171)

Without these changes, it can be hard to know which magic methods are not implemented on a given ScriptObject.

before:
```py
torch.ops.load_library(""somelib.so"")
c = torch.classes.somelib.SomeClass()
print(len(c))
# raise NotImplementedError
```

after:
```py
torch.ops.load_library(""somelib.so"")
c = torch.classes.somelib.SomeClass()
print(len(c))
# raise NotImplementedError: '__len__' is not implemented for __torch__.torch.classes.somelib.SomeClass
```

------

I could not find a linked issue, if you want me to open one as well I can do this.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100171
Approved by: https://github.com/ezyang
","['torch/csrc/Exceptions.cpp', 'torch/csrc/Exceptions.h', 'torch/csrc/jit/python/script_init.cpp']","CustomClass in PyTorch doesn't indicate which magic methods are not implemented, leading to hurdles in debugging when such methods are accessed."
caaf37a1116cf4ce0f372bbd9241f8a827dc33b7,1667259483,"Fix `PyTorchStreamWriter` exception handling (#88128)

Avoid double exception in destructor if attempting to serialize to
python object that does not have `write` method

Use `Finalizer` class in `PyTorchStreamWriter::writeEndOfFile()` to a
always set `finailized_` property even if excretion occurs. (as there
isn't much one can do at this point)

Add expicit check for the attribue to `_open_zipfile_writer_buffer` and
add unitests

Modernize code a bit by using Python-3 `super()` method

Fixes https://github.com/pytorch/pytorch/issues/87997

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88128
Approved by: https://github.com/albanD
","['caffe2/serialize/inline_container.cc', 'caffe2/serialize/inline_container.h', 'test/test_serialization.py', 'torch/csrc/jit/python/init.cpp', 'torch/serialization.py']","There's an issue with error handling in `PyTorchStreamWriter`, leading to a double exception in the destructor when trying to serialize to a Python object lacking a `write` method."
445ee5620ec203cfccefd6f3dca4f0962a83b03e,1658432067,"Simplify torch.nn.grad by calling into aten::convolution_backward (#81839)

`torch.nn.grad` has its own implementations of gradients for conv1d, conv2d, and conv3d. This PR simplifies them by calling into the unified `aten::convolution_backward` backend instead.

The existing implementation of conv2d_weight is incorrect for some inputs (see issue #51430). This PR fixes the issue.

This PR expands coverage in test_nn to include conv1d_weight, conv2d_weight, and conv3d_weight, which were previously untested. It also expands the cases for conv2d to cover issue #51430.

Fixes #51430

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81839
Approved by: https://github.com/albanD
","['test/test_nn.py', 'torch/nn/grad.py']","The implementation of gradients for conv1d, conv2d, and conv3d in torch.nn.grad is not unified, and conv2d_weight is showing incorrect results for some inputs. Additionally, these cases are not covered in test_nn."
7c48b9ee25b1d9c7ba7adf91f3808c7426a046e1,1635358340,"Sparse CSR CUDA: add `triangular_solve_out` (#61858)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61858

This PR adds `triangular_solve_out_sparse_csr_cuda`. The operation is
used to comput the solution to the linear system where coefficient
matrix is triangular.
Structured kernels are used and the meta function needed some changes to
support sparse csr layout. With sparse matrix input the `cloned_coefficient`
tensor is 0-sized tensor.

cc nikitaved pearu cpuhrsch IvanYashchuk ngimel

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D31948435

Pulled By: cpuhrsch

fbshipit-source-id: 7775fece83ca705a26d75f82aead10b956b14bfd
","['aten/src/ATen/cuda/CUDASparse.h', 'aten/src/ATen/cuda/CUDASparseDescriptors.cpp', 'aten/src/ATen/cuda/CUDASparseDescriptors.h', 'aten/src/ATen/native/BatchLinearAlgebra.cpp', 'aten/src/ATen/native/sparse/cuda/SparseBlas.cpp', 'aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp', 'aten/src/ATen/native/sparse/cuda/SparseBlasImpl.h', 'test/test_sparse_csr.py']","The current implementation of `triangular_solve` does not support Sparse CSR format on CUDA, limiting the solution computations of linear systems where coefficient matrix is triangular."
f1cbd10276995c8fa376891804da62352b5af316,1616183111,"[PyPer] Port c2 add to pt (#54229)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54229

Because caffe2 add uses Eigen for add with broadcasting which is not well supported by OSS PyTorch, it's easier to just keep the `c2_add_out` internal for now. Caffe2 does use mkl add when the input dims of A and B are the same and there is no broadcasting needed.

Reviewed By: bertmaher

Differential Revision: D27036279

fbshipit-source-id: 49f0ec5407ea1f641896f054cad2283faed81687
","['caffe2/operators/elementwise_ops_utils.cc', 'caffe2/operators/elementwise_ops_utils.h', 'torch/csrc/jit/runtime/static/ops.cpp']","The add operation in Caffe2 uses Eigen for addition with broadcasting, which is not supported by OSS PyTorch, leading to issues when the input dimensions of A and B are the same and there's no broadcast needed."
9a2b43085d26531c6e0d0606c9dadb71ec544cf2,1643142732,"Improve docs for `from_dlpack` and `to_dlpack` (#70437)

Summary:
This moves the warning to the legacy function where it belongs, improves the phrasing, and adds examples.

There may be more to do to make `from_dlpack` more discoverable as a follow-up, because in multiple issues/PR we discovered people wanted new things (e.g., a memoryview-like object, or `__array_interface__` support) that `from_dlpack` already provides.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70437

Reviewed By: albanD

Differential Revision: D33760552

Pulled By: mruberry

fbshipit-source-id: e8a61fa99d42331cc4bf3adfe494cab13ca6d499
(cherry picked from commit 880ad9665956078958af93132a4c6ae820bbaac9)
",['torch/utils/dlpack.py'],"The `from_dlpack` and `to_dlpack` functions' descriptions are unclear and lack examples; additionally, users are requesting features these functions already provide due to obscurity."
25dd2a0422cf5fe38937c5b9441b1f6abc1c40cb,1661289918,"Fix load_extra_only api for flatbuffers and enable flatbuffers in mobile for OSS properly (#83855)

`_load_extra_only_for_mobile` API hasn't handled flatbuffers logic yet. Update the api accordingly.

Also find out mobile build in OSS doesn't build with flatbuffers. Filed task T129996445 to track

Differential Revision: [D38890847](https://our.internmc.facebook.com/intern/diff/D38890847/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D38890847/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83855
Approved by: https://github.com/qihqi
","['build_variables.bzl', 'torch/csrc/jit/mobile/import.cpp']",The `_load_extra_only_for_mobile` API doesn't handle flatbuffers and the mobile build in OSS is not properly integrated with flatbuffers.
2738405a76a6210140b95912a2159969ece6a7fb,1653310144,"[primTorch] Adds any, all, equal, item references (#78072)

This PR adds the item, equal, any, and all references.

While doing this I found the following issues:
- https://github.com/pytorch/pytorch/issues/78070
- https://github.com/pytorch/pytorch/issues/78071

And I fixed a bug where the `convert_element_type` prim could not convert tensors requiring grad to datatypes that don't require grad.

Creating the item reference required adding item as a prim, but per @ngimel's suggestion I removed the prims for any and all and implemented them as references, so this is net negative one prim.

Reference OpInfos are added for any and all, but item and equal don't even have regular OpInfos.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78072
Approved by: https://github.com/ngimel
","['test/test_ops.py', 'torch/_prims/__init__.py', 'torch/_prims/utils.py', 'torch/_refs/__init__.py', 'torch/testing/_internal/common_methods_invocations.py']","The `convert_element_type` function can't convert tensors needing grad to data types that don't need grad. Also, functions like 'all', 'any', 'equal' and 'item' do not have respective prim or OpInfo references."
806878518f32c5b93acc7da576e57ab52f6f5232,1661803838,"[ONNX][Reland] Export node and value with scope name (#82040)

Introduce `_jit_pass_onnx_assign_node_and_value_names` to parse and assign
scoped name for nodes and values in exported onnx graph.
Module layer information is obtained from `ONNXScopeName` captured in `scope`
attribute in nodes. For nodes, the processed onnx node name are stored in
attribute `onnx_name`. For values, the processed onnx output name are stored
as `debugName`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82040
Approved by: https://github.com/AllenTiTaiWang, https://github.com/justinchuby, https://github.com/abock
","['test/onnx/test_utility_funs.py', 'torch/csrc/jit/passes/onnx/naming.cpp', 'torch/csrc/jit/passes/onnx/naming.h', 'torch/csrc/jit/serialization/export.cpp', 'torch/csrc/onnx/init.cpp', 'torch/csrc/onnx/onnx.h', 'torch/onnx/utils.py']",ONNX export lacks the assignment of scope names to nodes and values in the graph which is necessary for module layer information.
0cf46fb0deeb24ebaeae13b9476a0b215023d66a,1637008250,"[fx2trt] fix a bug in conversion from negative dim to positive dim (#68360)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68360

Added a helper function to do this. Only use `mod` to convert negative dim to positive. Do nothing when it's already positive.

Previously in `getitem` if we are slicing to the very end, we will get the dimension wrong.

Test Plan: Add a unit test

Reviewed By: yinghai, wushirong

Differential Revision: D32432893

fbshipit-source-id: 3c5d6a578d92a15207a5e52802750f9ea7f272a9
","['test/fx2trt/converters/acc_op/test_getitem.py', 'torch/fx/experimental/fx2trt/converters/acc_ops_converters.py', 'torch/fx/experimental/fx2trt/converters/converter_utils.py']",Conversion from negative dimension to positive dimension results in incorrect dimensions when slicing to the end in `getitem`.
2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,1696065176,"Skip launching kernels with zero grid in AOT Inductor (#110312)

Summary: with the grid computed in terms of unbacked `SymInt`s, it can happen that the grid is zero size. This causes CUDA error on `cuLaunchKernel` in the AOT Inductor codegen.

In this PR, when the grid contains unbacked `SymInt`s, a check is added around the `launchKernel` in the AOT Inductor's C++ wrapper codegen to make sure that the grid is not zero-size.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110312
Approved by: https://github.com/chenyang78
","['test/inductor/test_aot_inductor.py', 'torch/_dynamo/config.py', 'torch/_export/__init__.py', 'torch/_inductor/codegen/wrapper.py', 'torch/_inductor/ir.py']",Zero-sized grid computed in terms of unbacked `SymInt`s in AOT Inductor results in a CUDA error on `cuLaunchKernel`.
b96a6516a690e4e396f361c4e2bed99448f1b6dd,1611252050,"Add CPP Full Reduction Benchmarks. (#50193)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50193

* Supports aten, native reference implementation, and NNC TE implementations.
* Support functionality checks against aten, in addition to performance checks.

Test plans:

* After enable ""BUILD_TENSOREXPR_BENCHMARK"" in CMakeLists.txt,
* bin/tensorexpr_bench --benchmark_filter=Reduce1D

Measurements:

On a Broadwell E5-2686 CPU,

Reduce1D/Torch/16777216            5638547 ns    5638444 ns        119 BYTES=11.902G/s
Reduce1D/Naive/16777216           19308235 ns   19308184 ns         36 BYTES=3.47567G/s
Reduce1D/NativeRfactor/16777216    8433348 ns    8433038 ns         85 BYTES=7.95785G/s
Reduce1D/NativeVector/16777216     5608836 ns    5608727 ns        124 BYTES=11.9651G/s
Reduce1D/NativeTiled/16777216      5550233 ns    5550221 ns        126 BYTES=12.0912G/s
Reduce1D/TeNaive/16777216         21451047 ns   21450752 ns         33 BYTES=3.12851G/s
Reduce1D/TeSplitTail/16777216     23701732 ns   23701229 ns         30 BYTES=2.83145G/s
Reduce1D/TeSplitMask/16777216     23683589 ns   23682978 ns         30 BYTES=2.83363G/s
Reduce1D/TeRfactorV2/16777216      5378019 ns    5377909 ns        131 BYTES=12.4786G/s

Result summary:

* The single-threaded performance with NNC TeRfactorV2 matches and exceeds Aten and avx2 naive counterpart.

Follow-up items:

* rfactor does not work well with split
* We don't have a multi-threaded implementation yet.
  * Missing ""parallel"" scheduling primitive, which is not different from what we need for pointwise ops.

Test Plan: Imported from OSS

Reviewed By: bertmaher

Differential Revision: D25821880

Pulled By: zheng-xq

fbshipit-source-id: 8df3f40d1eed8749c8edcaacae5f0544dbf6bed3
",['benchmarks/cpp/tensorexpr/bench_reduce.cpp'],"Lack of CPP benchmarks for full reduction is causing performance mapping issue. There aren't any functionalities and performance checks against aten, as well as absence of a multi-threaded implementation."
9b46737fca9759c217804367521b654c3aa3efc3,1659391283,"Add tests for fake tensor striding (#82571)

Add tests for fake tensor striding in OpInfos. I know primtorch is not strictly committing to consistent stride propagation with ATen (see https://github.com/pytorch/pytorch/issues/78050), where as in fake tensor/meta the goal is be completely consistent. This is a little awkward because by default prim refs will register a meta implementation.

In any case, I think we can add the tests for fake with a disclaimer in the tests the failure is non-blocking for adding prims. At least as far as OpInfo tests get, the prims seem to do a pretty good job with stride propagation already.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82571
Approved by: https://github.com/ezyang
","['test/test_ops.py', 'torch/_prims_common/__init__.py']","There are no tests for fake tensor striding in OpInfos, leading to potential inconsistencies in stride propagation."
b00d388ada64f1d35022fee4cfc666342ba61843,1686672851,"Update test_misc.cpp (#97768)

Potential null dereference after dynamic cast was found during static analysis.

**Description:**
Dereference of `ctx` is performed in `TORCH_CHECK` on line 1176, while `ctx` pointer may equal `nullptr`.
Previous `TORCH_CHECK` on line 1175 checks the value of `ctx_ptr` pointer that may be of type that cannot be casted to `TestContext*`. In such case, `dynamic_cast` returns `nullptr` despite `ctx_ptr` is not equal to `nullptr`.

**Fix:**

- Check `ctx` instead of `ctx_ptr` for equality to zero.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97768
Approved by: https://github.com/kit1980
",['test/cpp/jit/test_misc.cpp'],"Null pointer dereference potential in test_misc.cpp after a dynamic cast, the variable 'ctx' may not be properly checked for nullity before being dereferenced."
8fee46693c1639eb3e6299dd461f698d8a90d5f6,1681086922,"Fused attention patterns (#97741)

Patterns based on https://github.com/pytorch/pytorch/pull/94729 mainly as a forcing function for implementing joint graph replacements.

Up until now, we had two places to do pattern matching
1) Pre-grad has janky infra (graph not normalized or functional), but is
   desirable for many types of passes where you want your change to
   affect grad formulas.
2) Post-grad has good infra, but cant change grad formulas.

This PR adds a third place to do pattern matching: the joint
forward+backwards graph.  The idea is to take the patterns and lower
them to a joint graph and replace both the forwards+backwards before
we partition them.  This allows us to do something similar to pre-grad
transforms, but run after normalization and functionalization.

Note that we don't seem to have kernels for all of these patterns, some get decomposed in the dispatcher.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97741
Approved by: https://github.com/Chillee
","['test/inductor/test_fused_attention.py', 'torch/_inductor/compile_fx.py', 'torch/_inductor/decomposition.py', 'torch/_inductor/fx_passes/fuse_attention.py', 'torch/_inductor/fx_passes/joint_graph.py', 'torch/_inductor/utils.py']","Pattern matching for joint forward+backwards graph is not available, limiting the implementation of transformations similar to pre-grad transforms after normalization and functionalization."
7bb5fb3c6dceb38581f55e1c9484013a562516c5,1680209858,"[vmap] Fix index_select support when dim is negative (#97916)

Fixes https://github.com/pytorch/pytorch/issues/96854

Previously, this would segfault (via indexing -2 into a SmallVector).
This PR fixes it so that we wrap negative dimensions.

Test Plan:
- changed the index_select OpInfo to use dim=-1 instead of dim=1,
because it's much more common that the negative dimension doesn't work
instead of the positive one.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97916
Approved by: https://github.com/ngimel, https://github.com/janeyx99
","['aten/src/ATen/functorch/BatchRulesScatterOps.cpp', 'torch/testing/_internal/common_methods_invocations.py']",Negative dimension indexing in 'index_select' function causes segmentation fault (segfault) due to indexing into out-of-bound areas in a SmallVector. This mainly happens while using vmap.
06166a13e0a03e710d03280d411b553f0316fb3a,1625884324,"Remove VS install step unless necessary from GHA Windows workflows (#60791)

Summary:
~~This should only be merged after our AMI has been deployed after https://github.com/fairinternal/pytorch-gha-infra/pull/1. (And will likely fail our current windows jobs)~~

I have revised this PR to install VS only when it's not already installed.

This should save ~5min per Windows workflow.
![image](https://user-images.githubusercontent.com/31798555/125141598-7e886c80-e0e3-11eb-9fe0-bb9e6bcc14f1.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60791

Reviewed By: soulitzer

Differential Revision: D29643876

Pulled By: janeyx99

fbshipit-source-id: 4bcfaf5bcad9e5636a1624c3e799e7cc97a87660
",['.circleci/scripts/vs_install.ps1'],"Windows workflows in GitHub Actions are installing Visual Studio unnecessarily, resulting in longer build times."
0d0d2f2ac596cda6bb785bbfe49447bbbe545d73,1631151168,"[PyTorch] move from input ivalues in ByteCodeDeserializer (#64029)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64029

This should save us a separate pass over the data structure to destroy it.
ghstack-source-id: 137566821

Test Plan:
Pixel3
before:
https://www.internalfb.com/intern/aibench/details/503337445067962
after:
https://our.intern.facebook.com/intern/aibench/details/320277034999340

overall mean time decreased from 373 ms to 358 ms. In flame graph, we
can see that some time spent destroying a vector of IValues was moved
into parseMethods, and the new parseMethods time is less than the old
time plus the recursive destruction time.

Reviewed By: dhruvbird

Differential Revision: D30559530

fbshipit-source-id: d080295a846745ea03ac50f08f4f6c95f4eaf3d8
",['torch/csrc/jit/mobile/import.cpp'],"The ByteCodeDeserializer in PyTorch requires a separate pass over the data structure to destroy it, causing additional time to be spent on ivalues destruction."
8c185e62f99c272bf991b8e0558fccb5f8c780c0,1613560340,"torchvision hipify revamp fix (#51453)

Summary:
The torchvision build error from hipify revamp, ""KeyError: '/usr/include/libpng16/png.h'"" is fixed in this PR

Description:

Traceback (most recent call last):
  File ""setup.py"", line 471, in <module>
    ext_modules=get_extensions(),
  File ""setup.py"", line 329, in get_extensions
    extra_compile_args=extra_compile_args
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 892, in CUDAExtension
    is_pytorch_extension=True,
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py"", line 978, in hipify
    clean_ctx=clean_ctx)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py"", line 212, in preprocess
    hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py"", line 175, in preprocess_file_and_save_result
    hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor
    output_source = RE_ANGLE_HEADER.sub(mk_repl('#include <{0}>', False), output_source)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py"", line 785, in repl
    value = HIPIFY_FINAL_RESULT[header_filepath][""hipified_path""]
KeyError: '/usr/include/libpng16/png.h'

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51453

Reviewed By: agolynski

Differential Revision: D26459979

Pulled By: fmassa

fbshipit-source-id: f653f55fd34c71314e6c6682217f84b2d1e49335
",['torch/utils/hipify/hipify_python.py'],The torchvision build is failing due to KeyError '/usr/include/libpng16/png.h' during the hipify process.
d51f6de9b8794aa1d5af6e2e4ea0ccf1a2d69f95,1664301988,"[quant][core][feature] Implement index_put for quantized CUDA tensors (#85685)

Summary:
- Add new cuda test for quantized index_put
- Add determinsitc test for CPU and CUDA quantized index_put
- Add in QuantizedCUDA implementation for index_put
    - wrote new `index_put_kernel_quantized_cuda`
    - CUDA index_put determinstic implemented in `index_put_with_sort_kernel_quantized`

I think quantize_val<scalar_t> is not CUDA compatible, because of the
reliance on std::numeric_limits. Might be something useful to add in the
future?

Test Plan:
```
python test/test_quantization.py -k test_qtensor_index_put
```

Reviewers:

Subscribers:

Tasks:

Tags: quant
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85685
Approved by: https://github.com/dzdang
","['aten/src/ATen/native/TensorAdvancedIndexing.cpp', 'aten/src/ATen/native/TensorAdvancedIndexing.h', 'aten/src/ATen/native/cuda/IndexKernel.cu', 'aten/src/ATen/native/cuda/Indexing.cu', 'aten/src/ATen/native/quantized/IndexKernel.h', 'aten/src/ATen/native/quantized/TensorAdvancedIndexing.cpp', 'test/quantization/core/test_quantized_tensor.py']","Quantized CUDA tensors do not support the 'index_put' operation, causing issues when trying to place specific values at given indices."
299dec1ca71f0fc1e570bfc054fab065df537495,1646764905,"[codemod][type-comments] Convert type comments in examples.py (#73085)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73085

I'm wrapping up the conversion of type comments to type annotations
in caffe2. The last remaining ""bulk"" codemod has test failures that
are hard for me to understand, so I'm going to submit PRs for each
module individually which makes it easier to see what's causing
problems.

All the codemods were produced via LibCST and then manually cleaned up.

Test Plan: Wait for github CI

Reviewed By: shannonzhu

Differential Revision: D34344276

fbshipit-source-id: f64edc13533a6f62fb278dd16fe68f74d89442a7
(cherry picked from commit 061c60e918169ac0006f73f27c4f2a7a83a76249)
",['torch/csrc/deploy/example/examples.py'],Conversion of type comments to type annotations in 'caffe2' module leads to challenging to comprehend test failures.
da0820e553a1ff89dbfd37c591154e8326748fab,1629496612,"add BFloat16 operators on CPU: range, sinh, cosh, frexp, nan_to_num (#61826)

Summary:
Added BFloat16 support for range, sinh, cosh, frexp, and nan_to_num on CPU, and collected the benchmark data of these OPs(range, sinh, cosh, frexp, and nan_to_num) for BFloat16 and Float32 data type by using the operator_benchmark tool of PyTorch on the platform of Intel(R) Xeon(R) Platinum 8180 CPU @ 2.50GHz

Number of cores: 1 core, 28 cores(1 socket)
[cosh_sinh_benchmark.txt](https://github.com/pytorch/pytorch/files/6974313/cosh_sinh_benchmark.txt)
[frexp_benchmark.txt](https://github.com/pytorch/pytorch/files/6974315/frexp_benchmark.txt)
[nan_to_num_benchmark.txt](https://github.com/pytorch/pytorch/files/6974317/nan_to_num_benchmark.txt)
[range_benchmark.txt](https://github.com/pytorch/pytorch/files/6974318/range_benchmark.txt)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61826

Reviewed By: saketh-are

Differential Revision: D30257259

Pulled By: VitalyFedyunin

fbshipit-source-id: 394cd713e6394050a8c90b2160633beb675d71dd
","['aten/src/ATen/native/RangeFactories.cpp', 'aten/src/ATen/native/cpu/UnaryOpsKernel.cpp', 'c10/util/BFloat16-math.h', 'torch/testing/_internal/common_methods_invocations.py']","BFloat16 data type lacks support for operators such as range, sinh, cosh, frexp, and nan_to_num on CPU in PyTorch."
6d4b93bd9648adf91692bb9dc6efb06a97a3f3b7,1632988532,"[quant] adding memoryless observers for embeddingbag QAT work (#65699)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65699

related to: https://github.com/pytorch/pytorch/pull/65443#discussion_r715132425

The QAT and PAT (pruning aware training) support for embedding bags needs a memoryless observer to work properly. This is necessitated by the changing pruned/non-pruned weights during training which can significantly change the quantization parameters.

This PR adds a memoryless flag to the simpler observer classes (not moving average since those explicitly have memory)

In addition to the above, I altered the reset_min_max_vals
function for MinMaxObserver so that it would preserve the device of the
existing self.min_val and self.max_val which was not preserved
previously compared to how it is initialized (using factory_kwargs)

Test Plan:
python test/test_quantization.py TestObserver

(added test_memoryless_minmaxobserver, test_memoryless_per_channel_minmaxobserver, test_memoryless_histogramobserver)

Imported from OSS

Reviewed By: supriyar

Differential Revision: D31209773

fbshipit-source-id: 44a63298e44880fbd3576f49ac568e781f3fd79a
","['test/quantization/core/test_workflow_module.py', 'torch/ao/quantization/observer.py']","Embedding bags in Quantization Aware Training (QAT) and Pruning Aware Training (PAT) requires memoryless observers which are currently lacking, leading to significant changes in quantization parameters due to dynamic pruned/non-pruned weights during the training process. Additionally, the reset_min_max_vals function for MinMaxObserver doesn't preserve the device as it should."
b7a5c793994258e605c30b3cd6d82d78e6129cf2,1675377739,"[inductor] Fix type inference in CPU masked operations (#93842)

Fixes #93351

The existing code guesses that `tmp3` is probably a `float`, and so truncates
any `double` values

```cpp
float tmp3 = 0.0;
if(tmp2)
{
    auto tmp4 = in_ptr0[i0];
    tmp3 = tmp4;
}
```

The proposed change is to generate a lambda expression that represents the body
of the masked operation, and infer the type from the return value:
```cpp
auto tmp3 = [&]
{
    auto tmp4 = in_ptr0[i0];
    return tmp4;
}
;
auto tmp5 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93842
Approved by: https://github.com/jgong5, https://github.com/Valentine233, https://github.com/jansel
","['test/inductor/test_torchinductor.py', 'test/inductor/test_torchinductor_opinfo.py', 'torch/_inductor/codegen/cpp.py']","CPU masked operations infer 'tmp3' as a float, truncating double values and possibly leading to precision loss or type mismatch issues."
6065e7a97cfad4c2ae2b8722969648a53265fa13,1694626216,"add Half support for BatchNorm on CPU (#102070)

Fixes #106543

### Testing

Single core:

shape | fp32 forward / ms | fp16 forward / ms | bf16 forward / ms | fp32 backward / ms | fp16 backward / ms | bf16 backward / ms
-- | -- | -- | -- | -- | -- | --
(1, 4, 256, 256) | 0.7116 | 0.1427 | 0.1744 | 0.2638 | 0.2002 | 0.2556
(1, 32, 100, 100) | 0.8579 | 0.1725 | 0.2077 | 0.3023 | 0.2399 | 0.2995
(32, 16, 200, 200) | 57.3466 | 12.2179 | 13.1320 | 45.9524 | 24.1526 | 24.9882

28 cores:

shape | fp32 forward / ms | fp16 forward / ms | bf16 forward / ms | fp32 backward / ms | fp16 backward / ms | bf16 backward / ms
-- | -- | -- | -- | -- | -- | --
(1, 4, 256, 256) | 0.2571 | 0.0713 | 0.0846 | 0.1140 | 0.0883 |  0.1043
(1, 32, 100, 100) | 0.1077 | 0.0510 | 0.0548 | 0.0700 | 0.0645 | 0.0713
(32, 16, 200, 200) | 5.5060 | 1.4195 | 1.4663 | 6.773 | 3.0886 | 3.1343

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102070
Approved by: https://github.com/jgong5, https://github.com/mikaylagawarecki
","['aten/src/ATen/native/Normalization.cpp', 'aten/src/ATen/native/cpu/batch_norm_kernel.cpp', 'aten/src/ATen/native/cpu/utils.h', 'test/onnx/test_fx_op_consistency.py', 'test/test_meta.py', 'test/test_mps.py', 'test/test_nn.py', 'torch/testing/_internal/common_methods_invocations.py']","BatchNorm functionality currently lacks half-precision support on CPU, requiring improvement in performance and execution time."
df665b1a9d7506ee147524d02fc4639a3b27c56f,1656692913,"[jiterator] Reduce templating in jitted_gpu_kernel_impl (#80103)

Previously, a new `jitted_gpu_kernel_impl` was instantiated for every
combination of kernel and data types. This adds a new intermediate,
`jitted_gpu_kernel_generic`, which is only templated on the arity of
the input function. So, the compiler is free to re-use this code
between different kernels. `UnaryOperators.cu` as an example will
only need to compile one version.

This is achieved by:
1. Hoisting static variables out of the `launch_` functions and into
   `JittedKernelVariantCache`, stored in `jitted_gpu_kernel_impl`,
   which is templated on the kernel name and dtypes.
2. Moving arguments describing the kernel's static properties
   (e.g. `name` and `f_inputs_type`) into runtime variables
   which are packaged into a new `jit::KernelDescriptor` struct.
3. changing `extra_args` from a tuple to `c10::ArrayRef<void*>`

We can expect benefits in both binary size and compile times. On my
build, I see an 11 MB reduction in binary size for `libtorch_cuda.so`
and this saving scales linearly with the number of jiterated kernels.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80103
Approved by: https://github.com/ngimel
","['aten/src/ATen/native/cuda/CUDAJitLoops.cuh', 'aten/src/ATen/native/cuda/MemoryAccess.cuh', 'aten/src/ATen/native/cuda/jit_utils.cpp', 'aten/src/ATen/native/cuda/jit_utils.h']","Existing `jitted_gpu_kernel_impl` is instantiated for every combination of kernel and data types, causing increased binary size and compile times."
5a1f8b85735b4537e4abaeb3dabd2453ff592d0d,1631064146,"Generalize expand logic (#63615)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63615

how to review: pretty much just check that the inputs generated are a good representation of the op semantics, that should be sufficient for correctness, and then you can also double check the op size semantics by going to https://codebrowser.bddppq.com/pytorch/pytorch/ typing in native::{op_name} and looking at the op implementation as a bonus if you want

Test Plan: Imported from OSS

Reviewed By: driazati

Differential Revision: D30738148

Pulled By: eellison

fbshipit-source-id: 4ef74a9c9b39c0beb73949e63aa844c46ab637eb
","['torch/csrc/jit/runtime/symbolic_shape_registry.cpp', 'torch/testing/_internal/common_methods_invocations.py']","The current logic for expanding operations doesn't handle all types of inputs correctly, potentially leading to incorrect operation semantics."
b5edf183343c8588bfb4105f7545525b644546be,1679679407,"`GradScaler` recomputes `optimizer_state[""found_inf_per_device""]` before `optimizer.step` (#97415)

I found a discrepancy between non-fused and fused optimizers, which is to use `optimizer_state[""found_inf""]` or to recompute `found_inf`.

- non fused: https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L289
- fused: https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L353
    - where `_check_inf_per_device` is https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L564-L573

The other way to align the behavior is to use the existing `found_inf` in https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L353.

I'd say this PR is for the sake of ""safety"" and the alternative is to keep the existing behavior.
I honestly have no idea if it's expected to double-check the sanity of gradients in `GradScaler.step`.

---

what I've observed in huggingface/transformers T5-base example so far seems like that non-fused optimizers lead to invalid parameters while the fused not.
The cause seems to be that `gradients` become inf/nan before `GradScaler.step(optimizer)` after `GradScaler._unscale_grads_` (more precicely, the call of `torch._amp_foreach_non_finite_check_and_unscale_`) in the script of the issue linked below, i.e. the gradient clipping and/or unscaling lead to inf/nan as these happen after the grad check. See
https://github.com/pytorch/pytorch/blob/788300cc2aa096d8d5c1e7fbfc87e5439a338251/aten/src/ATen/native/cuda/AmpKernels.cu#L165-L174.

Fixes #96755 🙏

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97415
Approved by: https://github.com/ngimel, https://github.com/janeyx99
","['test/test_cuda.py', 'torch/cuda/amp/grad_scaler.py']",Mismatch in handling 'found_inf' between fused and non-fused optimizers in GradScaler leading to invalid parameters due to gradients turning into inf/nan values before GradScaler.step(optimizer) from GradScaler._unscale_grads_.
9a0bb4b646f83ef5540120fcbe07daf99070db1a,1654665277,"[Vulkan] Fix bug in GRU op (#78945)

Summary:
All of the current Vulkan API Tests of the GRU op, use the same H_in and H_out sizes (H_in = 384 and H_out = 384). Which means, these tests don't test the behavior when H_in != H_out.

There is indeed a bug: H_in is used at some point to split the weights/biases when it should have been H_out (the hidden_size).

Differential Revision: D36895889

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78945
Approved by: https://github.com/SS-JIA
","['aten/src/ATen/native/vulkan/ops/Gru.cpp', 'aten/src/ATen/test/vulkan_api_test.cpp']",Vulkan API Tests for GRU op do not cover different H_in and H_out sizes resulting in a bug when splitting weights/biases.
4a1309035e4a1637b1e5957ea7aa96512431cf9e,1656543803,"[AutoAccept][Codemod][FBSourceBuckFormatLinter] Daily `arc lint --take BUCKFORMAT` (#80468)

Summary:
Meta:
**If you take no action, this diff will be automatically accepted on 2022-06-28.**
(To remove yourself from auto-accept diffs and just let them all land, add yourself to [this Butterfly rule](https://www.internalfb.com/butterfly/rule/904302247110220))

Produced by `tools/arcanist/lint/codemods/buckformat-fbsource`.

#nocancel

Rules run:
- CodemodTransformerSimpleShell

Config Oncall: [lint](https://our.intern.facebook.com/intern/oncall3/?shortname=lint)
CodemodConfig: [CodemodConfigFBSourceBuckFormatLinter](https://www.internalfb.com/code/www/flib/intern/codemod_service/config/fbsource_arc_f/CodemodConfigFBSourceBuckFormatLinter.php)
ConfigType: php
Sandcastle URL: https://www.internalfb.com/intern/sandcastle/job/9007199961796985/
This diff was automatically created with CodemodService.
To learn more about CodemodService, check out the [CodemodService wiki](https://fburl.com/CodemodService).

_____

## Questions / Comments / Feedback?

**[Click here to give feedback about this diff](https://www.internalfb.com/codemod_service/feedback?sandcastle_job_id=9007199961796985).**

* Returning back to author or abandoning this diff will only cause the diff to be regenerated in the future.
* Do **NOT** post in the CodemodService Feedback group about this specific diff.

drop-conflicts

Test Plan:
Meta:
No commands were run for this Codemod

Reviewed By: strulovich

Differential Revision: D37482777

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80468
Approved by: https://github.com/osalpekar
",['pt_template_srcs.bzl'],"The daily `arc lint --take BUCKFORMAT` operation might not be working properly, possibly leading to unexpected behavior or issues within the codemod service."
440f734169c1337dc84323adb1e88e11d7a72059,1666108435,"[inductor] Minifier fixes (#87062)

Fixes https://github.com/pytorch/torchdynamo/issues/1690

This fixes the error seen in the minifiers. But does not repro the original issue that prompted the above issue.

Fx minifiers work at the level of Fx-graphs, and the original issue lies outside of the Fx graph and is only visible on the second iteration. Therefore, the original issue escapes the abstraction of our existing Fx-based minifiers.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87062
Approved by: https://github.com/eellison
",['torch/_dynamo/debug_utils.py'],"Errors are occurring in the Fx-based minifiers, with an original issue manifesting outside of the Fx graph, only becoming visible on the second iteration and thus escaping the abstraction of the existing Fx minifiers."
346dc88bfa271403aab20affd5f8806c4370a06b,1620938569,"[ONNX] Support registering custom export for prim::PythonOp from torch.autograd.Function (#55630) (#57600)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57600

Demo script:

```python
import torch

class MyReLU(torch.autograd.Function):
    staticmethod
    def forward(ctx, input, scalar_tuple, scalar, scalar_list):
        ctx.save_for_backward(input)
        return input.clamp(min=scalar)
    staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_a = torch.nn.Linear(2, 2)
        self.linear_b = torch.nn.Linear(2, 2)
        self.relu = MyReLU.apply
    def forward(self, x):
        h = self.linear_a(x)
        h = self.relu(h, (5, 3), 2, [1, 2, 3])
        h = self.linear_b(h)
        return h

""""""
User define how to export prim::PythonOp into custom op.
""""""
def symbolic_pythonop(g, n, *args, **kwargs):
    # Print information:
    print('arguments of ', kwargs['name'], ':')
    print('original node: ', n)
    for i, out in enumerate(n.outputs()):
        print('original output {}: {}, requires grad: {}'.format(i, out, out.requiresGrad()))
    import torch.onnx.symbolic_helper as sym_helper
    for i, arg in enumerate(args):
        print('arg {}: {}, requires grad: {}'.format(i, arg, arg.requiresGrad() if sym_helper._is_value(arg) else False))
    for k, v in kwargs.items():
        print('key: ', k, ' v: ', v)

    # TODO: all inputs (tensors and scalars) are in args.
    #       backend can define CustomDomain::PythonOp and how info are stored however it deem fit.
    return g.op(""CustomDomain::PythonOp"", args[0], name_s=kwargs['name'])

torch.onnx.register_custom_op_symbolic(""::prim_PythonOp"", symbolic_pythonop, 9)

# Define input.
x = torch.tensor([[0.3971, 0.7544],
                  [0.5695, 0.4388]], requires_grad=True)

model = MyModule()
# Forward.
y = model(x)

torch.onnx.export(model, (x,), 'model.onnx', opset_version=12, verbose=True)
```

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D28393528

Pulled By: SplitInfinity

fbshipit-source-id: e0d55b7c737c5916fda08a3b26b3306037f970df

Co-authored-by: BowenBao <bowbao@microsoft.com>
","['test/onnx/test_custom_ops.py', 'test/onnx/test_models.py', 'torch/csrc/jit/passes/onnx.cpp', 'torch/csrc/jit/python/python_ir.cpp', 'torch/onnx/utils.py']","Custom export for 'prim::PythonOp' from 'torch.autograd.Function' in ONNX is not supported, causing issues while defining and exporting custom operations."
3d9706c46408ab9b1b22e322ef8f6608fa8942e4,1645669985,"Prefix c10d log messages with `[c10d]` for easier troubleshooting (#73144)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73144

This PR formats c10d log messages written by the `C10D_INFO/WARN/ERROR` macros by prefixing them with the `[c10d]` tag for easier troubleshooting. See #73121 for a specific customer request.

Note though that this is a temporary fix to unblock our users. Ideally our global logging facility should natively support component-based preambles.
ghstack-source-id: 149748943

Test Plan: N/A

Reviewed By: rohan-varma

Differential Revision: D34363975

fbshipit-source-id: 6b8096ac4b2fa344406c866a2e7665541cb60b34
(cherry picked from commit af14aef18d0239f04730545596a05536e0f9c857)
",['torch/csrc/distributed/c10d/logging.h'],Log messages created by c10d's `C10D_INFO/WARN/ERROR` macros are hard to identify and filter for troubleshooting.
ce7041d25793cc414b35e03838e92606adef6ee8,1647440548,"[Quant][core][refactorization] Refactored qconv_unpack.cpp into an implementation file and higher level call registration and definition file (Reland #73773) (#74227)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74227

This a reland of #73773 --
please look at this PR directly for a summary and test plan.

(cherry picked from commit 5ee42da0957aa6eb2e156b5fac24fee4c63a52c4)

Test Plan: Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D34886987

Pulled By: dzdang

fbshipit-source-id: 4a1f4580cdc12db12af55719debf55dbcd00cac3
(cherry picked from commit c8903f16b08ee06a35c70b43328379515ba7610a)
","['aten/src/ATen/native/quantized/cpu/qconv_unpack_impl.cpp', 'aten/src/ATen/native/quantized/qconv_unpack.cpp', 'tools/build_variables.bzl']","The codebase for qconv_unpack.cpp is not modular, causing difficulties in comprehension and maintenance."
f82f14de1714bd92ddc830b774e40eea80194f09,1637305760,"[libkineto] Refactor 4/n: Simplify activity logger step 2/3 (#68329)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68329

Pull Request resolved: https://github.com/pytorch/kineto/pull/466

1. Generalize ChromeTraceLogger::handleGenericActivity to enable it to handle Cuda runtime activities as well as the Roctracer generic activities.
This primarily involves enabling generic support for CPU -> GPU flows.

2. In the event of out-of-order GPU activities (an issue with Cuda11.0, likely fixed in later versions), no longer remove them but print warnings. Another diff will add these warnings to the metadata section.

Reviewed By: briancoutinho

Differential Revision: D31624496

fbshipit-source-id: dab04b3e3c0dd6799496ac87f837363de79eea25
",['torch/csrc/autograd/profiler_kineto.cpp'],"The activity logger struggles with handling generic CPU->GPU flows and out-of-order GPU activities, particularly an issue in Cuda 11.0, causing unnecessary removal of these activities."
e8d2916b8414257175d985bd549ff38935b2eefe,1626814470,"Add faulty tensorpipe implementation (#61421)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61421

This PR adds the faulty tensorpipe agent implementation and replaces all faulty process group agent tests with it. The faulty tensorpipe agent code is very similar to that of faulty process group agent. It allows the user to fail or delay certain types of rpc messages, which is used in the faulty agent tests. These changes are needed to deprecate the process group rpc backend.

Summary of changes:
- Add faulty tensorpipe agent class
- Update tensorpipe pipeWrite function to allow to be overwritten and add delay
- Update test backend registry and faulty agent tests to use the FAULTY_TENSORPIPE_AGENT backend.

This effects all faulty agent tests, here a few of them as sample commands:
`pytest test/distributed/rpc/test_faulty_agent.py -vs -k test_verify_backend_options`
`pytest test/distributed/rpc/test_faulty_agent.py -vs -k test_no_faulty_messages`
`pytest test/distributed/rpc/test_faulty_agent.py -vs -k test_builtin_remote_message_dropped_timeout`

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D29773739

Pulled By: H-Huang

fbshipit-source-id: 6b2bc366735d70b79943d4207f454bc9555bbf5f
","['tools/build_variables.bzl', 'torch/csrc/distributed/rpc/tensorpipe_agent.h', 'torch/csrc/distributed/rpc/testing/faulty_tensorpipe_agent.cpp', 'torch/csrc/distributed/rpc/testing/faulty_tensorpipe_agent.h', 'torch/csrc/distributed/rpc/testing/init.cpp', 'torch/distributed/rpc/_testing/__init__.py', 'torch/distributed/rpc/_testing/faulty_agent_backend_registry.py', 'torch/testing/_internal/dist_utils.py', 'torch/testing/_internal/distributed/rpc/dist_autograd_test.py', 'torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py', 'torch/testing/_internal/distributed/rpc/rpc_test.py']",Faulty Process Group Agent tests require an update as they intend to deprecate the process group RPC backend. The current setup fails or delays certain types of RPC messages and lacks a suitable testing backend.
8db30255c36fc7a93d8d5285415d7ab96911e1df,1665521758,"[ROCm] set nvfuser default to disabled, keep CI (#86369)

Bug fix. nvfuser is functional for ROCm on gfx906, but some tests are failing for other gfx targets. Disable nvfuser until all features are verified. Users may still opt-in by setting the known env var PYTORCH_JIT_ENABLE_NVFUSER=1. This PR sets this env var for the github actions workflow for ROCm since all current CI hosts are gfx906.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86369
Approved by: https://github.com/huydhn
",['torch/csrc/jit/codegen/cuda/interface.cpp'],The nvfuser in ROCm fails on some gfx targets other than gfx906 causing some test failures.
d33066ab3ff0d292d3930ad9b4a36bd6910c088c,1625700433,"adding a build_start_time_epoch to build meta info (#61322)

Summary:
Adding a `build_start_time_epoch` as a normal field in scribe reporting.
This should fix https://github.com/pytorch/pytorch/issues/60591.

The decision was made because:
- we would like only one build (test CI job) start time as partition key string
  - the alternative is to report the duration on each test case individually which would result in duplicate numeric value upload.
- we would be easily calculate the wall-time of a test job from `MAX('time') - build_start_time_epoch` for all reporting messages with the same normal keys.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61322

Test Plan:
CI should report the extra normal field.

See: https://fburl.com/scuba/pytorch_test_times/pm6chz9w

Reviewed By: driazati

Differential Revision: D29589020

Pulled By: walterddr

fbshipit-source-id: 309fc3b01cbce76cd62f8ccd2eb0ecad27782b88
",['tools/stats/print_test_stats.py'],"The partition key string lacks a consistent build start time, causing issues with calculating the wall-time of a test job and potentially leading to duplicated numeric value upload."
f1eedfa2c865e2c5f7c4fa67fd22d952af271306,1614918906,"[package] Add `allow_empty` flag to mock and extern (#53232)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53232

**Summary**
This commit adds an optional `allow_empty` argument to
`PackageExporter.mock` and `PackageExporter.extern` that allows certain
patterns for mocked modules and extern modules to be marked ones that
*must* be matched during the packaging process. If a mock or extern
module with `allow_empty=False` is not matched while packaging, an error
is thrown.

**Test Plan**
This commit adds two new test cases to `PackagingTest`,
`test_extern_glob_allow_empty` and `test_mock_glob_allow_empty` that
test this new flag. Existing tests already tests `allow_empty=True`.

**Fixes**
This commit fixes #53217.

Test Plan: Imported from OSS

Reviewed By: suo

Differential Revision: D26834011

Pulled By: SplitInfinity

fbshipit-source-id: 9cf4ea56079ae210d6cfa8604218849eb5cde5f4
","['test/test_package.py', 'torch/package/__init__.py', 'torch/package/package_exporter.py']","The packaging process does not highlight if a pattern for mocked modules and extern modules is not matched during packaging, leading to unnoticed errors."
d6d485fa8c55dd2763e3f70e12fb9561d5c0d276,1692514227,"Revamp guard debug logging (#107505)

The new guard printout looks like this:

```
[DEBUG] GUARDS:
[DEBUG]   ___check_type_id(L['name'], 7605632)                          # if name == ""special_attr"":  # test/dynamo/test_misc.py:1155 in __getattribute__
[DEBUG]   L['name'] == '_backward_pre_hooks'                            # if name == ""special_attr"":  # test/dynamo/test_misc.py:1155 in __getattribute__
[DEBUG]   ___check_obj_id(L['self'], 139746432564960)                   # return super().__getattribute__(name)  # test/dynamo/test_misc.py:1157 in __getattribute__
[DEBUG]   ___check_obj_id(L['__class__'], 1451499216)                   # return super().__getattribute__(name)  # test/dynamo/test_misc.py:1157 in __getattribute__
[DEBUG]   ___is_grad_enabled()                                          # _dynamo/output_graph.py:346 in init_ambient_guards
[DEBUG]   not ___are_deterministic_algorithms_enabled()                 # _dynamo/output_graph.py:342 in init_ambient_guards
[DEBUG]   ___is_torch_function_enabled()                                # _dynamo/output_graph.py:350 in init_ambient_guards
[DEBUG]   utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:348 in init_ambient_guards
```

Along with the guards, we also print what line of user code caused the guard to be added, or what line of Dynamo internal code added the guard (if there is no user stack trace, which is typically the case for ambient guards.)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107505
Approved by: https://github.com/mlazos, https://github.com/voznesenskym, https://github.com/anijain2305
","['torch/_dynamo/convert_frame.py', 'torch/_dynamo/guards.py', 'torch/_dynamo/output_graph.py', 'torch/_dynamo/symbolic_convert.py', 'torch/_guards.py', 'torch/utils/_traceback.py']","Guard debug logging lacking detail, making problem identification difficult. Specifically, it's unclear which line of user or internal code causes the addition of a guard."
f0fa3d1110fdd085b040ef82c8a547b205b98e38,1633787156,"Create separate documentation pages for quantization observers and fake_quants (#66125)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66125

Before this PR, the documentation for observers and fake_quants was inlined in the
Eager mode quantization page.  This was hard to discover, especially
since that page is really long, and we now have FX graph mode quantization reusing
all of this code.

This PR moves observers and fake_quants into their own documentation pages. It also
adds docstrings to all user facing module attributes such as the default observers
and fake_quants, so people can discover them from documentation without having
to inspect the source code.

For now, enables autoformatting (which means all public classes, functions, members
with docstrings will get docs).  If we need to exclude something in these files from
docs in the future, we can go back to manual docs.

Test Plan:
```
cd docs
make html
python -m server.http
// inspect docs on localhost, renders correctly
```

Reviewed By: dagitses

Differential Revision: D31447613

Pulled By: vkuzo

fbshipit-source-id: 63b4cf518badfb29ede583a5c2ca823f572c8599
","['torch/ao/quantization/fake_quantize.py', 'torch/ao/quantization/observer.py']",The documentation for observers and fake_quants is hard to discover and cumbersome as it is currently inlined in the Eager mode quantization page.
909694fd884bc092182b527358184a0c756128d8,1634571278,"Fix `nn.functional.max_poolNd` dispatch (for arg: `return_indices`) (#62544)

Summary:
Please see https://github.com/pytorch/pytorch/issues/62545 for context.

The order of `return_indices, ceil_mode` is different for `nn.functional.max_poolNd` functions to what seen with `torch.nn.MaxPoolNd` (modular form). While this should be resolved in the future, it was decided to first raise a warning that the behavior will be changed in the future. (please see https://github.com/pytorch/pytorch/pull/62544#issuecomment-893770955 for more context)

This PR thus raises appropriate warnings and updates the documentation to show the full signature (along with a note) for `torch.nn.functional.max_poolNd` functions.

**Quick links:**

(_upstream_)

* Documentation of [`nn.functional.max_pool1d`](https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.max_pool1d.html), [`nn.functional.max_pool2d`](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html), and [`nn.functional.max_pool3d`](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool3d.html).

(_this branch_)

* Documentation of [`nn.functional.max_pool1d`](https://docs-preview.pytorch.org/62544/generated/torch.nn.functional.max_pool1d.html?highlight=max_pool1d), [`nn.functional.max_pool2d`](https://docs-preview.pytorch.org/62544/generated/torch.nn.functional.max_pool2d.html?highlight=max_pool2d#torch.nn.functional.max_pool2d), and [`nn.functional.max_pool3d`](https://docs-preview.pytorch.org/62544/generated/torch.nn.functional.max_pool3d.html?highlight=max_pool3d#torch.nn.functional.max_pool3d).

cc mruberry jbschlosser

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62544

Reviewed By: gchanan

Differential Revision: D31179038

Pulled By: jbschlosser

fbshipit-source-id: 0a2c7215df9e132ce9ec51448c5b3c90bbc69030
","['test/test_fx.py', 'torch/nn/functional.py']","The order of `return_indices, ceil_mode` arguments in `nn.functional.max_poolNd` functions is inconsistent with `torch.nn.MaxPoolNd`, leading to unexpected results."
0351e2042b62f828f5adb3d553cfa9a92e8616c5,1695315854,"Avoid throwing exception in ClosingTHPObjectPtr (#109758)

Previously, if ClosingTHPObjectPtr was destructed because we
were unwinding the stack from an exception, we would attempt to call
close() which just isn't going to work.  Two fixes:

1. Detect if we're unwinding due to a Python error, and don't try
   to do more Python stuff if so.

2. If close() fails somehow, write an unraisable exception, don't
   try to throw because that will terminate if you're in an
   exception.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109758
Approved by: https://github.com/jansel
",['torch/csrc/dynamo/python_compiled_autograd.cpp'],Attempting to call close() while destructing ClosingTHPObjectPtr during stack unwinding from an exception leads to system failure.
065de4301200117c65607b213322cbde800b50f8,1678418466,"Fixing a bug where allocating a 4GB block results in using 8GB of memory (#95827)

I added two constants. First helps with avoiding rounding while we hit a certain threshold, and second, to control what blocks can be cached.

Allocations larger than `kMaxRoundThreshold` will not be rounded to the next power of two anymore. Generally it is expected that larger allocations happen less frequently, and this more or less matches what happens in `CudaCachingAllocator`.

Blocks larger than `kMaxCachedSize` will not be cached. This is a separate problem than the above but I noticed this caching is poorly implemented here and doesn't do anything to avoid fragmentation or to help with good resource utilization. For example, the following allocations:
```
t1 = alloc(4GB)
del t1
t2 = alloc(10k)
t3 = alloc(4GB)
```
this results in allocating 8GB, because the first 4GB block that is cached gets assigned to the 10k allocation wasting the rest of the block.

Lastly, ideally I would make this constants configurable, but looking around the code I didn't see any existing mechanisms in ATen to configure things at runtime.

Fixes #95823

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95827
Approved by: https://github.com/ngimel
",['aten/src/ATen/cuda/CachingHostAllocator.cpp'],"Allocating a 4GB block results in using 8GB of memory due to inappropriate caching and rounding logic for large allocations, leading to inefficient resource utilisation."
d3a176a156819d57ed442579b806ff027402f4dc,1660710761,"[PT-D][BE][TP perf 1/N] Get rid of unnecessary collectives in Embedding/EmbeddingBag and use autograd-enabled collectives (#81853)

These two ops (Embedding and EmbeddingBag for ShardedTensor) especially for row-wise sharding is very inefficient and hard to fit in the concept of future design. So this PR is trying to:
1. Remove all unnecessary collective communications. Only one gather and one reduce(or reduce scatter) is needed.
2. Use auto-grad enabled collectives so that we can use these ops in real model training.
3. Some minor code cleaning
4. Treat input differently when it's replicated tensor. (Will add more for this for the next few PRs).

Differential Revision: [D37965687](https://our.internmc.facebook.com/intern/diff/D37965687/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81853
Approved by: https://github.com/wanchaol
","['test/distributed/_shard/sharded_tensor/ops/test_embedding.py', 'test/distributed/_shard/sharded_tensor/ops/test_embedding_bag.py', 'torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/_common.py', 'torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding.py', 'torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py']","The Embedding and EmbeddingBag operations for ShardedTensor, especially with row-wise sharding, are currently inefficient and challenging to accommodate in future designs. This is due to unnecessary collective communications and lack of autograd-enabled collectives."
49450fe021cc5d439f56580463461ff438f9ac96,1686207425,"add github action to upload alerts to rockset / aws (#102995)

Successful test run found at Test run found at https://github.com/pytorch/pytorch/actions/runs/5179855118/jobs/9333292038 (uses equivalent PRs)

<!--
copilot:summary
-->
### <samp>🤖 Generated by Copilot at 8d7d860</samp>

This pull request adds a new feature to create and upload alerts for failing jobs in the pytorch/pytorch repo. It introduces a new script `tools/alerts/create_alerts.py` to generate alert entries and a new workflow `.github/workflows/upload-alerts.yml` to run the script and upload the alerts periodically.
<!--
copilot:poem
-->
### <samp>🤖 Generated by Copilot at 8d7d860</samp>

> _To upload alerts to Rockset_
> _We added a workflow, you bet_
> _It runs every ten_
> _With concurrency then_
> _And `create_alerts.py` we edit_
Pull Request resolved: https://github.com/pytorch/pytorch/pull/102995
Approved by: https://github.com/huydhn, https://github.com/ZainRizvi
",['tools/alerts/create_alerts.py'],"The repository currently lacks periodic alerts for failing jobs, impeding efficient error tracking and responses."
79e7544cb4ece28032fdeccf01778222cbbe99cd,1612269024,"[Gradient Compression] Check start_PowerSGD_iter > 1 and add guidance on tuning PowerSGD configs. (#51427)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51427

A user reported that `start_PowerSGD_iter` failed when it's set as 1. This is because allocating memory for error tensors somehow overlap with bucket rebuilding process at iteration 1.

Check `start_PowerSGD_iter > 1` instead of `start_PowerSGD_iter >= 1`.

Also add a unit test of `test_invalid_powerSGD_state` and some guidance on tuning PowerSGD configs.

Original PR issue: Investigate Applying PowerSGD to Communication Hook for Gradient Compression #47202
ghstack-source-id: 120834126

Test Plan: buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_invalid_powerSGD_state

Reviewed By: rohan-varma

Differential Revision: D26166897

fbshipit-source-id: 34d5b64bb3dd43acb61d792626c70e6c8bb44a5d
","['test/distributed/test_c10d.py', 'torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py']",Setting `start_PowerSGD_iter` as 1 fails due to overlapping of memory allocation for error tensors with bucket rebuilding process at the first iteration.
c377a8590b9b654a2cf03c005f5171ca63c38534,1681190016,"Add `nonzero_static()` op to pytorch to unblock export (#97417)

Summary: Add new experimental python op (`torch.nonzero_static`) for export. There is NO cuda impl included in this PR

Example:

Say input tensor is `x = torch.tensor([[1, 0], [3, 2]])`

call regular `nonzero()` on x will give you a tensor `tensor([[0, 0], [1, 0], [1, 1])`
call `nonzero_static(x, size=4)` on x will give you a tensor `tensor([[0, 0], [1, 0], [1, 1], [fill_value, fill_value])` (padded)
call `nonzero_static(x, size=2)` on x will give you a tensor `tensor([[0, 0], [1, 0])` (truncated)

Test Plan:
**Unit Tests**
```
buck test @mode/dev-nosan //caffe2/test:test_dynamo -- 'caffe2/test:test_dynamo - test_export.py::ExportTests::test_export_with_nonzero_static' -- 'caffe2/test:test_dynamo - test_misc.py::MiscTests::test_nonzero_static'
```

**PT2 Export with `nonzero_static()`**
Example of `GraphModule` in the exported graph
```
def forward(self, x):
    arg0, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
    nonzero_static_default = torch.ops.aten.nonzero_static.default(arg0, size = 4);  arg0 = None
    return pytree.tree_unflatten([nonzero_static_default], self._out_spec)
```

Differential Revision: D44324808

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97417
Approved by: https://github.com/ezyang
","['aten/src/ATen/native/TensorAdvancedIndexing.cpp', 'test/dynamo/test_export.py', 'test/dynamo/test_misc.py', 'test/test_mps.py', 'torch/_meta_registrations.py', 'torch/_tensor_docs.py', 'torch/overrides.py', 'torch/testing/_internal/common_methods_invocations.py']",The PyTorch library lacks a feature to perform a `nonzero` operation on static tensors that could be necessary for specific tensor manipulations during export.
de3a4eb583f1a63a45333359f225628c3b0bbb7e,1627405225,"Migrate thnn_conv_depthwise2d from THC to ATen (#62006)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62006

Closes gh-24646, gh-24647

There is no `TensorIterator` equivalent to these kernels so this is just
migrating the existing kernels over to the ATen style.

I've benchmarked for contiguous tensors with this script:
```
import torch
shape = (10, 10, 100, 100)
x = torch.randn(*shape, device='cuda')
w = torch.randn((10, 1, 5, 5), device='cuda')

for _ in range(100):
    torch.nn.functional.conv2d(x, w, groups=10)
```

and similarly for backwards. I see these as the same to within measurement error.

|                   | Master Forward (us) | This PR Forward (us) |
|------------------:|:-------------------:|:--------------------:|
|           Forward |        133.5        |         133.6        |
|  Backward (input) |        1,102        |         1,119        |
| Backward (weight) |        2,220        |         2,217        |

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D29883676

Pulled By: ngimel

fbshipit-source-id: 9b2ac62cdd8a84e1a23ffcd66035b2b2fe2374d8
","['aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp', 'aten/src/ATen/native/ConvUtils.h', 'aten/src/ATen/native/Convolution.cpp', 'aten/src/ATen/native/LegacyNNDefinitions.cpp', 'aten/src/ATen/native/cuda/DepthwiseConv2d.cu', 'aten/src/THCUNN/SpatialDepthwiseConvolution.cu', 'aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu', 'aten/src/THCUNN/generic/THCUNN.h', 'test/test_nn.py', 'tools/build_variables.bzl']","The thnn_conv_depthwise2d function in PyTorch is currently implemented in THC, not supporting ATen style and lacking an equivalent in TensorIterator."
76efbccc3b970c391ad147f29af960a6f028014e,1634289585,"[PyTorch Edge][tracing-based] Unify tracer between internal and external (#64152)

Summary:
As title, introduce the file `TracerRunner` shared by internal/external tracer and the main function is
```
TracerResult trace_run(const std::string& input_module_path);
```
which basically takes the path to model file and generate the trace result. The main difference between external tracer and internal tracer is
1. the dependency on `<yaml-cpp/yaml.h>`.
2. the output yaml file from internal tracer includes `model_version` and `model_asset`. These are only needed for internal.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64152

ghstack-source-id: 140692467

Test Plan:
```
./build/bin/model_tracer --model_input_path ""/Users/chenlai/Documents/pytorch/tracing/deeplabv3_scripted_with_bundled_input.ptl"" --build_yaml_path  ""/Users/chenlai/Documents/pytorch/tracing/tmp.yaml""
```
```
./fbcode/caffe2/fb/model_tracer/run_model_with_bundled_inputs.sh ~/local/notebooks/prod_models/deeplabv3_scripted_with_bundled_input.ptl
```
have the same operator output

selected_operators.yaml (P460296279)
selected_mobile_ops.h (P460296258)

Reviewed By: dhruvbird

Differential Revision: D30632224

fbshipit-source-id: eb0321dbc0f1fcf6d2e05384695eebb59ac04f8c
","['tools/build_variables.bzl', 'torch/csrc/jit/mobile/model_tracer/TracerRunner.cpp', 'torch/csrc/jit/mobile/model_tracer/TracerRunner.h', 'torch/csrc/jit/mobile/model_tracer/tracer.cpp']","Differences between internal and external tracer in PyTorch Edge hinder unified tracing, causing both dependency and output differences."
f8297d40fc0f40f96d369ba6c50a69764b202f04,1636447387,"Adds a `maximize` flag to SGD. (#67847)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/46480 -- for SGD.

## Notes:
- I have modified the existing tests to take a new `constructor_accepts_maximize` flag. When this is set to true, the ` _test_basic_cases_template` function will test both maximizing and minimizing the sample function.
- This was the clearest way I could think of testing the changes -- I would appreciate feedback on this strategy.

## Work to be done:
[] I need to update the docs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67847

Reviewed By: H-Huang

Differential Revision: D32252631

Pulled By: albanD

fbshipit-source-id: 27915a3cc2d18b7e4d17bfc2d666fe7d2cfdf9a4
","['test/distributed/optim/test_zero_redundancy_optimizer.py', 'test/test_optim.py', 'torch/distributed/optim/functional_sgd.py', 'torch/optim/_functional.py', 'torch/optim/_multi_tensor/sgd.py', 'torch/optim/optimizer.py', 'torch/optim/sgd.py']","SGD (Stochastic Gradient Descent) algorithm in PyTorch does not currently support maximization target, only minimization."
06f14c2d63b819a276ead7b12d9d387a9e47764a,1642699988,"Refactor convolution_backward's CudaDepthwise3d case (#71490)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71490

Deleted unnecessary .contiguous() calls in convolution_backward. The
CudaDepthwise3d case always hits _depthwise_3d_backward_cuda_out,
which will make arguments contiguous as necessary.

Changed _depthwise_3d_backward_cuda_out
- to make the input contiguous only when we're computing grad_weight
- to make the weight contiguous only when we're computing grad_input

Test Plan: - pytest test/test_nn.py -v -k ""conv""

Reviewed By: jbschlosser

Differential Revision: D33664696

Pulled By: zou3519

fbshipit-source-id: d01d4f213e21ef4778de089a158933737b191cdf
(cherry picked from commit c6eb977c94a07f9812567a43b125b453eb5c5051)
","['aten/src/ATen/native/Convolution.cpp', 'aten/src/ATen/native/cuda/DepthwiseConv3d.cu']","Unnecessary calls to "".contiguous()"" in convolution_backward for the CudaDepthwise3d case may be causing inefficiencies, as these calls are performed even when not necessarily required."
0df2e863fbd5993a7b9e652910792bd21a516ff3,1650143298,"[Profiler] Expose `profilerType` in Python

Summary: It's currently possible for C++ callers to check if there is an active profiler. This adds Python API parity. For now we just use `torch._C._autograd` namespace, as this is mostly for first party frameworks like RPC. (We can always move to public API if there is demand.)

Test Plan: Added unit test

Differential Revision: D35602425

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75754
Approved by: https://github.com/rohan-varma
","['test/test_profiler.py', 'torch/csrc/autograd/init.cpp']","Python callers cannot check if there is an active profiler, leading to a lack of feature parity with C++."
b8057aa16d376eefe081d852335541e2a7609c40,1673667483,"Remove unnecessary copies of Scalars for TensorBody template (#92162)

Inspired by #92156 , I realized our generated TensorBody.h has many methods that do an unnecessary copies. Scalar is backed by a ptr and is therefore not trivially copyable and care should be assigned over ownership of the params. Since it's a template, clang-tidy was never run on it in a way that was able to propogate the changes back to the source code.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92162
Approved by: https://github.com/ezyang
",['aten/src/ATen/templates/TensorBody.h'],"The TensorBody.h methods in template generate numerous unnecessary copies of Scalars, undermining efficient ownership of parameters and leading to potential issues with memory management."
7bb4b683b57d5ee48edc9c62f4d2752fae392e40,1639607372,"Codegen: Registration now only includes the functions used (#68689)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68689

Currently Register{DispatchKey}.cpp includes all of
`NativeFunctions.h`, so any operator signature change requires all
backend registration to be recompiled. However, most backends only
have registrations for a small fraction of operators so it makes sense
to only include the specific functions required.

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D32596273

Pulled By: albanD

fbshipit-source-id: 11d511f47937fbd5ff9f677c9914277b5d015c25
","['aten/src/ATen/native/Resize.h', 'aten/src/ATen/native/ResizeCommon.h', 'aten/src/ATen/native/SortingUtils.h', 'aten/src/ATen/native/sparse/SparseBlas.cpp', 'aten/src/ATen/templates/Function.h', 'aten/src/ATen/templates/RegisterDispatchKey.cpp', 'aten/src/ATen/templates/TensorBody.h', 'tools/codegen/dest/__init__.py', 'tools/codegen/dest/register_dispatch_key.py', 'tools/codegen/gen.py', 'tools/codegen/gen_backend_stubs.py']","Any operator signature change in NativeFunctions.h unnecessarily triggers recompilation of all backend registrations, even if a backend only uses a small fraction of the operators."
68b518c13e128997a0c7c9ab8ce9508cc4062e3a,1693333511,"Add check for out of range pointer. (#107510)

### Summary

Hi! We've been fuzzing pytorch with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz) and found an error of accessing arbitary address while parsing flatbuffer format using `torch::load` function.

pytorch version: 18bcf62bbcf7ffd47e3bcf2596f72aa07a07d65f (the last commit at the moment of reporting the issue)

### Details
The vulnerability appears while loading arbitrary user input using `torch::load` function. To detect the error the input must correspond to FlatbufferFileFormat, so the part of parsing flatbuffer in `import_ir_module` function must be executed.

Firstly error can occur in `GetMutableRoot` in `module.h`, where we add pointer to input data buffer with the value, got from dereference of this pointer (which data fully depends on the user input and can be arbitrary). so the resulting `flatbuffer_module` address can be corrupted.

Moreover, we can get the arbitrary address later at `flatbuffer_loader.cpp:305`, when we get `ival` pointer with `Get` method.
There in `IndirectHelper::Read` function we add pointer with the offset got from the dereference of this pointer, so the address can be corrupted again.

The corrupted `ival` pointer is dereferenced at `table.h` in flatbuffers project, where is used to get another address, which is later dereferenced again at `table.h` in flatbuffers project. The resulting corrupted address is written to `func` pointer at `flatbuffer_loader.cpp:274`, which is then used in `parseFunction`, where write access to the address occurs.

To fix the problem we can compute the end of memory area in `parse_and_initialize_mobile_module` function like this:
```
auto* end = static_cast<char*>(data) + size;
```
And then pass it to all the callees and insert corresponding checks.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107510
Approved by: https://github.com/albanD
",['torch/csrc/jit/mobile/flatbuffer_loader.cpp'],"Accessing arbitrary address while parsing flatbuffer format using `torch::load` function in PyTorch can cause an error, potentially allowing out of bound memory access due to pointer corruption."
c1415a0a72a09e16a93bb1900eb1a8541bb448d6,1631899918,"[Reland] [Model Averaging] Simplify PostLocalSGD Optimizer API (#65197)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65197

1. The constructor accepts a local optimizer instance instead of the inputs of local optimizer constructor and the class type.
2. The parameters are read from local optimizer's param_groups instead of a separate input.

Proposal: https://github.com/pytorch/pytorch/issues/59699
ghstack-source-id: 138307226

Test Plan: buck test mode/dev-nosan //caffe2/test/distributed:distributed_nccl_spawn -- test_post_localSGD_optimizer_parity

Reviewed By: rohan-varma

Differential Revision: D31007439

fbshipit-source-id: bbb0526e6763ef76775b85088571506b3942c722
","['torch/distributed/algorithms/model_averaging/utils.py', 'torch/distributed/optim/post_localSGD_optimizer.py', 'torch/testing/_internal/distributed/distributed_test.py']","Model Averaging PostLocalSGD Optimizer API is overly complex, requiring the class type and inputs of the local optimizer constructor and a separate input for parameters."
157a3d2a7cd25779258f3e3dcef14633f1930103,1665725928,"[Profiler] Move legacy profiler out of `torch/csrc/autograd` (#85512)

The legacy profiler is an eyesore in the autograd folder. At this point the implementation is almost completely decoupled from the rest of profiler, and it is in maintaince mode pending deprecation.

As a result, I'm moving it to `torch/csrc/profiler/standalone`. Unfortuantely BC requires that the symbols remain in `torch::autograd::profiler`, so I've put some basic forwarding logic in `torch/csrc/autograd/profiler.h`.

One strange bit is that `profiler_legacy.h` forward declares `torch::autograd::Node`, but doesn't seem to do anything with it. I think we can delete it, but I want to test to make sure.

(Note: this should not land until https://github.com/pytorch/torchrec/pull/595 is landed.)

Differential Revision: [D39108648](https://our.internmc.facebook.com/intern/diff/D39108648/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85512
Approved by: https://github.com/aaronenyeshi
","['build_variables.bzl', 'setup.py', 'torch/csrc/autograd/profiler.h', 'torch/csrc/autograd/profiler_kineto.cpp', 'torch/csrc/jit/mobile/profiler_edge.h', 'torch/csrc/profiler/api.h', 'torch/csrc/profiler/kineto_shim.cpp', 'torch/csrc/profiler/kineto_shim.h', 'torch/csrc/profiler/standalone/profiler_legacy.cpp', 'torch/csrc/profiler/standalone/profiler_legacy.h']","Legacy profiler, almost decoupled from rest of profiler, remains in `torch/csrc/autograd`, making it hard to maintain and may result in potential misplacement caused by unnecessary forward declarations."
11b753af01d110011a16db9dda2bee799ef967c9,1689730010,"Refactor causal mask generation and detection for nn.transformer (#105265)

Summary:
* Create a private global-scope function _generate_subsequent because static class attribute member functions not supported by TorchScript resulting in torchscripting errors.
* Make TransformerEncoder and TransformerDecoder consistent w.r.t. is_causal handling by calling _detect_casual_mask
* Clarify documentation that is_causal is a hint
* Move causal mask detection into a method _detect_causal_mask
* only accept input-size compatible causal mask as causal mask
* update _generate_subsequent_causal_mask to include factory kwargs for dtype and device:
   avoid extra copies & conversions by passing directly to torch.full.

Test Plan: sandcastle & github CICD
Continuation of #101487 (due to a tooling issue) which is a continuation-in-part of https://github.com/pytorch/pytorch/pull/98327 by @janEbert

Differential Revision: D47427117

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105265
Approved by: https://github.com/mikaylagawarecki
","['test/test_nn.py', 'test/test_transformers.py', 'torch/nn/modules/transformer.py']","Inconsistencies in handling `is_causal` in TransformerEncoder and TransformerDecoder, and torchscripting errors due to unsupported static class attribute member functions. Also, issues with accepting only input-size compatible causal mask."
f7294cd865aa437a3335802e5890507600b32984,1639503589,"[Static Runtime] Skip ReplaceWithCopy when inputs have writters (#69819)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69819

We should skip ReplaceWithCopy if the inputs to the operator can be updated during inference. For a set of tensors that share data, ReplaceWithCopy should not happen to any of them if there exists updates to any of them.

Currently, the check in place has missed some cases (suppose there exists updates, and uses <= 1). This diff addresses the missing cases by querying AliasDB.

Test Plan:
- Added test cases, including a one that is problematic before this diff
- CI

Reviewed By: mikeiovine

Differential Revision: D33052562

fbshipit-source-id: 61f87e471805f41d071a28212f2f457e8c6785e7
","['benchmarks/static_runtime/test_static_module.cc', 'benchmarks/static_runtime/test_utils.cc', 'benchmarks/static_runtime/test_utils.h', 'torch/csrc/jit/runtime/static/passes.cpp', 'torch/csrc/jit/runtime/static/passes.h']","The current ReplaceWithCopy implementation incorrectly modifies tensors that share data and will be updated during inference, causing inconsistencies in the inference results."
d5ce2bbed26a175e7bf69480759c2cfe73f42a75,1664223083,"[primTorch] decompositions for upsample_bicubic2d (#85403)

FYI, this decomposition seems to be significantly slower than the lowering in torchinductor:

```
------------------------------------- upsample_bicubic2d -------------------------------------]
                                                              |  lowering  |  Inductor  |  Eager
32 threads: ------------------------------------------------------------------------------------
      (torch.Size([16, 4, 128, 256]),), ((512, 1024), True)   |    1.8     |   3.880    |   1.4
      (torch.Size([16, 4, 128, 256]),), ((512, 1024), False)  |    1.9     |   3.887    |   1.4
```

This seems related to the fact that in the lowering we can use int32s as the indices and in the decomp we can only use int64s (see https://github.com/pytorch/torchdynamo/issues/1293).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85403
Approved by: https://github.com/ngimel
","['test/test_decomp.py', 'torch/_decomp/decompositions.py']",The decomposition implementation for upsample_bicubic2d in primTorch is significantly slower than the lowering in torchinductor due to index datatype restrictions.
56f9475625e6fc16d26ecff8ddaca041ec72e14e,1675276444,"ns: change PNP testing to use QNNPACK (#91421)

Summary:

Changes the PNP test cases to use QNNPACK. The only reason is because
I'm switching to Mac M1 as my primary machine, which supports QNNPACK
but not fbgemm, and it's convenient for me to be able to run these
locally.

PNP itself is not backend specific, so it does not matter which backend
the functionality is tested on.

Test plan:

```
python test/test_quantization.py -k NShadows
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91421
Approved by: https://github.com/jerryzh168
","['test/quantization/fx/test_numeric_suite_fx.py', 'torch/testing/_internal/common_quantization.py']","PNP test cases currently use fbgemm which is unsupported on Mac M1 machines, causing testing inconvenience for developers using such machines."
d2215f14ba1cd45c79f778a804a6125f140461b3,1692707885,"Fix: transactional translation validation insertion. (#107523)

This PR fixes transactional behavior of translation validation insertion.

Previously, this transactional behavior was implemented by removing the FX node if any
issues occurred until the end of `evaluate_expr`. However, since we cache FX nodes, we
might end up removing something that wasn't inserted in the same function call.

**Solution:** when creating an FX node for `call_function`, we also return whether this is
a fresh FX node or not. Then, we can appropriately handle each case.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/107523
Approved by: https://github.com/ezyang
",['torch/fx/experimental/symbolic_shapes.py'],The transactional behavior of translation validation insertion is broken; cached FX nodes may be removed inadvertently if any issues occur until the end of `evaluate_expr`.
bcc0f4bcabd8158160d18d76a313976cf011505f,1691040964,"Move ASAN to clang12 and Ubuntu-22.04 (Jammy) (#106355)

- Modify `install_conda` to remove libstdc++ from libstdcxx-ng to use one from OS
- Modify `install_torchvision` to workaround weird glibc bug, where malloc interposers (such as ASAN) are causing a hang in internationalization library, see https://sourceware.org/bugzilla/show_bug.cgi?id=27653 and https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90589
- Modify `torch.utils.cpp_extension` to recognize Ubuntu's clang as supported compiler

Extracted from https://github.com/pytorch/pytorch/pull/105260
Pull Request resolved: https://github.com/pytorch/pytorch/pull/106355
Approved by: https://github.com/huydhn
ghstack dependencies: #106354
","['.ci/docker/build.sh', '.ci/docker/common/install_conda.sh', '.ci/pytorch/common_utils.sh', '.ci/pytorch/test.sh', 'torch/utils/cpp_extension.py']","Issues with current ASAN setup, specifically with glibc leading to a hang in internationalization library. Additionally, Ubuntu's clang is not recognized as a supported compiler in `torch.utils.cpp_extension`."
ee9335a6082ffa9def1e6fad06b6d297eacf794c,1649081235,"[Quant][fx] Define native backend_config_dict for linear and conv (#74636)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74636

This commit changes how quantization patterns for linear
and conv are set up in prepare. Previously, these were set up
through ConvReluQuantizeHandler and LinearReLUQuantizeHandler.
After this commit, however, these were set up through the
corresponding entries in the native backend_config_dict,
rendering the above quantize handlers no longer necessary.
In future commits, we will do the same for the remaining ops.

Test Plan:
python test/test_quantization.py TestQuantizeFx
python test/test_quantization.py TestQuantizeFxOps

Imported from OSS

Reviewed By: jerryzh168, ngimel

Differential Revision: D35225680

fbshipit-source-id: 4a79f63a11fce46701eb17aaf3619c1e827d72a4
(cherry picked from commit 475f599821cd32d3ba71ba086885ecdc4cbee755)
","['test/quantization/fx/test_numeric_suite_fx.py', 'torch/ao/ns/fx/mappings.py', 'torch/ao/ns/fx/pattern_utils.py', 'torch/ao/quantization/fx/backend_config/native.py', 'torch/ao/quantization/fx/backend_config/utils.py', 'torch/ao/quantization/fx/pattern_utils.py', 'torch/ao/quantization/fx/prepare.py', 'torch/ao/quantization/fx/quantization_patterns.py']","Quantization patterns for linear and convolutional layers are not properly setup, making current quantize handlers redundant."
022119879065990ba6b65fbed17e97dc0e79f2f4,1682638617,"Added Typechecking to input tensor in RNN (#100100)

The input tensor of the RNN forward must be the same type as the weights.
While passing tensor of type long the error is:
    `RuntimeError: expected scalar type Long but found Float`
Which is misleading because it said to convert Something to Long, but the correct solution is to convert the input to Float (Which is the type of the weights).

The new error:
    `RuntimeError: input must have the type torch.float32, got type torch.int64`

Is correct and more verbose

Fixes #99998

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100100
Approved by: https://github.com/drisspg
",['torch/nn/modules/rnn.py'],The input tensor to the RNN forward call results in a misleading error message when passed a tensor of type 'long'; it is expected to match the type with the weights ('float').
76a2c22341e273211f710576306d9a6642f140dc,1643250499,"[c10d] Improve the ""not yet listening"" warning message of `socket` (#71864)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71864

A very minor change in one of the warning messages of `socket` to make it clear that it is a transient issue and not an error.

```
[W socket.cpp:634] The server socket on [localhost]:29501 is not yet listening (errno: 111 - Connection refused).
```

becomes

```
[W socket.cpp:634] The server socket on [localhost]:29501 is not yet listening (errno: 111 - Connection refused), will retry.
```
ghstack-source-id: 147716736

Test Plan: No behavioral change. Run the existing unit and integration tests.

Reviewed By: H-Huang

Differential Revision: D33792888

fbshipit-source-id: 79b287325945d0353c4568d84d1b52c820783cfc
(cherry picked from commit 9e5b627551fdf3bd6d06eb669883f9423d0999f1)
",['torch/csrc/distributed/c10d/socket.cpp'],"The ""not yet listening"" warning message of `socket` can be misinterpreted as a persistent error, rather than a transient issue."
d4f5f9fdb49312060dbb9c487bade00fa72c02ee,1678205542,"Profile dynamo guards (#96119)

Adds a profiler start and end callback to dynamo's C eval_frame impl, which can be used to profile a region providing a name for visualization.  Currently only hooks up one usage to profile cache lookup (primarily covering guards and linear search through  linked list).

Example profile taken from toy model:
`python benchmarks/dynamo/distributed.py --toy_model --profile --dynamo aot_eager`
<img width=""1342"" alt=""image"" src=""https://user-images.githubusercontent.com/4984825/223225931-b2f6c5a7-505a-4c90-9a03-34982f6dc033.png"">

Planning to measure overhead in CI, and probably can't afford to check this in enabled by default.  Will have to evaluate UX options such as `config.profile_dynamo_cache = True` or some other way.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96119
Approved by: https://github.com/jansel
","['test/dynamo/test_misc.py', 'torch/_dynamo/config.py', 'torch/_dynamo/eval_frame.py', 'torch/_dynamo/types.py', 'torch/csrc/dynamo/eval_frame.c']","Lack of profiling on the dynamo C eval_frame implementation makes it challenging to visualize and measure performance in regions like cache lookup, particularly for guards and linear search through linked lists."
a9f5e7229e92259bc44eb9898c04eaf200c7f81f,1613588324,"[PyTorch] Remove reference_cast in make_boxed_from_unboxed_functor (#51319)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51319

We were going out of our way to accommodate `IValue::to<Tensor>` returning a copy of the inner Tensor. `IValue::toTensor` is capable of returning a reference without copying, so if we use it directly, we can allow kernels that want to take `Tensor &` to do so!
As a bonus, we get reduced build times.
ghstack-source-id: 121378961

Test Plan:
Rely on CI for correctness.
Profiled build time with -ftime-trace for RegisterCPU.cpp using an extracted build invocation.

Before: P168244900

After: P168245014

Note reduced time spent compiling make_boxed_from_unboxed_functor.

I also ran the AdIndexer benchmark (https://fb.quip.com/ztERAYjuzdlr) with static runtime disabled and batch size 1 to see how big the effect on boxed call performance was (any kernels that take `Tensor&` or `const Tensor&` should now actually save a refcount bump). Looks like it was roughly 1% better:

Before: 124-125 usec/iter
After: 122-123 usec/iter

Reviewed By: bhosmer

Differential Revision: D26138549

fbshipit-source-id: b0f830527da360c542c815bef2f7e1692615b32a
","['aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h', 'aten/src/ATen/core/op_registration/op_registration_test.cpp', 'torch/custom_class_detail.h']","The current implementation unnecessarily copies the inner Tensor while using `IValue::to<Tensor>`, even where a reference could be returned, limiting kernels from taking `Tensor &` and increasing build times."
5ea74b4996d4dd9ed52263f12b2e7777c7e0f661,1645074557,"[Static Runtime] Remove ProcessedNode::num_outputs_ (#72592)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72592

Only code paths that are not perf-critical read `ProcessedNode::num_outputs_` and also its static feature of the op that `ProcessedNode` instance is executing.

Therefore, it's better to move `ProcessedNode::num_outputs_` into `ProcessedFunction::num_outputs_` and let `ProcessedNode` access it via `ProcessedNode::fn_` for its occasional use. Note that this prevents duplicating num_outputs_ per node & per Static Runtime instance since `ProcessedFunction` instances are shared across all runtime instances.

It's confirmed that this change reduces the `sizeof(ProcessedNode)` by 14% from local instrumentation as follows:

- Before
-- sizeof(ProcessedNode): 56

- After
-- sizeof(Processednode): 48

Test Plan: `buck test //caffe2/benchmarks/static_runtime:static_runtime_cpptest`

Reviewed By: mikeiovine

Differential Revision: D33984792

fbshipit-source-id: e29ffc97b799e679215f42e1e85cd3fcd7e88983
(cherry picked from commit 0f7003f4dfd6473a70355ca3c6f51498abf1d7be)
","['torch/csrc/jit/runtime/static/impl.cpp', 'torch/csrc/jit/runtime/static/impl.h']","The `ProcessedNode::num_outputs_` is being read via non-perf-critical paths, resulting in potential duplication of `num_outputs_` per node and Static Runtime instance affecting the performance."
a580e07d5dbac0ffe5db170890bb976a33a41c72,1652812280,"[BE] remove unused RDS pipeline from print_test_stats.py (#77654)

Now that Grafana is deprecated and we report test status + flaky tests to Rockset, we should remove the need to write to RDS at all.

Some runners (like ROCm) don't have permissions to write to RDS on pull requests anyway so this was causing a split in stats.

Next steps:
- Remove RDS in binary_size and sccache stats.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77654
Approved by: https://github.com/mehtanirav, https://github.com/malfet, https://github.com/seemethere
",['tools/stats/print_test_stats.py'],The RDS pipeline in print_test_stats.py is unused (especially for runners like ROCm) causing unnecessary fragmentation in reported statistics as some runners lack permissions to write to RDS.
332e43ed1a353a8b0a1d8e0167daaaed1e8b7a34,1655653287,"[Profiler] Expose extra fields to Python

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79623

Pybind11 has a really awesome feature where you can tell it how to move a type from C++ to Python just by specializing one template and it has out of the box support for variant types. (You do have to make one change to variant to avoid a bunch of chatty compiler warnings.) This will make it easy to both:
A) Write principled type driven analysis in Python similar to `c10::visit`
B) Expose fields that only make sense for certain events without cluttering up the API of the top level events.

For now I haven't added any fields; this PR is just to handle the foundation.

Differential Revision: [D36988611](https://our.internmc.facebook.com/intern/diff/D36988611/)

Approved by: https://github.com/aaronenyeshi
","['c10/util/variant.h', 'test/test_profiler.py', 'torch/csrc/autograd/init.cpp', 'torch/csrc/jit/tensorexpr/tensorexpr_init.cpp', 'torch/csrc/utils/pybind.h']","The Profiler's Python API lacks ability to expose additional fields specific to certain events, and it's challenging to implement type-driven analysis similar to `c10::visit`."
b8546bde09c7c00581fe4ceb061e5942c7b78b20,1615324616,"ci: Remove special versioning privileges for cu102 (#53133)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53133

In light of some issues where users were having trouble installing CUDA
specific versions of pytorch we should no longer have special privileges
for CUDA 10.2.

Recently I added scripts/release/promote/prep_binary_for_pypi.sh (https://github.com/pytorch/pytorch/pull/53056) to make
it so that we could theoretically promote any wheel we publish to
download.pytorch.org to pypi

Signed-off-by: Eli Uriegas <eliuriegas@fb.com>

Test Plan: Imported from OSS

Reviewed By: walterddr

Differential Revision: D26759823

Pulled By: seemethere

fbshipit-source-id: 2d2b29e7fef0f48c23f3c853bdca6144b7c61f22
","['.circleci/scripts/binary_populate_env.sh', '.github/scripts/generate_pytorch_version.py']",Special versioning privileges for CUDA 10.2 are causing installation issues for users trying to install CUDA specific versions of Pytorch.
3bb1f59a9c3801409a20d34a613214b4209e2a22,1617813073,"avoid CPU std::copysign segfault when compiling on arm64 with gcc 7.5 / 8 for CUDA (#51834)

Summary:
It seems that the std::copysign code introduced in https://github.com/pytorch/pytorch/issues/51706 is too much for gcc 7.5 / 8 when compiled on arm64 (e.g. on Jetson with latest Jetpack) and causes it to produce an internal compiler error with segfault during compilation. This avoids the compiler bug it by not using std::copysign.

A very kind person sent a Jetson Xavier NX {emoji:1f381} thank you {emoji:2764}.

After https://github.com/pytorch/pytorch/issues/51900 fixed this for CPU-only arm64 (eg Raspberry), this fixes it for CUDA-using arm64 (e.g. Jetson). CUDA device lambdas must also be present as host functions for technical reasons but they are never used, so we just assert in the CPU variant instead of actually doing the operation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51834

Reviewed By: mrshenli

Differential Revision: D27622277

Pulled By: malfet

fbshipit-source-id: a1dc4c3a67f925019782e24b796919e17339749f
","['aten/src/ATen/native/cuda/BinaryMulDivKernel.cu', 'c10/cuda/CUDAMathCompat.h']","The std::copysign code in the recent update causes an internal compiler error with a segmentation fault when compiled on arm64 with gcc 7.5/8, specifically on devices like Jetson Xavier NX with the latest Jetpack."
1d9a6862cd130bdc40ad86eceb3bfb6b41fbc3b0,1633965818,"fx quant: add a BC test for loading old torch.package models (#65538)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65538

Adds a test which verifies that `prepare_fx` and `convert_fx` work
on models created by `torch.package` in the past.  In detail:

1. (one time) create a model and save it with torch.package. Also save input,
expected output, and names of quantization related get_attrs added by
our passes.
2. (every time) load the model from (1), and verify that expected output
matches current output, and that get_attr targets did not change.

Test Plan:
```
python test/test_quantization.py TestSerialization.test_linear_relu_package_quantization_transforms
```

Imported from OSS

Reviewed By: supriyar

Differential Revision: D31512939

fbshipit-source-id: 718ad5fb66e09b6b31796ebe0dc698186e9a659f
","['test/quantization/bc/test_backward_compatibility.py', 'test/test_quantization.py', 'torch/testing/_internal/quantization_torch_package_models.py']",Using `prepare_fx` and `convert_fx` with models saved by `torch.package` in the past may produce inconsistent outputs and unstable `get_attr` targets.
0b8c3830890c1803d6c5e49728a63d203b9a37b4,1657658944,"Modules under migration in the public binding test (#81314)

If a module is being migrated, a common practice is to temporarily support
the old location. That might break the assertion that the `__module__`
of a function is pointing to the same location as where it is created.

 ## Example

1. Assume there is `torch/nn/quantized/functional.py`
2. The file is copied to `torch/ao/nn/quantzied/functional.py`
3. The old location is changed to have `from torch.ao.nn.quantized.functional import *`

In such a situation, importing from the old location will have `__module__`
pointing to the new `torch/ao/nn/...` location. This will break the
current test.

 ## What changed

This PR adds the following:

1. Added a key `""being_migrated""` to the `allowlist_for_publicAPI.json`
2. Added a check in the `test_public_bindings.py` to check if the JSON file has the `""being_migrated""` key.

 ## How to add migration entries

1. Add an entry to the `""being_migrated""`
   For the example above, add `""torch.nn.quantized.functional"": ""torch.ao.nn.quantized.functional""`
2. Change any existing keys for the old location
   For example, if there is an existing entry `""torch.nn.quantized.functional"": [...]`
   outside the `""being_migrated""`.
   Change it to `""torch.ao.nn.quantized.functional"": [...]`

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81314
Approved by: https://github.com/anjali411
",['test/test_public_bindings.py'],"During module migration, the `__module__` of a function might not point to the location where it is created, causing the test for public bindings to fail. Support for temporary old locations exacerbates this issue."
d0f82cd082fad7243226e0ab68fd995873ea7d76,1695763461,"Use Dr.CI results to classify flaky failures in trymerge (#110054)

After https://github.com/pytorch/test-infra/pull/4589, we can now query Dr.CI to get the list of flaky failures there.  This change queries Dr.CI API endpoint and check if the failure is a flaky one using `is_flaky` function.

Because the change is relatively large, I'm breaking it down to several smaller PRs in this order:

* [x] This PR queries Dr.CI and adds `is_flaky` check
* [ ] Clean up the flaky rules logic because it has already been implemented on Dr. CI
* [ ] Clean up the broken trunk logic for the same reason

### Testing

* Create a new `drci_mocks.json` file to catch the JSON response from Dr.CI API endpoint. The API requires `DRCI_BOT_KEY`.
*  `pytest -v test_trymerge.py`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110054
Approved by: https://github.com/clee2000
","['.github/scripts/github_utils.py', '.github/scripts/test_trymerge.py', '.github/scripts/trymerge.py']",Failure classification system is currently unable to identify and label flaky failures from the trymerge process.
7a3c3d63bf4c17ca24e8a90c6d3d62fe0dbced78,1697750145,"fix gloo cuda sparse_allreduce dispatch (#111485)

Fixes #111422

allreduce_sparse_cuda gets dispatched to allreduce_sparse which doesnt exist for gloo. However, gloo has an existing implementation so this is just fixing the dispatching to that.

The reason CI didn't catch this is because we are calling the backend directly. Added a test which calls the public API (dist.XYZ) and goes through the dispatcher

Pull Request resolved: https://github.com/pytorch/pytorch/pull/111485
Approved by: https://github.com/fduwjj
","['test/distributed/test_c10d_gloo.py', 'torch/csrc/distributed/c10d/Backend.hpp', 'torch/csrc/distributed/c10d/ProcessGroupGloo.cpp', 'torch/csrc/distributed/c10d/ProcessGroupGloo.hpp']","Incorrect dispatching in gloo causes nonexistence error for allreduce_sparse, even though an implementation already exists. Existing tests, calling the backend directly, missed this issue."
c08078031f73e34d046784ff14e63c7b2061bd43,1612221989,"[Gradient Compression] Allow BatchedPowerSGD to run vanilla allreduce for the first K iterations (#51270)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51270

Similar to #50973, allow the batched version to run vanilla allreduce for the first K iterations.

This may be useful if the batched version can be applied to some use cases where the accuracy requirement is not very strict.

Original PR issue: Investigate Applying PowerSGD to Communication Hook for Gradient Compression #47202
ghstack-source-id: 120725858

Test Plan:
buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_powerSGD_ddp_comm_hook_nccl

baseline: f248001754
batched PowerSGD: f246960752

The training time was reduced from 54m48s to 30m33s, and the accuracy is approximately the same: 44.21 vs 44.35

Reviewed By: rohan-varma

Differential Revision: D26077709

fbshipit-source-id: 6afeefad7a3fbdd7da2cbffb56dfbad855a96cb5
",['torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py'],"BatchedPowerSGD doesn't have an option to run vanilla allreduce for the initial iterations, potentially limiting its applicability in scenarios with less strict accuracy requirements."
1d3172130d93e7eabef652e349868d843b00c6f3,1614314749,"ns for fx: add node name and type to results (#52798)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52798

Adds the node name and node target type to Numerical Suite outputs.
This is useful to debug which node got matched to which node,
and what is the type of the operation.

```
// before
{
  layer_name: {
    model_name: {
      'type': 'weight',
      'values': [...],
    },
  },
}

// after
{
  layer_name: {
    model_name: {
      'type': 'weight',
      'values': [...],
      'node_name': '0',
      'node_target_type': ""<class 'torch.nn.modules.conv.Conv2d'>"",
    },
  },
}
```

Test Plan:
```
python test/test_quantization.py TestFXNumericSuiteCoreAPIs
```

Imported from OSS

Reviewed By: hx89

Differential Revision: D26652637

fbshipit-source-id: ba75b110cb91234f17a926ccbc5d0ccee2c3faeb
","['torch/quantization/ns/graph_passes.py', 'torch/quantization/ns/numeric_suite_core_apis_fx.py']",Numerical Suite results lack information for debugging - they do not include node name or node target type.
1188d89a1dd2c52a1d515f76accad472ced2f3bc,1639509013,"TestMathBits: Call functions with original sample input values (#68947)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68947

`_test_math_view` currently calls the operator with different values
than those specified in the `SampleInput`. This is undesirable as it
could break mathematical properties required by the operator. Instead,
this calls `math_op_view(math_op_physical(sample.input))` to get a
view that represents the same value as the original input.

`test_neg_view` already did this by returning `torch._neg_view(-x)`
from `math_op_view` but this moves the handling into `_test_math_view`
to make it apply to all view op tests.

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D33064327

Pulled By: anjali411

fbshipit-source-id: 4d87e0c04fc39b95f8dc30dcabda0d554d16a1d8
","['test/test_ops.py', 'torch/testing/_internal/common_methods_invocations.py']","The `_test_math_view` function in `TestMathBits` is using different values than those specified in `SampleInputs` to call the operator, potentially causing mathematical properties required by the operator to break."
2828ce53fd7a03df0488f6cffa5dd2bd18feb57a,1633069511,"Added jit log stream changing function and some refactor (#65768)

Summary:
Description:
- Have only added `stdout` and `stderr` as possible options from python
  API for now. We can do file path passing later maybe.
- Put the class `JitLoggingConfig` in the cpp file as none of its methods were being used outside of this file.

Python API:
`torch._C._jit_set_logging_stream('stdout|stderr')`
C++ API:
`::torch::jit::set_jit_logging_output_stream(ostream);`

Testing:
- Tested python API locally.
- Unit test for the C++ API is written

Fixes https://github.com/pytorch/pytorch/issues/54182

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65768

Reviewed By: mrshenli

Differential Revision: D31291739

Pulled By: ZolotukhinM

fbshipit-source-id: eee72edc20488efad78a01c5b0ed8a132886a08d
","['test/cpp/jit/test_jit_logging_levels.cpp', 'torch/csrc/jit/jit_log.cpp', 'torch/csrc/jit/jit_log.h', 'torch/csrc/jit/python/init.cpp']","Changing the JIT log stream is not currently possible from Python API, limiting debug and trace options. Furthermore, JitLoggingConfig methods have unnecessary global exposure."
4b29829ece227eef08d6887670db52cec2fc69b8,1689100962,"[quant][pt2] Fix QAT convert for mobilenetv2 (#104110)

Summary:
QAT convert for mobilenetv2 was previously not working
because we incorrectly applied dropout during eval as well as
training. This is because, for exported models, model.eval() does
not change the behavior of dropout, unlike models with torch ops.
This commit simulates the effects of model.eval() for exported
models as well by replacing the aten dropout pattern before eval.
As of this commit, end-to-end QAT numerics now match for
mobilenetv2 between FX and PT2.

Test Plan: python test/test_quantization.py TestQuantizePT2EModels.test_qat_mobilenet_v2

Differential Revision: D46750343

Pull Request resolved: https://github.com/pytorch/pytorch/pull/104110
Approved by: https://github.com/jerryzh168
","['test/quantization/pt2e/test_quantize_pt2e.py', 'torch/_dynamo/skipfiles.py', 'torch/ao/quantization/_pt2e/qat_utils.py', 'torch/ao/quantization/_pt2e/utils.py', 'torch/ao/quantization/_quantize_pt2e.py']","QAT conversion for mobilenetv2 is incorrect due to dropout being mistakenly applied during both evaluation and training, causing inconsistent end-to-end QAT numerics between FX and PT2."
7eb1a6a965486c99e5a16a7345056c200b7305ba,1657210888,"[Vulkan] Implement Batchnorm operator (#80510)

Summary:
Implemented BatchNorm operator for the Vulkan backend.

Special case implementation:
- Input tensor must be 4-dim, i.e. [N, C, H, W].
- C must be a multiple of 4.
- It expects that weight and bias be defined.
- Only supports evaluation mode. Therefore, running_mean and running_var must also be defined.

References
- PyTorch Docs > torch.nn > [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)

Test Plan:
Added 3 test cases to `/xplat/caffe2/aten/src/ATen/test/vulkan_api_test.cpp`

On Mac:
```
buck run //xplat/caffe2:pt_vulkan_api_test_binAppleMac
```
On Android:
```
buck build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 //xplat/caffe2:pt_vulkan_api_test_binAndroid\#android-arm64 --show-output
adb push buck-out/gen/xplat/caffe2/pt_vulkan_api_test_binAndroid\#android-arm64 /data/local/tmp/vulkan_api_test
adb shell ""/data/local/tmp/vulkan_api_test""
```

Differential Revision: D37519389

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80510
Approved by: https://github.com/SS-JIA
","['aten/src/ATen/native/vulkan/glsl/batchnorm.glsl', 'aten/src/ATen/native/vulkan/ops/Batchnorm.cpp', 'aten/src/ATen/test/vulkan_api_test.cpp']",BatchNorm operator for the Vulkan backend is not implemented. This limits the types of neural networks that can be built using Vulkan.
7acdb2d5642557053df00951b51b94929302a9b7,1661897947,"Don't start land checks if the PR hasn't been approved yet (#84239)

Per title, don't start land checks if the PR hasn't been approved yet. This is very important to make sure that we don't start CI jobs from unknown devs, i.e. first time contributor.

Also rename force to `skip_mandatory_checks` to make it clearer on what this flag does

### Testing

```
python .github/scripts/test_trymerge.py
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84239
Approved by: https://github.com/zengk95, https://github.com/ZainRizvi
","['.github/scripts/test_trymerge.py', '.github/scripts/trymerge.py']","Land checks are being started for PRs even if they haven't been approved yet, potentially triggering CI jobs from unknown developers."
78ea86a445aad27f6f0260a840cbf18fddef728b,1650891684,"[shard] Sharder and ShardingPlan prototype (#73873)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73873

Basic ShardingPlan interface and Sharder implemention:
1. We provide `ShardingPlan` to allow user to specify all parameter sharding strategies for a given model, this including `plan` for sharding the parameters, and `output_plan` for tagging the output layout, `return_local_tensor` for converting back to DDP.
2. Introduce `shard_module` API, that could take a nn.Module, a ShardingPlan, then shard the module according to the plan.

TODO:
next PR we will introduce Extensible Sharder and ShardingPlanner.
ghstack-source-id: 154682421

Test Plan: test_sharding_plann.py

Reviewed By: pritamdamania87, fduwjj

Differential Revision: D34695159

fbshipit-source-id: 3d695803c4b7e9a7543177ade5b709b5f847baa9
(cherry picked from commit 670cd279b0e5304a9bf0ce6e6651a08273a77035)
","['test/distributed/_shard/sharded_tensor/ops/test_linear.py', 'test/distributed/_shard/sharded_tensor/test_megatron_prototype.py', 'test/distributed/_shard/sharded_tensor/test_sharded_tensor.py', 'test/distributed/_shard/sharding_plan/test_sharding_plan.py', 'test/run_test.py', 'torch/distributed/_shard/__init__.py', 'torch/distributed/_shard/api.py', 'torch/distributed/_shard/sharded_tensor/__init__.py', 'torch/distributed/_shard/sharding_plan/__init__.py', 'torch/distributed/_shard/sharding_plan/api.py', 'torch/testing/_internal/distributed/_shard/test_common.py']","Lack of a structured way to specify parameter sharding strategies in a model, tag the output layout, and convert back to DDP, as well as to shard a module based on a specific plan."
84d18727bd0d91873d1a981f9bbfc7b4f78d3260,1617742406,"Added linalg.eig, linalg.eigvals (#52491)

Summary:
This PR adds `torch.linalg.eig`, and `torch.linalg.eigvals` for NumPy compatibility.

MAGMA uses a hybrid CPU-GPU algorithm and doesn't have a GPU interface for the non-symmetric eigendecomposition. It means that it forces us to transfer inputs living in GPU memory to CPU first before calling MAGMA, and then transfer results from MAGMA to CPU. That is rather slow for smaller matrices and MAGMA is faster than CPU path only for matrices larger than 3000x3000.
Unfortunately, there is no cuSOLVER function for this operation.

Autograd support for `torch.linalg.eig` will be added in a follow-up PR.

Ref https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52491

Reviewed By: anjali411

Differential Revision: D27563616

Pulled By: mruberry

fbshipit-source-id: b42bb98afcd2ed7625d30bdd71cfc74a7ea57bb5
","['aten/src/ATen/native/BatchLinearAlgebra.cpp', 'aten/src/ATen/native/BatchLinearAlgebra.h', 'aten/src/ATen/native/BatchLinearAlgebraKernel.cpp', 'aten/src/ATen/native/cuda/BatchLinearAlgebra.cu', 'c10/core/ScalarType.h', 'test/test_linalg.py', 'test/test_namedtuple_return_api.py', 'torch/csrc/api/include/torch/linalg.h', 'torch/linalg/__init__.py', 'torch/overrides.py', 'torch/testing/_internal/common_methods_invocations.py']","The Pytorch library lacks compatibility with NumPy as it currently does not have `torch.linalg.eig`, and `torch.linalg.eigvals` for the computation of eigenvalues and eigenvectors."
5b21f172a4ecba1712ae5338e2c8095af48e19a3,1631196331,"[doc][hackathon] To add AdamW Optimizer to the documentation (#63252)

Summary:
It has been discussed before that adding description of Optimization algorithms to PyTorch Core documentation may result in a nice Optimization research tutorial. In the following tracking issue we mentioned about all the necessary algorithms and links to the originally published paper  https://github.com/pytorch/pytorch/issues/63236.

In this PR we are adding description of AdamW Algorithm to the documentation.  For more details, we refer to the paper  here https://arxiv.org/abs/1711.05101

<img width=""442"" alt=""AdamWalgo"" src=""https://user-images.githubusercontent.com/73658284/132589957-6d381e96-cb62-40d0-990f-82a32ec455be.png"">

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63252

Reviewed By: datumbox

Differential Revision: D30839685

Pulled By: iramazanli

fbshipit-source-id: 1a426c874ab86408d286a34f41aefcf5b21167c0
",['torch/optim/adamw.py'],The PyTorch Core documentation lacks a description of the AdamW Optimization algorithm which is necessary for enriching the Optimization research tutorial.
2f9ffe7b0ae61c804d0aec898c4ac3946c109e41,1676644098,"Add torch.utils._sympy.interp (#94985)

This utility allows us to conveniently abstract interpret Sympy expressions with respect to some alternative domain. I am particularly interested in using ValueRanges to do range analysis on expressions (not this PR).

Some minor house-keeping:
* ReferenceAnalysis got moved to its own file, sprouted a constant() implementation, and some uses of math.* got converted to sympy.*
* ValueRangeAnalysis now understands mod
* Test file gets moved from `test_value_ranges.py` to `test_sympy_utils.py`

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94985
Approved by: https://github.com/eellison
","['test/test_sympy_utils.py', 'torch/utils/_sympy/interp.py', 'torch/utils/_sympy/reference.py', 'torch/utils/_sympy/value_ranges.py']","There is no existing utility to interpret Sympy expressions with respect to an alternative domain, hindering efficient range analysis on expressions."
a7b62abeb06931253c8d4067656e825a418958f0,1621379833,"[PyTorch Edge] bytecode version bump to v5 and enable share constant table (#57888)

Summary:
As title, main change:
1. Enable share constant table and reduce model size up to 50%
2. Bump bytecode version from v4 to v5.
3. Add the unittest back. (It was partially removed because `script_module_v5.ptl` bytecode version is v5. When current runtime is v4 and try to load a v5 model, it will raise an error because version is not within the range.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57888

As title
ghstack-source-id: 129255867

Test Plan:
CI
```
buck test papaya/toolkit/frontend/torch/...
buck test mode/opt papaya/integration/service/test/smartkeyboard:smartkeyboard_system_test
```

Reviewed By: raziel, iseeyuan

Differential Revision: D28309381

fbshipit-source-id: 6f5cf4296eaadde913d55f27d5bfb9d1dea2fbaf
","['caffe2/serialize/versions.h', 'torch/csrc/jit/serialization/export_module.cpp']","Current PyTorch runtime (v4) unable to load models using the latest bytecode version (v5), causing errors. Issue further extends to the constant table sharing unable to reduce model size."
600df80296cfca8d4ec388856c9d832da5b51214,1632243534,"[PT/ShardedTensor]Allow zero size local shard (#65007)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65007

Relax shard size check in ShardMetadata to allow zero size local shard.

When sharding a tensor on N ranks, some ranks may have empty shard allocated. As we are assuming SPMD, the ranks w/ empty shard still need to participate in all collectives, and we need to allow this in ShardMetadata.

Test Plan: Unit tests and CLI

Reviewed By: jiaqizhai, wanchaol

Differential Revision: D30926566

fbshipit-source-id: afa562c94ffa8f8d91d65ddb4c348156d871dc36
","['test/distributed/_sharding_spec/test_sharding_spec.py', 'torch/distributed/_sharding_spec/_internals.py']","ShardedTensor on certain ranks can have zero size local shard, however it fails to verify a shard due to ShardMetadata does not allow empty size."
596095dc8256c22613372c894ca5141a2c6dcbd2,1655401005,"[ao][sparsity] Support for sparsifying data operations on raw torch tensors.

The users can now pass in raw torch tensors and the base class handles all the parametrizations and masking

Example -
    >>> data_list = [('tensor_1', torch.randn(3,3)), ('tensor_2', torch.randn(4,4))]
    >>> defaults = {'sparsity_level': 0.7}
    >>> sparsifier = DerivedDataSparsifier(data_list = data_list, **defaults) # Some sparsifier that inherits BaseDataSparsifier
    >>> new_tensor_to_add = {'name': 'tensor_3', 'data': torch.randn(5,5), 'sparsity_level': 0.3}
    >>> sparsifier.add_data(**new_tensor_to_add)
    >>> # tensor_1 and tensor_2 will have sparsity_level of 0.7 but tensor_3 will have sparsity_level=0.3

Test Plan:
```python test/test_ao_sparsity.py TestBaseDataSparsifier```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79252

Approved by: https://github.com/HDCharles, https://github.com/z-a-f
","['test/ao/sparsity/test_data_sparsifier.py', 'torch/ao/sparsity/experimental/data_sparsifier/base_data_sparsifier.py']","Data sparsifying operations are not currently supported for raw torch tensors, limiting the scope of possibilities for parametrizations and masking."
b18d0f1dc9757be4ca58059ece28ac4e60bf6f0c,1667410747,"Add more debug information when installing NVIDIA driver (#88168)

This calls `lspci`, `lsmod`, and `modinfo nvidia` before and after the installation to gather more data about the ""No GPU available"" transient issue on G5 runner, i.e. https://hud.pytorch.org/pytorch/pytorch/commit/59fe272c1e698989228af5ad197bdd2985e4e9b9

This also handles `nvidia-smi` call and tries to re-install the driver if the first call fails, i.e. `No devices were found` https://hud.pytorch.org/pytorch/pytorch/commit/8ea19c802e38c061e79176360c1ecaa81ce2088a
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88168
Approved by: https://github.com/clee2000, https://github.com/malfet
",['.github/scripts/install_nvidia_utils_linux.sh'],"Transient ""No GPU available"" issue occurring on G5 runner post NVIDIA driver installation, with potential failure in `nvidia-smi` call resulting in ""No devices were found""."
de5e3b5eb045829041144e1c4e44448d13313d74,1618352931,"Fix OSS flaky test_destroy_full_group on MPI backend in pytorch_linux_xenial_cuda10_2_cudnn7_py3_multigpu_test environment by adding a barrier and retrying MPI_Comm_create 3 times (#55921)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55921

Fix this flaky test by adding a barrier and retrying the flaky function call `MPI_Comm_create` 3 times.

Couldn't figure out the root cause why `createProcessGroupMPI` can be flaky when just creating a subgroup communicator by mainly invoking `MPI_Comm_create`. Here `createProcessGroupMPI` does not involve any p2p or collective communication at all. Cannot further dig into `MPI_Comm_create`, which is in MPI codebase.

Also checked the commit history, and no commit on `ProcessGroupMPI.cpp` can be found within a few days before Mar 10th.

First failure (on Mar 10th):
https://app.circleci.com/pipelines/github/pytorch/pytorch/283704/workflows/d84ac4a0-42e3-4925-b1cf-32d3c3d1022a/jobs/11456129

Note that the test failure cannot be reproduced locally.

Verified the fix on CI:
https://app.circleci.com/pipelines/github/pytorch/pytorch/300586/workflows/a5c16db4-3ae2-44c7-a9c8-b0885dad2a64/jobs/12356852
test_destroy_full_group has rerun 100 times and pass.

#Closes: https://github.com/pytorch/pytorch/issues/53899
ghstack-source-id: 126414937

Test Plan:
```
export BACKEND=mpi
export WORLD_SIZE=2
pytest -k test_destroy_full_group test/distributed/test_distributed_fork.py -vs
```

```
#!/bin/bash
for i in {1..100}
do
pytest -k test_destroy_full_group test/distributed/test_distributed_fork.py
done
```

The CI tests triggered by a new branch:
https://app.circleci.com/pipelines/github/pytorch/pytorch?branch=ci-all%2Fwayi_mpi

Reviewed By: mrshenli

Differential Revision: D27245421

fbshipit-source-id: 86e7fe208e34eda8a33885e385d56ec6b60eca27
",['torch/lib/c10d/ProcessGroupMPI.cpp'],"The test_destroy_full_group is flaky on the MPI backend in the pytorch_linux_xenial_cuda10_2_cudnn7_py3_multigpu_test environment. The root cause is unknown, but appears to be connected with the `createProcessGroupMPI` and `MPI_Comm_create` functions. The issue cannot be reproduced locally."
91531d3047ba340d5f2bff0272b7afa49740c693,1615577550,"[caffe2] add a CAFFE2_NODISCARD macro to help support old compilers (#53754)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53754

Some of the PyTorch CircleCI builds still use gcc 5.4, and compile with
`-Werror=attributes` causing this old compiler to fail because it does not
understand the `[[nodiscard]]` attribute.

Let's define a `CAFFE2_NODISCARD` macro to work around this.
ghstack-source-id: 123594084

Test Plan: I'm using this macro in subsequent diffs in the stack.

Reviewed By: mraway

Differential Revision: D26959584

fbshipit-source-id: c7ba94f7ea944b6340e9fe20949ba41931e11d41
",['caffe2/core/common.h'],Older compilers like gcc 5.4 used in some PyTorch CircleCI builds fail to understand the `[[nodiscard]]` attribute due to `-Werror=attributes` compilation.
d35e3dbd06b16b1468ea4c9ddf9eb4411d16ef95,1696634052,"Fix concurrency limits for Create Release (#110759)

Also, don't run it on tags, but run on release branch and on `release` event.
Tweak linter to accept different concurrency limits for `create_release.yml`

Fixes https://github.com/pytorch/pytorch/issues/110569 as all the invocations of workflow in the past were cancelled by concurrently limit due to the tag push and release happening at roughly the same time, see https://github.com/pytorch/pytorch/actions/workflows/create_release.yml?query=event%3Arelease

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110759
Approved by: https://github.com/atalman
",['.github/scripts/ensure_actions_will_cancel.py'],Simultaneous execution of `create_release.yml` during tag push and release event leads to workflow invocation cancellations due to concurrency limits.
3c0c1c4ecbcbc08b7199483868fac76f066c6de1,1627666033,"Fix incorrectly sized tensors for svd when full_matrices=False (#62022)

Summary:
Before this PR for m x n input matrix, the return matrices were always allocated as m x m and n x n and then narrowed.
This unnecessarily requires a lot of memory that is then discarded.
With this PR when `compute_uv=True and full_matrices=False` correctly sized tensors are allocated. Moreover, if `compute_uv=False` U, V matrices are not allocated as they are not needed. However, cusolver's gesvdj routines fail when these matrices are not allocated, which is a bug, so this allocation is done separately in cusolver specific code path.

MAGMA doesn't work for this input because it tries to allocate a large matrix internally (ROCm doesn't work as it uses MAGMA). Example error:
```
CUBLAS error: memory mapping error (11) in magma_sgelqf at /opt/conda/conda-bld/magma-cuda110_1598416697386/work/src/sgelqf.cpp:161
CUBLAS error: out of memory (3) in magma_sgeqrf2_gpu at /opt/conda/conda-bld/magma-cuda110_1598416697386/work/src/sgeqrf2_gpu.cpp:145
CUBLAS error: not initialized (1) in magma_sgeqrf2_gpu at /opt/conda/conda-bld/magma-cuda110_1598416697386/work/src/sgeqrf2_gpu.cpp:145
MAGMA error: function-specific error, see documentation (1) in magma_sgeqrf2_gpu at /opt/conda/conda-bld/magma-cuda110_1598416697386/work/src/sgeqrf2_gpu.cpp:145
MAGMA error: function-specific error, see documentation (1) in magma_sgeqrf2_gpu at /opt/conda/conda-bld/magma-cuda110_1598416697386/work/src/sgeqrf2_gpu.cpp:145
python: /opt/conda/conda-bld/magma-cuda110_1598416697386/work/interface_cuda/interface.cpp:806: void magma_queue_create_internal(magma_device_t, magma_queue**, const char*, const char*, int): Assertion `queue->dAarray__ != __null' failed.
Aborted (core dumped)
```

Fixes https://github.com/pytorch/pytorch/issues/61949.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62022

Reviewed By: heitorschueroff

Differential Revision: D29994429

Pulled By: ngimel

fbshipit-source-id: c3f7744d7adc5fd6787f6cbb1ec41405f89a6d4c
","['aten/src/ATen/native/BatchLinearAlgebra.cpp', 'aten/src/ATen/native/LinearAlgebraUtils.h', 'aten/src/ATen/native/cuda/BatchLinearAlgebra.cu', 'aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu', 'test/test_linalg.py']","Tensors for singular value decomposition are incorrectly sized with `full_matrices=False`, causing over-allocation of memory and failure on MAGMA due to oversized matrix internal allocation."
c0814bff8753ef48213c92cd96f6dc757c1f9bea,1654219879,"[ONNX] Variable length argument support for quantized_args (#78775)

Add support for decorating functions with variable length arguments in `quantized_args`. This is needed to decorate functions like `symbolic_fn` in `_interpolate_helper` which takes `*args`.

Previously it is not possible to decorate functions like it. Now we can do

```python
@quantized_args(True)
def symbolic_fn(g, input, output_size, *args):
    ...
```

and the rest of the params are defaulted to non-quantized.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78775
Approved by: https://github.com/garymm
",['torch/onnx/symbolic_helper.py'],"`quantized_args` decorator lacks support for functions with variable length arguments, impeding certain function decorations like `_interpolate_helper`'s `symbolic_fn` ."
03b6e6979c11d4a0bfb566b2530bc721dc1246d8,1677981046,"Transformers: fix src and key padding mask bool regression (#96009)

Summary: fix src and pad mask bool regression

This fixes a regression introduced previously with #92733. That PR unified testing of masks to remove Byte Tensors as permissible mask, introduced mask compatibility check, and mask conversion to FP mask.  The problem addressed in this PR was that after the first mask had been converted, a check for mask compatibility would fail.

Test Plan: sandcastle & github

Differential Revision: D43782858

Fixes  https://github.com/pytorch/pytorch/issues/95702

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96009
Approved by: https://github.com/malfet
","['test/test_transformers.py', 'torch/nn/functional.py', 'torch/nn/modules/activation.py', 'torch/nn/modules/transformer.py']","After the first mask conversion, the mask compatibility check fails due to a regression issue introduced with #92733."
c139df407bf09f4a8908a8c1260855d3184e2b7f,1681149150,"Skip failing test_torchinductor_codegen_dynamic_shapes tests on CPU (#98621)

This test starts to fail in trunk after https://github.com/pytorch/pytorch/pull/97230.  The original PR missed this because these test are marked as slow and is only run periodically.  Is this ok to skip them like `test_upsample_cat_conv_dynamic_shapes`?

Here is an example failure https://github.com/pytorch/pytorch/actions/runs/4638277468/jobs/8208270657. The following tests are all failing with `Failed to find dynamic for loop variable` error like others in the list.  They are:

* `test_conv2d_binary_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98679
* `test_conv2d_unary_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98680
* `test_conv_bn_fuse_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98681
* `test_conv_transpose2d_unary_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98682

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98621
Approved by: https://github.com/malfet
",['test/inductor/test_torchinductor_codegen_dynamic_shapes.py'],"CPU tests for torchinductor_codegen_dynamic_shapes are failing with errors related to dynamic for loop variable, possibly skipped due to slow run-time."
3e7f23e04fd0e2834e5cdd32494c7456be6968d6,1695876789,"[inductor] Actually enable typing for sizevars.py and joint_graph.py (#110109)

The commit message of #107862 says it enabled mypy checking for
sizevars.py, but it seems that it neglected to update .lintrunner.toml.

New type errors appear to have crept in since then, so I've fixed them
accordingly.

A similar mistake happened with #109955 for joint_graph.py, though that
one is more recent and so hasn't had any new type errors to fix.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110109
Approved by: https://github.com/Skylion007
",['torch/_inductor/sizevars.py'],"'Mypy checking was not enabled for sizevars.py and joint_graph.py due to omission in .lintrunner.toml update, leading to unnoticed type errors.'"
2dc1726ab79d64de56f4778e0a609675f215e400,1697577430,"Compile NestedTensor with AOTAutograd (#110529)

This PR has a number of changes that improve subclass support for AOTAutograd/Inductor in general:
-  previously if a subclass does extra aliasing between graph outputs/inputs in a way, the partitioner would complain because grad_outputs are the outputs reused as-is. Now we do a view_as(self) to workaround this.
- Use dense -> dense metadata when working with fwd_output_strides during backward. This is important since the stride information comes from inductor which sees the dense to dense graph.
- Inductor requires that the inputs to the compiled backward to match some expected strides computed during compilation. We make sure to make the inner tensors of the subclass contiguous (previously, we only made the subclass itself contiguous)

Changes specific to NestedTensor relevant to compilation:
- Properly handle the case where `__tensor_unflatten__` is passed non-symbolic dense tensors and with meta extracted from fake subclasses.
- Skip var_to_range logic for singleton int
- Skip size hint logic in inductor for singleton int

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110529
Approved by: https://github.com/bdhirsh
","['test/dynamo/test_subclasses.py', 'test/test_nestedtensor.py', 'torch/_functorch/aot_autograd.py', 'torch/_inductor/codegen/wrapper.py', 'torch/autograd/__init__.py', 'torch/fx/experimental/symbolic_shapes.py', 'torch/nested/_internal/nested_tensor.py', 'torch/nested/_internal/ops.py']","NestedTensor compilation with AOTAutograd/Inductor is problematic due to issues with subclass aliasing, stride information handling, input stride expectations, handling of non-symbolic dense tensors, singleton int in var_to_range logic, and size hint logic."
d9547b9bb29ca5ba926b3707c5c8313ee65792b2,1629329101,"Nnapi Delegation: Quick improvements (#63489)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63489

A few quick improvements to the Android NNAPI Delegate, some of which were discussed here https://github.com/pytorch/pytorch/pull/62272:
1) `throw std::exception` replaced with `TORCH_CHECK` to reduce runtime
size (nnapi_backend_lib.cpp)
2) weights processing moved from compile to preprocess step, since it can
be done AOT (nnapi_backend_lib.cpp & nnapi_backend_preprocess.cpp)
3) `ser_model_` and `shape_compute_module_` member variables removed, since they are never used after
`init()`, so they are not needed (nnapi_backend_lib.cpp)

Test Plan:
Unit tests: `python test/test_jit.py TestNnapiBackend`
Run SparkAR segmentation with delegated NNAPI as done here D30259033 (can use `jf download GAekdAwsyGKXhggFALN4LnSBTzcubsIXAAAz --file ""v303-nnd-mod.ptl""` to get a preprocessed model from these changes)

Imported from OSS

Reviewed By: raziel, iseeyuan

Differential Revision: D30398880

fbshipit-source-id: b6872e1e9ccd583622b80659da00c83fdd82580e
","['torch/csrc/jit/backends/nnapi/nnapi_backend_lib.cpp', 'torch/csrc/jit/backends/nnapi/nnapi_backend_preprocess.cpp']","Android NNAPI Delegate has issues with runtime size, AOT weights processing, and the unnecessary retention of member variables post-init()."
49f1336106cc0f2a4f08b14332dcf95ef6ace0c1,1616170224,"Add Tensor::is_cpu, genericize TensorIterator (#54079)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54079

Fixes https://github.com/pytorch/pytorch/issues/53815

Instead of testing if something is CUDA, we instead test if something
is not CPU.  This in the general theming of ""Don't be so darn CUDA
centric"".

Intruigingly, we didn't have a is_cpu() method on Tensor.  Which seems
like a big oversight and one of the reasons how we ended up in this
mess.  So in it goes.  Maybe we should also get this for Python bindings
as well (but in that case, should probably look into redoing all of the
is_X bindings so they aren't done manually).

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D27109507

Pulled By: ezyang

fbshipit-source-id: abbe72c2e688c452ffe098d206cb79938b5824b1
","['aten/src/ATen/TensorIterator.cpp', 'aten/src/ATen/templates/TensorBody.h', 'aten/src/ATen/templates/TensorMethods.cpp', 'c10/core/TensorImpl.h', 'test/test_torch.py']","The Tensor class is lacking an is_cpu() method, which potentially led to issues by forcing more specific checks for CUDA rather than a generic CPU check."
0bf7506051c0cd78224e86d6a14cd059b3bea3fe,1674534846,"[CUDA] Drop CUDA < 11.0 test flags (#92605)

Follow-up of #89582 to drop flags like `CUDA11OrLater` in tests. Note that in some places it appears that `TEST_WITH_ROCM` is _implicitly_ guarded against via the `CUDA11OrLater` version check, based on my best-guess of how `torch.version.cuda` would behave in ROCM builds, so I've added `not TEST_WITH_ROCM` in cases where ROCM wasn't previously explicitly allowed.

CC @ptrblck @malfet @ngimel
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92605
Approved by: https://github.com/ngimel
","['test/distributed/fsdp/test_fsdp_mixed_precision.py', 'test/distributed/test_nccl.py', 'test/test_jit_cuda_fuser.py', 'test/test_linalg.py', 'test/test_matmul_cuda.py', 'test/test_sparse.py', 'test/test_sparse_csr.py', 'torch/testing/_internal/common_cuda.py', 'torch/testing/_internal/common_methods_invocations.py', 'torch/testing/_internal/opinfo/definitions/linalg.py']","Tests are still deploying CUDA < 11.0 flags like `CUDA11OrLater` and ROCM builds may be inadvertently guarded by this version check, causing potential incompatibility issues."
0988dc481ae4efcff57fa6bee837052473d71772,1647372532,"[Codemod][Codemod deprecated unittest asserts] fbcode//caffe2/test (#71708)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71708

In Python 3.2, a number of asserts were deprecated.

In Python 3.11, these asserts are deleted completely. The files in this change still use the deprecated asserts.

Switch over to the supported syntax for 3.2 onwards.

Test Plan: Tested on the internal test suite runner.

Reviewed By: ajtulloch

Differential Revision: D33503694

fbshipit-source-id: a150f296033260acf8365d77b837ce0679f57361
(cherry picked from commit abf60ed97409265222915d8265aaabedd625fd93)
","['test/jit/test_types.py', 'test/test_autograd.py', 'test/test_indexing.py', 'test/test_logging.py', 'test/test_python_dispatch.py', 'test/test_serialization.py', 'test/test_tensorboard.py']","Deprecated asserts from Python 3.2 are still being used in certain test files, which can cause compatibility issues with Python 3.11 where these are completely removed."
e20e5f5578a208d932c578ec26d8db99e5c16b03,1679367376,"[RFC] Add an API to remove autograd hooks from DDP (#96490)

Summary:
When creating a new DDP instance for the same model when an old DDP instance existed, the autograd hooks from the old DDP instance might not be cleared. Also, relying on python gc to clear out old autograd hooks is fragile and may not work 100% of the time.

As a result, in this PR I'm adding a way to explicitly remove these hooks from DDP

Test Plan:
Unit test added

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96490
Approved by: https://github.com/zhaojuanmao, https://github.com/rohan-varma
","['torch/csrc/distributed/c10d/init.cpp', 'torch/csrc/distributed/c10d/reducer.cpp', 'torch/csrc/distributed/c10d/reducer.hpp', 'torch/nn/parallel/distributed.py', 'torch/testing/_internal/distributed/distributed_test.py']","Creating new DDP instances for the same model does not always clear out old autograd hooks, causing issues due to reliance on Python garbage collection mechanisms.
"
608f44b24b783accd21a6bc663ce93f0503438d2,1614272019,"ns for fx: update graph matching to not match nodes with equal types (#52402)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52402

Before this PR, any pair of subgraphs with base nodes of equal
types matched.

While sometimes this is useful, this should be off by default to
properly handle user defined modules and functions, for which we do not
know how to extract weights or cast to the right input type.

In a future PR, we can add hooks to turn on matching for nodes
of equal types, for the situations where it makes sense.

Test Plan:
```
python test/test_quantization.py TestFXGraphMatcher.test_nodes_with_equal_types_do_not_get_matched
```

Imported from OSS

Reviewed By: hx89

Differential Revision: D26499848

fbshipit-source-id: 5818b88eb7fd8ed36390f60aa1a18228bb50507e
","['test/quantization/test_numeric_suite_fx.py', 'torch/quantization/ns/graph_matcher.py']","Graph-matching behavior causes subgraphs with base nodes of identical types to match, interfering with the handling of user-defined modules and functions."
15ceafb5c5633083b5eedf2dfb63f0b84dc4cbc5,1693140824,"[Quant][Inductor] Enable qlinear weight prepack inside inductor constant folding (#106782)

**Summary**
To realize weight prepack for quantized linear, we replace the following pattern
```
int8 activation
      |
dequant_per_tensor
      |
mm/addmm <- t <- dequant_per_channel <- int8_weight
```
with
```
int8 activation
  |
onednn.qlinear_pointwise <- onednn.qlinear_prepack <- int8_weight
```
And we register weight prepack path inside inductor constant folding. Constant folding evaluates the prepack op and replace it with prepacked weight (a constant parameter)

**Test plan**
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_unary

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106782
Approved by: https://github.com/jgong5, https://github.com/leslie-fang-intel, https://github.com/eellison
ghstack dependencies: #105818, #106781
","['test/inductor/test_mkldnn_pattern_matcher.py', 'torch/_inductor/fx_passes/quantization.py', 'torch/_meta_registrations.py']","Dequant per channel in quantized linear activation does not support weight prepacking, causing performance issues in Inductor constant folding."
d51d2bd6080726d7ff579ae70854734d963f17ec,1644288187,"[SR] Add a flag to disable copy variants (#71102)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71102

This graph pass is causing a major perf regression on some models. Ideally we would introduce maybe_copy variants for all these ops. But since those are tricky to write, I've introduced a flag to just turn the pass off for now.
ghstack-source-id: 148541673

Test Plan: `buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest`

Reviewed By: navahgar

Differential Revision: D33510080

fbshipit-source-id: bb4847f26561197ea5e6bbad0a4d25db4ef468eb
(cherry picked from commit 8f333d3e8138e2a7ba04bea7509ad84dd97844eb)
","['torch/csrc/jit/runtime/static/impl.cpp', 'torch/csrc/jit/runtime/static/impl.h']","Performance regression occurring on some models due to a graph pass, possible need for introducing maybe_copy variants for operations."
b4a8d98247c4eac60ae594ae626886408dc4e2c0,1614313441,"[caffe2] use AddNAlreadyReserved() when serializing blobs

Summary:
Optimize the blob serialization code by using `AddNAlreadyReserved()` when
serializing tensor data, rather than making N separate `Add()` calls.
`AddNAlreadyReserved()` is a simple addition operation, while each `Add()`
call checks to see if it needs to reserve new space, and then updates the
element data, which is unnecessary in this case.

Test Plan:
This appears to improve raw serialization performance by 30 to 35% for float,
double, and int64_t types which use this function.  This improvement appears
relatively consistent across large and small tensor sizes.

Differential Revision: D26617038

fbshipit-source-id: 97dedbae889d35463628f3016ac56986e685289e
",['caffe2/core/blob_serialization.h'],"Blob serialization code is inefficient due to multiple `Add()` calls for tensor data, potentially slowing down the serialization performance for different data types."
c19cda5782f4c38140cb5c6aec08d5e44ec0c4cd,1635535041,"[skip ci] Add test owners for a special hi-pri class of tests (#67553)

Summary:
Action following https://github.com/pytorch/pytorch/issues/66232

This change does require some context: there were several suggestions regarding what to do about this group of tests: tests that are core and crucial to all of PyTorch and are too broad to be owned by one team.
1. Let's add a ""module: core"" and put people behind it! This idea sounds appealing unless you are one of the people backing the label. From talking to albanD among others, this idea of putting all these core tests on the shoulder of a few people or one team isn't super fair and I have not yet found anyone willing to take on this job.
2. Taking advantage of the fact that we already have a triaging oncall that takes turns triaging issues, we can leave these tests essentially unlabeled and allow the oncall to triage these tests. Since these tests are crucial to PyTorch, we'll add the ""high priority"" label to mark them different from other unowned tests (see https://github.com/pytorch/pytorch/issues/67552).
3. I _could_ still create an unbacked label ""module: core"" and attribute these tests there, but I don't like the idea of creating a facade that the tests are ""triaged"" to a label when no one is actually taking a look.

Now we could potentially break these tests down into smaller files so that each piece _could_ be owned by a team, but 1. I don't know if this is currently feasible and 2. This approach does not prevent that from happening in the future.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67553

Reviewed By: albanD

Differential Revision: D32025004

Pulled By: janeyx99

fbshipit-source-id: 1fb1aa4c27e305695ab6e80ae3d02f90519939c0
","['test/test_foreach.py', 'test/test_functional_autograd_benchmark.py', 'test/test_module_init.py', 'test/test_modules.py', 'test/test_ops.py', 'test/test_overrides.py', 'test/test_python_dispatch.py', 'test/test_pytree.py', 'test/test_stateless.py', 'test/test_utils.py']","High-priority, core tests in PyTorch are currently without clear test ownership, posing triage fairness and issue addressing challenges."
cc46dc45e1b4b2a9ffab4ad5442f8b864148e45a,1638535527,"[SR] Factor logic that determines managed tensors out of MemoryPlanner (#68295)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68295

There's no reason we can't figure out what tensors we need to manage at model load time. It's also useful to have the set of ranges available at load time for integrating the ranges algorithm introduced in the previous diff.

Test Plan: `buck test caffe2/benchmarks/static_runtime/...`

Reviewed By: hlu1

Differential Revision: D32400593

fbshipit-source-id: 0466b2641166ddc9c14f72774f4ba151407be400
","['torch/csrc/jit/runtime/static/impl.cpp', 'torch/csrc/jit/runtime/static/impl.h', 'torch/csrc/jit/runtime/static/memory_planner.cpp', 'torch/csrc/jit/runtime/static/memory_planner.h']","The process of determining which tensors need to be managed is currently integrated with MemoryPlanner, and not available at model load time, limiting integration of the ranges algorithm."
83275d8cdf7721285c4e1b921c28295dc215ba7c,1675820912,"add torch.autograd._set_view_replay_enabled, use in aot autograd (#92588)

tldr; this should fix some minor perf regressions that were caused by adding more as_strided() calls in aot autograd.

This PR adds a new context manager, `torch.autograd._set_view_replay_enabled()`.

Context: AOT Autograd has special handling for ""outputs that alias graph intermediates"". E.g. given this function:

```
def f(x):
    y = torch.mul(x, 2)
    out = y.view(-1)
    return out
```

AOT Autograd will do the following:

```
def fn_to_compile(x):
    y = torch.mul(x, 2)
    out = y.view(-1)
    # return the graph intermediate
    return y, out

compiled_fn = compile(fn_to_compile)

def wrapper(x):
    y, out = compiled_fn(x)
    # regenerate the alias of the graph intermediate
    return out._view_func(y)
```

What's annoying is that `out._view_func()` will result in a `.as_strided` call, because `out` is an ordinary runtime tensor. This (likely?) caused a perf regression, because when running the backward, out `as_strided_backward()` is slower than our `view_backward()`.

In this PR, I added some TLS for instructing autograd to do view replay instead of as_strided, even when given a normal tensor. I'm definitely interested in thoughts from autograd folks (cc @albanD @soulitzer). A few points that I want to bring up:

(1) One reason that this API seems generally useful to me is because of the case where you `torch.compile()` a function, and you pass in two inputs that alias each other, and mutate one of the inputs. Autograd is forced to add a bunch of as_strided() calls into the graph when this happens, but this would give users an escape hatch for better compiled perf in this situation

(2) To be fair, AOT Autograd probably won't need this TLS in the long term. There's a better (more complicated) solution, where AOT Autograd manually precomputes the view chain off of graph intermediates during tracing, and re-applies them at runtime. This is kind of complicated though and feels lower priority to implement immediately.

(3) Given all of that I made the API private, but lmk what you all think.

This is a followup of https://github.com/pytorch/pytorch/pull/92255.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92588
Approved by: https://github.com/ezyang, https://github.com/albanD
","['c10/core/AutogradState.h', 'test/test_autograd.py', 'tools/autograd/gen_inplace_or_view_type.py', 'torch/_functorch/aot_autograd.py', 'torch/autograd/__init__.py', 'torch/autograd/grad_mode.py', 'torch/csrc/autograd/init.cpp']","Performance regressions due to increased `as_strided()` calls in AOT Autograd, particularly when handing outputs that alias graph intermediates. Issue specifically occurs during view replay of normal tensors."
ad6dad810e871da3ec9d19f1ec0f11a7b62fca03,1689655333,"[dynamo][profiler] More verbose profiler warning (#105362)

torch.profiler.record_function and torch.profiler.profile are ignored by dynamo. In the common case, users have `record_function` in the middle of their program in order to annotate a section of the profile.

The previous error message was `Profiler will be ignored`. Users would think that profiling would be completely ignored.

Now the message will look like `Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105362
Approved by: https://github.com/yanboliang, https://github.com/aaronenyeshi
",['torch/_dynamo/variables/torch.py'],Dynamo ignores torch.profiler.record_function and torch.profiler.profile causing ambiguous profiler warnings leading to confusion about the level of profiling ignored.
33b7e6ff239ef674ff3cf012b3a280405fae07b9,1644561342,"Convert type comments to annotations in torch/nn (#72662)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72662

This commit was produced by running
```
python -m libcst.tool codemod --no-format --jobs=1 convert_type_comments.ConvertTypeComments caffe2/torch/nn/ --no-quote-annotations
```
and then manually fixing unreadable lines by breaking up very
long function defintiion (unfortuantely
it's very difficult to fully automate tranforms of code that
isn't autoformatted).

Test Plan:
Wait for CI. This should be safe, the types all appear to be valid - but it's
always good to let the jit tests run, in some cases we find typing errors that
crash tests.

Reviewed By: jbschlosser, albanD

Differential Revision: D34147388

fbshipit-source-id: 40701228837a927b54239ab87699b4b3169546b7
(cherry picked from commit 05a900c43f194d139f21382fd8bb388f1a462c4a)
","['torch/nn/quantized/dynamic/modules/rnn.py', 'torch/nn/utils/rnn.py']","Manual conversion of type comments to annotations in torch/nn may result in unreadable lines in long function definitions, also automated transformation has potency to create typing errors."
60a3b7425dde97fe8b46183c154a9c3b24f0c733,1675589052,"Small refactor of shape guards to allow for 1:1 code_parts (#93894)

By moving guard string assembly into dynamo's default behavior and letting code_parts do the work, we can have much better shape guard failures.

Before this fix, the guard failure in the test would look like:

```
'x.size()[1] == x.size()[0] and x.stride()[0] == x.[264 chars]!= 1' != 'x.size()[0] < 3'
- x.size()[1] == x.size()[0] and x.stride()[0] == x.size()[0] and x.stride()[1] == 1 and x.storage_offset() == 0 and y.size()[0] == x.size()[0] and y.size()[1] == x.size()[0] and y.stride()[0] == x.size()[0] and y.stride()[1] == 1 and y.storage_offset() == 0 and x.size()[0] < 3 and x.size()[0] != 0 and x.size()[0] != 1
+ x.size()[0] < 3
```
now it is
```
""x.size()[0] < 3""
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93894
Approved by: https://github.com/ezyang
","['test/dynamo/test_export.py', 'test/dynamo/test_misc.py', 'test/test_proxy_tensor.py', 'torch/_dynamo/guards.py', 'torch/fx/experimental/symbolic_shapes.py']","Shape guard failures in code parts are verbose and unclear, making it difficult to understand the actual guard condition that failed."
03a79f43e33c1bef65fc8912c27160d01e0e15d5,1626266280,"adding support for index_select on quantized tensors (#61406)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61406

Only really needed to fix a few select functions so that they could work
for quantized tensors. Primarily creation and resizing of tensors
required a branch for quantized tensors. This doesn't work for
per_channel tensors

Test Plan:
```python test/test_quantization.py TestQuantizedTensor.test_qtensor_index_select_cuda```

```python test/test_quantization.py TestQuantizedTensor.test_qteensor_index_select_cpu```

Imported from OSS

Reviewed By: jerryzh168

Differential Revision: D29654446

fbshipit-source-id: 8fde9b2dd2c3e380cc330bbad71d6c4d2aeec0ab
","['aten/src/ATen/native/TensorAdvancedIndexing.cpp', 'aten/src/ATen/native/cuda/Indexing.cu', 'test/quantization/core/test_quantized_tensor.py']","Quantized tensors currently lack support for index select, impeding the creation and resizing of these tensors. This issue is also present on per_channel tensors."
9da032ecee8b0c7a5ce822bb4425af9208dc2fa1,1666067698,"[BE] Get rid of deprecation warnings in workflows (#87152)

- Per [deprecation announcement](https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/) replace `echo ""::set-output name=""` with echo to `${GITHUB_OUTPUT}` as shown in following [example](https://docs.github.com/en/actions/using-jobs/defining-outputs-for-jobs#example-defining-outputs-for-a-job)
- Update `actions/setup-python` from `v2` to `v4` to get rid of deprecated node version warning
- Update `actions/checkout-python` from `v2` to `v3` (and `silent-checkout` branch as well)
- Update `retry` action to https://github.com/nick-fields/retry/commit/3e91a01664abd3c5cd539100d10d33b9c5b68482
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87152
Approved by: https://github.com/kit1980, https://github.com/izaitsevfb
","['.github/scripts/filter_test_configs.py', '.github/scripts/parse_ref.py', '.github/scripts/test_filter_test_configs.py']",Deprecation warnings are occurring in workflows due to outdated actions and the use of deprecated commands in GitHub Actions.
0be334a1baba4a96a1341e8951e25a03bcf71301,1621015782,"optimize channels last for BatchNorm2d on CPU (#48919)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48919

move data indexing utils

parallel inference contiguous path

parallel inference channels last path

add dim apply

optimize update stats

add channels last support for backward

Revert ""add channels last support for backward""

This reverts commit cc5e29dce44395250f8e2abf9772f0b99f4bcf3a.

Revert ""optimize update stats""

This reverts commit 7cc6540701448b9cfd5833e36c745b5015ae7643.

Revert ""add dim apply""

This reverts commit b043786d8ef72dee5cf85b5818fcb25028896ecd.

bug fix

add batchnorm nhwc test for cpu, including C=1 and HW=1

Test Plan: Imported from OSS

Reviewed By: glaringlee

Differential Revision: D25399468

Pulled By: VitalyFedyunin

fbshipit-source-id: a4cd7a09cd4e1a8f5cdd79c7c32c696d0db386bd
","['aten/src/ATen/cpu/vec256/functional.h', 'aten/src/ATen/native/Normalization.cpp', 'aten/src/ATen/native/batch_norm.h', 'aten/src/ATen/native/cpu/batch_norm_kernel.cpp', 'aten/src/ATen/test/vec256_test_all_types.cpp', 'aten/src/ATen/test/vec256_test_all_types.h', 'test/test_nn.py']","BatchNorm2d on CPU is not optimized for channels-last data layout, causing inefficiency in indexing and updating statistics during parallel inference and backward passes. Also, lacks test coverage for NHWC (Batch x Height x Width x Channel) layout."
958651327f02763d1ccc4b30aeb80eb5e2a2b3f2,1660248113,"Set default qengine to QNNPACK on ARM for quantization tests (#83097)

The issue is that there is no FBGEMM or MKLDNN on ARM and QNNPACK is disabled by default for some reason. Not sure if this PR is the right approach, alternatives are probably:
1) keep these tests skipped
2) not disable QNNPACK by default on ARM
3) ?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83097
Approved by: https://github.com/jerryzh168, https://github.com/albanD
","['test/quantization/core/test_docs.py', 'test/quantization/fx/test_quantize_fx.py']","Quantization tests fail on ARM due to lack of FBGEMM or MKLDNN and disabling QNNPACK by default, needing an appropriate qengine."
74993dcf7b92f6c52ae321291d14b170fcc2ecac,1616029513,"Add repeats to Timer.collect_callgrind(...) (#53295)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53295

A lot of the time spent in `collect_callgrind` is spinning up Valgrind and executing the initial `import torch`. In most cases the actual run loop is a much smaller fraction. As a result, we can reuse the same process to do multiple replicates and do a much better job amortizing that startup cost. This also tends to result in more stable measurements: the kth run is more repeatable than the first because everything has been given a chance to settle into a steady state. The instruction microbenchmarks lean heavily on this behavior. I found that in practice doing several `n=100` replicates to be more reliable than one monolithic 10,000+ iteration run. (Since rare cases like memory consolidation will just contaminate that one replicate, as opposed to getting mixed into the entire long run.)

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D26907093

Pulled By: robieta

fbshipit-source-id: 72e5b48896911f5dbde96c8387845d7f9882fdb2
","['test/benchmark_utils/test_benchmark_utils.py', 'torch/csrc/Module.cpp', 'torch/utils/benchmark/utils/timer.py', 'torch/utils/benchmark/utils/valgrind_wrapper/compat_bindings.cpp', 'torch/utils/benchmark/utils/valgrind_wrapper/timer_callgrind_template.cpp', 'torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py']","`collect_callgrind` function in Timer module is plagued by high startup time costs due to Valgrind initiation and initial `import torch` execution, adversely affecting the stability and reliability of measurements."
d00de0d43598522b8f6ab2de553b6aaf6768faa5,1646076860,"Support dataclasses in TorchScript (#73066)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/72901.

Since we can't get access to the source code for synthesized magic methods on dataclasses, we have to synthesize our own versions. `torch/jit/_dataclass_impls.py` has the code that does this.

What's supported
- Synthesized `__init__`, `__eq__`, and the comparison magic methods when `order=True` is set on the dataclass decorator
- Default values for fields
- `__post_init__`, including using `InitVar` fields inside of `__post_init__`, on Python 3.8+
- Overriding `__eq__` or any of the comparison magic methods to provide your own implementation

What's not supported
- Default factory initializers for fields
- Frozen dataclasses
- `InitVar` on Python 3.7
- `__repr__` and `__hash__` (these are actually implemented, but the TorchScript interpreter won't call them)
- Using the `!=` operator on dataclasses inside TorchScript; this is because TorchScript requires that you implement `__ne__` to use this operator, whereas in regular Python the `!=` operator will resolve to the negation of whatever is returned by `__eq__` if there's no `__ne__`. Dataclasses don't actually synthesize an `__ne__` method for this reason. I've been toying with different ways to fix this but `!=` is not working in this PR at the moment.

qihqi

Pull Request resolved: https://github.com/pytorch/pytorch/pull/73066

Reviewed By: mrshenli

Differential Revision: D34398107

Pulled By: qihqi

fbshipit-source-id: f5a792555c88f3631f97837a96687e4890660a32
(cherry picked from commit ea7f077dc49a4ee75ca0d1409aedd85228952881)
","['test/jit/test_dataclasses.py', 'test/jit/test_misc.py', 'test/test_jit.py', 'torch/_jit_internal.py', 'torch/_sources.py', 'torch/jit/_dataclass_impls.py', 'torch/jit/frontend.py']","Dataclasses in TorchScript have certain unsupported features like default factory initializers, frozen dataclasses, Python 3.7's `InitVar`, and synthesized `__repr__`, `__hash__` methods, and the '!=' operator causing execution issues and inconsistencies."
a2392b000c1dc09b3631096f198ee0d79cb296fa,1650332459,"[quant][core][gpu][improvement] Set tensors as virtual in quantized cudnn linear op

Summary:
For cudnn, if certain intermediate tensors are not needed for other
parts of the operation graph in cudnn, the it can be set as virtual, so
that a pointer-uid pair does not need to be provided for the variant
pack, allowing for better performance.

Test plan:
```
python test/test_quantization.py -k test_qlinear_cudnn
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75967

Approved by: https://github.com/jerryzh168
","['aten/src/ATen/native/quantized/cudnn/Linear.cpp', 'aten/src/ATen/native/quantized/cudnn/utils.h']","Intermediate tensors in cuDNN not set as virtual, causing unnecessary pointer-uid pairs in the variant pack and hindering performance."
bf6481553ac03e3e6cdae11403f41a2797a738c0,1658471633,"Ensure `Transform` is pickleable. (#81707)

`Transform` is not currently pickleable if the inverse transform cache `_inv` is not `None` because `_inv` is a `weakref` which cannot be serialized by `pickle`.

The following succeeds.

```python
>>> import torch as th
>>> import pickle

>>> dist = th.distributions.TransformedDistribution(
...     th.distributions.Normal(0, 1),
...     [th.distributions.AffineTransform(2, 3)]
... )
>>> th.save(dist, ""some-file.pt"")
```

But the transformed distribution can no longer be pickled after evaluating `log_prob` (which implicitly creates `_inv`).

```python
>>> dist.log_prob(th.linspace(0, 1, 10))
>>> th.save(dist, ""some-file.pt"")
TypeError: cannot pickle 'weakref' object
```

This PR fixes the issue by setting `_inv` to `None` in `__getstate__`. cc @fritzo, @neerajprad
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81707
Approved by: https://github.com/fritzo
","['test/distributions/test_transforms.py', 'torch/distributions/transforms.py']","`Transform` object in PyTorch is unable to be pickled after evaluating `log_prob` due to the `_inv` weakref object, leading to TypeError: cannot pickle 'weakref' object."
e8690dacb2dc0c28846e5bbbc8638e9b6de8bd88,1624461703,"To add Nesterov Adam Algorithm to Optimizers (#59009)

Summary:
Fixes : https://github.com/pytorch/pytorch/issues/5804

In the paper : https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ  Timothy Dozat suggested a new optimization algorithm with an essence of combination of NAG and Adam algorithms.

It is known that the idea of momentum can be improved with the Nesterov acceleration in optimization algorithms, and Dozat is investigating to apply this idea to momentum component of Adam algorithm. Author provided experiment evidence in their work to show excellence of the idea.

In this PR we are implementing the proposed algorithm NAdam in the mentioned paper. Author has a preliminary work http://cs229.stanford.edu/proj2015/054_report.pdf  where he shows the decay base constant should be taken as 0.96 which we also followed the same phenomenon here in this implementation similar to Keras. Moreover, implementation / coding practice have been followed similar to Keras in some other places as well:

https://github.com/tensorflow/tensorflow/blob/f9d386849581d15d72f6f1f96f12aac230a8edbe/tensorflow/python/keras/optimizer_v2/nadam.py

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59009

Reviewed By: gchanan, vincentqb

Differential Revision: D29220375

Pulled By: iramazanli

fbshipit-source-id: 4b4bb4b15f7e16f7527f368bbf4207ed345751aa
","['test/test_optim.py', 'torch/optim/__init__.py', 'torch/optim/_functional.py', 'torch/optim/nadam.py']","PyTorch lacks Nesterov Adam (NAdam) optimization algorithm, an improved version of Adam optimizer with Nesterov acceleration, as proposed in Timothy Dozat's paper."
6c80d0a5a55de71d0cc56950f7459b5c334a14ab,1675908400,"[MPS] Fix correctness issues with Pool2D ops (#94348)

- Fix wrong results in AvgPool2D when `count_include_pad=True`
- Fix issues with adaptive average and max pool2d
- Remove the redundant blocking copies from `AdaptiveMaxPool2d`
- Add `divisor` to cached string key to avoid conflicts
- Add test case when both `ceil_mode` and `count_include_pad` are True (previously failed).
- Clean up redundant code
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94348
Approved by: https://github.com/kulinseth
","['aten/src/ATen/native/mps/operations/AdaptivePooling.mm', 'aten/src/ATen/native/mps/operations/Pooling.mm', 'test/test_mps.py']","`AvgPool2D` returns incorrect results when `count_include_pad=True`. Also, issues exist with adaptive average and max pool2d functionality, along with unnecessary blocking copies in `AdaptiveMaxPool2d`. Concurrently, a potential key conflict due to missing `divisor` has been observed. As a special case, tests fail when both `ceil_mode` and `count_include_pad` are True."
20374c991b20be8d2ccd8e1cdf181e5cdf19fffa,1632949772,"slow_conv2d_forward: avoid calling dispatcher in parallel region (#65724)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65724

See gh-56794

Avoid dispatch inside of parallel_for by:
1. Replacing Tensor slicing with TensorAccessor
2. Copy bias into output only once, outside of the parallel region
3. Replaces `addmm`_ with a direct call to gemm.

Technically this also adds a new requirement that the output always be
contiguous, but the out argument version isn't exposed or used
anywhere in the `torch.nn` API. So that should be fine.

Test Plan: Imported from OSS

Reviewed By: saketh-are

Differential Revision: D31257875

Pulled By: ngimel

fbshipit-source-id: 84d2b39e7f65334bdfcc2c4719f93ee3c514ca32
","['aten/src/ATen/native/ConvolutionMM2d.cpp', 'aten/src/ATen/native/Unfold2d.h', 'aten/src/ATen/native/cpu/Unfold2d.cpp']","The slow_conv2d_forward function causes issues when dispatcher is called inside a parallel region, due to Tensor slicing and addition of bias multiple times within this region. Additionally, the output is not always guaranteed to be contiguous."
0099796978a36f6f65e688d34565526c48d39265,1643247235,"[CUDA Pinned Memory] [Retry] Alternative implementation of pinned memory allocator focusing on multi-threaded scalability (#69299)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69299

https://github.com/pytorch/pytorch/pull/68906 + https://github.com/pytorch/pytorch/pull/68749 plugged one correctness hole (non-blocking copies of offset pinned memory tensors) while introducing another (non-blocking copies of pinned memory tensors with a non-standard DataPtr context).

In this revision, we use both the tensor data pointer and context to attempt to identify the originating block in the pinned memory allocator.

Test Plan: New unit tests added to cover the missing case previously.

Reviewed By: yinghai

Differential Revision: D32787087

fbshipit-source-id: 0cb0d29d7c39a13f433eb1cd423dc0d2a303c955
(cherry picked from commit 297157b1a13b5c75d860cac9eba4fe7fe1ad5e6f)
","['aten/src/ATen/cuda/CachingHostAllocator.cpp', 'aten/src/ATen/cuda/CachingHostAllocator.h', 'aten/src/ATen/native/cuda/Copy.cu', 'aten/src/ATen/test/cuda_caching_host_allocator_test.cpp', 'test/test_cuda.py']",Non-blocking copies of pinned memory tensors with a non-standard DataPtr context are causing incorrect identification of the originating block in the pinned memory allocator in multithreaded scenarios.
4a0f6e6c530b624bb5a4fbcaebe3fd43b1ff66c3,1647524447,"report an error if num_channels is not divisible by num_groups for nn.GroupNorm

 For a GroupNorm module, if num_channels is not divisible by num_groups, we need to report an error when defining a module other than at the running step.

example:
```
import torch
m = torch.nn.GroupNorm(5, 6)
x = torch.randn(1, 6, 4, 4)
y = m(x)
```
before:

```
Traceback (most recent call last):
  File ""group_norm_test.py"", line 8, in <module>
    y = m(x)
  File ""/home/xiaobinz/miniconda3/envs/pytorch_mater/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1111, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xiaobinz/miniconda3/envs/pytorch_mater/lib/python3.7/site-packages/torch/nn/modules/normalization.py"", line 271, in forward
    input, self.num_groups, self.weight, self.bias, self.eps)
  File ""/home/xiaobinz/miniconda3/envs/pytorch_mater/lib/python3.7/site-packages/torch/nn/functional.py"", line 2500, in group_norm
    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected number of channels in input to be divisible by num_groups, but got input of shape [1, 6, 4, 4] and num_groups=5
```

after:

```
Traceback (most recent call last):
  File ""group_norm_test.py"", line 6, in <module>
    m = torch.nn.GroupNorm(5, 6)
  File ""/home/xiaobinz/miniconda3/envs/pytorch_test/lib/python3.7/site-packages/torch/nn/modules/normalization.py"", line 251, in __init__
    raise ValueError('num_channels must be divisible by num_groups')
```

This PR also update the doc of num_groups.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74293
Approved by: https://github.com/jbschlosser
","['test/test_module_init.py', 'test/test_nn.py', 'torch/nn/modules/normalization.py']","Module definition in GroupNorm allows instantiation even when 'num_channels' isn't divisible by 'num_groups', resulting in runtime errors."
4d055ee5a1df0620c0daad9d8defd4a0523bed0a,1685671861,"RelaxUnspecConstraint some more (#102729)

One annoyance with mark_dynamic is if you use it on a user specified
tensor input (the idea being that you want to compile a function and
have it be polymorphic in size), you will get an error if the user
ever sends you a 0/1 size input, because of course we are probably
going to specialize it.  So I relax the constraint even more: even if we
find it's constant, if the value is 0/1, that's no big deal.

There's some irritating code duplication that I don't entirely know how
to resolve.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102729
Approved by: https://github.com/avikchaudhuri, https://github.com/voznesenskym
","['test/dynamo/test_unspec.py', 'torch/fx/experimental/symbolic_shapes.py']","Using mark_dynamic on a user-specified tensor input causes errors when the user inputs a size of 0/1, preventing function compilation and size polymorphism."
b3e7230efa30e03b9ee8ff3e2b31c38754a6b2b1,1652936793,"[symint] Fix SizesAndStridesTest to not use negative sizes/strides

With SymInt we are using the negative space of `int64_t` in our internal
representation. `SizesAndStridesTest` breaks this because it initializes
`SizesAndStrides` with negative sizes/strides. This PR fixes that.

As an aside: feels like `SizesAndStrides` (and `SymInt`) should really
take a uint64_t, but that would be BC-breaking so I don't do it here.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77820

Approved by: https://github.com/ezyang
",['c10/test/core/impl/SizesAndStrides_test.cpp'],"`SizesAndStridesTest` causes failures due to initialization with negative sizes/strides, which is inconsistent with SymInt's internal representation utilizing the negative space of `int64_t`."
731f417f60bfd5bb8d2ec756c23c0e6624ea3351,1671455156,"Use scalar implementation to keep the precision in linspace of integral types (#89048)

Fixes #88652

In the CPU implementation of linspace of integral types, `base` type in vectorized implementation is `int64_t`, which will drop the precision when `base` comes from a floating number. Meanwhile, its vectorized implementation tends to suffer from the catastrophic cancellation of floating point arithemtic since both the `base (start + step * idx)` and the `step` are not exact. Its scalar implementation is fine since start is always an integer and the result would be truncated to integer as well.

Therefore, in this PR , we will skip the vectorized implementation since the vec doesn't contribute to performance anyway. And now the behaviors between CPU and GPU are the same. In some cases, the results are the same as numpy's. In some other cases, the results are different from numpy's, but it is not related to the devices (CPU and GPU). https://github.com/pytorch/pytorch/issues/81996#issuecomment-1192980485

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89048
Approved by: https://github.com/mingfeima, https://github.com/jgong5, https://github.com/albanD
","['aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp', 'test/test_tensor_creation_ops.py']","The CPU implementation for linspace in integer types is dropping precision when 'base' comes from a floating number, leading to discrepancies with the GPU behaviors and Numpy results."
4144ad16afaa0772a51477a25249d63e80bbe3e4,1669840682,"add XPU backend to support torch.save and torch.load (#89679)

# Motivate
We need to add XPU backend to support torch.save and torch.load when parameter _use_new_zipfile_serialization=False.

# Solution
We give a design via wrap data as a tensor:
>1. and use an in-place copy for H2D
>2. directly call a tensor.to() for D2H.

This can help us:
>1. unify the generic code for all backends.
>2. support all the non-CPU device backends.

# Additional Context
No need more UT.
test/test_serialization.py will cover this code change.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89679
Approved by: https://github.com/ezyang
","['test/test_torch.py', 'torch/csrc/serialization.cpp']",The XPU backend currently does not support torch.save and torch.load functionality when _use_new_zipfile_serialization is set to False.
63d45275f462f7afd3c1fe3e9c36644348e78030,1691158608,"is causal hints for transformer (#106143)

Summary:
make is_causal hint flags available for the top level transformer module.

It's debatable whether this is useful -- at present we autodetect causal masks for src and tgt masks in transformer encoder and decoder, respectively. is_causal flags available woul enable users to short-cut this check by asserting whether they mask is causal, or not.

I am putting this diff up for discussion, not as a solution.  Not doing anything may be the right solution, unless there is strong (data-driven) user demand. -- it appears the consensus is to move ahead with this, as per discussions below.

@cpuhrsch @mikaylagawarecki @jbschlosser @janEbert

Test Plan: sandcastle

Differential Revision: D47373260

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106143
Approved by: https://github.com/mikaylagawarecki
","['test/functorch/test_aotdispatch.py', 'test/test_nn.py', 'torch/nn/modules/transformer.py']",The causal masks in transformer encoder and decoder are auto-detected which may not be efficient or accurate. An option to bypass this check by asserting if the mask is causal could be beneficial.
ebc7039bcb48b1723de922cc8f3eb8008e026285,1695365906,"New export API with dynamic shape specifications instead of constraints (#108448)

Our experience using `constraints` / `dynamic_dim` with the existing export API has found it to be (subjectively) clunky and (objectively) verbose in common cases.

This PR implements a new design for the export API that replaces the use of `constraints` / `dynamic_dim` with a new way of specifying dynamic shapes, involving the following concepts:
* a constructor `Dim` for first-class named dynamic dimensions with ranges (similar to `functorch.dim`, and analogous to internal symbolic sizes)
* a mechanism that uses the above in `export` calls to associate inputs to their dynamic shape specifications (`dynamic_shapes`)

Design doc: https://docs.google.com/presentation/d/168U7XK72C_WSsZpGESP6Cho9udh193fi0gfjxCNcJ4E/edit#slide=id.p (Meta-only). Note that we only implement Option 1 in that doc. An older version of this PR also implemented Option 3, which is an alternative way of specifying dynamic shapes using tensor type annotations on the exported callable; but we have moved that to future work for now.

See docs for these new features in `torch.export`. The existing `torch.export.export` is modified to use the new API, `torch._export.export__RC__`, whenever `constraints=None`. We have not deprecated the existing API yet, but will do in a follow-up.

Constraint violation errors arising through use of the new API will now contain suggested fixes using the new API. No longer do we need to report all specializations for static dimensions and suggest all constraints over dynamic dimensions to fix such errors. Instead, due to the redesign, the suggested fixes are much more concise, only involving modifying the definitions of relevant `Dim`s.

Differential Revision: [D48919204](https://our.internmc.facebook.com/intern/diff/D48919204/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108448
Approved by: https://github.com/suo, https://github.com/gmagogsfm
","['test/export/test_export.py', 'test/test_dynamic_shapes.py', 'torch/_dynamo/eval_frame.py', 'torch/_dynamo/variables/builder.py', 'torch/_export/__init__.py', 'torch/export/__init__.py', 'torch/fx/experimental/symbolic_shapes.py']","Current implementation of export API using `constraints` and `dynamic_dim` is clunky and verbose in common use cases, with constraint violation errors suggesting complicated fixes."
e70f3d118943cb0abfc045e635cf2aa39f13ba62,1616871186,"Nasty little hack to preserve NotImplementedError raised in interpreter (#54627)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54627

This is the simplest little fix to get interpreter to preserve
NotImplementedError, so that the test suite doesn't start choking
on meta tensors not working in interpreter.  It is sound and correct
but doesn't work for other c10::Error subclasses with special handling.
A more proper fix is requested at
https://github.com/pytorch/pytorch/issues/54612

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: wenleix, ngimel

Differential Revision: D27328666

Pulled By: ezyang

fbshipit-source-id: 483bef062de5a907d20e2d9e25eafe2d5197cf8d
",['torch/csrc/jit/runtime/interpreter.cpp'],"The Python interpreter fails to preserve NotImplementedError, causing issues with meta tensors during testing in PyTorch. This doesn't affect other c10::Error subclasses with special handling."
14ee608791b11efcd9b35e91a12eeab9bf67926c,1634710598,"[PyTorch] Make rearragement in sharded linear work as expected. (#66603)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66603

Found the issue here: https://github.com/pytorch/pytorch/issues/66281 by make the test cases more complicated.

By closely reading the code again, it turns out my original understanding is also wrong. Let's use the example mentioned in the issue to explain:

If the placement is like:
```
""rank:3/cuda:3"",
""rank:0/cuda:0"",
""rank:1/cuda:1"",
""rank:2/cuda:2"",
```

First, we split the column or row by the order of [3, 0, 1, 2].

In the case of column-wise sharding:
We get to reaggrage the result from rank0-4.
Step 1: we split the output based on the original sharding strategy, aka, rank3 gets the 1st shard, rank0 get the 2nd shard, etc.
Step 2: we need to rearrange the result from rank0-4 by ordering them following the order of [3, 0, 1, 2], aka, the result from rank3 needs to be put in the front, and so forth.

In the case of row-wise sharding:
We need to rearrange the input being sent to rank0-4.
Step 1: we reorder the input and follow the map of [3, 0, 1, 2]. For example, the first shard goes to rank 3 so we need to put in the 3rd part, the second shard goes to rank 0, so we put it in the 2nd part, and so on.
Step 2: the size of the sharding for each rank is decided by the original placement: [3, 0, 1, 2], aka, rank 3 gets the first shard and its size, etc.

Update the unit test to reflect this change.

Also, correct some format and comments in the sharded linear.
ghstack-source-id: 141055689

Test Plan: unit test and wait for CI.

Reviewed By: pritamdamania87, bowangbj

Differential Revision: D31634590

fbshipit-source-id: 677a9c2b42da1e2c63220523ed2c004565bbecc7
","['test/distributed/_sharded_tensor/ops/test_linear.py', 'torch/distributed/_sharded_tensor/ops/linear.py', 'torch/testing/_internal/distributed/_sharded_tensor/_test_ops_common.py']","Sharded linear operation in PyTorch is not functioning as expected due to issues with rearrangement during row-wise and column-wise sharding, potentially leading to calculation errors in the split and reaggregation processes."
6370ac0251b04e5a179545f15346f429f5480734,1683218183,"[codemod] Replace hasattr with getattr in caffe2/torch/ao/quantization/stubs.py (#100597)

Summary:
The pattern
```
X.Y if hasattr(X, ""Y"") else Z
```
can be replaced with
```
getattr(X, ""Y"", Z)
```

The [getattr](https://www.w3schools.com/python/ref_func_getattr.asp) function gives more succinct code than the [hasattr](https://www.w3schools.com/python/ref_func_hasattr.asp) function. Please use it when appropriate.

**This diff is very low risk. Green tests indicate that you can safely Accept & Ship.**

Test Plan: Sandcastle

Reviewed By: vkuzo

Differential Revision: D44886422

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100597
Approved by: https://github.com/Skylion007
",['torch/ao/quantization/stubs.py'],"The pattern ""X.Y if hasattr(X, ""Y"") else Z"" in caffe2/torch/ao/quantization/stubs.py can result in less succinct code than using ""getattr(X, ""Y"", Z)""."
2f6ada84b40cb165631a42a40156a875ad6a4fb3,1671164952,"[inductor] Remove flag of bmm's dim m and n in shape padding (#90937)

Summary: There was an OOM issue in two internal models when turning on padding bmm with dim m and n with shape padding optimization, so added a flag to turned on/off for the internal models. The issue was gone now so removing the flag.

Differential Revision: D42074557

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90937
Approved by: https://github.com/ngimel
","['torch/_inductor/config.py', 'torch/_inductor/decomposition.py']",Padding binary matrix multiplication with dimensions m and n caused Out of Memory (OOM) issues in two models when the shape padding optimization was enabled.
