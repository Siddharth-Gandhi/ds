commit_id,commit_date,commit_message,actual_files_modified,transformed_message_gpt4
ea36d4de17101f05b03d267a4afbae0f7b33a27c,1631630886,"Modules: Add remaining list API functions (#8439)

List functions operating on elements by index:

* RM_ListGet
* RM_ListSet
* RM_ListInsert
* RM_ListDelete

Iteration is done using a simple for loop over indices.
The index based functions use an internal iterator as an optimization.
This is explained in the docs:

```
 * Many of the list functions access elements by index. Since a list is in
 * essence a doubly-linked list, accessing elements by index is generally an
 * O(N) operation. However, if elements are accessed sequentially or with
 * indices close together, the functions are optimized to seek the index from
 * the previous index, rather than seeking from the ends of the list.
 *
 * This enables iteration to be done efficiently using a simple for loop:
 *
 *     long n = RM_ValueLength(key);
 *     for (long i = 0; i < n; i++) {
 *         RedisModuleString *elem = RedisModule_ListGet(key, i);
 *         // Do stuff...
 *     }
```","['src/module.c', 'src/quicklist.c', 'src/quicklist.h', 'src/redismodule.h', 'src/server.h', 'src/t_list.c', 'tests/modules/list.c', 'tests/unit/moduleapi/list.tcl']",Current list functions lack operations based on element index leading to inherently O(N) operation time due to doubly-linked list structure. This can result in inefficiency in certain scenarios.
17904780ae5b4793ad133020d6bfa7f4a266b20c,1692171926,"skip the rehashed entries in dictNext (#12386)

If dict is rehashing, the  entries in the head of table[0] is moved to table[1]
and all entries in `table[0][0:rehashidx]` is NULL.

`dictNext` start looking for non-NULL entry from table 0 index 0, and the first call
of `dictNext` on a rehashing dict will Iterate many times to skip those NULL entries.
We can easily skip those entries by setting `iter->index` as `iter->d->rehashidx` when
dict is rehashing and it's the first call of dictNext (`iter->index == -1 && iter->table == 0`).

Co-authored-by: sundb <sundbcn@gmail.com>",['src/dict.c'],"The first call of dictNext on a rehashing dict iterates excessively due to NULL entries in the head of table[0], resulting in suboptimal performance."
344e41c92228f3b8aade6220c48b7d8c6e7817a7,1644146036,"Fix PSYNC crash with wrong offset (#10243)

`PSYNC replicationid str_offset` will crash the server.

The reason is in `masterTryPartialResynchronization`,
we will call `getLongLongFromObjectOrReply` check the
offset. With a wrong offset, it will add a reply and
then trigger a full SYNC and the client become a replica.

So crash in `c->bufpos == 0 && listLength(c->reply) == 0`.
In this commit, we check the psync_offset before entering
function `masterTryPartialResynchronization`, and return.

Regardless of that crash, accepting the sync, but also replying
with an error would have corrupt the replication stream.",['src/replication.c'],"PSYNC replicationId str_offset causes a server crash due to wrong offset. Additionally, error reply during sync causes replication stream corruption."
eb94d6d36dec7323af5fb89a5899506d5c07adb1,1661152351,"Introduce unix socket connection type

Unix socket uses different accept handler/create listener from TCP,
to hide these difference to avoid hard code, use a new unix socket
connection type. Also move 'acceptUnixHandler' into unix.c.

Currently, the connection framework becomes like following:

                   uplayer
                      |
               connection layer
                 /    |     \
               TCP   Unix   TLS

It's possible to build Unix socket support as a shared library, and
load it dynamically. Because TCP and Unix socket don't require any
heavy dependencies or overheads, we build them into Redis statically.

Signed-off-by: zhenwei pi <pizhenwei@bytedance.com>
","['src/connection.c', 'src/connection.h', 'src/networking.c', 'src/server.c', 'src/server.h', 'src/unix.c']","Redis is unable to distinguish between Unix socket and TCP connections and handle them accordingly, causing hardcoding and organization issues."
8fb89a572892e600146bd933c4f8f99d46a519c7,1601484721,"Fixed Tracking test “The other connection is able to get invalidations” (#7871)

PROBLEM:

[$rd1 read] reads invalidation messages one by one, so it's never going to see the second invalidation message produced after INCR b, whether or not it exists. Adding another read will block incase no invalidation message is produced.

FIX:

We switch the order of ""INCR a"" and ""INCR b"" - now ""INCR b"" comes first. We still only read the first invalidation message produces. If an invalidation message is wrongly produces for b - then it will be produced before that of a, since ""INCR b"" comes before ""INCR a"".

Co-authored-by: Nitai Caro <caronita@amazon.com>",['tests/unit/tracking.tcl'],"'Invalidation messages sequentially read by [$rd1 read] fails to access multiple messages, which may lead to unnoticed invalid messages if they exist.'"
eef2d8303d7be6be50ea3b1e45f375765933da9b,1660478020,"Optimization in t_hash.c: Avoid looking for a same field twice by using dictAddRaw() instead of dictFind() and dictAdd() (#11110)

Before this change in hashTypeSet() function, we first use dictFind()
to look for the field and if it does not exist, we use dictAdd() to add it.
In dictAdd() function the dictionary will look for the field again and I
think this is meaningless as we already know that the field does not exist.

An optimization is to use dictAddRaw() instead of dictFind() and dictAdd().
If we use dictAddRaw(), a new entry will be added when the field does not
exist, and what we should do then is just set the value of that entry, and set
its key to 'sdsdup(field)' in the case that 'HASH_SET_TAKE_FIELD' flag wasn't set.",['src/t_hash.c'],Repeated lookup for the same field in hashTypeSet() function with dictFind() and dictAdd() causing unnecessary overhead.
65ef543f8cc4d470f554f06e50594e85aad0d617,1643887235,"Sentinel: return an error if configuration save fails (#10151)

When performing `SENTINEL SET`, Sentinel updates the local configuration file. Before this commit, failure to update the file would still result with an `+OK` reply. Now, a `-ERR Failed to save config file` error will be returned.

Co-authored-by: Yossi Gottlieb <yossigo@gmail.com>","['src/sentinel.c', 'tests/sentinel/tests/03-runtime-reconf.tcl']","Sentinel configuration update continues without interruption even when there is a failure in saving updated file, leading to potential misconfiguration."
fc9b4e79589c049276ff4c7cf536b49d700f8cee,1538107608,"bugfix: replace lastcmd with cmd when rewrite BRPOPLPUSH as RPOPLPUSH

There are two problems if we use lastcmd:

1. BRPOPLPUSH cannot be rewrited as RPOPLPUSH in multi/exec
    In mulit/exec context, the lastcmd is exec.
2. Redis will crash when execute RPOPLPUSH loading from AOF
    In fakeClient, the lastcmd is NULL.
",['src/t_list.c'],Using lastcmd to rewrite BRPOPLPUSH as RPOPLPUSH results in issues - cannot rewrite in multi/exec context and leads to Redis crash when executing RPOPLPUSH loaded from AOF.
1d9c8d61d895b72ecc35b2973c399d0c70a72c18,1629291622,"Skip OOM-related tests on incompatible platforms. (#9386)

We only run OOM related tests on x86_64 and aarch64, as jemalloc on other
platforms (notably s390x) may actually succeed very large allocations. As
a result the test may hang for a very long time at the cleanup phase,
iterating as many as 2^61 hash table slots.",['tests/integration/corrupt-dump.tcl'],"OOM-related tests may hang during the cleanup phase on platforms other than x86_64 and aarch64, due to jemalloc succeeding very large allocations."
dec529f4be3e3314300bb513e7a9f3af636e13b0,1672822376,"Introduce .is_local method for connection layer (#11672)

Introduce .is_local method to connection, and implement for TCP/TLS/
Unix socket, also drop 'int islocalClient(client *c)'. Then we can
hide the detail into the specific connection types.
Uplayer tests a connection is local or not by abstract method only.

Signed-off-by: zhenwei pi <pizhenwei@bytedance.com>

Signed-off-by: zhenwei pi <pizhenwei@bytedance.com>","['src/config.c', 'src/connection.h', 'src/networking.c', 'src/server.h', 'src/socket.c', 'src/tls.c', 'src/unix.c']","There is no abstract method to check if a connection is local or not, requiring manual inspection of specific connection types."
c2b5f1c15bbc5363f92f8e697538759c5d929934,1594806243,"Add registers dump support for Apple silicon (#7453)

Export following environment variables before building on macOS on Apple silicon

export ARCH_FLAGS=""-arch arm64""
export SDK_NAME=macosx
export SDK_PATH=$(xcrun --show-sdk-path --sdk $SDK_NAME)
export CFLAGS=""$ARCH_FLAGS -isysroot $SDK_PATH -I$SDK_PATH/usr/include""
export CXXFLAGS=$CFLAGS
export LDFLAGS=""$ARCH_FLAGS""
export CC=""$(xcrun -sdk $SDK_PATH --find clang) $CFLAGS""
export CXX=""$(xcrun -sdk $SDK_PATH --find clang++) $CXXFLAGS""
export LD=""$(xcrun -sdk $SDK_PATH --find ld) $LDFLAGS""

make
make test
..
All tests passed without errors!

Backtrack logging assumes x86 and required updating",['src/debug.c'],"The software doesn't support registers dump on Apple silicon, causing it to fail when built on macOS using this architecture."
14f802b360ef52141c83d477ac626cc6622e4eda,1688737244,"Initialize cluster owner_not_claiming_slot to avoid warning (#12391)

valgrind report a Uninitialised warning:
```
==25508==  Uninitialised value was created by a heap allocation
==25508==    at 0x4848899: malloc (in
/usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==25508==    by 0x1A35A1: ztrymalloc_usable_internal (zmalloc.c:117)
==25508==    by 0x1A368D: zmalloc (zmalloc.c:145)
==25508==    by 0x21FDEA: clusterInit (cluster.c:973)
==25508==    by 0x19DC09: main (server.c:7306)
```

Introduced in #12344",['src/cluster.c'],Uninitialised value created by a heap allocation in 'clusterInit' causing a warning reported by Valgrind.
85a0d29d04ac280151e3fed28ab4767fbcc7463b,1585841039,"Stale replica should allow MULTI/EXEC

Example: Client uses a pipe to send the following to a
stale replica:

MULTI
.. do something ...
DISCARD

The replica will reply the MUTLI with -MASTERDOWN and
execute the rest of the commands... A client using a
pipe might not be aware that MULTI failed until it's
too late.

I can't think of a reason why MULTI/EXEC/DISCARD should
not be executed on stale replicas...

Also, enable MULTI/EXEC/DISCARD during loading
",['src/server.c'],"Clients using pipe on a stale replica may unknowingly run into a -MASTERDOWN error upon attempting a MULTI command, potentially causing unexpected behaviour."
6d21560190fd5b09ff849ad1777e868d5e78da5f,1663836953,"Fix heap overflow vulnerability in XAUTOCLAIM (CVE-2022-35951) (#11301)

Executing an XAUTOCLAIM command on a stream key in a specific state, with a
specially crafted COUNT argument may cause an integer overflow, a subsequent
heap overflow, and potentially lead to remote code execution.
The problem affects Redis versions 7.0.0 or newer.","['src/t_stream.c', 'tests/unit/type/stream-cgroups.tcl']","Executing XAUTOCLAIM command with a specific COUNT argument on a stream key in a certain state leads to an integer and subsequent heap overflow, posing potential threat for remote code execution in Redis versions 7.0.0 or newer."
a0e19e3cf153ffbd8b1c7e72249d22b92f71532f,1618118071,"Fix wrong check for aof fsync and handle aof fsync errno (#8751)

The bio aof fsync fd may be closed by main thread (AOFRW done handler)
and even possibly reused for another socket, pipe, or file.
This can can an EBADF or EINVAL fsync error, which will lead to -MISCONF errors failing all writes.
We just ignore these errno because aof fsync did not really fail.

We handle errno when fsyncing aof in bio, so we could know the real reason
when users get -MISCONF Errors writing to the AOF file error

Issue created with #8419","['src/bio.c', 'src/server.c', 'src/server.h']","The main thread inadvertently closing the bio aof fsync fd can result in EBADF or EINVAL fsync errors, leading to -MISCONF errors and failed writes, while real reason for issues is obscured."
416f2773395ffcd72d8d8408e1558f49d59a0077,1619605195,"redis-cli: Do not use hostsocket when we got redirected in cluster mode (#8870)

When redis-cli was used with both -c (cluster) and -s (unix socket),
it would have kept trying to use that unix socket, even if it got
redirected by the cluster (resulting in an infinite loop).",['src/redis-cli.c'],"When using redis-cli with both -c (cluster) and -s (unix socket) options, redirection by the cluster results in infinite loop as the unix socket is continuously attempted."
63f606d3e3fa8196f98627a1c3924179811d07a1,1640247678,"redis-cli: Add OUTPUT_JSON format type (#9954)

Introduce `redis-cli --json` option.
CSV doesn't support Map type, then parsing SLOWLOG or HMSET with multiple args are not helpful.

By default the `--json` implies RESP3, which makes it much more useful, and a `-2`
option was added to force RESP2.
When `HELLO 3` fails, it prints a warning message (which can be silenced with `-2`).
If a user passed `-3` explicitly, the non-interactive mode will also exit with error without
running the command, while in interactive session it'll keep running after printing the warning.

JSON output would be helpful to parse Redis replies with other tools like jq.

```
redis-cli --json slowlog get | jq

[
  [
    1,
    1639677545,
    322362,
    [
      ""HMSET"",
      ""dummy-key"",
      ""field1"",
      ""123,456,789... (152 more bytes)"",
      ""field2"",
      ""111,222,333... (140 more bytes)"",
      ""field3"",
      ""... (349 more arguments)""
    ],
    ""127.0.0.1:49312"",
    """"
  ]
]

```",['src/redis-cli.c'],Parsing SLOWLOG or HMSET with multiple args using CSV is problematic due to its lack of support for Map type. This becomes troublesome when trying to interpret Redis replies with other tools.
08cd3bf2924da099facdc1fa30706c93ad3b4871,1678637283,"redis-cli reads specified number of replies for UNSUBSCRIBE/PUNSUBSCRIBE/SUNSUBSCRIBE (#11047)

In unsubscribe related commands, we need to read the specified
number of replies according to the number of parameters.

These commands may return multiple RESP replies, and currently
redis-cli only tries to read only one reply.

Fixes #11046, this redis-cli bug seems to be there forever.
Note that the [UN]SUBSCRIBE command response is a bit awkward
see: https://github.com/redis/redis-doc/pull/2327",['src/redis-cli.c'],"Redis-cli only reads one reply for unsubscribe commands even when multiple RESP replies are returned, leading to incomplete data retrieval."
448c435b1b5ff747d0d84c49a23435a83f0d40ac,1601388654,"Fix compilation warnings in Lua and jemalloc dependencies (#7785)

- The argument `u` in for `ar` is ignored (and generates warnings since `D` became the default.
  All it does is avoid updating unchanged objects (shouldn't have any impact on our build)
- Enable `LUA_USE_MKSTEMP` to force the use of `mkstemp()` instead of `tmpname()` (which is dead
  code in redis anyway).
- Remove unused variable `c` in `f_parser()`
- Removed misleadingly indented space in `luaL_loadfile()` and ``addfield()`

Co-authored-by: Oran Agra <oran@redislabs.com>","['deps/lua/src/lauxlib.c', 'deps/lua/src/ldo.c', 'deps/lua/src/ltablib.c']","Compilation warnings are arising in Lua and jemalloc dependencies due to ignored arguments, unused variables and incorrect indentations. Unsafe method `tmpname()` is being used over 'mkstemp()'."
bb875603fb7ff3f9d19aad906bd45d7db98d9a39,1644267431,"Fix protocol error caused by redis-benchmark (#10236)

The protocol error was caused by the buggy `writeHandler` in `redis-benchmark.c`,
which didn't handle one of the cases, thereby repeating data, leading to protocol errors
when the values being sent are very long.

This PR fixes #10233, issue introduced by #7959",['src/redis-benchmark.c'],"`writeHandler` in `redis-benchmark.c` is causing a protocol error due to repeated data, particularly when very long values are being sent."
18e72c5cc723c60489b1598de8c3c503ef42c7d0,1574933097,"Converting more configs to use generic infra, and moving defaults to config.c

Changes in behavior:
- Change server.stream_node_max_entries from int64_t to long long, so that it can be used by the generic infra
- standard error reply instead of ""repl-backlog-size must be 1 or greater"" and such
- tls-port and a few TLS booleans were readable (config get) even when USE_OPENSSL was off (now they aren't)
- syslog-enabled, syslog-ident, cluster-enabled, appendfilename, and supervised didn't have a get (now they do)
- pidfile was initialized to NULL in InitServerConfig but had CONFIG_DEFAULT_PID_FILE in rewriteConfig (so the real default was """", but rewrite would cause it to be set), fixed the rewrite.
- TLS config in server.h was uninitialized (if no tls config args were provided)

Adding test for sanity and coverage
","['src/cluster.h', 'src/config.c', 'src/server.c', 'src/server.h', 'tests/unit/introspection.tcl']","Configuration values are inconsistent and can't fully be utilized by the generic infrastructure. Several configurations are readable or writable when they shouldn't be, and some are wrongly initialized."
19d46f8f2f4239bea738a9578e083a64c1467012,1607879686,"Expose Redis main thread cpu time, and scrape system time via INFO command. (#8132)

Exposes the main thread CPU info via info modules ( linux specific only )
(used_cpu_sys_main_thread and used_cpu_user_main_thread). This is important for:

- distinguish between main thread and io-threads cpu time total cpu time consumed ( check
  what is the first bottleneck on the used config )
- distinguish between main thread and modules threads total cpu time consumed

Apart from it, this commit also exposes the server_time_usec within the Server section so that we can
properly differentiate consecutive collection and calculate for example the CPU% and or / cpu time vs
wall time, etc...
",['src/server.c'],"The CPU time for main thread and I/O-threads in Redis modules are indistinguishable, making it a bottleneck to calculate total CPU time consumed. There's also no exposure of server_time_usec to differentiate consecutive collections and calculate CPU usage.
"
4a45386e3c7c95dff65802f758feefd00091790b,1646118167,"Move most of the configuration to a hashtable (#10323)

* Moved configuration storage from a list to a hash table
* Configs are returned in a non-deterministic order. It's possible that a client was relying on order (hopefully not).
* Fixed an esoteric bug where if you did a set with an alias with an error, it would throw an error indicating a bug with the preferred name for that config.","['src/config.c', 'src/server.c', 'src/server.h']",Config storage from a list results in order dependent configurations and throws an unexpected error when setting a config with an alias that already has an error.
0bc8c9c8f975f326c2ce87a0759b8b65b94a4120,1613382005,"Modules: In RM_HashSet, add COUNT_ALL flag and set errno (#8446)

The added flag affects the return value of RM_HashSet() to include
the number of inserted fields, in addition to updated and deleted
fields.

errno is set on errors, tests are added and documentation updated.","['src/module.c', 'src/redismodule.h', 'tests/modules/hash.c', 'tests/unit/moduleapi/hash.tcl']",'RM_HashSet() function does not handle or return the count of inserted fields and lacks proper error reporting mechanism when it encounters errors.'
f7f77a746ab458566654dc159925528daaaf80da,1595313704,"Clarification on the bug that was fixed in PR #7539. (#7541)

Before that PR, processCommand() did not notice that cmd could be a module
command in which case getkeys_proc member has a different meaning.

The outcome was that a module command which doesn't take any key names in its
arguments (similar to SLOWLOG) would be handled as if it might have key name arguments
(similar to MEMORY), would consider cluster redirect but will end up with 0 keys
after an excessive call to getKeysFromCommand, and eventually do the right thing.",['src/server.c'],"A module command that doesn't require any key names in its arguments behaves as if it might have key name arguments, causing unnecessary calls to getKeysFromCommand and potential issues with cluster redirecting."
62c8be28ee7d15f90103f52bcf4a4c91f9ba7396,1644738758,"Regression test for sync psync crash (#10288)

Added regression tests for #10020 / #10081 / #10243.
The above PRs fixed some crashes due to an asserting,
see function `clientHasPendingReplies` (introduced in #9166).

This commit added some tests to cover the above scenario.
These tests will all fail in #9166, althought fixed not,
there is value in adding these tests to cover and verify
the changes. And it also can cover #8868 (verify the logs).

Other changes: 
1. Reduces the wait time in `waitForBgsave` and `waitForBgrewriteaof`
from 1s to 50ms, which should reduce the time for some tests.
2. Improve the test infra to print context when `assert_match` fails.
3. Improve the test infra to print `$error` when `assert_error` fails.
```
Expected an error matching 'ERR*' but got 'OK' (context: type eval line 4 cmd {assert_error ""ERR*"" {r set a b}} proc ::test)
```","['tests/integration/replication.tcl', 'tests/support/test.tcl', 'tests/support/util.tcl']","Assertion failure in `clientHasPendingReplies` function results in crashes, requiring regression tests. Additionally, significant wait time in `waitForBgsave` and `waitForBgrewriteaof` and lack of context in test output when `assert_match` and `assert_error` fails reduces test efficiency."
7cd6a64d2fa23dc665aab205d3aee4b78a896f44,1641562265,"lpGetInteger returns int64_t, avoid overflow (#10068)

Fix #9410

Crucial for the ms and sequence deltas, but I changed all
calls, just in case (e.g. ""flags"")

Before this commit:
`ms_delta` and `seq_delta` could have overflown, causing `currid` to be wrong,
which in turn would cause `streamTrim` to trim the entire rax node (see new test)","['src/t_stream.c', 'tests/unit/type/stream.tcl']","Potential overflow in 'ms_delta' and 'seq_delta' could lead to incorrect 'currid', resulting in erroneous trimming of the entire rax node in 'streamTrim'."
f687ac0c32e3857aff56ddc4711be18dd87c336c,1613892886,"Client tracking tracking-redir-broken push len is 2 not 3 (#8456)

When redis responds with tracking-redir-broken push message (RESP3),
it was responding with a broken protocol: an array of 3 elements, but only
pushes 2 elements.

Some bugs in the test make this pass. Read the push reply
will consume an extra reply, because the reply length is 3, but there
are only two elements, so the next reply will be treated as third
element. So the test is corrected too.

Other changes:
* checkPrefixCollisionsOrReply success should return 1 instead of -1,
  this bug didn't have any implications.
* improve client tracking tests to validate more of the response it reads.","['src/tracking.c', 'tests/support/redis.tcl', 'tests/unit/tracking.tcl']","Redis responds with tracking-redir-broken push message (RESP3) inaccurately, providing an array of 3 elements but only pushing 2 elements. This is likely causing an issue with consuming an extra reply."
aea6e71ef82701e07177744e600e1ef20d60b7d0,1612547641,"RM_ZsetRem: Delete key if empty (#8453)

Without this fix, RM_ZsetRem can leave empty sorted sets which are
not allowed to exist.

Removing from a sorted set while iterating seems to work (while
inserting causes failed assetions). RM_ZsetRangeEndReached is
modified to return 1 if the key doesn't exist, to terminate
iteration when the last element has been removed.","['src/module.c', 'tests/modules/zset.c', 'tests/unit/moduleapi/zset.tcl']","The RM_ZsetRem function can leave empty sorted sets which creates unwanted entries in the database, potentially causing failed assertions during iteration."
24187ed8e396625cc44a6bbeeb87e01aec55c27d,1695551472,"Fix overflow calculation for next timer event (#12474)

The `retval` variable is defined as an `int`, so with 4 bytes, it cannot properly represent
microsecond values greater than the equivalent of about 35 minutes. 

This bug shouldn't impact standard Redis behavior because Redis doesn't have timer
events that are scheduled as far as 35 minutes out, but it may affect custom Redis modules
which interact with the event timers via the RM_CreateTimer API.

The impact is that `usUntilEarliestTimer` may return 0 for as long as `retval` is scaled to
an overflowing value. While `usUntilEarliestTimer` continues to return `0`, `aeApiPoll`
will have a zero timeout, and so Redis will use significantly more CPU iterating through
its event loop without pause. For timers scheduled far enough into the future, Redis will
cycle between ~35 minute periods of high CPU usage and ~35 minute periods of standard
CPU usage.",['src/ae.c'],"Overflow in timer event calculation causes high CPU usage due to continuous iteration through the event loop without pause, particularly affecting custom modules which use RM_CreateTimer API for far future timers."
f898a9e97dbf711ee192aee60134a07841c735d0,1631790428,"Adds limit to SINTERCARD/ZINTERCARD. (#9425)

Implements the [LIMIT limit] variant of SINTERCARD/ZINTERCARD.
Now with the LIMIT, we can stop the searching when cardinality
reaching the limit, and return the cardinality ASAP.

Note that in SINTERCARD, the old synatx was: `SINTERCARD key [key ...]`
In order to add a optional parameter, we must break the old synatx.
So the new syntax of SINTERCARD will be consistent with ZINTERCARD.
New syntax: `SINTERCARD numkeys key [key ...] [LIMIT limit]`.

Note that this means that SINTERCARD has a different syntax than
SINTER and SINTERSTORE (taking numkeys argument)

As for ZINTERCARD, we can easily add a optional parameter to it.
New syntax: `ZINTERCARD numkeys key [key ...] [LIMIT limit]`","['src/db.c', 'src/server.c', 'src/server.h', 'src/t_set.c', 'src/t_zset.c', 'tests/unit/type/set.tcl', 'tests/unit/type/zset.tcl']","SINTERCARD/ZINTERCARD operations lack a LIMIT function, causing unnecessary prolonged searches for cardinality and inconsistencies in the command syntax."
528bb11d7a25cde99e6d48c418662575c64758b2,1670585618,"Solve issues with active defrag test failing on fast machines (#11598)

We do defrag during AOF loading, but aim to detect fragmentation only
once a second, so this test aims to slow down the AOF loading and mimic
loading of a large file.
On fast machines the sleep, plus the actual work we did was insufficient
making it sleep longer so the test won't fail.

The error we used to get is this one:
Expected 0 > 100000 (context: type eval line 106 cmd {assert {$hits > 100000}} proc ::test)",['tests/unit/memefficiency.tcl'],"Active defrag test fails on faster machines as the current sleep duration, along with the actual work, fails to effectively mimic the loading of a large file."
7d9b09adaaa92b07a11cb1b40aea6958fab049a6,1607937226,"Tests: fix new defrag test to be skipped when not supported (#8185)

Additionally the older defrag tests are using an obsolete way to check
if the defragger is suuported (the error no longer contains ""DISABLED"").
this doesn't usually makes a difference since these tests are completely
skipped if the allocator is not jemalloc, but that would fail if the
allocator is a jemalloc that doesn't support defrag.","['tests/unit/memefficiency.tcl', 'tests/unit/moduleapi/defrag.tcl']","Defrag tests aren't skipped on unsupported structures due to obsolete validation method, potentially causing failures when allocator is a jemalloc that doesn't support defrag."
62f9ac6f436356dc7679d06bedeb98948bcebeb0,1529309121,"Streams: Change XADD MAXLEN handling of values <= 0.

Now a MAXLEN of 0 really does what it means: it will create a zero
entries stream. This is useful in order to make sure that the behavior
is identical to XTRIM, that must be able to reduce the stream to zero
elements when MAXLEN is given.

Also now MAXLEN with a count < 0 will return an error.
",['src/t_stream.c'],Using XADD MAXLEN with values <= 0 creates inconsistency in stream behavior and does not handle error for negative count values.
497351ad07365109a57c831c5e99b15dfbefdeb3,1616660427,"Fix SLOWLOG for blocked commands (#8632)

* SLOWLOG didn't record anything for blocked commands because the client
  was reset and argv was already empty. there was a fix for this issue
  specifically for modules, now it works for all blocked clients.
* The original command argv (before being re-written) was also reset
  before adding the slowlog on behalf of the blocked command.
* Latency monitor is now updated regardless of the slowlog flags of the
  command or its execution (their purpose is to hide sensitive info from
  the slowlog, not hide the fact the latency happened).
* Latency monitor now uses real_cmd rather than c->cmd (which may be
  different if the command got re-written, e.g. GEOADD)

Changes:
* Unify shared code between slowlog insertion in call() and
  updateStatsOnUnblock(), hopefully prevent future bugs from happening
  due to the later being overlooked.
* Reset CLIENT_PREVENT_LOGGING in resetClient rather than after command
  processing.
* Add a test for SLOWLOG and BLPOP

Notes:
- real_cmd == c->lastcmd, except inside MULTI and Lua.
- blocked commands never happen in these cases (MULTI / Lua)
- real_cmd == c->cmd, except for when the command is rewritten (e.g.
  GEOADD)
- blocked commands (currently) are never rewritten
- other than the command's CLIENT_PREVENT_LOGGING, and the
  execution flag CLIENT_PREVENT_LOGGING, other cases that we want to
  avoid slowlog are on AOF loading (specifically CMD_CALL_SLOWLOG will
  be off when executed from execCommand that runs from an AOF)","['src/blocked.c', 'src/module.c', 'src/networking.c', 'src/server.c', 'src/server.h', 'tests/unit/slowlog.tcl']","SLOWLOG fails to record anything for blocked commands due to client reset and empty argv. Also, latency monitor doesn't always update, and it uses potentially rewritten commands instead of original ones."
14b198868fd8f8c7cbd319bec369fbc3fb24aff6,1648481756,"introduce MAX_D2STRING_CHARS instead of 128 const (#10487)

There are a few places that use a hard coded const of 128 to allocate a buffer for d2string.
Replace these with a clear macro.
Note that In theory, converting double into string could take as much as nearly 400 chars,
but since d2string uses `%g` and not `%f`, it won't pass some 40 chars.

unrelated:
restore some changes to auto generated commands.c that got accidentally reverted in #10293","['src/commands.c', 'src/module.c', 'src/t_zset.c', 'src/util.h', 'tests/unit/type/zset.tcl']","Several places are using hard coded const of 128 to allocate a buffer for d2string, which could in theory, require up to 400 chars. Possible buffer overflow."
bb7891f080602fb92197a0f398be71eb7aba9e3c,1649838998,"Keyspace event for new keys (#10512)

Add an optional keyspace event when new keys are added to the db.

This is useful for applications where clients need to be aware of the redis keyspace.
Such an application can SCAN once at startup and then listen for ""new"" events (plus
others associated with DEL, RENAME, etc).","['src/config.c', 'src/db.c', 'src/notify.c', 'src/server.h', 'tests/unit/pubsub.tcl']","No keyspace event in Redis is available for newly added keys, causing inconvenience for applications requiring awareness of the Redis keyspace."
c7dc17fc0f9341f61be1a1318468409249310316,1639422985,"Fix possible int overflow when hashing an sds. (#9916)

This caused a crash when adding elements larger than 2GB to a set (same goes for hash keys). See #8455.

Details:
* The fix makes the dict hash functions receive a `size_t` instead of an `int`. In practice the dict hash functions
  call siphash which receives a `size_t` and the callers to the hash function pass a `size_t` to it so the fix is trivial.
* The issue was recreated by attempting to add a >2gb value to a set. Appropriate tests were added where I create
  a set with large elements and check basic functionality on it (SADD, SCARD, SPOP, etc...).
* When I added the tests I also refactored a bit all the tests code which is run under the `--large-memory` flag.
  This removed code duplication for the test framework's `write_big_bulk` and `write_big_bulk` code and also takes
  care of not allocating the test frameworks helper huge string used by these tests when not run under `--large-memory`.
* I also added the _violoations.tcl_ unit tests to be part of the entire test suite and leaned up non relevant list related
  tests that were in there. This was done in this PR because most of the _violations_ tests are ""large memory"" tests.","['src/dict.c', 'src/dict.h', 'tests/support/util.tcl', 'tests/test_helper.tcl', 'tests/unit/type/list.tcl', 'tests/unit/type/set.tcl', 'tests/unit/violations.tcl']",Adding elements larger than 2GB to a set or hash keys leads to an integer overflow in sds hashing causing a system crash.
ad55fbaabb853d553b58ed951da1890590d3b835,1640167589,"Shorten timeouts of CLIENT PAUSE to avoid hanging when tests fail. (#9975)

If a test fails at `wait_for_blocked_clients_count` after the `PAUSE` command,
It won't send `UNPAUSE` to server, leading to the server hanging until timeout,
which is bad and hard to debug sometimes when developing.
This PR tries to fix this.

Timeout in `CLIENT PAUSE` shortened from 1e5 seconds(extremely long) to 50~100 seconds.",['tests/unit/pause.tcl'],"Test failures in 'wait_for_blocked_clients_count' after the 'PAUSE' command are causing server hangs due to the overly long 'CLIENT PAUSE' timeout, making it harder to debug during development."
6790d848c5cf76ad5833d155fc4debc50b9ef2c4,1641920456,"Reuse temporary client objects for blocked clients by module (#9940)

Added a pool for temporary client objects to reuse in module operations.
By reusing temporary clients, we are avoiding expensive createClient()/freeClient()
calls and improving performance of RM_BlockClient() and  RM_GetThreadSafeContext() calls. 

This commit contains two optimizations: 

1 - RM_BlockClient() and RM_GetThreadSafeContext() calls create temporary clients and they are freed in
RM_UnblockClient() and RM_FreeThreadSafeContext() calls respectively. Creating/destroying client object
takes quite time. To avoid that, added a pool of temporary clients. Pool expands when more clients are needed.
Also, added a cron function to shrink the pool and free unused clients after some time. Pool starts with zero
clients in it. It does not have max size and can grow unbounded as we need it. We will keep minimum of 8
temporary clients in the pool once created. Keeping small amount of clients to avoid client allocation costs
if temporary clients are required after some idle period.

2 - After unblocking a client (RM_UnblockClient()), one byte is written to pipe to wake up Redis main thread.
If there are many clients that will be unblocked, each operation requires one write() call which is quite expensive.
Changed code to avoid subsequent calls if possible. 

There are a few more places that need temporary client objects (e.g RM_Call()). These are now using the same
temporary client pool to make things more centralized. ","['src/module.c', 'src/networking.c', 'src/server.c', 'src/server.h']","Large amounts of time are consumed due to repetitive creation and deletion of temporary client objects for blocked clients by a module, slowing down performance of RM_BlockClient() and RM_GetThreadSafeContext() calls. In addition, each unblocking of a client (via RM_UnblockClient()) requires an expensive write call.
"
7c3916ae6e5bc31a6991986dd52a5ae9b4147fae,1666697168,"Fix command GEOHASH and GEOPOS argument doc, mark member as optional (#11417)

These commands take a list of members, which can be empty (i.e. running
the command with just a key name).
this always results in an empty array reply, so it doesn't make much sense,
but changing it is a breaking change.

This PR fixes the documentation, making the member field as optional, just makes
sure the command format documentation is consistent with the command behavior.

The command format will be:

127.0.0.1:6381> GEOPOS key [member [member ...]]
127.0.0.1:6381> GEOHASH key [member [member ...]]

",['src/commands.c'],"The documentation for GEOHASH and GEOPOS commands doesn't accurately describe the members as optional, causing inconsistency between actual command behavior and its documentation."
a2046c1eb1bcfcdeffadfffffad3b1f635965652,1693710888,"Check shard_id pointer validity in updateShardId (#12538)

When connecting between a 7.0 and 7.2 cluster, the 7.0 cluster will not populate the shard_id field, which is expect on the 7.2 cluster. This is not intended behavior, as the 7.2 cluster is supposed to use a temporary shard_id while the node is in the upgrading state, but it wasn't being correctly set in this case.",['src/cluster.c'],"When connecting a 7.0 cluster to a 7.2 cluster, the 7.0 cluster fails to populate the shard_id field, which is expected on the 7.2 cluster. This leads to incorrect shard_id usage during the upgrading state."
febc3f63b2b57afe3553b22c69c3ce27b5a3e842,1639996273,"Fix recent daily CI test failures (#9966)

Recent PRs have introduced some failures, this commit
try to fix these CI failures. Here are the changes:

1. Enable debug-command in sentinel test.
```
Master reboot in very short time: ERR DEBUG command not allowed. If the
enable-debug-command option is set to ""local"", you can run it from a
local connection, otherwise you need to set this option in the
configuration file, and then restart the server.
```

2. Enable protected-config in sentinel test.
```
SDOWN is triggered by misconfigured instance replying with errors: ERR
CONFIG SET failed (possibly related to argument 'dir') - can't set
protected config
```

3. Enable debug-command in cluster test.
```
Verify slaves consistency: ERR DEBUG command not allowed. If the
enable-debug-command option is set to ""local"", you can run it from a
local connection, otherwise you need to set this option in the
configuration file, and then restart the server.
```

4. quicklist fill should be signed int.
The reason for the modification is to eliminate the warning.
Modify `int fill: QL_FILL_BITS` to `signed int fill: QL_FILL_BITS`

The first three were introduced at #9920 (same issue).
And the last one was introduced at #9962.","['src/quicklist.h', 'src/server.c', 'tests/cluster/run.tcl', 'tests/sentinel/run.tcl']","Recent PRs have resulted in daily CI test failures due to issues, such as debug-command not enabled in Sentinel and Cluster tests, protected-config not enabled in Sentinel tests, and use of unsigned int in quicklist fill leading to warnings."
824f5f0b7a1f32a6e6abf3d355d523d2c6e7bf09,1572945274,"Update PR #6537 patch to for generality.

After the thread in #6537 and thanks to the suggestions received, this
commit updates the original patch in order to:

1. Solve the problem of updating the time in multiple places by updating
it in call().
2. Avoid introducing a new field but use our cached time.

This required some minor refactoring to the function updating the time,
and the introduction of a new cached time in microseconds in order to
use less gettimeofday() calls.
","['src/db.c', 'src/rdb.c', 'src/server.c', 'src/server.h']","The software is updating time in multiple locations and frequently making unnecessary gettimeofday() calls, leading to inefficient operation."
d4890c20c10d91e990b8af044a233d3e1da7bda0,1549623039,"ACL: ignore modules commands when adding categories.

We can't trust modules commands flagging, so module commands must be
always explicitly added, with the exception of +@all that will include
everything. However something like +@readonly should not include command
from modules that may be potentially dangerous: our categories must be
safe and reliable and modules may not be like that.
",['src/acl.c'],Module commands flagging in ACL is unreliable and including potentially dangerous modules in categories like +@readonly could compromise safety.
eab865a0a1d7e3cb2494c02e601dbdb93b48533c,1480352044,"PSYNC2: stop sending newlines to sub-slaves when master is down.

This actually includes two changes:

1) No newlines to take the master-slave link up when the upstream master
is down. Doing this is dangerous because the sub-slave often is received
replication protocol for an half-command, so can't receive newlines
without desyncing the replication link, even with the code in order to
cancel out the bytes that PSYNC2 was using. Moreover this is probably
also not needed/sane, because anyway the slave can keep serving
requests, and because if it's configured to don't serve stale data, it's
a good idea, actually, to break the link.

2) When a +CONTINUE with a different ID is received, we now break
connection with the sub-slaves: they need to be notified as well. This
was part of the original specification but for some reason it was not
implemented in the code, and was alter found as a PSYNC2 bug in the
integration testing.
","['src/networking.c', 'src/replication.c', 'tests/integration/psync2.tcl']","Sending newlines to sub-slaves when the main master is down causes replication link desync. Additionally, sub-slaves don't break connection when a +CONTINUE with a different ID is received."
e9ae03787e0a2e0484914737f82bfe216f8e9d52,1645524586,"Delete key doesn't dirty client who watched stale key (#10256)

When WATCH is called on a key that's already logically expired, avoid discarding the
transaction when the keys is actually deleted.

When WATCH is called, a flag is stored if the key is already expired
at the time of watch. The expired key is not deleted, only checked.

When a key is ""touched"", if it is deleted and it was already expired
when a client watched it, the client is not marked as dirty.

Co-authored-by: Oran Agra <oran@redislabs.com>
Co-authored-by: zhaozhao.zz <zhaozhao.zz@alibaba-inc.com>","['src/db.c', 'src/multi.c', 'tests/unit/multi.tcl']",Clients watching a logically expired key aren't marked as dirty when the key is deleted.
b5a879e1c2df3dc83cbc17e0a8203decd2dc98ac,1631637906,"Added URI support to redis-benchmark (cli and benchmark share the same uri-parsing methods) (#9314)

- Add `-u <uri>` command line option to support `redis://` URI scheme.
- included server connection information object (`struct cliConnInfo`),
  used to describe an ip:port pair, db num user input, and user:pass to
  avoid a large number of function arguments.
- Using sds on connection info strings for redis-benchmark/redis-cli

Co-authored-by: yoav-steinberg <yoav@monfort.co.il>","['src/cli_common.c', 'src/cli_common.h', 'src/redis-benchmark.c', 'src/redis-cli.c', 'tests/integration/redis-benchmark.tcl', 'tests/support/benchmark.tcl']","Redis-benchmark tool does not support URI input format, causing difficulties in providing db number, user input, user pass, and IP:port pair information."
169be0426c5a4b4eedc04ad94a165c002101e5b2,1615988738,"Fixes for systems with 64-bit time (#8662)

Some operating systems (e.g., NetBSD and OpenBSD) have switched to
using a 64-bit integer for time_t on all platforms. This results in currently
harmless compiler warnings due to potential truncation.
These changes fix these minor portability concerns.

* Fix format string for systems with 64 bit time
* use llabs to avoid truncation with 64 bit time","['src/debug.c', 'src/networking.c']","Compiler warnings due to potential truncation are being caused by some operating systems switching to a 64-bit integer for time_t on all platforms. This results in minor portability concerns.
"
7885faf18b776eaed4a7ddcf8fb8bce699820eea,1606907845,"Modify help msg PING_BULK to PING_MBULK in benchmark (#8109)

As described in redis-benchamrk help message 'The test names are the same as the ones produced as output.', In redis-benchmark output, we can only see PING_BULK, but the cmd `redis-benchmark -t ping_bulk` is not supported. We  have to run it with ping_mbulk which is not user friendly.
","['src/redis-benchmark.c', 'tests/integration/redis-benchmark.tcl']","Redis-benchmark help message contains discrepancy between described test names and actual supported commands, specifically around PING_BULK, causing confusion for users."
9cfcf37968c9890e6f183461f382528cfbe4d375,1542992221,"Clarify the ""Creating Server TCP listening socket"" error.

This really helps spot it in the logs, otherwise it does not look like a
warning/error. For example:

  Creating Server TCP listening socket ::1:6379: bind: Cannot assign requested address

... is not nearly as clear as:

  Could not create server TCP listening listening socket ::1:6379: bind: Cannot assign requested address
",['src/server.c'],"""Creating Server TCP listening socket"" error message is not clear and identifiable as a warning/error in the logs."
7c179f9bf4390512196b3a2b2ad6d0f4cb625c8a,1691650733,"Fixed a bug where sequential matching ACL rules weren't compressed (#12472)

When adding a new ACL rule was added, an attempt was made to remove
any ""overlapping"" rules. However, there when a match was found, the search
was not resumed at the right location, but instead after the original position of
the original command.

For example, if the current rules were `-config +config|get` and a rule `+config`
was added. It would identify that `-config` was matched, but it would skip over
`+config|get`, leaving the compacted rule `-config +config`. This would be evaluated
safely, but looks weird.

This bug can only be triggered with subcommands, since that is the only way to
have sequential matching rules. Resolves #12470. This is also only present in 7.2.
I think there was also a minor risk of removing another valid rule, since it would start
the search of the next command at an arbitrary point. I couldn't find a valid offset that
would have cause a match using any of the existing commands that have subcommands
with another command. ","['src/acl.c', 'tests/unit/acl.tcl']","Adding new ACL rule can skip over matching subcommands, resulting in sequential matching rules not being compressed correctly, possibly removing valid rules."
693acc0114af6298ab6601ab6d6c668c4e795049,1657542095,"Trying to fix cluster test (#10963)

#10942 break the new test added in #10449
```
Testing unit: 29-slot-migration-response.tcl
Cluster Join and auto-discovery test: FAILED: Cluster failed to join into a full mesh.
```

It looks like we need to wait for the cluster in 28 to become stable.",['tests/cluster/tests/28-cluster-shards.tcl'],"Cluster fails to join into a full mesh during the auto-discovery test, possibly due to instability in the 28 cluster."
3f2d7e277e1754a5a421948bfbd45b5725a05148,1512120264,"Streams: items compression implemented.

The approach used is to set a fixed header at the start of every
listpack blob (that contains many entries). The header contains a
""master"" ID and fields, that are initially just obtained from the first
entry inserted in the listpack, so that the first enty is always well
compressed. Later every new entry is checked against these fields, and
if it matches, the SAMEFIELD flag is set in the entry so that we know to
just use the master entry flags. The IDs are always delta-encoded
against the first entry. This approach avoids cascading effects in which
entries are encoded depending on the previous entries, in order to avoid
complexity and rewritings of the data when data is removed in the middle
(which is a planned feature).
","['src/stream.h', 'src/t_stream.c']","Current implementation in Streams unnecessarily stores repetitive data in listpack blobs, not utilizing a compression technique leading to potential storage inefficiency."
0a2f78837d82046ec7ffe77f8a2addcb591ff4f7,1655133833,"redis-check-rdb add when_opcode check for module aux (#10859)

In #9199, we add a `goto eoferr` in when_opcode check,
this means that if the when_opcode check fails, we will
abort the rdb loading, but this not reflected in the
rdb-check tool. So someone can modify when_opcode to make
rdb load fail, but rdb-check will report OK. Just a cleanup
or a code consistency issue.
```
serverLog: # Internal error in RDB reading offset 0, function at rdb.c:3055 -> bad when_opcode
[offset 0] Checking RDB file dump.rdb
[offset 109] \o/ RDB looks OK! \o/
```

Plus a minor memory leak fix like #9860, note that it will
exit immediately after the eoferr, so it is not strictly a
leak, so it is also a small cleanup.",['src/redis-check-rdb.c'],"RDB-check tool fails to reflect aborting of RDB loading when when_opcode check fails, reporting false positives. Small memory leak also found on immediate exit post eoferr."
aab479f8cfaa4493f5618ba05cfec0e2b406e77c,1613485058,"Optimize listpack for stream usage to avoid repeated reallocs (#6281)

Avoid repeated reallocs growing the listpack while entries are being added.
This is done by pre-allocating the listpack to near maximum size, and using
malloc_size to check if it needs realloc or not.
When the listpack reaches the maximum number of entries, we shrink it to fit it's used size.

Co-authored-by: Viktor Söderqvist <viktor@zuiderkwast.se>
Co-authored-by: Oran Agra <oran@redislabs.com>","['src/listpack.c', 'src/listpack.h', 'src/listpack_malloc.h', 'src/t_stream.c']","Adding multiple entries to a listpack causes repeated reallocations, slowing down the system performance."
56fa48ffc136b7540f8144562d8aa2a13b8a3fa7,1645101168,"aof rewrite and rdb save counters in info (#10178)

Add aof_rewrites and rdb_snapshots counters to info.
This is useful to figure our if a rewrite or snapshot happened since last check.
This was part of the (ongoing) effort to provide a safe backup solution for multipart-aof backups.","['src/aof.c', 'src/rdb.c', 'src/server.c', 'src/server.h']","There's no ability to track or determine if an AOF rewrite or RDB snapshot occurred since the last check, leading to difficulties in managing multipart-AOF backups."
109b5ccdcd6e6b8cecdaeb13a246bc49ce7a61f4,1595924129,"Fix failing tests due to issues with wait_for_log_message (#7572)

- the test now waits for specific set of log messages rather than wait for
  timeout looking for just one message.
- we don't wanna sample the current length of the log after an action, due
  to a race, we need to start the search from the line number of the last
  message we where waiting for.
- when attempting to trigger a full sync, use multi-exec to avoid a race
  where the replica manages to re-connect before we completed the set of
  actions that should force a full sync.
- fix verify_log_message which was broken and unused","['tests/integration/replication.tcl', 'tests/support/util.tcl', 'tests/unit/moduleapi/testrdb.tcl']","Tests are failing due to race conditions and inefficient log message waiting, causing issues with full sync triggering and log message verification."
37a10cef028f0ed16e6768dabdd3ffa56fc77761,1574408569,"Propagation: wrap commands in also_propagate array with MULIT/EXEC

Random command like SPOP with count is replicated as
some SREM operations, and store them in also_propagate
array to propagate after the call, but this would break
atomicity.

To keep the command's atomicity, wrap also_propagate
array with MULTI/EXEC.
","['src/module.c', 'src/multi.c', 'src/scripting.c', 'src/server.c', 'src/server.h']","Atomicity of commands like SPOP with count is compromised when replicated as SREM operations, due to storing these operations in the also_propagate array."
b3dc23c5a8365a5e61b9ef8113970c1c36de6a13,1608786227,"add a read-only variant for HELLO
As discussed in https://github.com/antirez/redis/issues/7364, it is good
to have a HELLO command variant, which does not switch the current proto
version of a redis server.

While `HELLO` will work, it introduced a certain difficulty on parsing
options of the command. We will need to offset the index of authentication
and setname option by -1.

So 0 is marked a special version meaning non-switching. And we do not
need to change the code much.
",['src/networking.c'],"The existing HELLO command in Redis switchs the current proto version of the server, leading to difficulties in parsing command options, specifically offsetting the index of authentication and setname option by -1."
810e28a397b77fa16037354477370b564474d9be,1600236961,"Incremental eviction processing (#7653)

Rather than blindly evicting until maxmemory limit is achieved, this
update adds a time limit to eviction.  While over the maxmemory limit,
eviction will process before each command AND as a timeProc when no
commands are running.

This will reduce the latency impact on many cases, especially pathological
cases like massive used memory increase during dict rehashing.

There is a risk that some other edge cases (like massive pipelined use
of MGET) could cause Redis memory usage to keep growing despite the
eviction attempts, so a new maxmemory-eviction-tenacity config is
introduced to let users mitigate that.","['src/config.c', 'src/evict.c', 'src/networking.c', 'src/server.c', 'src/server.h']","Redis memory usage dangerously increasing due to blind eviction until maxmemory limit is reached, causing high latency and potential edge cases such as large memory use during dict rehashing or massive pipelining of MGET."
2d3240f31b4e7111e6162476bdffd4c1c3794002,1658040007,"Add optional for FCALL and FCALL_RO command json file (#10988)

According to the Redis functions documentation, FCALL command format could be
FCALL function_name numberOfKeys [key1, key2, key3.....] [arg1, arg2, arg3.....]

So in the json file of fcall and fcall_ro, we should add optional for key and arg part.
Just like EVAL...

Co-authored-by: Binbin <binloveplay1314@qq.com>",['src/commands.c'],The JSON files for FCALL and FCALL_RO commands may not adequately handle optional key and argument parts as specified in the Redis functions documentation.
af7489886dab9362e4661a3015ac606025710686,1637225269,"Obliterate STRALGO! add LCS (which only works on keys) (#9799)

Drop the STRALGO command, now LCS is a command of its own and it only works on keys (not input strings).
The motivation is that STRALGO's syntax was really messed-up...
- assumes all (future) string algorithms will take similar arguments
- mixes command that takes keys and one that doesn't in the same command.
- make it nearly impossible to expose the right key spec in COMMAND INFO (issues cluster clients)
- hard for cluster clients to determine the key names (firstkey, lastkey, etc)
- hard for ACL / flags (is it a read command?)

This is a breaking change.","['src/server.c', 'src/server.h', 'src/t_string.c', 'tests/support/util.tcl', 'tests/unit/type/string.tcl']","The STRALGO command has a complicated syntax, assuming all future string algorithms will take similar arguments, and is difficult to use with cluster clients and access control lists (ACL) due to inconsistencies in functioning with keys and non-key inputs causing difficulty in determining key names and flags."
27a68a4d1b4526f37b8e9b2e89f3e32596734035,1627882225,"improve quicklist insert head/tail while head/tail node is full. (#9113)

In _quicklistInsert when `at_head` / `at_tail` is true, but `prev` / `next` is NULL,
the code was reaching the last if-else block at the bottom of the function,
and would have unnecessarily executed _quicklistSplitNode, instead of just creating a new node.
This was because the penultimate if-else was checking `node->next && full_next`.
but in fact it was unnecessary to check if `node->next` exists, if we're gonna create one anyway,
we only care that it's not full, or doesn't exist, so the condition could have been changed to `!node->next || full_next`.

Instead, this PR makes a small refactory to negate `full_next` to a more meaningful variable
`avail_next` that indicates that the next node is available for pushing additional elements or
not (this would be true only if it exists and it is non-full)",['src/quicklist.c'],Insertion at head/tail in quicklist encounters unnecessary node splits when previous/next node is null and the head/tail node is full.
7f342020dcbdf9abe754d6b666efdeded7063870,1623824997,"Change return value type for ZPOPMAX/MIN in RESP3 (#8981)

When using RESP3, ZPOPMAX/ZPOPMIN should return nested arrays for consistency
with other commands (e.g. ZRANGE).

We do that only when COUNT argument is present (similarly to how LPOP behaves).
for reasoning see https://github.com/redis/redis/issues/8824#issuecomment-855427955

This is a breaking change only when RESP3 is used, and COUNT argument is present!","['src/t_zset.c', 'tests/unit/type/zset.tcl']","Inconsistency in return types for ZPOPMAX/ZPOPMIN when used with RESP3 and COUNT argument, breaking the uniformity with other commands like ZRANGE."
65a9740fa880a8e5b4640037a3670f8a2f33080b,1512140470,"Fix loading of RDB files lua AUX fields when the script is defined.

In the case of slaves loading the RDB from master, or in other similar
cases, the script is already defined, and the function registering the
script should not fail in the assert() call.
","['src/rdb.c', 'src/scripting.c', 'src/server.h']","RDB files with lua AUX fields fail to load when the script is already defined, especially in cases where slaves load the RDB from master."
fac188b49d9680fdeb90332f163e5634cec1ea13,1667580397,"Introduce socket shutdown into connection type, used if a fork is active (#11376)

Introduce socket `shutdown()` into connection type, and use it
on normal socket if a fork is active. This allows us to close
client connections when there are child processes sharing the
file descriptors.

Fixes #10077. The reason is that since the `fork()` child is holding
the file descriptors, the `close` in `unlinkClient -> connClose`
isn't sufficient. The client will not realize that the connection is
disconnected until the child process ends.

Let's try to be conservative and only use shutdown when the fork is active.","['src/connection.h', 'src/networking.c', 'src/socket.c', 'src/tls.c', 'src/unix.c', 'tests/unit/introspection.tcl']","When a fork is active, `close` in `unlinkClient -> connClose` fails to properly disconnect client connections due to shared file descriptors, leading clients unaware of the disconnection until the child process ends."
cf1d49adbfd1713bd832ade9d57ffdc032e40c33,1625030479,"Simplify logic in raxSeek, eliminate it->key reassembly on gt and lt (#9115)

In the original version, the operation of traversing the stack only seems to
reconstruct the key that does not contain the current node.

But in fact We have got the matched length and splitpos in the key in the
raxlowwalk, so I think we can simplify the logic of this part.

Co-authored-by: lizhaolong.lzl <lizhaolong.lzl@B-54MPMD6R-0221.local>",['src/rax.c'],"The logic in raxSeek involves unnecessary key reassembly for 'greater than' and 'less than' operations, despite having the matched length and splitpos available from raxlowwalk."
ae1de549006c1f15bade4969ba25932e3509f17a,1669306196,"GEOSEARCH BYBOX: Reduce wastefull computation on geohashGetDistanceIfInRectangle and geohashGetDistance (#11535)

Optimize geohashGetDistanceIfInRectangle when there are many misses.
It calls 3x geohashGetDistance. The first 2 times we call them to produce intermediate results.
This PR focus on optimizing for those 2 intermediate results.

1 Reduce expensive computation on intermediate geohashGetDistance with same long
2 Avoid expensive lon_distance calculation if lat_distance fails beforehand

Co-authored-by: Oran Agra <oran@redislabs.com>",['src/geohash_helper.c'],"The geohashGetDistanceIfInRectangle function performs excessive computations especially with numerous misses, resulting in inefficient lat_distance and lon_distance calculations."
8c829d9e4323b3636afa43d0ad8eb1ce2397c3f9,1492160482,"Cluster: fix gossip section ping/pong times encoding.

The gossip section times are 32 bit, so cannot store the milliseconds
time but just the seconds approximation, which is good enough for our
uses. At the same time however, when comparing the gossip section times
of other nodes with our node's view, we need to convert back to
milliseconds.

Related to #3929. Without this change the patch to reduce the traffic in
the bus message does not work.
",['src/cluster.c'],"Gossip section times in the cluster are only storing second approximations, causing issues when comparing times with other nodes requiring millisecond precision."
51f9bed3dd8d35a59e9124e49450c9795822dbf5,1642494592,"Fix `FUNCTION LOAD` ignores unknown parameter. (#10131)

Following discussion on: https://github.com/redis/redis/issues/9899#issuecomment-1014689385
Raise error if unknows parameter is given to `FUNCTION LOAD`.

Before the fix:
```
127.0.0.1:6379> function load LUA lib2 foo bar ""local function test1() return 5 end redis.register_function('test1', test1)""
OK
```

After the fix:
```
127.0.0.1:6379> function load LUA lib2 foo bar ""local function test1() return 5 end redis.register_function('test1', test1)""
(error) ERR Unkowns option given: foo
```","['src/functions.c', 'tests/unit/functions.tcl']",The `FUNCTION LOAD` command in Redis does not raise an error when an unknown parameter is passed to it.
b93ccee45136992fe08398cc9058f9546708062b,1639161351,"Fix timing issue in strem blocking tests (#9927)

A test failure was reported in Daily CI (FreeBSD).
`XREAD: XADD + DEL should not awake client`

```
*** [err]: XREAD: XADD + DEL should not awake client in tests/unit/type/stream.tcl
Expected [lindex  0 0] eq {s1} (context: type eval line 11 cmd {assert {[lindex $res 0 0] eq {s1}}} proc ::test)
```

It seems that `r` is executed before `rd` enters the blocking
state. And ended up getting a empty reply by timeout.

We use `wait_for_blocked_clients_count` to wait for the
blocking client to be ready and avoid this situation.
Also fixed other test cases that may have the same issue.",['tests/unit/type/stream.tcl'],"The `XREAD: XADD + DEL` command isn't waking up client due to premature execution of `r` before `rd` transitions to blocking state in stream blocking tests, leading to empty replies by timeout."
e8f5d2194020886b39da19fc98b8a77ede516256,1532384420,"string2ll(): remove duplicated check for special case.

Related to #5157. The PR author correctly indentified that the check was
duplicated, but removing the second one introduces a bug that was fixed
in the past (hence the duplication). Instead we can remove the first
instance of the check without issues.
",['src/util.c'],Duplicated check in string2ll() function for special case potentially causing redundancy and reintroduction of a past bug.
43229e4f1028777a7d0e1b44425e3e3cb91ea8e9,1640690756,"Safe and organized exit when receiving sigterm while loading (#10003)

on the signal handler we are enabling server.shutdown_asap flag instead of just doing `exit()`,
and then catch it on the whileBlockedCron() where we prepare for shutdown correctly.

this is a more ore organized and safe termination, the old approach was missing these for example:
1. removal of the pidfile
2. shutdown event to modules",['src/server.c'],"During server load, abrupt termination with SIGTERM results in unclean exit without proper removal of pidfile and shut down events notification to modules."
06dd202a051a6788ebb2e13cd4603ffbf7e6dbd0,1636011788,"Fixes LPOP/RPOP wrong replies when count is 0 (#9692)

Introduced in #8179, this fixes the command's replies in the 0 count edge case.
[BREAKING] changes the reply type when count is 0 to an empty array (instead of nil)
Moves LPOP ... 0 fast exit path after type check to reply with WRONGTYPE","['src/t_list.c', 'tests/unit/type/list.tcl']","LPOP/RPOP commands return incorrect replies when the count parameter is set to 0, potentially breaking existing expectations on handling null responses."
fad0b0d2a680498fce1cd7e153f8ad7396a79edf,1645435241,"Fix error stats and failed command stats for blocked clients (#10309)

This is a followup work for #10278, and a discussion about #10279

The changes:
- fix failed_calls in command stats for blocked clients that got error.
  including CLIENT UNBLOCK, and module replying an error from a thread.
- fix latency stats for XREADGROUP that filed with -NOGROUP

Theory behind which errors should be counted:
- error stats represents errors returned to the user, so an error handled by a
  module should not be counted.
- total error counter should be the same.
- command stats represents execution of commands (even with RM_Call, and if
  they fail or get rejected it counts these calls in commandstats, so it should
  also count failed_calls)

Some thoughts about Scripts:
for scripts it could be different since they're part of user code, not the infra (not an extension to redis)
we certainly want commandstats to contain all calls and errors
a simple script is like mult-exec transaction so an error inside it should be counted in error stats
a script that replies with an error to the user (using redis.error_reply) should also be counted in error stats
but then the problem is that a plain `return redis.call(""SET"")` should not be counted twice (once for the SET
and once for EVAL)
so that's something left to be resolved in #10279","['src/blocked.c', 'src/module.c', 'src/networking.c', 'src/server.c', 'src/server.h', 'tests/unit/info.tcl', 'tests/unit/moduleapi/blockedclient.tcl', 'tests/unit/type/stream-cgroups.tcl']","Statistics for blocked clients exhibit inaccuracies, including incorrect failed_calls in command stats and improper latency stats for XREADGROUP, notably when an error occurs. Complications also arise in error counting for scripts."
4a12047c61570ec3a4dbf9513b0881b41b224399,1573643850,"XADD with ID 0-0 stores an empty key

Calling XADD with 0-0 or 0 would result in creating an
empty key and storing it in the database.
Even worse, because XADD will reply with error the action
will not be replicated, creating a master-replica
inconsistency
","['src/t_stream.c', 'tests/unit/type/stream.tcl']","Using XADD with ID 0-0 or 0 leads to creation of empty key in the database and isn't replicated, causing master-replica inconsistency."
b08ebff31f1e3a64ae4c537c3acc40dde7a83976,1665044779,"Pass -flto flag to the linker  (#11350)

Currently, we add -flto to the compile flags only. We are supposed
to add it to the linker flags as well. Clang build fails because of this.

Added a change to add -flto to REDIS_CFLAGS and REDIS_LDFLAGS
if the build optimization flag is -O3. (noopt build will not use -flto)","['src/quicklist.c', 'src/t_zset.c']",The Clang build is failing as the -flto flag is only added to the compile flags and not to the linker flags as it should be.
bbc2c44541fbb50c5d1fc001afe2d2fca3a42eda,1606340341,"INFO client_recent_max_input_buffer includes argv array (#8065)

this metric already includes the argv bytes, like what clientsCronTrackClientsMemUsage does, but it's missing the array itself.

p.s. For the purpose of tracking expensive clients we don't need to include the size of the client struct and the static reply buffer in it.",['src/server.c'],"The metric client_recent_max_input_buffer does not account for the memory used by the argv array itself, potentially leading to inaccurate memory usage tracking."
457b7073b5d512c10aaeeb0dbf1763f0cb10462c,1603029403,"INFO report peak memory before eviction (#7894)

In some cases one command added a very big bulk of memory, and this
would be ""resolved"" by the eviction before the next command.

Seeing an unexplained mass eviction we would wish to
know the highest momentary usage too.

Tracking it in call() and beforeSleep() adds some hooks in AOF and RDB
loading.

The fix in clientsCronTrackExpansiveClients is related to #7874",['src/server.c'],"Mass evictions occur without clear visibility on the highest momentary usage leading to unexplained decrease in memory, with issue specifically noted during AOF and RDB loading."
625bdaf3d1faabff956b29699a560b459c9103d3,1649332027,"Fix auto-aof-rewrite-percentage based AOFRW trigger after restart (#10550)

The `auto-aof-rewrite-percentage` config defines at what growth percentage
an automatic AOF rewrite is triggered.
This normally works OK since the size of the AOF file at the end of a rewrite
is stored in `server.aof_rewrite_base_size`.
However, on startup, redis used to store the entire size of the AOF file into that
variable, resulting in a wrong automatic AOF rewrite trigger (could have been
triggered much later than desired).
This issue would only affect the first AOFRW after startup, after that future AOFRW
would have been triggered correctly.
This bug existed in all previous versions of Redis.

This PR unifies the meaning of `server.aof_rewrite_base_size`, which only represents
the size of BASE AOF.
Note that after an AOFRW this size includes the size of the incremental file (all the
commands that executed during rewrite), so that auto-aof-rewrite-percentage is the
ratio from the size of the AOF after rewrite.
However, on startup, it is complicated to know that size, and we compromised on
taking just the size of the base file, this means that the first rewrite after startup can
happen a little bit too soon.

Co-authored-by: Oran Agra <oran@redislabs.com>
Co-authored-by: yoav-steinberg <yoav@redislabs.com>",['src/aof.c'],"Upon startup, Redis is incorrectly initializing `server.aof_rewrite_base_size` with the total AOF file size, causing the first automatic AOF rewrite (AOFRW) to be potentially triggered later than desired."
cee3d67f50a53fa388e8fe4ca32ab5cd8c1f1991,1631176349,"Delay to discard cached master when full synchronization (#9398)

* Delay to discard cache master when full synchronization
* Don't disconnect with replicas before loading transferred RDB when full sync

Previously, once replica need to start full synchronization with master,
it will discard cached master whatever full synchronization is failed or
not. 
Now we discard cached master only when transferring RDB is finished
and start to change data space, this make replica could start partial
resynchronization with another new master if new master is failed
during full synchronization.","['src/replication.c', 'tests/integration/replication.tcl']",Full synchronization discards cached master prematurely leading to the inability for the replica to initiate partial resynchronization with a new master if the new master fails during full synchronization.
ac84b1cd826a39c80cbc80a83787b7ba2cbc3ec8,1641296267,"Ban snapshot-creating commands and other admin commands from transactions (#10015)

Creating fork (or even a foreground SAVE) during a transaction breaks the atomicity of the transaction.
In addition to that, it could mess up the propagated transaction to the AOF file.

This change blocks SAVE, PSYNC, SYNC and SHUTDOWN from being executed inside MULTI-EXEC.
It does that by adding a command flag, so that modules can flag their commands with that flag too.

Besides it changes BGSAVE, BGREWRITEAOF, and CONFIG SET appendonly, to turn the
scheduled flag instead of forking righ taway.

Other changes:
* expose `protected`, `no-async-loading`, and `no_multi` flags in COMMAND command
* add a test to validate propagation of FLUSHALL inside a transaction.
* add a test to validate how CONFIG SET that errors reacts in a transaction

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/aof.c', 'src/commands.c', 'src/rdb.c', 'src/server.c', 'src/server.h', 'tests/unit/aofrw.tcl', 'tests/unit/multi.tcl']",Running snapshot-creating and other admin commands during a transaction break the atomicity of the transaction and may result in AOF file corruption.
238cebdd5eb0fd0d8f2db87d895a007bc492dcdb,1492784858,"Check event loop creation return value. Fix #3951.

Normally we never check for OOM conditions inside Redis since the
allocator will always return a pointer or abort the program on OOM
conditons. However we cannot have control on epool_create(), that may
fail for kernel OOM (according to the manual page) even if all the
parameters are correct, so the function aeCreateEventLoop() may indeed
return NULL and this condition must be checked.
",['src/server.c'],"The event loop creation may fail and return NULL due to kernel out-of-memory (OOM) conditions, which currently isn't checked for."
7f19a04f0f049720ff5f84f3ab1aa81014f2f4ed,1594551326,"update release scripts for new hosts, and CI to run more tests (#7480)

* update daily CI to include cluster and sentinel tests
* update daily CI to run when creating a new release
* update release scripts to work on the new redis.io hosts","['utils/releasetools/01_create_tarball.sh', 'utils/releasetools/02_upload_tarball.sh', 'utils/releasetools/03_test_release.sh', 'utils/releasetools/04_release_hash.sh']","The existing release scripts and daily CI tests are not configured for the new hosts on redis.io, causing limited test coverage and unsuccessful runs for cluster and sentinel tests during a new release."
45c99d7092926c0774608520b9a399dbd40f9714,1658922005,"Adds RM_Microseconds and RM_CachedMicroseconds (#11016)

RM_Microseconds
Return the wall-clock Unix time, in microseconds

RM_CachedMicroseconds
Returns a cached copy of the Unix time, in microseconds.
It is updated in the server cron job and before executing a command.
It is useful for complex call stacks, such as a command causing a
key space notification, causing a module to execute a RedisModule_Call,
causing another notification, etc.
It makes sense that all these callbacks would use the same clock.","['src/module.c', 'src/redismodule.h', 'tests/modules/keyspace_events.c']","Unix time retrieval inconsistencies occur during complex call stacks, creating different timestamps for interconnected operations like command execution, key space notifications and module calls."
f4b5a4d8698cfee563ee0c054bd97ef5b46c5d26,1618736154,"Improve testsuite print of log file (#8805)

1. the `dump_logs` option would have printed only logs of servers that were
   spawn before the test proc started, and not ones that the test proc
   started inside it.
2. when a server proc catches an exception it should normally forward the
   exception upwards, specifically when it's an assertion that should be
   caught by a test proc above. however, in `durable` mode, we caught all
   exceptions printed them to stdout and let the code continue,
   this was wrong to do for assertions, which should have still been
   propagated to the test function.
3. don't bother to search for crash log to print if we printed the the
   entire log anyway
4. if no crash log was found, no need to print anything (i.e. the fact it
   wasn't found)
5. rename warnings_from_file to crashlog_from_file","['tests/support/server.tcl', 'tests/support/test.tcl', 'tests/support/util.tcl']","Testsuite's 'dump_logs' option doesn't print logs of servers started by the test proc. Also, when a server proc catches an exception, including assertions, in 'durable' mode, it just prints them to stdout and lets the code continue, which isn't ideal."
f8547e53f0d80b4cd2a84c6d39d1e08a470f0a2c,1498809817,"Added GEORADIUS(BYMEMBER)_RO variants for read-only operations.

Issue #4084 shows how for a design error, GEORADIUS is a write command
because of the STORE option. Because of this it does not work
on readonly slaves, gets redirected to masters in Redis Cluster even
when the connection is in READONLY mode and so forth.

To break backward compatibility at this stage, with Redis 4.0 to be in
advanced RC state, is problematic for the user base. The API can be
fixed into the unstable branch soon if we'll decide to do so in order to
be more consistent, and reease Redis 5.0 with this incompatibility in
the future. This is still unclear.

However, the ability to scale GEO queries in slaves easily is too
important so this commit adds two read-only variants to the GEORADIUS
and GEORADIUSBYMEMBER command: GEORADIUS_RO and GEORADIUSBYMEMBER_RO.
The commands are exactly as the original commands, but they do not
accept the STORE and STOREDIST options.
","['src/geo.c', 'src/server.c', 'src/server.h']","GEORADIUS command, due to its STORE option, functions as a write command and doesn't work on readonly slaves, or redirects to masters in Redis Cluster even while in READONLY mode, thus hampering the scalability of GEO queries."
70b2c4f5fd4dc5ca592aad2128d0fd43e1e8f82c,1678871224,"Fix WAITAOF reply when using last_offset and last_numreplicas (#11917)

WAITAOF wad added in #11713, its return is an array.
But forget to handle WAITAOF in last_offset and last_numreplicas,
causing WAITAOF to return a WAIT like reply.

Tests was added to validate that case (both WAIT and WAITAOF).
This PR also refactored processClientsWaitingReplicas a bit for better
maintainability and readability.","['src/replication.c', 'tests/support/util.tcl', 'tests/unit/wait.tcl']","The WAITAOF function isn't handled in last_offset and last_numreplicas, leading to a single WAIT-like reply instead of the array expected."
93d951556be78e7a0b9fca40f0ebddb4e946ca27,1643462519,"commands arguments improvement about unix-time type (#10203)

Change the name to unix-time-seconds or unix-time-milliseconds
to be consistent. Change the type from INTEGER to UNIX_TIME.

SET (EXAT and PXAT) was already ok.
and naming aside, both PXAT and EXAT everywhere used unit-time (for both milliseconds and seconds).
the only ones that where wrong are GETEX and XCLAIM (using ""integer"" for both seconds and milliseconds)",['src/commands.c'],Inconsistency in command argument names and types for unix-time across GETEX and XCLAIM commands.
9483ab0b8ece47eaa0c6b725e87d9348b521a3e3,1676397990,"Minor changes around the blockonkeys test module (#11803)

All of the POP commands must not decr length below 0.
So, get_fsl will delete the key if the length is 0 (unless
the caller wished to create if doesn't exist)

Other:
1. Use REDISMODULE_WRITE where needed (POP commands)
2. Use wait_for_blokced_clients in test

Unrelated:
Use quotes instead of curly braces in zset.tcl, for variable expansion","['tests/modules/blockonkeys.c', 'tests/unit/moduleapi/blockonkeys.tcl', 'tests/unit/type/zset.tcl']","POP commands in the blockonkeys test module are decrementing length below 0, resulting in key deletion when length is zero."
665e4284356dad675baefd4aa3246094def3abc6,1635226470,"Testsuite: attempt to find / avoid valgrind warnings of killed processes (#9679)

I recently started seeing a lot of empty valgrind reports in the daily CI.
i.e. prints showing valgrind header but no leak report, which causes the tests to fail
https://github.com/redis/redis/runs/3991335416?check_suite_focus=true

This commit change 2 things:
* first, considering valgrind is just slow, we used to give processes 60 seconds timeout on shutdown
  instead of 10 seconds we give normally. this commit changes that to 120.
* secondly, when we reach the timeout, we first try to use SIGSEGV so that maybe we'll get a stack
  trace indicating where redis is hang, and we only resort to SIGKILL if double that time passed.

note that if there are indeed hang processes, we will normally not see that in the non-valgrind runs,
since the tests didn't use to detect any failure in that case, and now they will since `crashlog_from_file`
is run after `kill_server`.","['tests/instances.tcl', 'tests/support/server.tcl']","Daily CI prints empty Valgrind headers with missing leak reports, leading to failing tests, potentially due to premature process termination under Valgrind."
380f6048e0bbc762f12fa50b57b73cf29049f967,1603809360,"Fix cluster access to unaligned memory (SIGBUS on old ARM) (#7958)

Turns out this was broken since version 4.0 when we added sds size
classes.
The cluster code uses sds for the receive buffer, and then casts it to a
struct and accesses a 64 bit variable.
This commit replaces the use of sds with a simple reallocated buffer.","['src/cluster.c', 'src/cluster.h']",Cluster code using sds for receive buffer and casting to a struct causes unaligned memory access errors (SIGBUS) on older ARM architectures.
0215324a66af949be39b34be2d55143232c1cb71,1633338617,"Fix redis-cli / redis-sential overflow on some platforms (CVE-2021-32762) (#9587)

The redis-cli command line tool and redis-sentinel service may be vulnerable
to integer overflow when parsing specially crafted large multi-bulk network
replies. This is a result of a vulnerability in the underlying hiredis
library which does not perform an overflow check before calling the calloc()
heap allocation function.

This issue only impacts systems with heap allocators that do not perform their
own overflow checks. Most modern systems do and are therefore not likely to
be affected. Furthermore, by default redis-sentinel uses the jemalloc allocator
which is also not vulnerable.

Co-authored-by: Yossi Gottlieb <yossigo@gmail.com>","['deps/hiredis/hiredis.c', 'deps/hiredis/test.c']",Redis-cli and Redis-sentinel have a vulnerability to integer overflow while parsing large multi-bulk network replies due to an issue in the underlying hiredis library.
985430b4fca5ac55b121a98ac0407909c6767530,1637061145,"Change lzf to handle values larger than UINT32_MAX (#9776)

Redis supports inserting data over 4GB into string (and recently for lists too, see #9357),
But LZF compression used in RDB files (see `rdbcompression` config), and in quicklist
(see `list-compress-depth` config) does not support compress/decompress data over
UINT32_MAX, which will result in corrupting the rdb after compression.

Internal changes:
1. Modify the `unsigned int` parameter of `lzf_compress/lzf_decompress` to `size_t`.
2. Modify the variable types in `lzf_compress` involving offsets and lengths to `size_t`.
3. Set LZF_USE_OFFSETS to 0.
    When LZF_USE_OFFSETS is 1, lzf store offset into `LZF_HSLOT`(32bit). 
    Even in 64-bit, `LZF_USE_OFFSETS` defaults to 1, because lzf assumes that it only
    compresses and decompresses data smaller than UINT32_MAX.
    But now we need to make lzf support 64-bit, turning on `LZF_USE_OFFSETS` will make
    it impossible to store 64-bit offsets or pointers.
    BTW, disable LZF_USE_OFFSETS also brings a few performance improvements.

Tests:
1. Add test for compress/decompress string large than UINT32_MAX.
2. Add unittest for compress/decompress quicklistNode.
","['src/lzf.h', 'src/lzfP.h', 'src/lzf_c.c', 'src/lzf_d.c', 'src/quicklist.c', 'tests/unit/bitops.tcl']","Redis RDB files and quicklists have issues when handling LZF compression/decompression of data exceeding 4GB, leading to corrupted RDB after compression."
4375b01cc7ab617579e17a4682a690a95f08ff8d,1681411706,"Adding missing test cases for substring (#12039)

There is are some missing test cases for SUBSTR command.
These might already be covered by GETRANGE, but no harm in adding them since they're simple.

Added 3 test case.

* start > stop
* start and stop both greater than string length
* when no key is present.
",['tests/unit/type/string.tcl'],"Absence of certain test cases for SUBSTR command, such as when start > stop, start and stop both exceed string length, and when no key exists, can potentially lead to undetected issues."
992f9e23c7ee819ad0dfac0bd6224d8330366960,1651008054,"Move user eval function to be located on Lua registry.

Today, Redis wrap the user Lua code with a Lua function.
For example, assuming the user code is:

```
return redis.call('ping')
```

The actual code that would have sent to the Lua interpreter was:

```
f_b3a02c833904802db9c34a3cf1292eee3246df3c() return redis.call('ping') end
```

The wraped code would have been saved on the global dictionary with the
following name: `f_<script sha>` (in our example `f_b3a02c833904802db9c34a3cf1292eee3246df3c`).

This approach allows one user to easily override the implementation a another user code, example:

```
f_b3a02c833904802db9c34a3cf1292eee3246df3c = function() return 'hacked' end
```

Running the above code will cause `evalsha b3a02c833904802db9c34a3cf1292eee3246df3c 0` to return
hacked although it should have returned `pong`.

Another disadventage is that Redis basically runs code on the loading (compiling) phase without been
aware of it. User can do code injection like this:

```
return 1 end <run code on compling phase> function() return 1
```

The wraped code will look like this and the entire `<run code on compling phase>` block will run outside
of eval or evalsha context:

```
f_<sha>() return 1 end <run code on compling phase> function() return 1 end
```
","['src/eval.c', 'src/script_lua.c', 'src/script_lua.h']",Lua code wrapping in Redis allows for unintended script overriding and potential code injection during the compiling phase.
2aad03fa399f520febb8b7db972459bc97484104,1695310892,"Use server.current_client to decide whether cluster commands should return TLS info. (#12569)

Starting a change in #12233 (released in 7.2), CLUSTER commands use client's
connection to decide whether to return TLS port or non-TLS port, but commands
called by Lua script and module's RM_Call don't have a real client with connection,
and would currently be regarded as non-TLS connections.

We can use server.current_client instead when it is available. When it is not (module calls
commands without a real client), we may see this as an undefined behavior, and return null
or default port (currently in this PR it returns default port, judged by server.tls_cluster).
",['src/cluster.c'],"CLUSTER commands used by Lua script and module's RM_Call lack a real client, causing them to be incorrectly treated as non-TLS connections, as they don't have a real client with connection."
4985c11bd6ef67a8eb58716f6c2b6b3d96e8538e,1611054933,"Bugfix: Make modules blocked on keys unblock on commands like LPUSH (#8356)

This was a regression from #7625 (only in 6.2 RC2).

This makes it possible again to implement blocking list and zset
commands using the modules API.

This commit also includes a test case for the reverse: A module
unblocks a client blocked on BLPOP by inserting elements using
RedisModule_ListPush(). This already works, but it was untested.","['src/blocked.c', 'tests/modules/blockonkeys.c', 'tests/unit/moduleapi/blockonkeys.tcl']",A regression in version 6.2 RC2 causes modules blocked on keys not to unblock on commands like LPUSH negatively affecting implementation of blocking list and zset commands using the modules API.
b95beeb595c94649cfda4b2bd8049cf5ddb07868,1644233253,"Fix redis-cli with sentinel crash due to SENTINEL DEBUG missing summary (#10250)

Fix redis-cli with sentinel crash due to SENTINEL DEBUG missing summary

Because SENTINEL DEBUG missing summary in its json file,
with the change in #10043, the following assertion will fail.
```
[redis]# src/redis-cli -p 26379
redis-cli: redis-cli.c:678: cliInitCommandHelpEntry: Assertion `reply->type == 1' failed.
```

This commit add the summary and complexity for SENTINEL DEBUG,
which introduced in #9291, and also improved the help message.","['src/commands.c', 'src/sentinel.c']","Sentinel debug feature in Redis-cli is missing summary in its json file, causing an assertion failure when initializing command help entry."
b4123663c31aaf1e97f4dbd630cbd7b8d0e91e31,1673869878,"Obuf limit, exit during loop in *RAND* commands and KEYS (#11676)

Related to the hang reported in #11671
Currently, redis can disconnect a client due to reaching output buffer limit,
it'll also avoid feeding that output buffer with more data, but it will keep
running the loop in the command (despite the client already being marked for
disconnection)

This PR is an attempt to mitigate the problem, specifically for commands that
are easy to abuse, specifically: KEYS, HRANDFIELD, SRANDMEMBER, ZRANDMEMBER.
The RAND family of commands can take a negative COUNT argument (which is not
bound to the number of elements in the key), so it's enough to create a key
with one field, and then these commands can be used to hang redis.
For KEYS the caller can use the existing keyspace in redis (if big enough).","['src/db.c', 'src/t_hash.c', 'src/t_set.c', 'src/t_zset.c', 'tests/unit/obuf-limits.tcl']","Redis can hang due to continuous execution of loop in commands such as KEYS, HRANDFIELD, SRANDMEMBER, ZRANDMEMBER even after client disconnection upon reaching output buffer limit."
2f1a1c3835efefec8de97d3222538a45ccdd257e,1582785293,"fix github actions failing latency test for active defrag - part 2

it seems that running two clients at a time is ok too, resuces action
time from 20 minutes to 10. we'll use this for now, and if one day it
won't be enough we'll have to run just the sensitive tests one by one
separately from the others.

this commit also fixes an issue with the defrag test that appears to be
very rare.
",['tests/unit/memefficiency.tcl'],"Github Actions latency test for active defrag fails sporadically; also, running two clients concurrently causes the action time to double."
bebc7f8470a5c6093514abf02f2514268c78b28f,1632398437,"Add RM_TrimStringAllocation(). (#9540)

This commit makes it possible to explicitly trim the allocation of a
RedisModuleString.

Currently, Redis automatically trims strings that have been retained by
a module command when it returns. However, this is not thread safe and
may result with corruption in threaded modules.

Supporting explicit trimming offers a backwards compatible workaround to
this problem.","['src/module.c', 'src/redismodule.h', 'tests/modules/basics.c']","Thread safety issue with automatically trimming strings retained by a module command, potentially leading to corruption in threaded modules."
79db037a4f7bf18962f4255de84d1767d63da1f5,1647938041,"config rewrite enhancement, in case of line longer than 1024 (#8009)

When rewrite the config file, we need read the old config file first,
but the CONFIG_MAX_LEN is 1024, so if some lines are longer than it,
it will generate a wrong config file, and redis cannot reboot from
the new config file.

Rename CONFIG_MAX_LINE to CONFIG_READ_LEN","['src/config.c', 'src/server.h']",Redis fails to reboot from the new config file because lines longer than 1024 characters generate an incorrect configuration file when rewritten.
86fed3fe093cfa5342b1965b89adb4ac987890ba,1595240067,"Refactor streamAppendItem() by deleting redundancy condition. (#7487)

It will never happen that ""lp != NULL && lp_bytes >= server.stream_node_max_bytes"".
Assume that ""lp != NULL && lp_bytes >= server.stream_node_max_bytes"",
we got the following conditions:
a. lp != NULL
b. lp_bytes >= server.stream_node_max_bytes

If server.stream_node_max_bytes is 0, given condition a, condition b is always satisfied
If server.stream_node_max_bytes is not 0, given condition a and condition b, the codes just a
	few lines above set lp to NULL, a controdiction with condition a

So that condition b is recundant. We could delete it safely.",['src/t_stream.c'],There are redundant conditions in streamAppendItem() function causing unnecessary checks and potential contradictions when server.stream_node_max_bytes is 0.
3a955d9ad443adcb1d0495f4b702612f6b250f6e,1618816087,"Fix ouput buffer limit test (#8803)

The tail size of c->reply is 16kb, but in the test only publish a
few chars each time, due to a change in #8699, the obuf limit
is now checked a new memory allocation is made, so this test
would have sometimes failed to trigger a soft limit disconnection
in time.

The solution is to write bigger payloads to the output buffer, but
still limit their rate (not more than 100k/s).",['tests/unit/obuf-limits.tcl'],The test checking the output buffer's soft limit disconnection is failing occasionally due to small payload sizes and a shifted timing of obuf limit check after a new memory allocation.
d1a005ab3963c16b65e805675a76f0e40c723158,1570428572,"fix issues found by a static analyzer

cluster.c - stack buffer memory alignment
    The pointer 'buf' is cast to a more strictly aligned pointer type
evict.c - lazyfree_lazy_eviction, lazyfree_lazy_eviction always called
defrag.c - bug in dead code
server.c - casting was missing parenthesis
rax.c - indentation / newline suggested an 'else if' was intended
","['src/cluster.c', 'src/defrag.c', 'src/evict.c', 'src/rax.c', 'src/server.c']","Various issues spotted by static analyzer: misaligned stack buffer cast, missing parenthesis in casting operation, issues with function call in 'evict.c', an error in 'defrag.c' dead code and a misleading indentation in 'rax.c'."
432bf4770e8f2c11e63477620e4765abfbdf91f5,1521032498,"Cluster: ability to prevent slaves from failing over their masters.

This commit, in some parts derived from PR #3041 which is no longer
possible to merge (because the user deleted the original branch),
implements the ability of slaves to have a special configuration
preventing that they try to start a failover when the master is failing.

There are multiple reasons for wanting this, and the feautre was
requested in issue #3021 time ago.

The differences between this patch and the original PR are the
following:

1. The flag is saved/loaded on the nodes configuration.
2. The 'myself' node is now flag-aware, the flag is updated as needed
   when the configuration is changed via CONFIG SET.
3. The flag name uses NOFAILOVER instead of NO_FAILOVER to be consistent
   with existing NOADDR.
4. The redis.conf documentation was rewritten.

Thanks to @deep011 for the original patch.
","['src/cluster.c', 'src/cluster.h', 'src/config.c', 'src/server.c', 'src/server.h']","There's no way to prevent slave nodes from initiating a failover when the master node fails, causing unwanted failover in certain conditions."
ed9bfe226289759756126f35583394e5db93048f,1599994223,"fix broken PEXPIREAT test (#7791)

This test was nearly always failing on MacOS github actions.
This is because of bugs in the test that caused it to nearly always run
all 3 attempts and just look at the last one as the pass/fail creteria.

i.e. the test was nearly always running all 3 attempts and still sometimes
succeed. this is because the break condition was different than the test
completion condition.

The reason the test succeeded is because the break condition tested the
results of all 3 tests (PSETEX/PEXPIRE/PEXPIREAT), but the success check
at the end was only testing the result of PSETEX.

The reason the PEXPIREAT test nearly always failed is because it was
getting the current time wrong: getting the current second and loosing
the sub-section time, so the only chance for it to succeed is if it run
right when a certain second started.

Because i now get the time from redis, adding another round trip, i
added another 100ms to the PEXPIRE test to make it less fragile, and
also added many more attempts.

Adding many more attempts before failure to account for slow platforms,
github actions and valgrind",['tests/unit/expire.tcl'],The PEXPIREAT test fails frequently on MacOS due to inaccurate time retrieval. It is also noticing incorrect pass/fail criteria due to mismatched break and test completion conditions. Neglect of time fractions results in inaccurate results.
09f99c2a925a0351985e799c106614082d6053cf,1559478794,"make redis purge jemalloc after flush, and enable background purging thread

jemalloc 5 doesn't immediately release memory back to the OS, instead there's a decaying
mechanism, which doesn't work when there's no traffic (no allocations).
this is most evident if there's no traffic after flushdb, the RSS will remain high.

1) enable jemalloc background purging
2) explicitly purge in flushdb
","['src/config.c', 'src/db.c', 'src/debug.c', 'src/server.c', 'src/server.h', 'src/zmalloc.c', 'src/zmalloc.h']","Jemalloc 5 doesn't release memory back to OS instantly causing high RSS especially with no traffic post flushdb, indicating ineffective memory management."
189a12afb4df2986d4671cdebae3c96c4f0945c2,1492603372,"PSYNC2: discard pending transactions from cached master.

During the review of the fix for #3899, @yangsiran identified an
implementation bug: given that the offset is now relative to the applied
part of the replication log, when we cache a master, the successive
PSYNC2 request will be made in order to *include* the transaction that
was not completely processed. This means that we need to discard any
pending transaction from our replication buffer: it will be re-executed.
",['src/replication.c'],"In the current implementation, when caching a master, PSYNC2 requests aim to include transactions that were not completely processed, leading to re-execution of any pending transaction from our replication buffer."
0c00fd7834215ec311a91def5d6737892222b9c3,1512120264,"Streams: reduce listpack max size to 2k to speedup range queries.

Listpack max size is a tradeoff between space and time. A 2k max entry
puts the memory usage approximately at a similar order of magnitude (5
million entries went from 96 to 120 MB), but the range queries speed
doubled (because there are half entries to scan in the average case).

Lower values could be considered, or maybe this parameter should be
made tunable.
",['src/t_stream.c'],"Current listpack max size impacting time and space efficiency, causing slow range query speeds due to increased entries to scan."
49e098234abc2c5cc7042c931e3d611b0ae2055d,1523267684,"Modules API: blocked client free callback modified to get a context.

Note that this was an experimental API that can only be enabled with
REIDSMODULE_EXPERIMENTAL_API, so it is subject to change until its
promoted to stable API. Sorry for the breakage, it is trivial to
resolve btw. This change will not be back ported to Redis 4.0.
","['src/module.c', 'src/modules/helloblock.c', 'src/modules/hellocluster.c', 'src/redismodule.h']","Blocked client free callback in experimental Modules API lacks context, potentially causing functionality issues in Redis 4.0."
fa5474e15319743908e4cd32bd003272ed018ed3,1670520570,"Normalize NAN to a single nan type, like we do with inf (#11597)

From https://en.wikipedia.org/wiki/NaN#Display, it says
that apart from nan and -nan, we can also get NAN and even
nan(char-sequence) from libc.

In #11482, our conclusion was that we wanna normalize it in
Redis to a single nan type, like we already normalized inf.

For this, we also reverted the assert_match part of the test
added in #11506, using assert_equal to validate the changes.","['src/networking.c', 'src/util.c', 'tests/unit/moduleapi/reply.tcl']","Different NAN types returned from libc are not being normalized in Redis, leading to inconsistencies and potential unexpected behavior."
b45b0d81bbbb26a793567fbf5236364ba28d4686,1616567604,"Fix crash report killed by message (#8683)

We sometimes see the crash report saying we were killed by a random
process even in cases where the crash was spontanius in redis.
for instance, crashes found by the corrupt-dump test.

It looks like this si_pid is sometimes left uninitialized, and a good
way to tell if the crash originated in redis or trigged by outside is to
look at si_code, real signal codes are always > 0, and ones generated by
kill are have si_code of 0 or below.",['src/debug.c'],"Redis crashes are reported as being triggered by an external process instead of being recognized as spontaneous, possibly due to uninitialized si_pid."
f55858349344b40168cd83d645b5636295eb92a6,1654493364,"Split instantaneous_repl_total_kbps to instantaneous_input_repl_kbps and instantaneous_output_repl_kbps. (#10810)

A supplement to https://github.com/redis/redis/pull/10062
Split `instantaneous_repl_total_kbps` to `instantaneous_input_repl_kbps` and `instantaneous_output_repl_kbps`. 
## Work:
This PR:
- delete 1 info field:
    - `instantaneous_repl_total_kbps`
- add 2 info fields:
    - `instantaneous_input_repl_kbps / instantaneous_output_repl_kbps`
## Result:
- master
```
total_net_input_bytes:26633673
total_net_output_bytes:21716596
total_net_repl_input_bytes:0
total_net_repl_output_bytes:18433052
instantaneous_input_kbps:0.02
instantaneous_output_kbps:0.00
instantaneous_input_repl_kbps:0.00
instantaneous_output_repl_kbps:0.00
```
- slave
```
total_net_input_bytes:18433212
total_net_output_bytes:94790
total_net_repl_input_bytes:18433052
total_net_repl_output_bytes:0
instantaneous_input_kbps:0.00
instantaneous_output_kbps:0.05
instantaneous_input_repl_kbps:0.00
instantaneous_output_repl_kbps:0.00
```","['src/server.c', 'src/server.h']",A single `instantaneous_repl_total_kbps` value is unable to provide separated information about the input and output rates in the replication process.
b00a2351865bb1d87e2d2701b77c8a68daefb054,1686931987,"Use Reservoir Sampling for random sampling of dict, and fix hang during fork (#12276)

## Issue:
When a dict has a long chain or the length of the chain is longer than
the number of samples, we will never be able to sample the elements
at the end of the chain using dictGetSomeKeys().
This could mean that SRANDMEMBER can be hang in and endless loop.
The most severe case, is the pathological case of when someone uses SCAN+DEL
or SSCAN+SREM creating an unevenly distributed dict.
This was amplified by the recent change in #11692 which prevented a
down-sizing rehashing while there is a fork.

## Solution
1. Before, we will stop sampling when we reach the maximum number
  of samples, even if there is more data after the current chain.
  Now when we reach the maximum we use the Reservoir Sampling
  algorithm to fairly sample the end of the chain that cannot be sampled
2. Fix the rehashing code, so that the same as it allows rehashing for up-sizing
  during fork when the ratio is extreme, it will allow it for down-sizing as well.

Issue was introduced (or became more severe) by #11692

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/dict.c', 'tests/unit/type/set.tcl']","Long chains in dict cause never-ending loops for SRANDMEMBER, particularly when SCAN+DEL or SSCAN+SREM produce uneven dict distributions. Recent changes in #11692 amplify this issue by preventing down-sizing rehashes during a fork."
8ef4f1dbad3c40dc90920c4aaa8a2f9f72637786,1655230190,"Script that made modification will not break with unexpected NOREPLICAS error (#10855)

If a script made a modification and then was interrupted for taking too long.
there's a chance redis will detect that a replica dropped and would like to reject
write commands with NOREPLICAS due to insufficient good replicas.
returning an error on a command in this case breaks the script atomicity.

The same could in theory happen with READONLY, MISCONF, but i don't think
these state changes can happen during script execution.","['src/script.c', 'tests/unit/scripting.tcl']",Scripts that modify Redis data and get interrupted for taking too long can break script atomicity due to an unexpected NOREPLICAS error upon insufficient good replicas.
f24ad5d83140eee04bd1cda8ee19bced0de26c11,1539969314,"cli: pass auth through REDISCLI_AUTH

This adds support for passing a password through a REDISCLI_AUTH
environment variable (which is safer than the CLI), which might often be
safer than passing it through a CLI argument.

Passing a password this way does not trigger the warning about passing a
password through CLI arguments, and CLI arguments take precedence over
it.
",['src/redis-cli.c'],Password provision through CLI argument for Redis is not safe and triggers warnings; requires safer alternative.
0413fbc7d015d2ad9a4f57cc50a68e581f0e7744,1618814061,"fix invalid master_link_down_since_seconds in info repication (#8785)

When replica never successfully connect to master, server.repl_down_since
will be initialized to 0, therefore, the info master_link_down_since_seconds
was showing the current unix timestamp, which does not make much sense.

This commit fixes the issue by showing master_link_down_since_seconds to -1.
means the replica never connect to master before.

This commit also resets this variable back to 0 when a replica is turned into
a master, so that it'll behave the same if the master is later turned into a
replica again.

The implication of this change is that if some app is checking if the value is > 60
do something, like conclude the replica is stale, this could case harm (changing
a big positive number with a small one).","['src/replication.c', 'src/server.c']","When a replica server never successfully connects to the master, incorrect server replication downtime is displayed, represented by the current Unix timestamp instead of a representative error value."
79ffc3524dc3d1222685cacc236f417d09459bae,1650922441,"fix broken protocol regression from #10612 (#10639)

A change in #10612 introduced a regression.
when replying with garbage bytes to the caller, we must make sure it
doesn't include any newlines.

in the past it called rejectCommandFormat which did that trick.
but now it calls rejectCommandSds, which doesn't, so we need to make sure
to sanitize the sds.",['src/server.c'],"A regression from change #10612 causes protocol issues, as garbage byte replies might include newlines post rejection of CommandSds."
78960ad57b8a5e6af743d789ed8fd767e37d42b8,1655181163,"Throw -TRYAGAIN instead of -ASK on migrating nodes for multi-key commands when the node only has some of the keys (#9526)

* In cluster getNodeByQuery when target slot is in migrating state and
the slot lack some keys but have at least one key, should return TRYAGAIN.

Before this commit, when a node is in migrating state and recevies
multiple keys command, if some keys don't exist, the command emits
an `ASK` redirection.

After this commit, if some keys exist and some keys don't exist, the
command emits a TRYAGAIN error. If all keys don't exist, the command
emits an `ASK` redirection.",['src/cluster.c'],"When a node in migrating state receives multi-key commands and only contains some (but not all) of the keys, inappropriate 'ASK' redirection occurs instead of the expected 'TRYAGAIN' error."
9273d09dd46353015406c26cd7d3826b78109a38,1637739563,"Add tests to cover EXPIRE overflow fix (#9839)

In #8287, some overflow checks have been added. But when
`when *= 1000` overflows, it will become a positive number.
And the check not able to catch it. The key will be added with
a short expiration time and will deleted a few seconds later.

In #9601, will check the overflow after `*=` and return an
error first, and avoiding this situation.

In this commit, added some tests to cover those code paths.
Found it in #9825, and close it.",['tests/unit/expire.tcl'],"There is a potential overflow issue when `when *= 1000` occurs, causing keys to be added with a too-short expiration time, and are deleted within seconds."
8b2c0f90442c0646d7265ef150dd5afa3172b86e,1573030649,"Update PR #6537: use a fresh time outside call().

One problem with the solution proposed so far in #6537 is that key
lookups outside a command execution via call(), still used a cached
time. The cached time needed to be refreshed in multiple places,
especially because of modules callbacks from timers, cluster bus, and
thread safe contexts, that may use RM_Open().

In order to avoid this problem, this commit introduces the ability to
detect if we are inside call(): this way we can use the reference fixed
time only when we are in the context of a command execution or Lua
script, but for the asynchronous lookups, we can still use mstime() to
get a fresh time reference.
","['src/db.c', 'src/server.c', 'src/server.h']","Key lookups outside a command execution via call() are using cached time, causing issues with modules callbacks from timers, cluster bus and thread safe contexts that use RM_Open()."
430dadaf834e6de066006d5bf700d2daa70987ca,1610216540,"CLIENT PAUSE - don't drop together with other blocked clients (#8302)

When the server state changes and blocked clients are being dropped, the
paused clients should not be dropped, they're safe to keep since unlike
other blocked types, these commands are not half way though processing,
and the commands they sent may get rejected according to the new server
state.",['src/blocked.c'],"Server state changes causing both blocked and paused clients to be dropped simultaneously, possibly rejecting commands sent by paused clients mid-processing."
2cc99c692c4984f5d9c42e4d2d25fa70125ef0a7,1679809144,"Add COMMAND COUNT test to cover reply-schemas-validator test (#11971)

Since we remove the COMMAND COUNT call in sentinel test in #11950,
reply-schemas-validator started reporting this error:
```
WARNING! The following commands were not hit at all:
  command|count
  ERROR! at least one command was not hit by the tests
```

This PR add a COMMAND COUNT test to cover it and also fix some
typos in req-res-log-validator.py","['tests/unit/introspection-2.tcl', 'utils/req-res-log-validator.py']","Commands in sentinel tests are not completely covered, leading to errors reported by reply-schemas-validator related to COMMAND COUNT not getting hit."
5977a94842a25140520297fe4bfda15e0e4de711,1594364557,"RESTORE ABSTTL won't store expired keys into the db (#7472)

Similarly to EXPIREAT with TTL in the past, which implicitly deletes the
key and return success, RESTORE should not store key that are already
expired into the db.
When used together with REPLACE it should emit a DEL to keyspace
notification and replication stream.","['src/cluster.c', 'src/expire.c', 'src/server.h', 'tests/unit/dump.tcl']","'RESTORE command potentially stores already expired keys into the database, causing unnecessary usage of space. The issue also impacts keyspace notifications and replication stream when used with REPLACE.'"
5b17909c4f69830e93a569b6e4994c9775657e35,1644072856,"redis-cli generates command help tables from the results of COMMAND  (#10043)

This is a followup to #9656 and implements the following step mentioned in that PR:

* When possible, extract all the help and completion tips from COMMAND DOCS (Redis 7.0 and up)
* If COMMAND DOCS fails, use the static help.h compiled into redis-cli.
* Supplement additional command names from COMMAND (pre-Redis 7.0)

The last step is needed to add module command and other non-standard commands.

This PR does not change the interactive hinting mechanism, which still uses only the param
strings to provide somewhat unreliable and inconsistent command hints (see #8084).
That task is left for a future PR. 

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/redis-cli.c', 'tests/support/util.tcl', 'utils/generate-command-help.rb']","Redis-cli's command help tables extraction relies solely on static help.h, lacking support for COMMAND DOCS in Redis 7.0 and above, and doesn't accommodate non-standard commands like module commands."
14176484697dc96df62c76630eef2b97a89bb8d0,1637497820,"Prevent LCS from allocating temp memory over proto-max-bulk-len (#9817)

LCS can allocate immense amount of memory (sizes of two inputs multiplied by each other).
In the past this caused some possible security issues due to overflows, which we solved
and also added use of `trymalloc` to return ""Insufficient memory"" instead of OOM panic zmalloc.

But in case overcommit is enabled, it could be that we won't get the OOM panic, and zmalloc
will succeed, and then we can get OOM killed by the kernel.

The solution here is to prevent LCS from allocating transient memory that's bigger than
`proto-max-bulk-len` config.
This config is not directly related to transient memory, but using a hard coded value ad well as
introducing a specific config seems wrong.

This comes to solve an error in the corrupt-dump-fuzzer test that started in the daily CI see #9799","['src/t_string.c', 'tests/integration/corrupt-dump.tcl']","When overcommit is enabled, LCS allocates large amounts of transient memory, possibly leading to an OOM kill by the kernel. This correlates with errors in the corrupt-dump-fuzzer test as observed in daily CI."
f24d3a7de05d213f702621186f31a4c227f366c6,1512120264,"Streams: delta encode IDs based on key. Add count + deleted fields.

We used to have the master ID stored at the start of the listpack,
however using the key directly makes more sense in order to create a
space efficient representation: anyway the key at the radix tree is very
unlikely to change because of how the stream is implemented. Moreover on
nodes merging, to rewrite the merged listpacks is anyway the most
sensible operation, and we can use the iterator and the append-to-stream
function in order to avoid re-implementing the code needed for merging.

This commit also adds two items at the start of the listpack: the
number of valid items inside the listpack, and the number of items
marked as deleted. This means that there is no need to scan a listpack
in order to understand if it's a good candidate for garbage collection,
if the ration between valid/deleted items triggers the GC.
","['src/rdb.c', 'src/t_stream.c']","Storing master ID in listpack inefficient and causing extra storage usage. In addition, counting valid items and deleted items lacks an optimized solution causing unnecessary scans for garbage collection."
e3ef73dc2a557232c60c732705e8e6ff2050eba9,1646508352,"redis-cli: Better --json Unicode support and --quoted-json (#10286)

Normally, `redis-cli` escapes non-printable data received from Redis, using a custom scheme (which is also used to handle quoted input). When using `--json` this is not desired as it is not compatible with RFC 7159, which specifies JSON strings are assumed to be Unicode and how they should be escaped.

This commit changes `--json` to follow RFC 7159, which means that properly encoded Unicode strings in Redis will result with a valid Unicode JSON.

However, this introduces a new problem with `--json` and data that is not valid Unicode (e.g., random binary data, text that follows other encoding, etc.). To address this, we add `--quoted-json` which produces JSON strings that follow the original redis-cli quoting scheme.

For example, a value that consists of only null (0x00) bytes will show up as:
* `""\u0000\u0000\u0000""` when using `--json`
* `""\\x00\\x00\\x00""` when using `--quoted-json`","['src/cli_common.c', 'src/cli_common.h', 'src/redis-cli.c', 'tests/integration/redis-cli.tcl']","`redis-cli`'s escaping of non-printable data from Redis using a custom scheme is not compatible with RFC 7159 when used with `--json`, causing issues with properly encoded Unicode strings. This also introduces problems with data not valid in Unicode."
4d1094e8be3150b92b3e96d3a743c66b1a95988a,1620312349,"fix redis-benchmark to ignore unsupported configs (#8916)

Redis Enterprise supports the CONFIG GET command, but it replies with am
empty array since the save and appendonly configs are not supported.
before this fix redis-benchmark would segfault for trying to access the
error string on an array type reply.
see #8869",['src/redis-benchmark.c'],Redis-benchmark segfaults when trying to access the error string on an array type reply due to unsupported save and appendonly configs.
fe37e4fc874a92dcf61b3b0de899ec6f674d2442,1696503017,"Cleanup nested module keyspace notifications (#12630)

Recently we added a way for the module to declare that it wishes to
receive nested KSN, by setting ALLOW_NESTED_KEYSPACE_NOTIFICATIONS.
but it looks like this flow has a bug, clearing the `active` member
when it was previously set. however, since nesting is permitted,
this bug has no implications, since regardless of the active member,
the notification is permitted.",['src/module.c'],"Nested Keyspace Notifications module is improperly clearing the 'active' member variable despite nesting being allowed, potentially impacting module's notification permissions."
5f4a2801cb2eb42c2ed5c3ebe1b357147779963d,1595853113,"TLS: support cluster/replication without tls-port. (#7573)

Initialize and configure OpenSSL even when tls-port is not used, because
we may still have tls-cluster or tls-replication.

Also, make sure to reconfigure OpenSSL when these parameters are changed
as TLS could have been enabled for the first time.","['src/config.c', 'src/server.c']","OpenSSL is not initializing or configuring when tls-port is not in use, even when tls-cluster or tls-replication may be active. OpenSSL configuration is not updating when tls-cluster or tls-replication parameters are changed."
641c64ada10404356fc76c0b56a69b32c76f253c,1574182544,"Use libsystemd's sd_notify for communicating redis status to systemd

Instead of replicating a subset of libsystemd's sd_notify(3) internally,
use the dynamic library provided by systemd to communicate with the
service manager.

When systemd supervision was auto-detected or configured, communicate
the actual server status (i.e. ""Loading dataset"", ""Waiting for
master<->replica sync"") to systemd, instead of declaring readiness right
after initializing the server process.
","['src/replication.c', 'src/server.c', 'src/server.h']","The current communication between Redis server and systemd service manager is only conveying readiness post initialization, thereby missing out real-time updates such as 'Loading dataset' or 'Master-Replica sync'."
e7f18432b8e9e1d1998d3a898006497e6c5e5a32,1681649366,"avoid incorrect shrinking of querybuf when client is reading a big argv (#12000)

this pr fix two wrongs：
1. When client’s querybuf is pre-allocated for a fat argv, we need to update the
  querybuf_peak of the client immediately to completely avoid the unexpected
  shrinking of querybuf in the next clientCron (before data arrives to set the peak).
2. the protocol's bulklen does not include `\r\n`, but the allocation and the data we
  read does. so in `clientsCronResizeQueryBuffer`, the `resize` or `querybuf_peak`
  should add these 2 bytes.

the first bug is likely to hit us on large payloads over slow connections, in which case
transferring the payload can take longer and a cron event will be triggered (specifically
if there are not a lot of clients)","['src/networking.c', 'src/server.c', 'tests/unit/querybuf.tcl']","Pre-allocated querybuf for large argv in client is unexpectedly shrinking during clientCron before data arrives, and protocol's bulklen allocation and read data mismatch owing to missing `\r\n` characters."
1234e3a5628260658adfe9065cb58ec5c1cb5ebe,1661152304,"Fully abstract connection type

Abstract common interface of connection type, so Redis can hide the
implementation and uplayer only calls connection API without macro.

               uplayer
                  |
           connection layer
             /          \
          socket        TLS

Currently, for both socket and TLS, all the methods of connection type
are declared as static functions.

It's possible to build TLS(even socket) as a shared library, and Redis
loads it dynamically in the next step.

Also add helper function connTypeOfCluster() and
connTypeOfReplication() to simplify the code:
link->conn = server.tls_cluster ? connCreateTLS() : connCreateSocket();
-> link->conn = connCreate(connTypeOfCluster());

Signed-off-by: zhenwei pi <pizhenwei@bytedance.com>
","['src/cluster.c', 'src/connection.c', 'src/connection.h', 'src/networking.c', 'src/replication.c', 'src/socket.c', 'src/tls.c']","Redis's connection type methods are static, leading to an interconnectedness that prohibits building TLS or socket as a shared library and limits scaling capabilities."
95ea74549cc454d6d6a7462b366462589cd96712,1614077865,"Fix failed tests on Linux Alpine and add a CI job. (#8532)

* Remove linux/version.h dependency.

This introduces unnecessary dependencies, and generally not a good idea
as the platform we build on may be different than the platform we run
on.

To determine if sync_file_range exists we can simply rely on header file
hints.

* Fix setproctitle() on libmusl.

The previous ifdef checks were a bit too strict for no apparent
reason.

* Fix tests failure on Linux with no backtrace.

* Add alpine daily CI job.","['src/config.h', 'src/fmacros.h', 'src/setproctitle.c', 'tests/integration/logging.tcl']","The software dependency on linux/version.h is causing failed tests on Linux Alpine, and there is a discrepancy between the build and run platform. Additionally, tests are failing on Linux with no backtrace provided. Issues with setproctitle() on libmusl also noted."
1f598fc2bb6975417751486405303d98246bb7bc,1494403456,"Modules TSC: use atomic var for server.unixtime.

This avoids Helgrind complaining, but we are actually not using
atomicGet() to get the unixtime value for now: too many places where it
is used and given tha time_t is word-sized it should be safe in all the
archs we support as it is.

On the other hand, Helgrind, when Redis is compiled with ""make helgrind""
in order to force the __sync macros, will detect the write in
updateCachedTime() as a read (because atomic functions are used) and
will not complain about races.

This commit also includes minor refactoring of mutex initializations and
a ""helgrind"" target in the Makefile.
","['src/server.c', 'src/server.h']","The usage of a non-atomic variable for server.unixtime is leading to false-positive race condition detections in Helgrind, especially when Redis is compiled with ""make helgrind""."
317464a386c0ba8e027817c0743e7d802f4abb1d,1640611097,"Fix failing test due to recent change in transaction propagation (#10006)

PR #9890 may have introduced a problem.
There are tests that use MULTI-EXEC to make sure two BGSAVE / BGREWRITEAOF are executed together.
But now it's not valid to run run commands that create a snapshot inside a transaction (gonna be blocked soon)
This PR modifies the test not to rely on MULTI-EXEC.

Co-authored-by: Oran Agra <oran@redislabs.com>",['tests/unit/aofrw.tcl'],"Recent changes have caused tests relying on MULTI-EXEC for simultaneous execution BGSAVE / BGREWRITEAOF to fail, as snapshot creation inside a transaction will soon be blocked."
897c7bddf5dcdc2fd6b4071ecfb15a88a6afaa04,1633446019,"Attempt to fix rare pubsub oubuf maxmemory eviction test failure (#9603)

* Reduce delay between publishes to allow less time to write the obufs.
* More subscribed clients to buffer more data per publish.
* Make sure main connection isn't evicted (it has a large qbuf).",['tests/unit/maxmemory.tcl'],"The pubsub oubuf maxmemory eviction test is failing intermittently, potentially due to the delay between publishes and eviction of the main connection with a large qbuf."
4e012daee97f1d3a3ea8dcef338ce8ef26f2325b,1646898715,"Fix outdated comments on updateSlavesWaitingBgsave (#10394)

* fix-replication-comments

The described capacity
 `and to schedule a new BGSAVE if there are slaves that attached while a BGSAVE was in progress`
was moved to `checkChildrenDone()`  named by `replicationStartPendingFork`

But the comment was not changed, may misleading others.

* remove-misleading-comments

The described capacity
 `to schedule a new BGSAVE if there are slaves that attached while a BGSAVE was in progress` 
and 
`or when the replication RDB transfer strategy is modified from disk to socket or the other way around` 
were not correct now.",['src/replication.c'],"The comments on `updateSlavesWaitingBgsave` function are outdated and misleading, not accurately reflecting the current code functionality and potentially causing confusion."
439b0315c8dfce4e33cd43d349264b13846c665c,1686928451,"Fix SPOP/RESTORE propagation when doing lazy free (#12320)

In SPOP, when COUNT is greater than or equal to set's size,
we will remove the set. In dbDelete, we will do DEL or UNLINK
according to the lazy flag. This is also required for propagate.

In RESTORE, we won't store expired keys into the db, see #7472.
When used together with REPLACE, it should emit a DEL or UNLINK
according to the lazy flag.

This PR also adds tests to cover the propagation. The RESTORE
test will also cover #7472.
","['src/cluster.c', 'src/t_set.c', 'tests/unit/dump.tcl', 'tests/unit/type/set.tcl']","SPOP and RESTORE operations do not propagate correctly while performing lazy free, affecting deletion and un-link operations. Issue is especially prominent when COUNT in SPOP is greater than or equal to the set's size and when REPLACE is used with RESTORE for expired keys."
2017407b4d1d19a91af1e7c0b199f2c1775dbaf9,1609828160,"Fix wrong order of key/value in Lua map response (#8266)

When a Lua script returns a map to redis (a feature which was added in
redis 6 together with RESP3), it would have returned the value first and
the key second.

If the client was using RESP2, it was getting them out of order, and if
the client was in RESP3, it was getting a map of value => key.
This was happening regardless of the Lua script using redis.setresp(3)
or not.

This also affects a case where the script was returning a map which it got
from from redis by doing something like: redis.setresp(3); return redis.call()

This fix is a breaking change for redis 6.0 users who happened to rely
on the wrong order (either ones that used redis.setresp(3), or ones that
returned a map explicitly).

This commit also includes other two changes in the tests:
1. The test suite now handles RESP3 maps as dicts rather than nested
   lists
2. Remove some redundant (duplicate) tests from tracking.tcl","['src/scripting.c', 'tests/support/redis.tcl', 'tests/unit/scripting.tcl', 'tests/unit/tracking.tcl']","Lua scripts returning a map to Redis are producing results with keys and values in reverse order, affecting both RESP2 and RESP3 clients."
d32f2e9999ce003bad0bd2c3bca29f64dcce4433,1614001292,"Fix integer overflow (CVE-2021-21309). (#8522)

On 32-bit systems, setting the proto-max-bulk-len config parameter to a high value may result with integer overflow and a subsequent heap overflow when parsing an input bulk (CVE-2021-21309).

This fix has two parts:

Set a reasonable limit to the config parameter.
Add additional checks to prevent the problem in other potential but unknown code paths.","['src/config.c', 'src/sds.c', 'src/zmalloc.c']",Setting the 'proto-max-bulk-len' config parameter to a high value on 32-bit systems leads to integer overflow and could potentially trigger a heap overflow when parsing input bulk.
482785ac62aab0bcaf53eb7eb6f7e005629ff0fa,1529421503,"add malloc_usable_size for libc malloc

this reduces the extra 8 bytes we save before each pointer.
but more importantly maybe, it makes the valgrind runs to be more similiar
to our normal runs.

note: the change in malloc_stats struct in server.h is to eliminate an name conflict.
structs that are not typedefed are resolved from a separate name space.
","['src/server.h', 'src/zmalloc.h']",Malloc_stats struct in server.h causing name conflict. Additional 8 bytes saved before each pointer leading to dissimilar valgrind runs compared to normal runs.
3f068b92b98c664ccefb457103068583879609e3,1492185191,"Test: fix, hopefully, false PSYNC failure like in issue #2715.

And many other related Github issues... all reporting the same problem.
There was probably just not enough backlog in certain unlucky runs.
I'll ask people that can reporduce if they see now this as fixed as
well.
",['tests/integration/replication-psync.tcl'],"Unpredictable PSYNC failures are being reported, potentially due to insufficient backlog during specific run circumstances."
1f75ce30dfdb1c8c4df8474f9580f2aff032cfb4,1577354497,"Stream: Handle streamID-related edge cases

This commit solves several edge cases that are related to
exhausting the streamID limits: We should correctly calculate
the succeeding streamID instead of blindly incrementing 'seq'
This affects both XREAD and XADD.

Other (unrelated) changes:
Reply with a better error message when trying to add an entry
to a stream that has exhausted last_id
","['src/blocked.c', 'src/stream.h', 'src/t_stream.c', 'tests/unit/type/stream.tcl']","Exhausting the streamID limits leads to incorrect calculations for the succeeding streamID in both XREAD and XADD operations. In addition, adding an entry to a stream that has exhausted its last_id is not properly handled."
4f4676a1420a446e4233c04a80f2009aa819b21f,1557761230,"Fix test false positive introduced by threaded I/O.

Now clients that are ready to be terminated asynchronously are processed
more often in beforeSleep() instead of being processed in serverCron().
This means that the test will not be able to catch the moment the client
was terminated, also note that the 'omem' figure now changes in big
steps, because of the new client output buffers layout.

So we have to change the test range in order to accomodate for that.
Yet the test is useful enough to be worth taking, even if its precision
is reduced by this commit. Probably if we get more problems, a thing
that makes sense is just to check that the limit is < 200k. That's more
than enough actually.
",['tests/unit/obuf-limits.tcl'],"Threaded I/O termination handling in beforeSleep() instead of serverCron() results in tests not capturing exact client termination moment, and 'omem' figure changes in big steps, affecting the test precision."
ffb691f6f1b0f176b1f03c474f1dfd854ee6bf78,1675255696,"Fix handshake timeout replication test race (#11773)

Test on x86 + TLS fail with this error:
```
*** [err]: Slave is able to detect timeout during handshake in tests/integration/replication.tcl
Replica is not able to detect timeout
```

The replica logs is:
```
 ### Starting test Slave is able to detect timeout during handshake in tests/integration/replication.tcl
7681:S 05 Jan 2023 00:21:56.635 * Non blocking connect for SYNC fired the event.
7681:S 05 Jan 2023 00:21:56.638 * Master replied to PING, replication can continue...
7681:S 05 Jan 2023 00:21:56.638 * Trying a partial resynchronization (request ef70638885500aad12dd673c68ca1541116a59fe:1).
7681:S 05 Jan 2023 00:22:56.894 # Failed to read response from the server: error:0A000126:SSL routines::unexpected eof while reading
7681:S 05 Jan 2023 00:22:56.894 # Master did not reply to PSYNC, will try later
```

This is another issue that appeared after #11640 was merged. This PR try to fix it.
The idea is to make it stable in `wait_bgsave`, for example, it may wait until the
next psync retry in the following situation: `Master did not reply to PSYNC, will try later`

Other than that, the change will make the test more consistent / predictable since
it'll mean the master is always frozen in the desired state (waiting for repl-diskless-sync-delay
to happen, rather than earlier stages of the handshake).",['tests/integration/replication.tcl'],"The replication test 'Slave is able to detect timeout during handshake' occasionally fails, showing an error 'Failed to read response from the server' and a log message 'Master did not reply to PSYNC, will try later'."
8759c1e14bafd026ddc8a097b3fbe2aa914b7578,1638106655,"Improve stability in some blocking command tests (#9856)

In order to test the situation where multiple clients are
blocked, we set up multiple clients to execute some blocking
commands. These tests depend on the order of command processing.

Those tests are based on the wrong assumption that the command
send first will be executed by the server first, which is obviously
wrong in some network delyas.

This commit ensures orderly execution of commands by waiting
and judging the number of blocked clients each time.

Fix #9850","['tests/unit/type/list.tcl', 'tests/unit/type/zset.tcl']","Blocking command tests assuming first-sent command is first-executed by the server, resulting in instability with network delays."
ae771ea77b1eb7b55444fd6189d9db77cb724990,1648457674,"fix crash in debug protocol push (#10483)

a missing of resp3 judgement which may lead to the crash using `debug protocol push`
introduced in #9235
Similar improvement in RM_ReplySetAttributeLength in case the module ignored the
error that returned from RM_ReplyWithAttribute.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/debug.c', 'src/module.c']",The absence of resp3 judgement in `debug protocol push` can potentially lead to system crashes.
8a59c193105b6ad4cb2b9cc910f0b1ab34d0a3d8,1661152602,"Introduce redis module ctx flag 'server startup' & 'sentinel'

A module may be loaded only during initial stage, a typical case is
connection type shared library.

Introduce REDISMODULE_CTX_FLAGS_SERVER_STARTUP context flag
to tell the module the stage of Redis. Then the module gets the flag
by RedisModule_GetContextFlags(ctx), tests flags and returns error in
onload handler.

Also introduce 'REDISMODULE_CTX_FLAGS_SENTINEL' context flag to tell
the module the sentinel mode or not.

Suggested-by: Oran Agra <oran@redislabs.com>
Signed-off-by: zhenwei pi <pizhenwei@bytedance.com>
","['src/module.c', 'src/redismodule.h']","Modules may be loaded only during certain stages, like server startup or during sentinel mode, but there's no current way to inform the modules of these stages causing potential errors or inconsistencies."
d5a3b3f5ec879ac596d70036551a3e716b52656d,1641305968,"Setup dependabot for github-actions and codespell (#9857)

This sets up  dependabot to check weekly updates for pip and github-actions dependencies.
If it finds an update it will create a PR to update the dependency. More information can be found here

It includes the update of:

* vmactions/freebsd-vm from 0.1.4 to 0.1.5
* codespell from 2.0.0 to 2.1.0

Also includes spelling fixes found by the latest version of codespell.
Includes a dedicated .codespell folder so dependabot can read a requirements.txt file and every files dedicated to codespell can be grouped in the same place

Co-Authored-By: Matthieu MOREL <mmorel-35@users.noreply.github.com>
Co-Authored-By: MOREL Matthieu <matthieu.morel@cnp.fr>","['src/db.c', 'src/module.c', 'src/replication.c', 'src/server.h']","Dependabot is not set up for checking updates for pip and github-actions dependencies, causing out-of-date dependency issues. There's difficulty in grouping and reading files dedicated to codespell."
b55a827ea2e19bd6cd48f216e8e6caa34299f9b9,1606910171,"Backup keys to slots map and restore when fail to sync if diskless-load type is swapdb in cluster mode (#8108)

When replica diskless-load type is swapdb in cluster mode, we didn't backup
keys to slots map, so we will lose keys to slots map if fail to sync.
Now we backup keys to slots map at first, and restore it properly when fail.

This commit includes a refactory/cleanup of the backups mechanism (moving it to db.c and re-structuring it a bit).

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/bio.c', 'src/db.c', 'src/lazyfree.c', 'src/replication.c', 'src/server.h', 'tests/cluster/tests/17-diskless-load-swapdb.tcl']","In cluster mode, when the diskless-load type for replicas is 'swapdb', failure to sync leads to a loss of the keys to slots map due to a missing backup."
7329cc39818a05c168e7d1e791afb03c089f1933,1487516828,"ARM: Avoid fast path for BITOP.

GCC will produce certain unaligned multi load-store instructions
that will be trapped by the Linux kernel since ARM v6 cannot
handle them with unaligned addresses. Better to use the slower
but safer implementation instead of generating the exception which
should be anyway very slow.
",['src/bitops.c'],"GCC produces unaligned multi-load/store instructions for BITOP on ARMv6, causing exceptions due to ARM's inability to handle unaligned addresses."
8dd16caec833aebde3a12c7d7f9b6456554c8616,1610141730,"Fix last COW INFO report, Skip test on non-linux platforms (#8301)

- the last COW report wasn't always read from the pipe
  (receiveLastChildInfo wasn't used)
- but in fact, there's no reason we won't always try to drain that pipe
  so i'm unifying receiveLastChildInfo with receiveChildInfo
- adjust threshold of the COW test when run in accurate mode
- add some prints in case this test fails again
- fix indentation, page size, and PID! in MacOS proc info

p.s. it seems that pri_pages_dirtied is always 0","['src/childinfo.c', 'src/server.h', 'src/zmalloc.c', 'tests/integration/rdb.tcl']","The last COW report info is not always read correctly, especially on non-linux platforms, and inconsistent in test modes, leading to potential inaccuracies and failure in the test."
f40ca9cb58049683b563245006892001a5ebfacd,1618841762,"Modules: Replicate lazy-expire even if replication is not allowed (#8816)

Before this commit using RM_Call without ""!"" could cause the master
to lazy-expire a key (delete it) but without replicating to replicas.
This could cause the replica's memory usage to gradually grow and
could also cause consistency issues if the master and replica have
a clock diff.
This bug was introduced in #8617

Added a test which demonstrates that scenario.","['src/db.c', 'tests/modules/propagate.c', 'tests/unit/moduleapi/propagate.tcl']","Using RM_Call without ""!"" leads to inconsistent lazy-expiration across master and replicas, causing potential memory usage growth in replicas and data inconsistency due to clock differences."
a64b29485d4f2359b9d698c0e21e890a212ad1bb,1656172940,"changing min,max in ZRANGE -> start stop (#10097)

In 6.2.0 with the introduction of the REV subcommand in ZRANGE, there was a semantic shift in the arguments of ZRANGE when the REV sub-command is executed. Without the sub-command `min` and `max` (the old names of the arguments) are appropriate because if you put the min value and the max value in everything works fine.

```bash
127.0.0.1:6379> ZADD myset 0 foo
(integer) 1
127.0.0.1:6379> ZADD myset 1 bar
(integer) 1
127.0.0.1:6379> ZRANGE myset 0 inf BYSCORE
1) ""foo""
2) ""bar""
``` 

However - if you add the `REV` subcommand, ordering the arguments `min` `max` breaks the command:

```bash
127.0.0.1:6379> ZRANGE myset 0 inf BYSCORE REV
(empty array)
```

why? because `ZRANGE` with the `REV` sub-command is expecting the `max` first and the `min` second (because it's a reverse range like `ZREVRANGEBYSCORE`):

```bash
127.0.0.1:6379> ZRANGE myset 0 inf BYSCORE REV
(empty array)
```",['src/commands.c'],"There is an inconsistency with the ZRANGE command in Redis 6.2.0. Using REV subcommand results in an argument order shift, making the current min and max argument names misleading and causing command failure.
"
92a483bca2df734aff5caada6c23409ed6256773,1608643059,"Fix issue where fork process deletes the parent pidfile (#8231)

Turns out that when the fork child crashes, the crash log was deleting
the pidfile from the disk (although the parent is still running.

Now we set the pidfile of the fork process to NULL so the fork process
will never deletes it.","['src/debug.c', 'src/server.c']","Fork child crash leads to unwarranted deletion of the parent's pidfile from disk, even though parent process is still running."
28beb05aa322e4e72ac6b7b477f38f8c5eab0d57,1574932267,"More improvements and fixes to generic config infra

- Adding is_valid_fn and update_fn, both return 1 for success and 0 for failure with an optional error message.
- Bugfix in handling boundary check of unsigned numeric types (was boundaries as signed)
- Adding more numeric types to generic mechanism: uint, ulonglong, long, time_t, off_t
- More verbose error replies (""argument must be between"" in out of range CONFIG SET (like config file parsing)
",['src/config.c'],"Handling boundary checks for unsigned numeric types is flawed. ""CONFIG SET"" with out of range argument doesn't provide verbose error messages. Missing numeric types in generic mechanism: uint, ulonglong, long, time_t, off_t."
bedecec786767b84215f4002a02d18110585915a,1682316100,"iterate clients fairly in clientsCron() (#12025)

Every time when accept a connection, we add the client to `server.clients` list's tail, but in `clientsCron` we rotate the tail to head at first, and then process the head. It means that the ""new"" client would be processed before ""old"" client, moreover if connections established and then freed frequently, the ""old"" client may have no chance to be processed.

To fix it, we need take a fair way to iterate the list, that is take the current head and process, and then rotate the head to tail, thus we can make sure all clients could be processed step by step.

p.s. client has `client_list_node` pointer, we don't need put the current client to head to avoid O(N) when remove it.",['src/server.c'],"The current implementation of `clientsCron` is processing newer connections before older ones. This could potentially prevent older clients from being processed, especially under frequent connection changes."
4ef4c4a686b6edaf825635d942b698349f67bcc6,1672823602,"Make redis-cli support PSYNC command (#11647)

The current redis-cli does not support the real PSYNC command, the older
version of redis-cli can support PSYNC is because that we actually issue
the SYNC command instead of PSYNC, so it act like SYNC (always full-sync).
Noted that in this case we will send the SYNC first (triggered by sendSync),
then send the PSYNC (the one in redis-cli input).

Didn't bother to find which version that the order changed, we send PSYNC
first (the one in redis-cli input), and then send the SYNC (the one triggered
by sendSync). So even full-sync is not working anymore, and it will result
this output (mentioned in issue #11246):
```
psync dummy 0
Entering replica output mode...  (press Ctrl-C to quit)
SYNC with master, discarding bytes of bulk transfer until EOF marker...
Error reading RDB payload while SYNCing
```

This PR adds PSYNC support to redis-cli, which can handle +FULLRESYNC and
+CONTINUE responses, and some examples will follow.


Co-authored-by: Oran Agra <oran@redislabs.com>",['src/redis-cli.c'],"The redis-cli does not properly support the PSYNC command, leading to output errors during sync. Even in full-sync mode, the issue persists."
eaafea482870c839adc60d8453cbc0c1efb062c5,1523269039,"Modules API: experimental APIs version.

This way it is possible to use conditional compilation to be compatible
with a larger amount of Redis versions, however note that this breaks
binary compatibiltiy, so the module must be compiled with the
corresponding redismodule.h file depending on the version of Redis
targeted.
",['src/redismodule.h'],"Conditional compilation for Redis modules breaks binary compatibility due to version mismatches with redismodule.h, affecting compatibility with multiple Redis versions."
129d14e1431e913426485526663e1a9aac67838c,1574182544,"Auto-detect and link libsystemd at compile-time

This adds Makefile/build-system support for USE_SYSTEMD=(yes|no|*). This
variable's value determines whether or not libsystemd will be linked at
build-time.

If USE_SYSTEMD is set to ""yes"", make will use PKG_CONFIG to check for
libsystemd's presence, and fail the build early if it isn't
installed/detected properly.

If USE_SYSTEM is set to ""no"", libsystemd will *not* be linked, even if
support for it is available on the system redis is being built on.

For any other value that USE_SYSTEM might assume (e.g. ""auto""),
PKG_CONFIG will try to determine libsystemd's presence, and set up the
build process to link against it, if it was indicated as being
installed/available.

This approach has a number of repercussions of its own, most importantly
the following: If you build redis on a system that actually has systemd
support, but no libsystemd-dev package(s) installed, you'll end up
*without* support for systemd notification/status reporting support in
redis-server. This changes established runtime behaviour.

I'm not sure if the build system and/or the server binary should
indicate this. I'm also wondering if not actually having
systemd-notify-support, but requesting it via the server's config,
should result in a fatal error now.
","['src/server.c', 'src/server.h']","The build system fails to auto-detect libsystemd at compile-time causing it to not link, potentially removing support for systemd notification/status reporting support in redis-server, changing established runtime behaviour."
7d1ad6ca9663e3f771d1f27eec3ab67d0a1b6faf,1650282960,"Fix RM_Yield bug processing future commands of the current client. (#10573)

RM_Yield was missing a call to protectClient to prevent redis from
processing future commands of the yielding client.

Adding tests that fail without this fix.

This would be complicated to solve since nested calls to RM_Call used to
replace the current_client variable with the module temp client.

It looks like it's no longer necessary to do that, since it was added
back in #9890 to solve two issues, both already gone:
1. call to CONFIG SET maxmemory could trigger a module hook calling
   RM_Call. although this specific issue is gone, arguably other hooks
   like keyspace notification, can do the same.
2. an assertion in lookupKey that checks the current command of the
   current client, introduced in #9572 and removed in #10248","['src/module.c', 'tests/unit/moduleapi/blockedclient.tcl']","RM_Yield does not call protectClient leading to Redis processing future commands of the yielding client, causing issues especially with nested calls to RM_Call replacing current_client variable with module temp client."
f3bf8485d8c3f52cf05edc043f47c59e5c732305,1697713112,"Fix resize hash table dictionary iterator (#12660)

Dictionary iterator logic in the `tryResizeHashTables` method is picking the next
(incorrect) dictionary while the cursor is at a given slot. This could lead to some
dictionary/slot getting skipped from resizing.

Also stabilize the test.

problem introduced recently in #11695","['src/server.c', 'src/server.h', 'tests/unit/expire.tcl']","The dictionary iterator in the `tryResizeHashTables` method is incorrectly picking the subsequent dictionary at a certain slot, causing some dictionaries or slots to be skipped during the resizing process."
cbaf3c5bbafd43e009a2d6b38dd0e9fc450a3e12,1597903142,"Fix flock cluster config may cause failure to restart after kill -9 (#7674)

After fork, the child process(redis-aof-rewrite) will get the fd opened
by the parent process(redis), when redis killed by kill -9, it will not
graceful exit(call prepareForShutdown()), so redis-aof-rewrite thread may still
alive, the fd(lock) will still be held by redis-aof-rewrite thread, and
redis restart will fail to get lock, means fail to start.

This issue was causing failures in the cluster tests in github actions.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/cluster.c', 'src/server.c', 'src/server.h', 'tests/instances.tcl']","Redis fails to restart after being killed (via kill -9) due to concurrent redis-aof-rewrite thread not releasing the parental file descriptor(lock), impacting the cluster tests in Github actions."
d0244bfc3df101bca8b2d94eec00feceaaff2ee0,1628151969,"Make sure execute SLAVEOF command in the right order in psync2 test. (#9316)

The psync2 test has failed several times recently.
In #9159 we only solved half of the problem.
i.e. reordering of the replica that's already connected to
the newly promoted master.

Consider this scenario:
0 slaveof 2
1 slaveof 2
3 slaveof 2
4 slaveof 1
0 slaveof no one, became a new master got a new replid
2 slaveof 0, partial resync and got the new replid
3 reconnect 2, inherit the new replid
3 slaveof 4, use the new replid and got a full resync

And another scenario:
1 slaveof 3
2 slaveof 4
3 slaveof 0
4 slaveof 0
4 slaveof no one, became a new master got a new replid
2 reconnect 4, inherit the new replid
2 slaveof 1, use the new replid and got a full resync

So maybe we should reattach replicas in the right order.
i.e. In the above example, if it would have reattached 1, 3 and 0 to
the new chain formed by 4 before trying to attach 2 to 1, it would succeed.

This commit break the SLAVEOF loop into two loops. (ideas from oran)

First loop that uses random to decide who replicates from who.
Second loop that does the actual SLAVEOF command.
In the second loop, we make sure to execute it in the right order,
and after each SLAVEOF, wait for it to be connected before we proceed.

Co-authored-by: Oran Agra <oran@redislabs.com>",['tests/integration/psync2.tcl'],"The psync2 test is failing due to the incorrect order of execution of the SLAVEOF command, causing replica reconnection issues leading to full resyncs."
ad7d4c6b709ce94ed9a2fa3a1fcf9b8cf7843eff,1611760390,"Implement rdb-only replication (#8303)

In some scenarios, such as remote backup, we only want to get remote
redis server db snapshot. Currently, redis-cli acts as a replica and
sends SYNC to redis, but redis still accumulates replication buffer
in the replica client output buffer, that may result in using vast
memory, or failing to transfer RDB because of client-output-buffer-limit.
In this commit, we add 'replconf rdb-only 0|1', redis doesn't send
incremental replication buffer to them if they send 'replconf rdb-only 1',
so we can reduce used memory and improve success of getting RDB.","['src/redis-cli.c', 'src/replication.c', 'src/server.h']","Redis server accumulates excessive replication buffer while performing remote backup, resulting in significant memory usage and potential failure in RDB transfer due to client-output-buffer-limit."
f06d782f5abcb30efb0117841232828ed3e129bf,1625548877,"Avoid exiting to allow diskless loading to recover from RDB short read on module AUX data (#9199)

Currently a replica is able to recover from a short read (when diskless loading
is enabled) and avoid crashing/exiting, replying to the master and then the rdb
could be sent again by the master for another load attempt by the replica.
There were a few scenarios that were not behaving similarly, such as when
there is no end-of-file marker, or when module aux data failed to load, which
should be allowed to occur due to a short read.",['src/rdb.c'],"Replica crashes when attempting to recover from a short read during diskless loading, specifically when there's no end-of-file marker or when module aux data fails to load."
07ea220419103ac16c06a86d4e0790d5f14790a0,1684855844,"add a new loglevel 'nothing' to disable logging (#12133)

Users can record logs of different levels by setting the `loglevel`.
However, sometimes there are many logs even at the warning level,
which can affect the performance of Redis.

For example, when a user accesses the tls-port using a non-encrypted link,
Redis will log lots of ""# Error accepting a client connection: ..."".

We can provide the ability to disable logging so that users can temporarily turn
off logging and turn it back on after the problem is resolved.","['src/config.c', 'src/server.h']","Excessive logs, even at the warning level, affecting Redis performance. Notably, instances where non-encrypted links access the tls-port result in numerous error logs."
06bfeb482d5652e55bdd9d93a9909f26d6c3999a,1482162107,"Only show Redis logo if logging to stdout / TTY.

You can still force the logo in the normal logs.
For motivations, check issue #3112. For me the reason is that actually
the logo is nice to have in interactive sessions, but inside the logs
kinda loses its usefulness, but for the ability of users to recognize
restarts easily: for this reason the new startup sequence shows a one
liner ASCII ""wave"" so that there is still a bit of visual clue.

Startup logging was modified in order to log events in more obvious
ways, and to log more events. Also certain important informations are
now more easy to parse/grep since they are printed in field=value style.

The option --always-show-logo in redis.conf was added, defaulting to no.
","['src/config.c', 'src/server.c', 'src/server.h']","Redis logo display in non-interactive sessions disrupting log readability, impacting user's ability to easily recognize service restarts."
e819c2ef5b6f6b7c196cc7c00c543c4af8c703a7,1550071844,"ACL: tag LASTSAVE as dangerous.

That's not REALLY needed, but... right now with LASTSAVE being the only
command tagged as ""admin"" but not ""dangerous"" what happens is that after
rewrites the rewrite engine will produce from the rules:

    user default on +@all ~* -@dangerous nopass

The rewrite:

    user default on nopass ~* +@all -@admin -@dangerous +lastsave

Which is correct but will have users wondering about why LASTSAVE has
something special.

Since LASTSAVE after all also leaks information about the underlying
server configuration, that may not be great for SAAS vendors, let's tag
it as dangerous as well and forget about this issue :-)
",['src/server.c'],"The command LASTSAVE is currently tagged as ""admin"" but not ""dangerous"", causing confusion during rewrites and potentially leaking server configuration information which isn't ideal for SAAS vendors."
1eb85249e7e9dafe4fbd023771e53c9a804c0a2f,1617270315,"Handle remaining fsync errors (#8419)

In `aof.c`, we call fsync when stop aof, and now print a log to let user know that if fail.
In `cluster.c`, we now return error, the calling function already handles these write errors.
In `redis-cli.c`, users hope to save rdb, we now print a message if fsync failed.
In `rio.c`, we now treat fsync errors like we do for write errors. 
In `server.c`, we try to fsync aof file when shutdown redis, we only can print one log if fail.
In `bio.c`, if failing to fsync aof file, we will set `aof_bio_fsync_status` to error , and reject writing just like last writing aof error,  moreover also set INFO command field `aof_last_write_status` to error.","['src/aof.c', 'src/bio.c', 'src/cluster.c', 'src/redis-cli.c', 'src/rio.c', 'src/server.c', 'src/server.h']",Various components in the code are not effectively handling `fsync` errors which can lead to data loss or corruption without proper logging or error handling mechanisms.
0b643e930d978862b3284ba742fba1a659832b8c,1628496239," Cleanup: createAOFClient uses createClient to avoid overlooked mismatches (#9338)

AOF fake client creation (createAOFClient) was doing similar work as createClient,
with some minor differences, most of which unintended, this was dangerous and
meant that many changes to createClient should have always been reflected to aof.c

This cleanup changes createAOFClient to call createClient with NULL, like we
do in module.c and elsewhere.","['src/aof.c', 'src/networking.c']","Duplicate code in createAOFClient and createClient can lead to mismatch issues if changes are not reflected in both, risking overlooked discrepancies."
d08f0552ee13fc9feaa7afbb2c493d1bf92ee6ac,1635843232,"rebuild replication backlog index when master restart (#9720)

After PR #9166 , replication backlog is not a real block of memory, just contains a
reference points to replication buffer's block and the blocks index (to accelerate
search offset when partial sync), so we need update both replication buffer's block's
offset and replication backlog blocks index's offset when master restart from RDB,
since the `server.master_repl_offset` is changed.
The implications of this bug was just a slow search, but not a replication failure.","['src/replication.c', 'src/server.c', 'src/server.h']","Restarting the master from RDB changes the 'server.master_repl_offset', causing the replication backlog blocks index's offset and replication buffer's block's offset to become outdated, resulting in slower searches."
94fded4f4f9c3d52d69969c4a2a9d82786a3ac16,1634485695,"Code cleanup, resolve an issue identified by cppcheck (#4373)

[src/bitops.c:512] -> [src/bitops.c:507]: (warning) Either the condition 'if(o&&o->encoding==1)' is redundant or there is possible null pointer dereference: o.

This function has checks for `o` to be null or non-null, so it is odd that it accesses it first..",['src/bitops.c'],Accessing the object 'o' before null-check may lead to possible null pointer dereference in bitops.c.
945a83d40661981f5098de0d89d8cbcd88883f15,1629834861,"Fix boundary problem of adjusting open files limit. (#5722)

When `decr_step` is greater than `oldlimit`, the final `bestlimit` may be invalid.

    For example, oldlimit = 10, decr_step = 16.
    Current bestlimit = 15 and setrlimit() failed. Since bestlimit  is less than decr_step , then exit the loop.
    The final bestlimit is larger than oldlimit but is invalid.

Note that this only matters if the system fd limit is below 16, so unlikely to have any actual effect.",['src/server.c'],"When `decr_step` exceeds `oldlimit`, `bestlimit` might become invalid, especially if the system file descriptor limit is below 16."
8827aae83bf58b91f155005102675ff57061d9eb,1621259560,"redis-cli: fix bugs in hints of commands with subcommands. (#8914)

There are two bugs in redis-cli hints:
* The hints of commands with subcommands lack first params.
* When search matching command of currently input, we should find the
command with longest matching prefix. If not COMMAND INFO will always
match COMMAND and display no hints.",['src/redis-cli.c'],"Redis-cli hints for commands with subcommands are missing first parameters, and input matching command only matches the command with the longest matching prefix, causing no hints to be displayed for COMMAND INFO."
4c52aa9faa4d6e3ba2243a4f45563cb4a1439bb8,1624178044,"chdir to dir before fopen logfile in loadServerConfigFromString() (#6741)

Open the log file only after parsing the entire config file, so that it's
location isn't dependent on the order of configs (`dir` and `logfile`).
Also solves the problem of creating multiple log files if the `logfile`
directive appears many times in the config file.",['src/config.c'],Order of `dir` and `logfile` in config file affects logfile location and causes creation of multiple logfiles when the directive appears more than once.
df346bca3938de04dbf9794edc2dbaee64d5fc68,1549898911,"ACL: ACLFreeUserAndKillClients(): free user later.

Soon or later we may have code in freeClient() that may have to deal
with ACLs. Imagine for instance the command proposed multiple times (not
sure if this will ever be accepted but still...):

    ONCLOSE DEL mykey

Accumulating commands to run when a client is disconnected. Now the
function is compatible with such use cases.

Related to #5829.
",['src/acl.c'],"Potential commands to run when a client is disconnected could interact with ACLs, leading to issues when the client is freed."
00613bed06af890b9f09befc71de3f2aedabbacb,1528715406,"Improved regression test for #4906.

Removing the fix about 50% of the times the test will not be able to
pass cleanly. It's very hard to write a test that will always fail, or
actually, it is possible but then it's likely that it will consistently
pass if we change some random bit, so better to use randomization here.
",['tests/unit/scan.tcl'],Regression test for issue #4906 sporadically doesn't pass even when the fix is removed due to issues with randomization.
4ce3fd51b9fc6f627f4e81f1a98c8e350cc2e660,1659358360,"Fix wrong commands json docs for CLIENT KILL (#10970)

The docs state that there is a new and an old argument format.
The current state of the arguments allows mixing the old and new format,
thus the need for two additional oneof blocks.
One for differentiating the new from the old format and then one to
allow setting multiple filters using the new format.",['src/commands.c'],"The documentation for CLIENT KILL incorrectly states the argument format, allowing for a mix of old and new formats which causes confusion and inconsistency."
78dc29217846d7706397c5518de95e0f49795b43,1668337942,"Add test to cover NAN reply using a module (#11482)

Adding a test to cover the already existing behavior of NAN replies,
to accompany the PR that adds them to the RESP3 spec:
https://github.com/redis/redis-specifications/pull/10

This PR also covers Inf replies that are already in the spec, as well as RESP2 coverage.","['tests/modules/reply.c', 'tests/unit/moduleapi/reply.tcl']","The current Redis specifications lack tests for verifying the behaviour of NAN and Inf replies, providing an incomplete test coverage for RESP2 and RESP3."
fa9aa90813ae67509208022f1315cbe82869c2cc,1584880923,"Conns: Fix connClose() / connAccept() behavior.

We assume accept handlers may choose to reject a connection and close
it, but connAccept() callers can't distinguish between this state and
other error states requiring connClose().

This makes it safe (and mandatory!) to always call connClose() if
connAccept() fails, and safe for accept handlers to close connections
(which will defer).
","['src/connection.c', 'src/connection.h', 'src/connhelpers.h']","Failure in connAccept() function results in ambiguous states, making it indistinguishable between a rejected connection and other error states that require calling connClose()."
a7936ef96d521ef7e28a255e5105934f820e774a,1591783660,"LRANK: Add command (the command will be renamed LPOS).

The `LRANK` command returns the index (position) of a given element
within a list. Using the `direction` argument it is possible to specify
going from head to tail (acending, 1) or from tail to head (decending,
-1). Only the first found index is returend. The complexity is O(N).

When using lists as a queue it can be of interest at what position a
given element is, for instance to monitor a job processing through a
work queue. This came up within the Python `rq` project which is based
on Redis[0].

[0]: https://github.com/rq/rq/issues/1197

Signed-off-by: Paul Spooren <mail@aparcar.org>
","['src/help.h', 'src/server.c', 'src/server.h', 'src/t_list.c']","When using lists as queues, there's no native capability in Redis to find the position of a specific element, causing additional complexity for job monitoring in work queues."
b0e18f804d016dd6664c71920d6c290c0af08909,1653224579,"Scripts that declare the `no-writes` flag are implicitly `allow-oom` too. (#10699)

Scripts that have the `no-writes` flag, cannot execute write commands,
and since all `deny-oom` commands are write commands, we now act
as if the `allow-oom` flag is implicitly set for scripts that set the `no-writes` flag.
this also implicitly means that the EVAL*_RO and FCALL_RO commands can
never fails with OOM error.

Note about a bug that's no longer relevant:
There was an issue with EVAL*_RO using shebang not being blocked correctly
in OOM state:
When an EVAL script declares a shebang, it was by default not allowed to run in
OOM state.
but this depends on a flag that is updated before the command is executed, which
was not updated in case of the `_RO` variants.
the result is that if the previous cached state was outdated (either true or false),
the script will either unjustly fail with OOM, or unjustly allowed to run despite
the OOM state.
It doesn't affect scripts without a shebang since these depend on the actual
commands they run, and since these are only read commands, they don't care
for that cached oom state flag.
it did affect scripts with shebang and no allow-oom flag, bug after the change in
this PR, scripts that are run with eval_ro would implicitly have that flag so again
the cached state doesn't matter.

p.s. this isn't a breaking change since all it does is allow scripts to run when they
should / could rather than blocking them.","['src/script.c', 'src/server.c', 'tests/unit/functions.tcl', 'tests/unit/scripting.tcl']","Scripts with `no-writes` flag are incorrectly running under OOM state due to outdated cached state flag, particularly affecting `EVAL*_RO` scripts with a shebang and no `allow-oom` flag."
b9fa42a197702c0da0baa2edb184d984dffa2933,1587049762,"testsuite run the defrag latency test solo

this test is time sensitive and it sometimes fail to pass below the
latency threshold, even on strong machines.

this test was the reson we're running just 2 parallel tests in the
github actions CI, revering this.
","['tests/test_helper.tcl', 'tests/unit/memefficiency.tcl']","'Time-sensitive defrag latency test is inconsistently failing to meet latency threshold even on high-performance machines, impacting parallel testing capability in the CI environment.'"
3ea4c43add4f6038830ae0a6e42821481f7ce9a0,1614237881,"Cleanup usage of malloc_usable_size. (#8554)

* Add better control of malloc_usable_size() usage.
* Use malloc_usable_size on alpine libc daily job.
* Add no-malloc-usable-size daily jobs.
* Fix zmalloc(0) when HAVE_MALLOC_SIZE is undefined.

In order to align with the jemalloc behavior, this should never return
NULL or OOM panic.","['src/zmalloc.c', 'src/zmalloc.h']",Current usage of malloc_usable_size() is not well-controlled and there can be issues of NULL or OOM panic with zmalloc(0) operation when HAVE_MALLOC_SIZE is undefined.
2f282aee0b8e500697b627254b170f21d93dd4a8,1533143093,"Fix zslUpdateScore() edge case.

When the element new score is the same of prev/next node, the
lexicographical order kicks in, so we can safely update the node in
place only when the new score is strictly between the adjacent nodes
but never equal to one of them.

Technically speaking we could do extra checks to make sure that even if the
score is the same as one of the adjacent nodes, we can still update on
place, but this rarely happens, so probably not a good deal to make it
more complex.

Related to #5179.
",['src/t_zset.c'],"When updating the score of an element in zslUpdateScore(), if the new score is the same as one of the adjacent nodes, incorrect updates occur, disrupting the lexicographical order."
0bfb6d55826ce128b8752430b4499146450deef4,1686514325,"Reset command duration for rejected command. (#12247)

In 7.2, After 971b177fa we make sure (assert) that
the duration has been recorded when resetting the client.

This is not true for rejected commands.
The use case I found is a blocking command that an ACL rule changed before
it was unblocked, and while reprocessing it, the command rejected and triggered the assert.

The PR reset the command duration inside rejectCommand / rejectCommandSds.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/server.c', 'tests/unit/acl.tcl']","A rejected command, particularly following an ACL rule change, causes an assertion error due to the command duration not being correctly reset."
448d696549dc156fc26f93208d4e2a52d655dcea,1537871506,"Modules: dictionary API work in progress #5: rename API for consistency.

By using the ""C"" suffix for functions getting pointer/len, we can do the
same in the future for other modules APIs that need a variant with
pointer/len and that are now accepting a RedisModuleString.
",['src/module.c'],"The naming convention for functions in the dictionary API that accept pointer/length parameters is inconsistent with other module APIs, leading to confusions."
49147f36e942b3367a3d1c3f7f83582fcbc06006,1527590262,"Don't expire keys while loading RDB from AOF preamble.

The AOF tail of a combined RDB+AOF is based on the premise of applying
the AOF commands to the exact state that there was in the server while
the RDB was persisted. By expiring keys while loading the RDB file, we
change the state, so applying the AOF tail later may change the state.

Test case:

* Time1: SET a 10
* Time2: EXPIREAT a $time5
* Time3: INCR a
* Time4: PERSIT A. Start bgrewiteaof with RDB preamble. The value of a is 11 without expire time.
* Time5: Restart redis from the RDB+AOF: consistency violation.

Thanks to @soloestoy for providing the patch.
Thanks to @trevor211 for the original issue report and the initial fix.

Check issue #4950 for more info.
","['src/aof.c', 'src/rdb.c', 'src/rdb.h']","Expiring keys while loading the RDB file from AOF preamble changes the server state, leading to potential consistency violation when restarting Redis from the combined RDB+AOF."
0c3fe52ef7e3acf135428be81dd8e27b4ef191e5,1577366175,"config.c adjust config limits and mutable

- make lua-replicate-commands mutable (it never was, but i don't see why)
- make tcp-backlog immutable (fix a recent refactory mistake)
- increase the max limit of a few configs to match what they were before
the recent refactory
","['src/config.c', 'tests/unit/introspection.tcl']","Recent refactoring caused tcp-backlog to become mutable and reduced the max limit of some configs, also lua-replicate-commands configuration is not mutable."
8ad0cfa56cdf2accf25f1eb1a66c3e2e5b09a953,1650805920,"isSafeToPerformEvictions: Remove redundant condition (#10610)

If was first added in #9890 to solve the problem of
CONFIG SET maxmemory causing eviction inside MULTI/EXEC,
but that problem is already fixed (CONFIG SET doesn't evict
directly, it just schedules a later eviction)

Keep that condition may hide bugs (i.e. performEvictions
should always expect to have an empty server.also_propagate)",['src/evict.c'],"Redundant condition in `isSafePerformEvictions` potentially hides bugs, as `performEvictions` should always expect to have an 'empty server.also_propagate' even with 'CONFIG SET maxmemory' causing eviction inside 'MULTI/EXEC'."
fe4b4806b3f39256a82020a1cd2e832ebabc2ef9,1650260062,"Fix long long to double implicit conversion warning (#10595)

There is a implicit conversion warning in clang:
```
util.c:574:23: error: implicit conversion from 'long long' to 'double'
changes value from -4611686018427387903 to -4611686018427387904
[-Werror,-Wimplicit-const-int-float-conversion]
    if (d < -LLONG_MAX/2 || d > LLONG_MAX/2)
```

introduced in #10486

Co-authored-by: sundb <sundbcn@gmail.com>",['src/util.c'],"Implicit conversion from 'long long' to 'double' in util.c at line 574 leads to value change, causing a warning in Clang."
56f97bfa5fb082f812c711676d5a77d29af56940,1665898201,"Fix wrong replication on cluster slotmap changes with module KSN propagation (#11377)

As discussed on #11084, `propagatePendingCommands` should happened after the del
notification is fired so that the notification effect and the `del` will be replicated inside MULTI EXEC.

Test was added to verify the fix.","['src/cluster.c', 'tests/modules/keyspace_events.c', 'tests/test_helper.tcl', 'tests/unit/moduleapi/cluster.tcl']","Cluster slotmap changes with module KSN propagation results in incorrect replication, as `propagatePendingCommands` takes place before the del notification is fired, causing inconsistencies inside MULTI EXEC."
1769c222486d1deb33b394899eccab35be975c96,1549017448,"ACL: set modules help clients to the root user.

It does not make much sense to limit what modules can do: the admin
should instead limit what module commnads an user may call. So
RedisModule_Call() and other module operations should be able to execute
everything they want: the limitation should be posed by the API exported
by the module itself.
",['src/module.c'],"Current Access Control Limitation (ACL) set modules restrict operations excessively, creating limitations on what the RedisModule_Call() and other module operations can execute."
26e638a8e917608d8e75b07e5e941c7a24462c4d,1498830118,"redis-cli --latency: ability to run non interactively.

This feature was proposed by @rosmo in PR #2643 and later redesigned
in order to fit better with the other options for non-interactive modes
of redis-cli. The idea is basically to allow to collect latency
information in scripts, cron jobs or whateever, just running for a
limited time and then producing a single output.
",['src/redis-cli.c'],"There's no way for redis-cli's latency mode to run non-interactively, which limits its usability in scripts, cron jobs, or similar setups that run for a limited period and require single output data."
81879bc171ed8726f8c0bba1aa123862ee2a08fd,1596913157,"Create PUSH handlers in redis-cli

Add logic to redis-cli to display RESP3 PUSH messages when we detect
STDOUT is a tty, with an optional command-line argument to override
the default behavior.

The new argument:  --show-pushes <yn>

Examples:

$ redis-cli -3 --show-pushes no
$ echo ""client tracking on\nget k1\nset k1 v1""| redis-cli -3 --show-pushes y
",['src/redis-cli.c'],"redis-cli lacks the functionality to handle and display RESP3 PUSH messages, particularly in cases where STDOUT is detected as a tty. No command-line argument exists to override the default behavior."
5fd9756d2e1e73371c29e981d0c5a7a91976a27c,1686445876,"Fix ACLAppendUserForLoading memory leak when merge error (#12296)

This leak will only happen in loadServerConfigFromString,
that is, when we are loading a redis.conf, and the user is wrong.

Because it happens in loadServerConfigFromString, redis will
exit if there is an error, so this is actually just a cleanup.",['src/acl.c'],"Memory leak detected in the function loadServerConfigFromString, when loading a faulty user from redis.conf."
d71478a889935382f3bf8b9a39ab073eba0a856a,1684910411,"postpone the initialization of oject's lru&lfu until it is added to the db as a value object (#11626)

This pr can get two performance benefits:
1. Stop redundant initialization when most robj objects are created
2. LRU_CLOCK will no longer be called in io threads, so we can avoid the `atomicGet`

Another code optimization:
deleted the redundant judgment in dbSetValue, no matter in LFU or LRU, the lru field inold
robj is always the freshest (it is always updated in lookupkey), so we don't need to judge if in LFU
","['src/db.c', 'src/evict.c', 'src/object.c', 'src/server.c', 'src/server.h', 'tests/unit/maxmemory.tcl']","Redundant initialization occurs when most robj objects are created, leading to potential performance issues. Additionally, LRU_CLOCK is being called in IO threads, utilizing unnecessary 'atomicGet'."
d4cbd8140bb18ccec6af1561f6c2067f8c00a7b0,1650359199,"Fixes around AOF failed rewrite rate limiting (#10582)

Changes:
1. Check the failed rewrite time threshold only when we actually consider triggering a rewrite.
  i.e. this should be the last condition tested, since the test has side effects (increasing time threshold)
  Could have happened in some rare scenarios 
2. no limit in startup state (e.g. after restarting redis that previously failed and had many incr files)
3. the “triggered the limit” log would be recorded only when the limit status is returned
4. remove failure count in log (could be misleading in some cases)

Co-authored-by: chenyang8094 <chenyang8094@users.noreply.github.com>
Co-authored-by: Oran Agra <oran@redislabs.com>","['src/aof.c', 'src/server.c']","AOF rewrite failure rate limiting is not functioning properly, potentially due to issues with time threshold checks and inappropriate log recording. The rate limiting is also erroneously applied during startup state."
a71e8148533449097b12233fbebafef135fd4a80,1531214346,"Streams: send an error to consumers blocked on non-existing group.

To detect when the group (or the whole key) is destroyed to send an
error to the consumers blocked in such group is a problem, so we leave
the consumers listening, the sysadmin is free to create or destroy
groups assuming she/he knows what to do. However a client may be blocked
in a given consumer group, that is later destroyed. Then the stream
receives new elements. In that case there is no sane behavior to serve
the consumer... but to report an error about the group no longer
existing.

More about detecting this synchronously and why it is not done:

1. Normally we don't do that, we leave clients blocked for other data
types such as lists.

2. When we free a stream object there is no longer information about
what was the key it was associated with, so while destroying the
consumer groups we miss the info to unblock the clients in that moment.

3. Objects can be reclaimed in other threads where it is no longer safe
to do client operations.
",['src/blocked.c'],"Consumers blocked on a non-existing group are not notified with an error, resulting in an undefined behavior when the stream receives new elements."
c3fb48da8b76ee143d1c380ccf9899752607c5d5,1670580385," Reduce rewriteClientCommandVector usage on EXPIRE command  (#11602)

There is overhead on Redis 7.0 EXPIRE command that is not present on 6.2.7. 

We could see that on the unstable profile there are around 7% of CPU cycles
spent on rewriteClientCommandVector that are not present on 6.2.7.
This was introduced in #8474.
This PR reduces the overhead by using 2X rewriteClientCommandArgument instead of
rewriteClientCommandVector. In this scenario rewriteClientCommandVector creates 4 arguments.
the above usage of rewriteClientCommandArgument reduces the overhead in half.

This PR should also improve PEXPIREAT performance by avoiding at all
rewriteClientCommandArgument usage. 

Co-authored-by: Oran Agra <oran@redislabs.com>",['src/expire.c'],"Redis 7.0 EXPIRE command has surplus CPU cycle usage due to rewriteClientCommandVector overhead, not present in version 6.2.7, potentially impacting PEXPIREAT performance."
b1fc06f7eb90b2b8a3c68085ac0970e3d3525b52,1480943806,"Geo: improve fuzz test.

The test now uses more diverse radius sizes, especially sizes near or
greater the whole earth surface are used, that are known to trigger edge
cases. Moreover the PRNG seeding was probably resulting into the same
sequence tested over and over again, now seeding unsing the current unix
time in milliseconds.

Related to #3631.
",['tests/unit/geo.tcl'],"Fuzz test on Geo lacks diversity, especially for radius sizes testing edge cases. Furthermore, the current PRNG seeding might be causing repetitive sequence testing."
2ee1bbb53baea26b442d602b0b3335176170736d,1690965811,"Ensure that the function load timeout is disabled during loading from RDB/AOF and on replicas. (#12451)

When loading a function from either RDB/AOF or a replica, it is essential not to
fail on timeout errors. The loading time may vary due to various factors, such as
hardware specifications or the system's workload during the loading process.
Once a function has been successfully loaded, it should be allowed to load from
persistence or on replicas without encountering a timeout failure.

To maintain a clear separation between the engine and Redis internals, the
implementation refrains from directly checking the state of Redis within the
engine itself. Instead, the engine receives the desired timeout as part of the
library creation and duly respects this timeout value. If Redis wishes to disable
any timeout, it can simply send a value of 0.","['src/function_lua.c', 'src/functions.c', 'src/functions.h', 'src/rdb.c']",Function load from RDB/AOF or a replica fails due to timeout errors caused by varied loading times which depend on factors like hardware specs or the system's current workload.
5def65008ff92519a828e1ba403e9a46836ca802,1519203853,"Fix zrealloc to behave similarly to je_realloc when size is 0

According to C11, the behavior of realloc with size 0 is now deprecated.
it can either behave as free(ptr) and return NULL, or return a valid pointer.
but in zmalloc it can lead to zmalloc_oom_handler and panic.
and that can affect modules that use it.

It looks like both glibc allocator and jemalloc behave like so:
  realloc(malloc(32),0) returns NULL
  realloc(NULL,0) returns a valid pointer

This commit changes zmalloc to behave the same
",['src/zmalloc.c'],"The zmalloc function doesn't handle a realloc with size 0 consistently, leading to a potential zmalloc_oom_handler panic. This inconsistency affects modules relying on zmalloc."
d659c734569be4ed32a270bac2527ccf35418c43,1682577134,"Add missing reply schema and coverage tests (#12079)

The change in #12018 break the CI (fixed by #12083).
There are quite a few sentinel commands that are missing both test coverage and also schema.

PR added reply-schema to the following commands:
- sentinel debug
- sentinel info-cache
- sentinel pendding-scripts
- sentinel reset
- sentinel simulate-failure

Added some very basic tests for other sentinel commands, just so that they have some coverage.
- sentinel help
- sentinel masters
- sentinel myid
- sentinel sentinels
- sentinel slaves

These tests should be improved / replaced in a followup PR.","['src/sentinel.c', 'tests/sentinel/tests/00-base.tcl', 'tests/sentinel/tests/05-manual.tcl', 'utils/req-res-log-validator.py']","Several sentinel commands lack test coverage and have missing reply schema, causing continuous integration (CI) failures."
a8b6596d233556980adc09589c9022e8a7901755,1619441526,"Fail fast when systemic error occurs in poll (#8749)

Most of the ae.c backends didn't explicitly handle errors, and instead
ignored all errors and did an implicit retry.
This is desired for EAGAIN and EINTER, but in case of other systematic
errors, we prefer to fail and log the error we got rather than get into a busy loop.
","['src/ae.c', 'src/ae_epoll.c', 'src/ae_evport.c', 'src/ae_kqueue.c', 'src/ae_select.c', 'src/redis-benchmark.c', 'src/redis-cli.c', 'src/redisassert.c']","Systemic errors in ae.c backends are not adequately handled and instead lead to unnecessary retries, potentially causing a busy loop."
0f370f9b667fab4bede084f1b8b47c93f0d56069,1603385252,"do not add save parameter during config rewrite in sentinel mode (#7945)

Previous code would have added default redis save parameters
to the config file on rewrite, which would have been silently ignored
when the config file is loaded.

The new code avoids adding this, and also actively removes these lines
If added by a previous config rewrite. ",['src/config.c'],"In Sentinel mode, the config rewrite erroneously adds default Redis save parameters, which are subsequently ignored during config file load, leading to unwanted configuration changes."
df327b8bd56023931cd41e233f8703de7bbaa82c,1671092778,"Call postExecutionUnitOperations in active-expire of writable replicas (#11615)

We need to honor the post-execution-unit API and call it after each KSN

Note that this is an edge case that only happens in case volatile keys were
created directly on a writable replica, and that anyway nothing is propagated to sub-replicas

Co-authored-by: Oran Agra <oran@redislabs.com>",['src/expire.c'],"The post-execution-unit API is not honored and is not called as expected after each KSN, in active-expire of writable replicas, specifically notable when dealing with volatile keys."
0f85713174150044e17d2c4dfbcaaf7f7475f333,1669063993,"Fix sentinel update loglevel tls test (#11528)

Apparently we used to set `loglevel debug` for tls in spawn_instance.
I.e. cluster and sentinel tests used to run with debug logging, only when tls mode was enabled.
this was probably a leftover from when creating the tls mode tests.
it cause a new test created for #11214 to fail in tls mode.

At the same time, in order to better distinguish the tests, change the
name of `test-centos7-tls` to `test-centos7-tls-module`, change the name
of `test-centos7-tls-no-tls` to `test-centos7-tls-module-no-tls`.

Note that in `test-centos7-tls-module`, we did not pass `--tls-module`
in sentinel test because it is not supported, see 4faddf1, added in #9320.
So only `test-ubuntu-tls` fails in daily CI.

Co-authored-by: Oran Agra <oran@redislabs.com>",['tests/instances.tcl'],"Setting `loglevel debug` for TLS in spawn_instance causes tests to fail in tls mode while using test-centos7-tls-module and test-ubuntu-tls, as sentinel test does not support `--tls-module`."
311a757cb14c3420c7547377d250ef0a66e781d6,1646836827,"Some adjustments to command hints. (#10375)

* stats and latency commands have non-deterministic output.
* the ones about latency should be sent to ALL_NODES (considering
  reads from replicas)
* the ones about running scripts and memory usage only to masters.
* stats aggregation is SPECIAL (like in INFO)",['src/commands.c'],"Commands about latency and stats produce non-deterministic output, possibly due to misdirection of commands to incorrect nodes and incorrect special treatments for stats aggregation."
61c85a2b2081dffaf45eb8c6b7754b8d9a80c60d,1670141498,"Speedup GEODIST with fixedpoint_d2string as an optimized version of snprintf %.4f (#11552)

GEODIST used snprintf(""%.4f"") for the reply using addReplyDoubleDistance,
which was slow. This PR optimizes it without breaking compatibility by following
the approach of ll2string with some changes to match the use case of distance
and precision. I.e. we multiply it by 10000 format it as an integer, and then add
a decimal point. This can achieve about 35% increase in the achievable ops/sec. 

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/geo.c', 'src/util.c', 'src/util.h', 'tests/unit/geo.tcl']","Current GEODIST implementation with snprintf ""%.4f"" results in slow response while using addReplyDoubleDistance."
0283db5883e8dc08e8d3c7019a213712adb3d420,1576169411,"Improve RM_ModuleTypeReplaceValue() API.

With the previous API, a NULL return value was ambiguous and could
represent either an old value of NULL or an error condition. The new API
returns a status code and allows the old value to be returned
by-reference.

This commit also includes test coverage based on
tests/modules/datatype.c which did not exist at the time of the original
commit.
","['src/module.c', 'src/redismodule.h', 'tests/modules/datatype.c', 'tests/unit/moduleapi/datatype.tcl']",Current RM_ModuleTypeReplaceValue() API returns ambiguous NULL value; can't distinguish between actual NULL values and error conditions.
b9b140e2e2f900bc7a9396e813862b46e86af364,1551557862,"Use the RDB info fields to provide info to users.

Fix #5790 and 5878.

Maybe a better option was to have such fields named with the first
byte '%' as those are info fields for specification, however now to
break it in a backward incompatible way is not an option, so let's use
the fields actively to provide info when sensible, otherwise ignore
when they are not really helpful.
",['src/rdb.c'],RDB info fields are currently not providing useful information to users because they are either being ignored or not sensibly used.
9dcf878f1b735e4714a2f7bbea7ce567768605bf,1585645245,"Fix module commands propagation double MULTI bug.

37a10cef introduced automatic wrapping of MULTI/EXEC for the
alsoPropagate API. However this collides with the built-in mechanism
already present in module.c. To avoid complex changes near Redis 6 GA
this commit introduces the ability to exclude call() MUTLI/EXEC wrapping
for also propagate in order to continue to use the old code paths in
module.c.
","['src/module.c', 'src/server.c', 'src/server.h', 'tests/modules/propagate.c']","Automatic wrapping of MULTI/EXEC for the alsoPropagate API creates a clash with an existing mechanism in module.c, causing duplicate MULTI commands."
6b8a24a665d0f9bb7cb55cca9ef9e413bdf94e41,1528819875,"Streams: generate a few additional events.

Currently it does not look it's sensible to generate events for streams
consumer groups modification, being them metadata, however at least for
key-level events, like the creation or removal of a consumer group, I
added a few events here and there. Later we can evaluate if it makes
sense to add more. From the POV instead of WAIT (in Redis transaciton)
and signaling the key as modified, it looks like that the transaction
should not fail when a stream is modified, so no calls are made in
consumer groups related functions to signalModifiedKey().
",['src/t_stream.c'],"Current implementation lacks the generation of key-level events (like creation or removal of consumer groups) for stream consumer groups modifications, potentially affecting transactions and key modification signals."
45d3310694406fb0f338f7c639cda9754edb88f8,1692611626,"BITCOUNT: check for argument, before checking for key (#12394)

Generally, In any command we first check for  the argument and then check if key exist.

Some of the examples are

```
127.0.0.1:6379> getrange no-key invalid1 invalid2
(error) ERR value is not an integer or out of range
127.0.0.1:6379> setbit no-key 1 invalid
(error) ERR bit is not an integer or out of range
127.0.0.1:6379> xrange no-key invalid1 invalid2
(error) ERR Invalid stream ID specified as stream command argument
```

**Before change** 
```
bitcount no-key invalid1 invalid2
0
```

**After change**
```
bitcount no-key invalid1 invalid2
(error) ERR value is not an integer or out of range
```","['src/bitops.c', 'tests/unit/bitops.tcl']","The BITCOUNT command is not correctly checking for argument before checking if key exists, resulting in successful execution even with invalid arguments."
61a73de64dd8e900c0fe720b0fbf063ddf77676b,1615385383,"Cleanup ZADD_* flags (#8559)

Have a clear separation between in and out flags

Other changes:

delete dead code in RM_ZsetIncrby: if zsetAdd returned error (happens only if
the result of the operation is NAN or if score is NAN) we return immediately so
there is no way that zsetAdd succeeded and returned NAN in the out-flags","['src/module.c', 'src/server.h', 'src/t_zset.c']",Inconsistent handling and lack of distinction between in and out flags in RM_ZsetIncrby potentially obscure error states caused by NAN return values from zsetAdd.
bdd9a8002a6fcc93135eb4125da703b87a1959fa,1549977681,"Trim SDS free space of retained module strings

In some cases processMultibulkBuffer uses sdsMakeRoomFor to
expand the querybuf, but later in some cases it uses that query
buffer as is for an argv element (see ""Optimization""), which means
that the sds in argv may have a lot of wasted space, and then in case
modules keep that argv RedisString inside their data structure, this
space waste will remain for long (until restarted from rdb).
","['src/module.c', 'src/object.c', 'src/sds.c', 'src/server.h']","ProcessMultibulkBuffer's use of sdsMakeRoomFor can cause argv elements to retain wasted space, which persists when kept in module data structures, causing a potential memory inefficiency."
b6260a027026d1a5a6fa4fa0d8527de86066de8e,1531394379,"Streams: when re-delivering because of SETID, reset deliveries counter.

We don't want to increment the deliveries here, because the sysadmin
reset the consumer group so the desire is likely to restart processing,
and having the PEL polluted with old information is not useful but
probably confusing.

Related to #5111.
",['src/t_stream.c'],"Re-delivering streams due to SETID results in unwanted increment of deliveries counter, causing confusion due to persistence of old information in the Pending Entries List (PEL)."
29900d4e6bccdf3691bedf0ea9a5d84863fa3592,1620057587,"Fix integer overflow in intset (CVE-2021-29478)

An integer overflow bug in Redis 6.2 could be exploited to corrupt the heap and
potentially result with remote code execution.

The vulnerability involves changing the default set-max-intset-entries
configuration value, creating a large set key that consists of integer values
and using the COPY command to duplicate it.

The integer overflow bug exists in all versions of Redis starting with 2.6,
where it could result with a corrupted RDB or DUMP payload, but not exploited
through COPY (which did not exist before 6.2).
",['src/intset.c'],Redis versions starting from 2.6 are experiencing integer overflow which can potentially result in heap corruption and remote code execution. This especially happens when changing the default set-max-intset-entries configuration value and duplicating a large set key consisting of integer values.
501d7755831527b4237f9ed6050ec84203934e4d,1622029913,"unregister AE_READABLE from the read pipe in backgroundSaveDoneHandlerSocket (#8991)

In diskless replication, we create a read pipe for the RDB, between the child and the parent.
When we close this pipe (fd), the read handler also needs to be removed from the event loop (if it still registered).
Otherwise, next time we will use the same fd, the registration will be fail (panic), because
we will use EPOLL_CTL_MOD (the fd still register in the event loop), on fd that already removed from epoll_ctl","['src/rdb.c', 'tests/integration/replication.tcl']","In the diskless replication, not removing the read pipe's read handler from the event loop during closure leads to failed registration (panic) because of the reuse of the same file descriptor."
543e0daa6302f87478c660d2b96b2151afbf2c6a,1669127907,"Make assert_refcount skip the OBJECT REFCOUNT check with needs:debug tag (#11487)

This PR add `assert_refcount_morethan`, and modify `assert_refcount` to skip
the `OBJECT REFCOUNT` check with `needs:debug` flag. Use them to modify all
`OBJECT REFCOUNT` calls and also update the tests/README to be more specific.

The reasoning is that some of these tests could be testing something important,
and along the way also add a check for the refcount, and it could be a shame to skip
the whole test just because the refcount functionality is missing or blocked.
but much like the fact that some redis variants may not support DEBUG,
and still we want to run the majority of the test for coverage, and just skip the digest match.","['tests/support/test.tcl', 'tests/unit/maxmemory.tcl', 'tests/unit/type/incr.tcl']","The current 'assert_refcount' function may skip entire tests when the refcount capability is missing or blocked, potentially missing out on other important checks within these tests."
58e5feb3f49c50b9c18f38fd8f6cad2317c02265,1599146148,"redis-cli: fix writeConn() buffer handling. (#7749)

Fix issues with writeConn() which resulted with corruption of the stream by leaving an extra byte in the buffer. The trigger for this is partial writes or write errors which were not experienced on Linux but reported on macOS.",['src/redis-cli.c'],"Extra byte left in buffer after writeConn() operation, leading to stream corruption particularly observed on macOS. Triggered by partial writes or write errors."
7ace7231c6d306e3df92e6b4ec044b0e8cd8665b,1604569576,"Better INFO fields to track diskless and disk-based replication progress (#7981)

Expose new `loading_rdb_used_mem` showing the used memory of the server
that saved the RDB file we're currently using.
This is useful in diskless replication when the total size of the rdb is
unkown, and can be used as a rought estimation of progres.

Use that new field to calculate the ""user friendly""
`loading_loaded_perc` and `loading_eta_seconds`.

Expose `master_sync_total_bytes` and `master_sync_total_bytes` to complement
on the existing `master_sync_total_bytes` (which cannot be used on its own
to calculate progress).

Add ""user friendly"" field for `master_sync_perc`","['src/rdb.c', 'src/server.c', 'src/server.h']","The current INFO fields are insufficient for accurately tracking the progress of both diskless and disk-based replications, causing difficulty in estimating the total size of RDB and calculating progress."
68e87eb0883abf542fd8a38fc51f462eca8b5a4f,1669838892,"changing addReplySds and sdscat to addReplyStatusLength() within luaReplyToRedisReply() (#11556)

profiling EVALSHA\ we see that luaReplyToRedisReply takes 8.73% out of the
56.90% of luaCallFunction CPU cycles. 

Using addReplyStatusLength instead of directly composing the protocol to avoid
sdscatprintf and addReplySds ( which imply multiple sdslen calls ).

The new approach drops
luaReplyToRedisReply CPU cycles to 3.77%                                                                                                                                           ","['src/script_lua.c', 'src/server.h']","Profiling EVALSHA reveals that luaReplyToRedisReply function takes a significantly high percentage of luaCallFunction CPU cycles, indicating possible inefficiencies."
d484b8a04ed67e79030fcb060e88641acb6e4f98,1594922378,"Support passing stack allocated module strings to moduleCreateArgvFromUserFormat (#7528)

Specifically, the key passed to the module aof_rewrite callback is a stack allocated robj. When passing it to RedisModule_EmitAOF (with appropriate ""s"" fmt string) redis used to panic when trying to inc the ref count of the stack allocated robj. Now support such robjs by coying them to a new heap robj. This doesn't affect performance because using the alternative ""c"" or ""b"" format strings also copies the input to a new heap robj.",['src/module.c'],"When passing stack allocated robj keys to module aof_rewrite callback and using RedisModule_EmitAOF, Redis panics trying to increase the ref count of the stack allocated robj."
4aa927d16d4458124157bcc1d6a31b54df4f3a3f,1627982489,"Enabled -x option (Read last argument from STDIN) on redis-benchmark  (#9130)

Add the -x option (Read last argument from STDIN) on redis-benchmark.

Other changes:
To be able to use the code from redis-cli some helper methods were moved to cli_common.(h|c)

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/cli_common.c', 'src/cli_common.h', 'src/redis-benchmark.c', 'src/redis-cli.c']","Redis-benchmark is unable to read the last argument from STDIN, limiting its flexibility in test scenarios."
eddd8d34c4e82120ff9b0748aa48051b452f123b,1499162105,"Add symmetrical assertion to track c->reply_buffer infinite growth.

Redis clients need to have an instantaneous idea of the amount of memory
they are consuming (if the number is not exact should at least be
proportional to the actual memory usage). We do that adding and
subtracting the SDS length when pushing / popping from the client->reply
list. However it is quite simple to add bugs in such a setup, by not
taking the objects in the list and the count in sync. For such reason,
Redis has an assertion to track counts near 2^64: those are always the
result of the counter wrapping around because we subtract more than we
add. This commit adds the symmetrical assertion: when the list is empty
since we sent everything, the reply_bytes count should be zero. Thanks
to the new assertion it should be simple to also detect the other
problem, where the count slowly increases because of over-counting.
The assertion adds a conditional in the code that sends the buffer to
the socket but should not create any measurable performance slowdown,
listLength() just accesses a structure field, and this code path is
totally dominated by write(2).

Related to #4100.
",['src/networking.c'],Redis clients are experiencing reply buffer memory mismanagement—provided values are not proportional to actual memory usage. Possible counting bugs could be creating slow progressive over-counting when the list is empty.
37d761ba299909007ad27368b07f0d293e59398b,1648392981,"Fix Sentinel reconnect test following ACL change (#10480)

Replace condition with wait_for_condition On ""Verify sentinel that restarted 
failed to reconnect master after ACL change""

The reason we reach it, is because the test is fast enough to modify ACL and
test sentinel connection status with the server - before its scheduled operation
got the chance to update connection status with the server:
```
/* Perform scheduled operations for the specified Redis instance. */
void sentinelHandleRedisInstance(sentinelRedisInstance *ri) {
    /* ========== MONITORING HALF ============ */
    /* Every kind of instance */
    sentinelReconnectInstance(ri);
```",['tests/sentinel/tests/03-runtime-reconf.tcl'],"Test for Sentinel reconnect is failing due to rapid ACL modification and checking of connection status, before the scheduled operation could update the connection status."
82845f8d045a1fdbac17cf1959ad51906a8840ed,1569578398,"TerminateModuleForkChild(): use wait4 for safety.

In theory currently there is only one active child, but the API may
change or for bugs in the implementation we may have several (it was
like that for years because of a bug). Better to wait for a specific
pid and avoid consuing other pending children information.
",['src/module.c'],Multiple active child processes due to API changes or implementation bugs cause consumption of pending children information.
e58118cda6cacb5a845a9cf8eda45074470e31b2,1615222819,"Fix edge-case when a module client is unblocked (#8618)

Scenario:
1. A module client is blocked on keys with a timeout
2. Shortly before the timeout expires, the key is being populated and signaled
   as ready
3. Redis calls moduleTryServeClientBlockedOnKey (which replies to client) and
   then moduleUnblockClient
4. moduleUnblockClient doesn't really unblock the client, it writes to
   server.module_blocked_pipe and only marks the BC as unblocked.
5. beforeSleep kics in, by this time the client still exists and techincally
   timed-out. beforeSleep replies to the timeout client (double reply) and
   only then moduleHandleBlockedClients is called, reading from module_blocked_pipe
   and calling unblockClient

The solution is similar to what was done in moduleTryServeClientBlockedOnKey: we
should avoid re-processing an already-unblocked client",['src/module.c'],"When a module client is unblocked shortly before a timeout expires, it results in duplicate replies to the client due to re-processing of an already-unblocked client."
e532c95dfc3dd9f072b452db7b2d535cc2c12304,1692548271,"Added tests for Client commands (#10276)

In our test case, now we missed some test coverage for client sub-commands.
This pr goal is to add some test coverage cases of the following commands:

Client caching
Client kill
Client no-evict
Client pause
Client reply
Client tracking
Client setname

At the very least, this is useful to make sure there are no leaks and crashes in these code paths.",['tests/unit/introspection.tcl'],Missing test coverage for client sub-commands can potentially lead to undetected leaks and crashes.
de6f3ad0170fd69d23530960f406d19beb119300,1612366528,"Fix FreeBSD tests and CI Daily issues. (#8438)

* Add bash temporarily to allow sentinel fd leaks test to run.
* Use vmactions-freebsd rdist sync to work around bind permission denied
  and slow execution issues.
* Upgrade to tcl8.6 to be aligned with latest Ubuntu envs.
* Concat all command executions to avoid ignoring failures.
* Skip intensive fuzzer on FreeBSD. For some yet unknown reason, generate_fuzzy_traffic_on_key causes TCL to significantly bloat on FreeBSD resulting with out of memory.",['tests/integration/corrupt-dump-fuzzer.tcl'],"FreeBSD tests failing due to sentinel fd leaks, bind permission issues, slow execution, misalignment with Ubuntu environment, ignored failures, and memory bloat caused by TCL under intensive fuzzing."
f5235b2d7660b163c739325ec9284f97d5b963be,1613998859,"SRANDMEMBER RESP3 return should be Array, not Set (#8504)

SRANDMEMBER with negative count (non unique) can return the same member
multiple times, and the order of elements in the returned collection matters.
For these reasons returning a RESP3 Set type is not valid for the negative
count, but also not really valid for the positive (unique) variant either (the
command returns an array of random picks, not a set)

This PR also contains a minor optimization for SRANDMEMBER, HRANDFIELD,
and ZRANDMEMBER, to avoid the temporary dict from being rehashed while it grows.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/t_hash.c', 'src/t_set.c', 'src/t_zset.c']",The SRANDMEMBER command with negative count returning a RESP3 Set type can cause issues as the same member can be returned multiple times and order of the elements matter.
35e8ae3eb5f80ebb5cad5b509d1fde56176bca0d,1657527811,"Add cluster-port support to redis-cli --cluster (#10344)

In #9389, we add a new `cluster-port` config and make cluster bus port configurable,
and currently redis-cli --cluster create/add-node doesn't support with a configurable `cluster-port` instance.
Because redis-cli uses the old way (port + 10000) to send the `CLUSTER MEET` command.

Now we add this support on redis-cli `--cluster`, note we don't need to explicitly pass in the
`cluster-port` parameter, we can get the real `cluster-port` of the node in `clusterManagerNodeLoadInfo`,
so the `--cluster create` and `--cluster add-node` interfaces have not changed.

We will use the `cluster-port` when we are doing `CLUSTER MEET`, also note that `CLUSTER MEET` bus-port
parameter was added in 4.0, so if the bus_port (the one in redis-cli) is 0, or equal (port + 10000),
we just call `CLUSTER MEET` with 2 arguments, using the old form.

Co-authored-by: Madelyn Olson <34459052+madolson@users.noreply.github.com>","['src/redis-cli.c', 'tests/support/util.tcl', 'tests/unit/cluster.tcl']","Redis-cli --cluster create/add-node does not support a configurable `cluster-port` instance, instead following the old convention of using (port + 10000) to send the `CLUSTER MEET` command."
c789fb0aa7e4a634c66db8113699f06fd995c4d9,1661359155,"Fix assertion when a key is lazy expired during cluster key migration (#11176)

Redis 7.0 has #9890 which added an assertion when the propagation queue
was not flushed and we got to beforeSleep.
But it turns out that when processCommands calls getNodeByQuery and
decides to reject the command, it can lead to a key that was lazy
expired and is deleted without later flushing the propagation queue.

This change prevents lazy expiry from deleting the key at this stage
(not as part of a command being processed in `call`)","['src/cluster.c', 'src/db.c', 'src/server.h', 'tests/test_helper.tcl', 'tests/unit/cluster/misc.tcl']","Lazy expiration of a key during cluster key migration leads to it being deleted without flushing the propagation queue, triggering an assertion caused by #9890 in Redis 7.0."
e4eb18b303d716aeebc4153176a6cd93b8bd5d66,1668465635,"Module CLIENT_CHANGE, Fix crash on free blocked client with DB!=0 (#11500)

In moduleFireServerEvent we change the real client DB to 0 on freeClient in case the event is REDISMODULE_EVENT_CLIENT_CHANGE.
It results in a crash if the client is blocked on a key on other than DB 0.

The DB change is not necessary even for module-client, as we set its DB to 0 on either createClient or moduleReleaseTempClient.

Co-authored-by: Madelyn Olson <34459052+madolson@users.noreply.github.com>
Co-authored-by: Binbin <binloveplay1314@qq.com>","['src/module.c', 'tests/unit/moduleapi/hooks.tcl']","The system crashes when a client blocked on a key in a non-zero DB is freed, due to an unnecessary DB change in moduleFireServerEvent."
0047702aabdf53651e65cda8f1e7f7ed432353e7,1603082035,"Support ACL for Sentinel Mode (#7888)

This commit implements ACL for Sentinel mode, main work of this PR includes:

- Update Sentinel command table in order to better support ACLs.
- Fix couple of things which currently blocks the support for ACL on sentinel mode.
- Provide ""sentinel sentinel-user"" and ""sentinel sentinel-pass "" configuration in order to let sentinel authenticate with a specific user in other sentinels.
- requirepass is kept just for compatibility with old config files

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/acl.c', 'src/sentinel.c', 'src/server.c', 'src/server.h']","Sentinel mode does not support ACLs correctly, blocking configurations and causing authentication issues with other Sentinels."
00a9d6b3147143b7ed3f208f3924ff46c31e1fd9,1653206126,"Add SIGINT handler to redis-cli --bigkeys, --memkeys, --hotkeys, --scan (#10736)

Finish current loop and display the scanned keys summery on SIGINT (Ctrl-C) signal.
It will also prepend the current scanned percentage to the scanned keys summery 1st line.

In this commit I've renamed and relocated `intrinsicLatencyModeStop` function as I'm using the exact same logic.",['src/redis-cli.c'],"SIGINT (Ctrl-C) signal handling is missing in redis-cli when using --bigkeys, --memkeys, --hotkeys, --scan commands, preventing display of scanned keys summary and scanned percentage."
9dc6f93e9d0aadc7a8d6cd3617fa99bdfbba10a5,1686919140,"Add command being unblocked cause another command to get unblocked execution order test (#12324)

* Add command being unblocked cause another command to get unblocked execution order test

In #12301, we observed that if the
`while(listLength(server.ready_keys) != 0)`
in handleClientsBlockedOnKeys is changed to
`if(listLength(server.ready_keys) != 0)`,
the order of command execution will change.

It is wrong to change that. It means that if a command
being unblocked causes another command to get unblocked
(like a BLMOVE would do), then the new unblocked command
will wait for later to get processed rather than right away.

It'll not have any real implication if we change that since
we do call handleClientsBlockedOnKeys in beforeSleep again,
and redis will still behave correctly, but we don't change that.

An example:
1. $rd1 blmove src{t} dst{t} left right 0
2. $rd2 blmove dst{t} src{t} right left 0
3. $rd3 set key1{t}, $rd3 lpush src{t}, $rd3 set key2{t} in a pipeline

The correct order would be:
1. set key1{t}
2. lpush src{t}
3. lmove src{t} dst{t} left right
4. lmove dst{t} src{t} right left
5. set key2{t}

The wrong order would be:
1. set key1{t}
2. lpush src{t}
3. lmove src{t} dst{t} left right
4. set key2{t}
5. lmove dst{t} src{t} right left

This PR adds corresponding test to cover it.

* Add comment near while(listLength(server.ready_keys) != 0)","['src/blocked.c', 'tests/unit/type/list.tcl']",Unblocking a command that triggers unblocking of another command may lead to incorrect execution order; an issue observed when changing `while` to `if` for `listLength(server.ready_keys)`.
f21dc38a6ed3851a5e6501199e803ff0b93795cf,1638395826,"Redis Functions - Moved invoke Lua code functionality to script_lua.c

The functionality was moved to script_lua.c under
callFunction function. Its purpose is to call the Lua
function already located on the top of the Lua stack.

Used the new function on eval.c to invoke Lua code.
The function will also be used to invoke Lua
code on the Lua engine.
","['src/eval.c', 'src/script_lua.c', 'src/script_lua.h', 'src/server.h']","Lua code invocation currently scattered across different files, causing difficulty in maintenance and potential functionality inconsistency."
d0819d618e97c81ada5b09adffc127f332a4ce73,1624349411,"solve test timing issues in replication tests (#9121)

# replication-3.tcl
had a test timeout failure with valgrind on daily CI:
```
*** [err]: SLAVE can reload ""lua"" AUX RDB fields of duplicated scripts in tests/integration/replication-3.tcl
Replication not started.
```
replication took more than 70 seconds.
https://github.com/redis/redis/runs/2854037905?check_suite_focus=true

on my machine it takes only about 30, but i can see how 50 seconds isn't enough.

# replication.tcl
loading was over too quickly in freebsd daily CI:
```
*** [err]: slave fails full sync and diskless load swapdb recovers it in tests/integration/replication.tcl
Expected '0' to be equal to '1' (context: type eval line 44 cmd {assert_equal [s -1 loading] 1} proc ::start_server)
```

# rdb.tcl
loading was over too quickly.
increase the time loading takes, and decrease the amount of work we try to achieve in that time.","['tests/integration/rdb.tcl', 'tests/integration/replication-3.tcl', 'tests/integration/replication.tcl']",Replication tests are suffering from timing issues causing test failures in different scenarios. The test timeout failure is observed with valgrind on daily CI in replication-3.tcl and premature loading termination in replication.tcl and rdb.tcl.
3b9be93fdab81e27d68814aa794807897055af0d,1512063006,"Prevent corruption of server.executable after DEBUG RESTART.

Doing the following ended with a broken server.executable:

1. Start Redis with src/redis-server
2. Send CONFIG SET DIR /tmp/
3. Send DEBUG RESTART

At this point we called execve with an argv[0] that is no longer related
to the new path. So after the restart the absolute path of the
executable is recomputed in the wrong way. With this fix we pass the
absolute path already computed as argv[0].
",['src/server.c'],DEBUG RESTART post CONFIG SET DIR /tmp/ results in broken server.executable due to incorrect recomputation of the absolute path of the executable.
4fb39b6700c2a08b7de595fbff8102b84c79e562,1633509181,"Added module-acquire-GIL latency stats (#9608)

The new value indicates how long Redis wait to
acquire the GIL after sleep. This can help identify
problems where a module perform some background
operation for a long time (with the GIL held) and
blocks the Redis main thread.",['src/server.c'],Redis waiting for a long time to acquire the Global Interpreter Lock (GIL) due to background operations in a module could potentially block the Redis main thread.
ed2269762b3be6c384025dcf3237b58f10babf03,1574168511,"try to fix an unstable test (module hook for loading progress)

there were two lssues, one is taht BGREWRITEAOF failed since the initial one was still in progress
the solution for this one is to enable appendonly from the server startup so there's no initial aofrw.

the other problem was 0 loading progress events, theory is that on some
platforms a sleep of 1 will cause a much greater delay due to the context
switch, but on other platform it doesn't. in theory a sleep of 100 micro
for 1k keys whould take 100ms, and with hz of 500 we should be gettering
50 events (one every 2ms). in practise it doesn't work like that, so trying
to find a sleep that would be long enough but still not cause the test to take
too long.
",['tests/unit/moduleapi/hooks.tcl'],Test instability issues due to BGREWRITEAOF failing when initial one is in progress and erratic loading progress events generation with sleep-related delays on different platforms.
d56a7d9b0510017ca5096074f7772ce2bc5db6b0,1688538750,"Fix and increase tollerance of event loop test, add verbose logs (#12385)

The test fails on freebsd CI:
```
*** [err]: stats: eventloop metrics in tests/unit/info.tcl
Expected '31777' to be less than '16183' (context: type eval line 17 cmd
{assert_lessthan $el_sum2 [expr $el_sum1+10000] } proc ::test)
```

The test added in #11963, fails on freebsd CI which is slow,
increase tollerance and also add some verbose logs, now we can
see these logs in verbose mode (for better views):
```
eventloop metrics cycle1: 12, cycle2: 15
eventloop metrics el_sum1: 315, el_sum2: 411
eventloop metrics cmd_sum1: 126, cmd_sum2: 137
[ok]: stats: eventloop metrics (111 ms)
instantaneous metrics instantaneous_eventloop_cycles_per_sec: 8
instantaneous metrics instantaneous_eventloop_duration_usec: 55
[ok]: stats: instantaneous metrics (1603 ms)
[ok]: stats: debug metrics (112 ms)
```",['tests/unit/info.tcl'],"The event loop test fails on the slower FreeBSD CI due to insufficient tolerance, making it difficult to debug without verbose logs."
306a5ccd2d053ff653988b61a779e3cbce408874,1630570071,"Fix the timing of read and write events under kqueue (#9416)

Normally we execute the read event first and then the write event.
When the barrier is set, we will do it reverse.
However, under `kqueue`, if an `fd` has both read and write events,
reading the event using `kevent` will generate two events, which will
result in uncontrolled read and write timing.

This also means that the guarantees of AOF `appendfsync` = `always` are
not met on MacOS without this fix.

The main change to this pr is to cache the events already obtained when reading
them, so that if the same `fd` occurs again, only the mask in the cache is updated,
rather than a new event is generated.

This was exposed by the following test failure on MacOS:
```
*** [err]: AOF fsync always barrier issue in tests/integration/aof.tcl
Expected 544 != 544 (context: type eval line 26 cmd {assert {$size1 != $size2}} proc ::test)
```",['src/ae_kqueue.c'],"Uncontrolled read and write timing on MacOS under 'kqueue' due to double event generation when an 'fd' has both read and write events, violating AOF 'appendfsync' = 'always' guarantees."
88d71f479338c1a70fac15ea37f87315f9401f99,1589979882,"fix a rare active defrag edge case bug leading to stagnation

There's a rare case which leads to stagnation in the defragger, causing
it to keep scanning the keyspace and do nothing (not moving any
allocation), this happens when all the allocator slabs of a certain bin
have the same % utilization, but the slab from which new allocations are
made have a lower utilization.

this commit fixes it by removing the current slab from the overall
average utilization of the bin, and also eliminate any precision loss in
the utilization calculation and move the decision about the defrag to
reside inside jemalloc.

and also add a test that consistently reproduce this issue.
","['deps/jemalloc/include/jemalloc/internal/jemalloc_internal_inlines_c.h', 'deps/jemalloc/src/jemalloc.c', 'src/defrag.c', 'tests/unit/memefficiency.tcl']",Active defragger undergoes stagnation and keeps scanning the keyspace without moving any allocations in rare cases where all allocator slabs of a certain bin have same % utilization but the slab for new allocations has a lower utilization.
f11a7585a8498689e8fd1afbcab4fdc2ba38c38f,1512063472,"PSYNC2: Save Lua scripts state into RDB file.

This is currently needed in order to fix #4483, but this can be
useful in other contexts, so maybe later we may want to remove the
conditionals and always save/load scripts.

Note that we are using the ""lua"" AUX field here, in order to guarantee
backward compatibility of the RDB file. The unknown AUX fields must be
discarded by past versions of Redis.
","['src/rdb.c', 'src/server.h']","Lua scripts states are not being saved into RDB file, causing issues with backward compatibility and other contexts."
63acfe4b00b9d3e34a53559f965c0bc44c03db61,1619069563,"Expire key firstly and then notify keyspace event (#8830)

Modules event subscribers may get wrong things in notifyKeyspaceEvent callback,
such as wrong number of keys, or be able to lookup this key.
This commit changes the order to be like the one in evict.c.

Cleanup:
Since we know the key exists (it expires now), db*Delete is sure to return 1,
so there's no need to check it's output (misleading).",['src/db.c'],"Modules event subscribers receive incorrect data in notifyKeyspaceEvent callback, including wrong number of keys or being able to lookup an expired key."
3f494cc49d25929f27fa75a78d9921a9dee771f2,1596942707,"see #7544, added RedisModule_HoldString api. (#7577)

Added RedisModule_HoldString that either returns a
shallow copy of the given String (by increasing
the String ref count) or a new deep copy of String
in case its not possible to get a shallow copy.

Co-authored-by: Itamar Haber <itamar@redislabs.com>","['src/module.c', 'src/redismodule.h', 'tests/modules/keyspace_events.c', 'tests/unit/moduleapi/keyspace_events.tcl']","Lack of functionality to handle situations where a shallow copy of a String is needed or a new deep copy when shallow not possible, causing inefficiencies in memory management."
71e6abe423a36e9dfa9c74b7c0c9f45b5836adb7,1684841368,"Fix memory leak when RM_Call's RUN_AS_USER fails (#12158)

previously the argv wasn't freed so would leak.  not a common case, but should be handled.

Solution: move RUN_AS_USER setup and error exit to the right place.
this way, when we do `goto cleanup` (instead of return) it'll automatically do the right thing (including autoMemoryAdd)
Removed the user argument from moduleAllocTempClient (reverted to the state before 6e993a5)

Co-authored-by: Oran Agra <oran@redislabs.com>",['src/module.c'],"Memory leak occurs when RM_Call's RUN_AS_USER fails, due to not freeing argv."
63dae5232415d216dfc1acce8b5335e20aa3b178,1596000356,"Avoid an out-of-bounds read in the redis-sentinel (#7443)

The Redis sentinel would crash with a segfault after a few minutes
because it tried to read from a page without read permissions. Check up
front whether the sds is long enough to contain redis:slave or
redis:master before memcmp() as is done everywhere else in
sentinelRefreshInstanceInfo().

Bug report and commit message from Theo Buehler. Fix from Nam Nguyen.

Co-authored-by: Nam Nguyen <namn@berkeley.edu>",['src/sentinel.c'],"Redis sentinel crashes due to a segmentation fault when attempting to read from a page without read permissions, potentially due to insufficient sds length checks before memcmp()."
3ca451c46fed894bf49e7561fa0282d2583f1c06,1684934864,"Make a light weight version (default) of DEBUG HTSTATS (#12212)

The light version only shows the table sizes, while the pre-existing
version that shows chain length stats is reachable with the `full` argument.

This should allow looking into rehashing state, even on huge dicts, on
which we're afraid to run the command for fear of causing a server freeze.

Also, fix a possible overflow in dictGetStats.","['src/debug.c', 'src/dict.c', 'src/dict.h']","Running DEBUG HTSTATS on huge dicts risks server freeze, and potential overflow issue detected in dictGetStats."
a031d268b127d67969e9a4ec6b728dbb321ea0ce,1614607484,"Make port, tls-port and bind configurations modifiable (#8510)

Add ability to modify port, tls-port and bind configurations by CONFIG SET command.

To simplify the code and make it cleaner, a new structure
added, socketFds, which contains the file descriptors array and its counter,
and used for TCP, TLS and Cluster sockets file descriptors.","['src/cluster.c', 'src/config.c', 'src/server.c', 'src/server.h', 'tests/test_helper.tcl', 'tests/unit/introspection.tcl', 'tests/unit/networking.tcl', 'tests/unit/tls.tcl']","Unable to modify port, tls-port and bind configurations dynamically via the CONFIG SET command, causing limitations in configuration flexibility."
e53bf6524599dec89c08250c5d0f5bed096ae394,1663834925,"Replica that asks for rdb only should be closed right after the rdb part (#11296)

The bug is that the the server keeps on sending newlines to the client.
As a result, the receiver might not find the EOF marker since it searches
for it only on the end of each payload it reads from the socket.
The but only affects `CLIENT_REPL_RDBONLY`.
This affects `redis-cli --rdb` (depending on timing)

The fixed consist of two steps:
1. The `CLIENT_REPL_RDBONLY` should be closed ASAP (we cannot
   always call to `freeClient` so we use `freeClientAsync`)
2. Add new replication state `SLAVE_STATE_RDB_TRANSMITTED`","['src/replication.c', 'src/server.h']","Server continues to send newlines to client, causing issues with EOF marker detection specifically affecting `CLIENT_REPL_RDBONLY` and `redis-cli --rdb`."
d4ae76d1a6cde03c957000f870ff3d1c0e8c3fb9,1532420667,"fix slave buffer test suite false positives

it looks like on slow machines we're getting:
[err]: slave buffer are counted correctly in tests/unit/maxmemory.tcl
Expected condition '$slave_buf > 2*1024*1024' to be true (16914 > 2*1024*1024)

this is a result of the slave waking up too early and eating the
slave buffer before the traffic and the test ends.
",['tests/unit/maxmemory.tcl'],"On slower machines, the slave buffer is prematurely consumed, causing test failures in the 'tests/unit/maxmemory.tcl' suite, particularly for the condition '$slave_buf > 2*1024*1024'."
624742d9b40d20afe8b694c2dc0b0349f8b7aaad,1589642135,"Remove the client from CLOSE_ASAP list before caching the master.

This was broken in 1a7cd2c: we identified a crash in the CI, what
was happening before the fix should be like that:

1. The client gets in the async free list.
2. However freeClient() gets called again against the same client
   which is a master.
3. The client arrived in freeClient() with the CLOSE_ASAP flag set.
4. The master gets cached, but NOT removed from the CLOSE_ASAP linked
   list.
5. The master client that was cached was immediately removed since it
   was still in the list.
6. Redis accessed a freed cached master.

This is how the crash looked like:

=== REDIS BUG REPORT START: Cut & paste starting from here ===
1092:S 16 May 2020 11:44:09.731 # Redis 999.999.999 crashed by signal: 11
1092:S 16 May 2020 11:44:09.731 # Crashed running the instruction at: 0x447e18
1092:S 16 May 2020 11:44:09.731 # Accessing address: 0xffffffffffffffff
1092:S 16 May 2020 11:44:09.731 # Failed assertion:  (:0)

------ STACK TRACE ------
EIP:
src/redis-server 127.0.0.1:21300(readQueryFromClient+0x48)[0x447e18]

And the 0xffff address access likely comes from accessing an SDS that is
set to NULL (we go -1 offset to read the header).
","['src/networking.c', 'src/replication.c']",Caching of master client while it's in the CLOSE_ASAP linked list causes a crash; freeClient() is invoked against a master client in the async free list causing redirection to invalid address during re-access.
d697daa7a5600ca82a0098bc7c857bc7167bb35e,1642514120,"Use const char pointer in redismodule.h as far as possible (#10064)

When I used C++ to develop a redis module. i  used `string.data()` as the second parameter `ele`
of  `RedisModule_DigestAddStringBuffer`, but there is a warning, since we never change the `ele`,
i think we should use `const char` for it.

This PR adds const to just a handful of module APIs that required it, all not very widely used.
The implication is a breaking change in terms of compilation error that's easy to resolve, and no ABI impact.
The affected APIs are around Digest, Info injection, and Cluster bus messages.","['src/cluster.c', 'src/cluster.h', 'src/debug.c', 'src/module.c', 'src/modules/hellocluster.c', 'src/redismodule.h', 'src/server.h', 'tests/modules/datatype2.c']","Using `string.data()` as a parameter in `RedisModule_DigestAddStringBuffer` causes a warning in C++ module development for Redis, indicating issues with constant characters."
fb2a94af3fbb3f3cf8b26b8bd89387669cb111a1,1597675010,"TLS: relax verification on CONFIG SET. (#7665)

Avoid re-configuring (and validating) SSL/TLS configuration on `CONFIG
SET` when TLS is not actively enabled for incoming connections, cluster
bus or replication.

This fixes failures when tests run without `--tls` on binaries that were
built with TLS support.

An additional benefit is that it's now possible to perform a multi-step
configuration process while TLS is disabled. The new configuration will
be verified and applied only when TLS is effectively enabled.",['src/config.c'],Failures occur when tests run without `--tls` on binaries built with TLS support due to SSL/TLS configuration being re-configured and validated on `CONFIG SET` even when TLS is not actively enabled. This also affects multi-step configuration.
39feee8e3a6fee885aee60672a9dde0dfd08896c,1641903973,"LPOP/RPOP with count against non existing list return null array (#10095)

It used to return `$-1` in RESP2, now we will return `*-1`.
This is a bug in redis 6.2 when COUNT was added, the `COUNT`
option was introduced in #8179. Fix #10089.

the documentation of [LPOP](https://redis.io/commands/lpop) says
```
When called without the count argument:
Bulk string reply: the value of the first element, or nil when key does not exist.

When called with the count argument:
Array reply: list of popped elements, or nil when key does not exist.
```","['src/t_list.c', 'tests/unit/type/list.tcl']","Using LPOP/RPOP with count on non-existing list unexpectedly returns `$-1` instead of expected `*-1`, contradicting existing LPOP documentation."
047b609335436db9a38ad10d5e51aa9161c7d3cf,1649059139,"Fix #10508, on error, pop function and error handler from Lua stack. (#10519)

If, for some reason, Redis decides not to execute the script, we need
to pop the function and error handler from Lua stack. Otherwise, eventually
the Lua stack will explode.

Relevant only for 7.0-rc1 and 7.0-rc2.","['src/eval.c', 'tests/unit/scripting.tcl']","When Redis decides not to execute a script, the function and error handler are not removed from the Lua stack, which can eventually cause the Lua stack to overflow."
4355145a6268a6ae811a2bf39911966879512877,1611843583,"Add modules API for streams (#8288)

APIs added for these stream operations: add, delete, iterate and
trim (by ID or maxlength). The functions are prefixed by RM_Stream.

* RM_StreamAdd
* RM_StreamDelete
* RM_StreamIteratorStart
* RM_StreamIteratorStop
* RM_StreamIteratorNextID
* RM_StreamIteratorNextField
* RM_StreamIteratorDelete
* RM_StreamTrimByLength
* RM_StreamTrimByID

The type RedisModuleStreamID is added and functions for converting
from and to RedisModuleString.

* RM_CreateStringFromStreamID
* RM_StringToStreamID

Whenever the stream functions return REDISMODULE_ERR, errno is set to
provide additional error information.

Refactoring: The zset iterator fields in the RedisModuleKey struct
are wrapped in a union, to allow the same space to be used for type-
specific info for streams and allow future use for other key types.","['src/module.c', 'src/redismodule.h', 'src/stream.h', 'src/t_stream.c', 'tests/modules/stream.c', 'tests/unit/moduleapi/stream.tcl']","Lack of API functionality for specific Redis stream operations like add, delete, iterator and trim operations, and inability to convert between `RedisModuleStreamID` and `RedisModuleString`."
0992ada2fe1cbc8f5f25c0cdec67cb69bb8d3810,1577787390,"Fix petential cluster link error.

Funcion adjustOpenFilesLimit() has an implicit parameter, which is server.maxclients.
This function aims to ajust maximum file descriptor number according to server.maxclients
by best effort, which is ""bestlimit"" could be lower than ""maxfiles"" but greater than ""oldlimit"".
When we try to increase ""maxclients"" using CONFIG SET command, we could increase maximum
file descriptor number to a bigger value without calling aeResizeSetSize the same time.
When later more and more clients connect to server, the allocated fd could be bigger and bigger,
and eventually exceeds events size of aeEventLoop.events. When new nodes joins the cluster,
new link is created, together with new fd, but when calling aeCreateFileEvent, we did not
check the return value. In this case, we have a non-null ""link"" but the associated fd is not
registered.

So when we dynamically set ""maxclients"" we could reach an inconsistency between maximum file
descriptor number of the process and server.maxclients. And later could cause cluster link and link
fd inconsistency.

While setting ""maxclients"" dynamically, we consider it as failed when resulting ""maxclients"" is not
the same as expected. We try to restore back the maximum file descriptor number when we failed to set
""maxclients"" to the specified value, so that server.maxclients could act as a guard as before.
",['src/config.c'],"Dynamically setting ""maxclients"" can lead to inconsistency between maximum file descriptor number and server.maxclients, eventually resulting in cluster link and link fd inconsistency."
18920813a9842ec3dec895e2888d0b5d2955b916,1664175825,"Ignore RM_Call deny-oom flag if maxmemory is zero (#11319)

If a command gets an OOM response and then if we set maxmemory to zero
to disable the limit, server.pre_command_oom_state never gets updated
and it stays true. As RM_Call() calls with ""respect deny-oom"" flag checks
server.pre_command_oom_state, all calls will fail with OOM.

Added server.maxmemory check in RM_Call() to process deny-oom flag
only if maxmemory is configured.","['src/module.c', 'tests/unit/moduleapi/misc.tcl']","When maxmemory is set to zero to disable limit after an OOM response, server's previous OOM state remains true causing all subsequent RM_Call() with ""respect deny-oom"" flag to fail."
0bc3dab0954e481b882a722f768ec0a5a7f725ae,1573563929,"Adjustments for active defrag defaults and tuning

Reduce default minimum effort, so that when fragmentation is just detected,
the impact on the latency will be minor.

Reduce the default maximum effort, mainly to prevent a case were a sudden
massive deletions, won't trigger an aggressive defrag that will cause latency.

When activedefrag is disabled mid-run, reset the 'running' info field, and
clear the scan cursor, so that when it'll be re-enabled, a new fresh scan will
start.

Clearing the 'running' variable is important since lowering the defragger
tunables mid-scan won't help, the defragger only considers new threshold when
a new scan starts, and during a scan it can only become more aggressive,
(when more severe fragmentation is detected), it'll never go less aggressive.
So by temporarily disabling activedefrag, one can lower th the tunables.

Removing the experimantal warning.
","['src/defrag.c', 'src/server.c', 'src/server.h']","The active defrag feature causes significant latency issues due to overly aggressive defrag following large deletions and retains unnecessary information from mid-run disablements, preventing fresh scans on re-enabling. This setting also fails to consider new thresholds if altered during a scan."
82c3158ad5fe5aab002a6d0565832d5bd15082f5,1627882293,"Fix if consumer is created as a side effect without notify and dirty++ (#9263)

Fixes:
- When a consumer is created as a side effect, redis didn't issue a keyspace notification,
  nor incremented the server.dirty (affects periodic snapshots).
  this was a bug in XREADGROUP, XCLAIM, and XAUTOCLAIM.
- When attempting to delete a non-existent consumer, don't issue a keyspace notification
  and don't increment server.dirty
  this was a bug in XGROUP DELCONSUMER

Other changes:
- Changed streamLookupConsumer() to always only do lookup consumer (never do implicit creation),
  Its last seen time is updated unless the SLC_NO_REFRESH flag is specified.
- Added streamCreateConsumer() to create a new consumer. When the creation is successful,
  it will notify and dirty++ unless the SCC_NO_NOTIFY or SCC_NO_DIRTIFY flags is specified.
- Changed streamDelConsumer() to always only do delete consumer.
- Added keyspace notifications tests about stream events.
","['src/blocked.c', 'src/rdb.c', 'src/stream.h', 'src/t_stream.c', 'tests/unit/pubsub.tcl']","Issues in Redis related to consumer creation: when consumer is created as a side effect, it is not issuing a keyspace notification and not incrementing server.dirty. Additionally, deleting a non-existent consumer still issues a keyspace notification and increments server.dirty."
2fec7d9c6c630db3bcb13a07a08c39404abad447,1559478458,"Jemalloc: Avoid blocking on background thread lock for stats.

Background threads may run for a long time, especially when the # of dirty pages
is high.  Avoid blocking stats calls because of this (which may cause latency
spikes).

see https://github.com/jemalloc/jemalloc/issues/1502

cherry picked from commit 1a71533511027dbe3f9d989659efeec446915d6b
",['deps/jemalloc/src/background_thread.c'],"Stats calls are experiencing latency spikes due to blocking, most likely when background threads run for extended durations with a high number of dirty pages."
1aad55b66fac0f8b7a855cafc1ee80ef37a80414,1611646314,"Sentinel: Fix Config Dependency and Rewrite Sequence (#8271)

This commit fixes a well known and an annoying issue in Sentinel mode.

Cause of this issue:
Currently, Redis rewrite process works well in server mode, however in sentinel mode,
the sentinel config has variant semantics for different configurations, in example configuration
https://github.com/redis/redis/blob/unstable/sentinel.conf, we put comments on these.
However the rewrite process only treat the sentinel config as a single option. During rewrite
process, it will mess up with the lines and comments.

Approaches:
In order to solve this issue, we need to differentiate different subconfig options in sentinel separately,
for example, sentinel monitor <master-name> <ip> <redis-port> <quorum>
we can treat it as sentinel monitor option, instead of the sentinel option.

This commit also fixes the dependency issue when putting configurations in sentinel.conf.
For example before this commit,we must put
`sentinel monitor <master-name> <ip> <redis-port> <quorum>` before
`sentinel auth-pass <master-name> <password>` for a single master,
otherwise the server cannot start and will return error. This commit fixes this issue, as long as
the monitoring master was configured, no matter the sequence is, the sentinel can start and run properly.","['src/config.c', 'src/sentinel.c', 'src/server.c', 'src/server.h', 'tests/instances.tcl', 'tests/sentinel/run.tcl', 'tests/sentinel/tests/00-base.tcl', 'tests/sentinel/tests/includes/start-init-tests.tcl']","In Sentinel mode, Redis rewrite process mixes lines and comments while treating sentinel config as a single option. Also, there's a dependency issue when adding configurations to sentinel.conf causing the server not to start if sequence is incorrect."
cec404f099e3a1a3ce6e94c01ce45d851bd3e843,1528729948,"Use a less aggressive query buffer resize policy.

A user with many connections (10 thousand) on a single Redis server
reports in issue #4983 that sometimes Redis is idle becuase at the same
time many clients need to resize their query buffer according to the old
policy.

It looks like this was created by the fact that we allow the query
buffer to grow without problems to a size up to PROTO_MBULK_BIG_ARG
normally, but when the client is idle we immediately are more strict,
and a query buffer greater than 1024 bytes is already enough to trigger
the resize. So for instance if most of the clients stop at the same time
this issue should be easily triggered.

This behavior actually looks odd, and there should be only a clear limit
after we say, let's look at this query buffer to check if it's time to
resize it. This commit puts the limit at PROTO_MBULK_BIG_ARG, and the
check is performed both if compared to the peak usage the current usage
is too big, or if the client is idle.

Then when the check is performed, to waste just a few kbytes is
considered enough to proceed with the resize. This should fix the issue.
",['src/server.c'],Redis server with many connections experiences idleness due to aggressive query buffer resize policy being triggered when a significant number of clients stop simultaneously.
687210f1550cf9048bed5f5539c9411fb22cd3b0,1639670305,"Add FUNCTION FLUSH command to flush all functions (#9936)

Added `FUNCTION FLUSH` command. The new sub-command allows delete all the functions.
An optional `[SYNC|ASYNC]` argument can be given to control whether or not to flush the
functions synchronously or asynchronously. if not given the default flush mode is chosen by
`lazyfree-lazy-user-flush` configuration values.

Add the missing `functions.tcl` test to the list of tests that are executed in test_helper.tcl,
and call FUNCTION FLUSH in between servers in external mode","['src/commands.c', 'src/functions.c', 'src/functions.h', 'src/lazyfree.c', 'src/server.h', 'tests/support/server.tcl', 'tests/test_helper.tcl', 'tests/unit/functions.tcl']","Lack of FUNCTION FLASH command leads to inability to delete all functions either synchronously or asynchronously, impacting function management in various execution environments."
11b3325e9999721d35ec64afac7b917664f6291b,1608235138,"Not over-allocate client query buffer when reading large objects. (#5954)

In response to large client query buffer optimization introduced in 1898e6c. The calculation of the amount of
remaining bytes we need to write to the query buffer was calculated wrong, as a result we are unnecessarily
growing the client query buffer by sdslen(c->querybuf) always. This fix corrects that behavior.

Please note the previous behavior prior to the before-mentioned change was correctly calculating the remaining
additional bytes, and this change makes that calculate to be consistent.

Useful context, the argument of size `ll` starts at qb_pos (which is now the beginning of the sds), but much of it
may have already been read from the socket, so we only need to grow the sds for the remainder of it.",['src/networking.c'],"The client query buffer is over-allocated when reading large objects due to incorrect calculation of the remaining bytes needed, leading to unnecessary growth."
e5882da92bd0f39e74bf306ab34280e992956952,1681885427,"Move startup system check to before daeomniztion and modules init (#12067)

1. it's a bad idea to print these errors and exit after daemonization (if we'll exit,
  the failure may not be detected)
2. it's not nice to exit (or even do the check that uses `fork`) after modules already
  started (could create threads, or do some changes)",['src/server.c'],Startup system check post daemonization and modules initialization may result in unnoticed exits and potential disruptions due to premature `fork` usage.
537893420bc5a51ece060b53e5492b101bcfaba3,1582646003,"fix github actions failing latency test for active defrag

seems that github actions are slow, using just one client to reduce
false positives.

also adding verbose, testing only on latest ubuntu, and building on
older one.

when doing that, i can reduce the test threshold back to something saner
",['tests/unit/memefficiency.tcl'],Latency tests for active defrag are failing in GitHub actions due to slow performance and elevated false positives.
a7726cdf517eca16dec37655881ddae2688aaaeb,1638946742,"Fix SENTINEL subcommands's arity (#9909)

For `SENTINEL SET`, we can use in these ways:
1. SENTINEL SET mymaster quorum 3
2. SENTINEL SET mymaster quorum 5 parallel-syncs 1

For `SENTINEL SIMULATE-FAILURE`, although it is only used for testing:
1. SENTINEL SIMULATE-FAILURE CRASH-AFTER-ELECTION
2. SENTINEL SIMULATE-FAILURE CRASH-AFTER-ELECTION CRASH-AFTER-PROMOTION
","['src/sentinel.c', 'src/server.c']",Arity inconsistencies exist within 'SENTINEL SET' and 'SENTINEL SIMULATE-FAILURE' commands leading to incorrect command behaviour.
f22bfe86b6db59ae6d5357b44f9e86418746c31c,1654616821,"Update musl libc detection pattern (#10826)

This change fixes failing `integration/logging.tcl` test in Gentoo with
musl libc, where `ldd` returns
```
libc.so => /lib/ld-musl-x86_64.so.1 (0x7f9d5f171000)
```
unlike Alpine's
```
libc.musl-x86_64.so.1 => /lib/ld-musl-x86_64.so.1 (0x7f82cfa16000)
```
The solution is to extend matching pattern introduced in #8532.",['tests/integration/logging.tcl'],The `integration/logging.tcl` test in Gentoo with musl libc is failing due to an inconsistency in the detection pattern of musl libc in the output of `ldd`.
8945067544706951fd60007ad8cba8e8941b970d,1661675846,"bugfix:del keys in slot replicate to replica, and trigger other invalidations (#11084)

Bugfix:
with the scenario if we force assigned a slot to other master,
old master will lose the slot ownership, then old master will
call the function delKeysInSlot() to delete all keys which in
the slot. These delete operations should replicate to replicas,
avoid the data divergence issue in master and replicas.

Additionally, in this case, we now call:
* signalModifiedKey (to invalidate WATCH)
* moduleNotifyKeyspaceEvent (key space notification for modules)
* dirty++ (to signal that the persistence file may be outdated)

Co-authored-by: weimeng <weimeng@didiglobal.com>
Co-authored-by: Madelyn Olson <madelyneolson@gmail.com>","['src/cluster.c', 'tests/test_helper.tcl', 'tests/unit/cluster/slot-ownership.tcl']","Forced slot reassignment on master can cause data divergence between master and replicas. Additionally, invalidations such as WATCH, keyspace notification for modules, and persistence file updates are not being triggered."
689b64c3ad921c6f82cee38c0940c6637e9ba88f,1516090242,"PSYNC2 fix - promoted slave should hold on to it's backlog

after a slave is promoted (assuming it has no slaves
and it booted over an hour ago), it will lose it's replication
backlog at the next replication cron, rather than waiting for slaves
to connect to it.
so on a simple master/slave faiover, if the new slave doesn't connect
immediately, it may be too later and PSYNC2 will fail.
",['src/replication.c'],"After a slave is promoted, it prematurely loses its replication backlog before new slaves can connect, potentially leading to PSYNC2 failures."
4836ae32c7eb485dd4b0196ec67b9838c6d87f80,1640859004,"redis-cli: Add -X option and extend --cluster call take arg from stdin (#9980)

There are two changes in this commit:

1. Add -X option to redis-cli.
Currently `-x` can only be used to provide the last argument,
so you can do `redis-cli dump keyname > key.dump`,
and then do `redis-cli -x restore keyname 0 < key.dump`.

But what if you want to add the replace argument (which comes last?).
oran suggested adding such usage:
`redis-cli -X <tag> restore keyname <tag> replace < key.dump`

i.e. you're able to provide a string in the arguments that's gonna be
substituted with the content from stdin.

Note that the tag name should not conflict with others non-replaced args.
And the -x and -X options are conflicting.

Some usages:
```
[root]# echo mypasswd | src/redis-cli -X passwd_tag mset username myname password passwd_tag                                                   OK
[root]# echo username > username.txt
[root]# head -c -1 username.txt | src/redis-cli -X name_tag mget name_tag password
1) ""myname""
2) ""mypasswd\n""
```

2. Handle the combination of both `-x` and `--cluster` or `-X` and `--cluster`
Extend the broadcast option to receive the last arg or <tag> arg from the stdin.

Now we can use `redis-cli -x --cluster call <host>:<port> cmd`,
or `redis-cli -X <tag> --cluster call <host>:<port> cmd <tag>`.
(support part of #9899)
","['src/redis-cli.c', 'tests/integration/redis-cli.tcl']",redis-cli lacks the ability to provide multiple arguments from stdin and to handle the combination of `-x` and `--cluster` (or `-X` and `--cluster`) leading to limitations in command input from users.
f8ae991717f10c837c1a76b2954dae56ecb0e6bc,1605690981,"EXISTS should not alter LRU, OBJECT should not reveal expired keys on replica (#8016)

The bug was introduced by #5021 which only attempted avoid EXIST on an
already expired key from returning 1 on a replica.

Before that commit, dbExists was used instead of
lookupKeyRead (which had an undesired effect to ""touch"" the LRU/LFU)

Other than that, this commit fixes OBJECT to also come empty handed on
expired keys in replica.

And DEBUG DIGEST-VALUE to behave like DEBUG OBJECT (get the data from
the key regardless of it's expired state)","['src/db.c', 'src/debug.c', 'src/object.c', 'src/server.h', 'tests/unit/introspection-2.tcl']","Expired keys on a replica can still be revealed using OBJECT, while EXISTS inadvertently alters LRU/LFU, and DEBUG DIGEST-VALUE does not respect the expired state of a key."
b8eb2a73408fa3b8845760857dd6fcccb62107fe,1648729570,"Fix failing moduleconfigs tests and memory leak (#10501)

Fix global `strval` not reset to NULL after being freed, causing a crash on alpine
(most likely because the dynamic library loader doesn't init globals on reload)
By the way, fix the memory leak of using `RedisModule_Free` to free `RedisModuleString`,
and add a corresponding test.
","['tests/modules/moduleconfigs.c', 'tests/unit/moduleapi/moduleconfigs.tcl']",Freed `strval` not being reset leads to crashes on Alpine due to dynamic library loader not initializing globals on reload. There's also a memory leak issue from using `RedisModule_Free` to free `RedisModuleString`.
7d269d5e633911628625d279b54e48c8b38fec90,1527921136,"Fix DEBUG LOADAOF so that redis-server will not crash unexpectedly
and will not be inconsistent after we call debug loadaof.
Before this commit, there were 2 problems:

1, When appendonly is set to no and there is not a appendonly file,
   redis-server will crash if we call DEBUG LOADAOF.
2, When appendonly is set to no and there is a appendonly file,
   redis-server will hold different data after loading  appendonly
   file.
",['src/debug.c'],"Calling DEBUG LOADAOF when appendonly is set to 'no' leads to unexpected crashes or inconsistent data in redis-server, depending on the presence of an appendonly file."
a8c1253b6fd1d85ba33a8749e14e4fa515508df0,1638091989,"Fix Lua C API violation on lua msgpack lib. (#9832)

msgpack lib missed using lua_checkstack and so on rare
cases overflow the stack by at most 2 elements. This is a
violation of the Lua C API. Notice that Lua allocates
additional 5 more elements on top of lua->stack_last
so Redis does not access an invalid memory. But it is an
API violation and we should avoid it.

This PR also added a new Lua compilation option. The new
option can be enable using environment variable called
LUA_DEBUG. If set to `yes` (by default `no`), Lua will be
compiled without optimizations and with debug symbols (`-O0 -g`).
In addition, in this new mode, Lua will be compiled with the
`-DLUA_USE_APICHECK` flag that enables extended Lua C API
validations.

In addition, set LUA_DEBUG=yes on daily valgrind flow so we
will be able to catch Lua C API violations in the future.",['deps/lua/src/lua_cmsgpack.c'],"Lua msgpack library is violating the Lua C API by potentially overflowing the stack by up to 2 elements in rare cases, due to missing use of lua_checkstack."
cd6b3d558be0703b1a43f9fe58fd5f1ed7452829,1636628642,"Archive external redis log in external tests (#9765)

On test failure store the external redis server logs as CI artifacts so we can review them.

Write test name to server log for external server tests.
This is attempted and silently failed in case external server doesn't support it.
Note that in non-external server mode we use a more robust method of writing to the log which doesn't depend on the
server actually running/working. This isn't possible for externl servers and required for some complex tests which are
skipped in external mode anyway.

Cleanup: remove dup code.","['tests/support/test.tcl', 'tests/support/util.tcl']","In case of test failure in external server mode, Redis server logs are not being stored as CI artifacts for review. Also, cannot write test name to server log for external server tests."
ffcf7d5ab1e98d84c28af9bea7be76c6737820ad,1509439282,"Fix buffer overflows occurring reading redis.conf.

There was not enough sanity checking in the code loading the slots of
Redis Cluster from the nodes.conf file, this resulted into the
attacker's ability to write data at random addresses in the process
memory, by manipulating the index of the array. The bug seems
exploitable using the following techique: the config file may be altered so
that one of the nodes gets, as node ID (which is the first field inside the
structure) some data that is actually executable: then by writing this
address in selected places, this node ID part can be executed after a
jump. So it is mostly just a matter of effort in order to exploit the
bug. In practice however the issue is not very critical because the
bug requires an unprivileged user to be able to modify the Redis cluster
nodes configuration, and at the same time this should result in some
gain. However Redis normally is unprivileged as well. Yet much better to
have this fixed indeed.

Fix #4278.
",['src/cluster.c'],"Manipulation of the nodes.conf file index in Redis Cluster can lead to data being written at random addresses in the process memory, potentially allowing executable data to be run maliciously."
488aecb3abca3088735f1bffa60a74e22832d44b,1645598836,"Fix timing issue in EXEC fail on lazy expired WATCHed key test (#10332)

The test will fail on slow machines (valgrind or FreeBsd).
Because in #10256 when WATCH is called on a key that's already
logically expired, we will add an `expired` flag, and we will
skip it in `isWatchedKeyExpired` check.

Apparently we need to increase the expiration time so that
the key can not expire logically then the WATCH is called.
Also added retries to make sure it doesn't fail. I suppose
100ms is enough in valgrind, tested locally, no need to retry.",['tests/unit/multi.tcl'],Test fails on slow machines when WATCH is called on a logically expired key due to timing issues.
95360c2e0cb65956bfcd2c57fb909a9c93723e21,1615898999,"Fix issue where error replies are not counted on stats (#8659)

lookupKeyReadOrReply and lookupKeyWriteOrReply might decide to reply to the user
with the given robj reply. This reply might be an error reply and if so addReply
function is used instead of addReplyErrorObject which will cause the error reply not
to be counted on stats. The fix checks the first char in the reply and if its '-' (error)
it uses addReplyErrorObject.",['src/db.c'],Error replies to user are not being counted in stats due to misuse of addReply function over addReplyErrorObject in lookupKeyReadOrReply and lookupKeyWriteOrReply.
0c10f0e1c0815b9fc24a933e3dd4560f506e8313,1637251770,"Fix crashes when list-compress-depth is used. (#9779)

Recently we started using list-compress-depth in tests (was completely untested till now).
Turns this triggered test failures with the external mode, since the tests left the setting enabled
and then it was used in other tests (specifically the fuzzer named ""Stress tester for #3343-alike bugs"").

This PR fixes the issue of the `recompress` flag being left set by mistake, which caused the code to
later to compress the head or tail nodes (which should never be compressed)

The solution is to reset the recompress flag when it should have been (when it was decided not to compress).

Additionally we're adding some assertions and improve the tests so in order to catch other similar bugs.","['src/quicklist.c', 'tests/unit/type/list-3.tcl', 'tests/unit/type/list.tcl']","Usage of list-compress-depth in tests is causing failures in external mode, particularly with the ""Stress tester for #3343-alike bugs"" fuzzer, due to the `recompress` flag not being reset appropriately."
b541ccef25d20749c739b858ef8694512b7f3bce,1505899090,"PSYNC2: make persisiting replication info more solid

This commit is a reinforcement of commit c1c99e9.

1. Replication information can be stored when the RDB file is
generated by a mater using server.slaveseldb when server.repl_backlog
is not NULL, or set repl_stream_db be -1. That's safe, because
NULL server.repl_backlog will trigger full synchronization,
then master will send SELECT command to replicaiton stream.
2. Only do rdbSave* when rsiptr is not NULL,
if we do rdbSave* without rdbSaveInfo, slave will miss repl-stream-db.
3. Save the replication informations also in the case of
SAVE command, FLUSHALL command and DEBUG reload.
","['src/db.c', 'src/debug.c', 'src/rdb.c', 'src/replication.c']","Replication information storage issues could occur during the generation of RDB file by a master, especially when server.repl_backlog is not NULL. This could potentially lead to missing repl-stream-db for slaves. The problem also persists with SAVE, FLUSHALL commands and DEBUG reload."
1591e3479d46e7e21a98ae685fab2ccab076d094,1601401701,"TLS: Do not require CA config if not used. (#7862)

The tls-ca-cert or tls-ca-cert-dir configuration parameters are only
used when Redis needs to authenticate peer certificates, in one of these
scenarios:

1. Incoming clients or replicas, with `tls-auth-clients` enabled.
2. A replica authenticating the master's peer certificate.
3. Cluster nodes authenticating other nodes when establishing the bus
   protocol connection.",['src/tls.c'],"Redis requires CA configuration even in scenarios where it is not used, causing unnecessary configuration parameters for TLS."
adf2701cc95085a4a9980da7b7367b1836c5a0f2,1511450127,"fix string to double conversion, stopped parsing on \0 even if the string has more data.

getLongLongFromObject calls string2ll which has this line:
/* Return if not all bytes were used. */
so if you pass an sds with 3 characters ""1\01"" it will fail.

but getLongDoubleFromObject calls strtold, and considers it ok if eptr[0]==`\0`
i.e. if the end of the string found by strtold ends with null terminator

127.0.0.1:6379> set a 1
OK
127.0.0.1:6379> setrange a 2 2
(integer) 3
127.0.0.1:6379> get a
""1\x002""
127.0.0.1:6379> incrbyfloat a 2
""3""
127.0.0.1:6379> get a
""3""
","['src/object.c', 'tests/unit/type/incr.tcl']","String to double conversion function, strtold, incorrectly parses strings upon encountering '\0' character, even though the string contains more data."
c98a28a8480feeb00ccca80d23fcbe6d1718f6ed,1692611441,"Fix LREM count LONG_MIN overflow minor issue (#12465)

Limit the range of LREM count to -LONG_MAX ~ LONG_MAX.
Before the fix, passing -LONG_MAX would cause an overflow
and would effectively be the same as passing 0. (Because
this condition `toremove && removed == toremove `can never
be satisfied).

This is a minor fix as it shouldn't really affect users,
more like a cleanup.",['src/t_list.c'],"Passing -LONG_MAX to LREM count results in an overflow, causing it to effectively behave like a zero."
f11a2d4dd764c996b2d0c0cb5abde13f2445b40c,1635943106,"Fix COMMAND GETKEYS on EVAL without keys (#9733)

Add new no-mandatory-keys flag to support COMMAND GETKEYS of commands
which have no mandatory keys.

In the past we would have got this error:
```
127.0.0.1:6379> command getkeys eval ""return 1"" 0
(error) ERR Invalid arguments specified for command
```
","['src/module.c', 'src/server.c', 'src/server.h', 'tests/unit/introspection-2.tcl']","Using COMMAND GETKEYS on EVAL without keys returns an ""Invalid arguments specified for command"" error."
3fcf959e609e850a114d4016843e4c991066ebac,1493803581,"Modules TSC: Release the GIL for all the time we are blocked.

Instead of giving the module background operations just a small time to
run in the beforeSleep() function, we can have the lock released for all
the time we are blocked in the multiplexing syscall.
","['src/ae.c', 'src/ae.h', 'src/module.c', 'src/modules/helloblock.c', 'src/server.c', 'src/server.h']","GIL not released during module background operations' blocked time, causing limited operation time in the beforeSleep() function."
6b297cd64627a383ccb8d2d5a2ad736118a014ed,1635083564,"Improve errno reporting on fork and fopen rdbLoad failures (#9649)

I moved a bunch of stats in redisFork to be executed only on successful
fork, since they seem wrong to be done when it failed.
I guess when fork fails it does that immediately, no latency spike.","['src/rdb.c', 'src/replication.c', 'src/scripting.c', 'src/server.c']","When fork fails in redisFork, multiple inaccurate and potentially misleading stats get reported, despite the absence of any apparent latency spike."
6468cb2e825cf8258654f83e82324332e9879745,1512120264,"Streams: fix XREAD ready-key signaling.

With lists we need to signal only on key creation, but streams can
provide data to clients listening at every new item added.
To make this slightly more efficient we now track different classes of
blocked clients to avoid signaling keys when there is nobody listening.
A typical case is when the stream is used as a time series DB and
accessed only by range with XRANGE.
","['src/blocked.c', 'src/db.c', 'src/server.c', 'src/server.h', 'src/t_stream.c']","Blocking on Streams doesn't signal keys on the addition of new items, causing inefficiency with clients listening for these key signals, especially when stream is used as a time series DB."
e2d64485b8262971776fb1be803c7296c98d1572,1596942480,"Reduce the probability of failure when start redis in runtest-cluster #7554 (#7635)

When runtest-cluster, at first, we need to create a cluster use spawn_instance,
a port which is not used is choosen, however sometimes we can't run server on
the port. possibley due to a race with another process taking it first.
such as redis/redis/runs/896537490. It may be due to the machine problem or
In order to reduce the probability of failure when start redis in
runtest-cluster, we attemp to use another port when find server do not start up.

Co-authored-by: Oran Agra <oran@redislabs.com>
Co-authored-by: yanhui13 <yanhui13@meituan.com>",['tests/instances.tcl'],Redis server occasionally fails to start on a chosen port during runtest-cluster due to potential port conflicts with other processes.
29251f58e279738da6d45c5af3d12f35259f3d45,1542642087,"Streams: fix XREADGROUP history reading when CG last_id is low.

This fixes the issue reported in #5570.
This was fixed the hard way, that is, propagating more information to
the lower level API about this being a request to read just the history,
so that the code is simpler and less likely to regress.
",['src/t_stream.c'],Reading history with XREADGROUP in Streams generates an issue when consumer group last_id is at a low value.
e6fa47380a5274119ed37c7a5ea7455d4b7dbdcc,1606046451,"Fix bug with module GIL being released prematurely (#8061)

This is hopefully usually harmles.
The server.ready_keys will usually be empty so the code after releasing
the GIL will soon be done.
The only case where it'll actually process things is when a module
releases a client (or module) blocked on a key, by triggering this NOT
from within a command (e.g. a timer event).

This bug was introduced in redis 6.0.9, see #7903",['src/server.c'],"The GIL in module is being released prematurely, potentially causing issues when a module releases a client or module blocked on a key outside a command context."
7997874f4d2be9da1e26c77804569b0057c1e0a2,1678716749,"Fix tail->repl_offset update in feedReplicationBuffer (#11905)

In #11666, we added a while loop and will split a big reply
node to multiple nodes. The update of tail->repl_offset may
be wrong. Like before #11666, we would have created at most
one new reply node, and now we will create multiple nodes if
it is a big reply node.

Now we are creating more than one node, and the tail->repl_offset
of all the nodes except the last one are incorrect. Because we
update master_repl_offset at the beginning, and then use it to
update the tail->repl_offset. This would have lead to an assertion
during PSYNC, a test was added to validate that case.

Besides that, the calculation of size was adjusted to fix
tests that failed due to a combination of a very low backlog size,
and some thresholds of that get violated because of the relatively
high overhead of replBufBlock. So now if the backlog size / 16 is too
small, we'll take PROTO_REPLY_CHUNK_BYTES instead.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/replication.c', 'src/server.h', 'tests/integration/replication-buffer.tcl']","Splitting a large reply node into multiple nodes results in incorrect updates of 'tail->repl_offset' for all nodes except the last one, leading to assertion during PSYNC. Additionally, very low backlog size with high overhead of replBufBlock causes violation of thresholds."
19418b6b28b70818e67c50bbdfad8568da7c3458,1602657527,"Allow requirepass config to clear the password (#7899)

This is a compatibility issue with redis 5.0 that was introduced by ACL.
Before this commit, setting requirepass to an empty string will result
in a server that needs an empty AUTH, unlike redis 5.0 which would
accept connections without an AUTH.",['src/config.c'],"Setting requirepass to an empty string prompts an empty AUTH requirement, instead of enabling connections without an AUTH as in Redis 5.0."
6dd213558b0f77e2233743718e23556fb06a557e,1666460210,"Fix crash due to to reuse iterator entry after list deletion in module (#11383)

In the module, we will reuse the list iterator entry for RM_ListDelete, but `listTypeDelete` will only update
`quicklistEntry->zi` but not `quicklistEntry->node`, which will result in `quicklistEntry->node` pointing to
a freed memory address if the quicklist node is deleted. 

This PR sync `key->u.list.index` and `key->u.list.entry` to list iterator after `RM_ListDelete`.

This PR also optimizes the release code of the original list iterator.

Co-authored-by: Viktor Söderqvist <viktor@zuiderkwast.se>","['src/module.c', 'tests/modules/list.c', 'tests/unit/moduleapi/list.tcl']","Reuse of list iterator entry after list deletion in module causes crash, as it points to a freed memory address."
91011100ba57b138b729a819787eec75df27b844,1689478302,"Hide the comma after cport when there is no hostname. (#12411)

According to the format shown in https://redis.io/commands/cluster-nodes/
```
<ip:port@cport[,hostname[,auxiliary_field=value]*]>
```
when there is no hostname, and the auxiliary fields are hidden, the cluster topology should be
```
<ip:port@cport>
```
However in the code we always print the hostname even when it is an empty string, leaving an unnecessary comma tailing after cport, which is weird and conflicts with the doc.
```
94ca2f6cf85228a49fde7b738ee1209de7bee325 127.0.0.1:6379@16379, myself,master - 0 0 0 connected 0-16383
```
",['src/cluster.c'],"The current code prints an unnecessary comma after 'cport' even when the hostname is an empty string, resulting in conflict with https://redis.io/commands/cluster-nodes/ documentation."
7fa493912e5de72edd7b1720d1a7d1f4287998f8,1535726702,"After slave Lua script leaves busy state, re-process the master buffer.

Technically speaking we don't really need to put the master client in
the clients that need to be processed, since in practice the PING
commands from the master will take care, however it is conceptually more
sane to do so.
","['src/networking.c', 'src/scripting.c']","When a slave Lua script transitions out of a busy state, the master buffer is not appropriately re-processed."
fcb3dfe56d3e2e0732c6787fe11789252eaf9ba2,1610710436,"Rename non-API RM-prefixed functions to hide them from API docs

The prefix is changed from `RM_` to `module` on the following
internal functions, to prevent them from appearing in the API docs:

    RM_LogRaw -> moduleLogRaw
    RM_FreeCallReplyRec -> moduleFreeCallReplyRec
    RM_ZsetAddFlagsToCoreFlags -> moduleZsetAddFlagsToCoreFlags
    RM_ZsetAddFlagsFromCoreFlags -> moduleZsetAddFlagsFromCoreFlags
",['src/module.c'],Certain internal functions with `RM_` prefix are displaying in the API documentation unintentionally.
526cbb5cff5fec072069f66d874cb6e983f7c539,1635843851,"Fix not updating backlog histlen when trimming repl backlog (#9713)

Since the loop in incrementalTrimReplicationBacklog checks the size of histlen,
we cannot afford to update it only when the loop exits, this may cause deleting
much more replication blocks, and replication backlog may be less than setting size.

introduce in #9166 

Co-authored-by: sundb <sundbcn@gmail.com>",['src/replication.c'],"Updating histlen only after exiting the loop in incrementalTrimReplicationBacklog can cause deletion of extra replication blocks, resulting in a replication backlog smaller than the set size."
aa2403ca98f6a39b6acd8373f8de1a7ba75162d5,1679809618,"Fix redis-cli cluster test timing issue (#11887)

This test fails sporadically:
```
*** [err]: Migrate the last slot away from a node using redis-cli in tests/unit/cluster/cli.tcl
cluster size did not reach a consistent size 4
```

I guess the time (5s) of wait_for_cluster_size is not enough,
usually, the waiting time for our other tests for cluster
consistency is 50s, so also changing it to 50s.",['tests/support/cluster_util.tcl'],"The redis-cli cluster test is sporadically failing due to inconsistency in cluster size, potentially due to timed out wait_for_cluster_size."
3abdec9969b85fd28ddfa1fbe9730172f5b78b68,1658041054,"Fix cluster hostnames test causing failover while running valgrind (#10991)

In the newly added cluster hostnames test, the primary is failing over during the reboot
for valgrind so we are validating the wrong node. This change just sets the replica to
prevent taking over, which seems to fix the test.

We could have also set the timeout higher, but it slows down the test.",['tests/unit/cluster/hostnames.tcl'],The current cluster hostnames test results in the wrong node being validated due to an unexpected failover of the primary during a valgrind reboot.
51887e61b8a0ae36fa977351f34ace3e38efbc29,1668939135,"sanitize dump payload: fix crash with empty set with listpack encoding (#11519)

The following example will create an empty set (listpack encoding):
```
> RESTORE key 0
""\x14\x25\x25\x00\x00\x00\x00\x00\x02\x01\x82\x5F\x37\x03\x06\x01\x82\x5F\x35\x03\x82\x5F\x33\x03\x00\x01\x82\x5F\x31\x03\x82\x5F\x39\x03\x04\xA9\x08\x01\xFF\x0B\x00\xA3\x26\x49\xB4\x86\xB0\x0F\x41""
OK
> SCARD key
(integer) 0
> SRANDMEMBER key
Error: Server closed the connection
```

In the spirit of #9297, skip empty set when loading RDB_TYPE_SET_LISTPACK.
Introduced in #11290","['src/rdb.c', 'src/rdb.h', 'tests/integration/corrupt-dump-fuzzer.tcl', 'tests/integration/corrupt-dump.tcl']",Creating an empty set with listpack encoding using RESTORE command results in server connection closure when SRANDMEMBER is executed.
b7559d9f3b5e6ebb3fe3ad4570e3313ccaffa89e,1688377518,"Fix possible crash in command getkeys  (#12380)

When getKeysUsingKeySpecs processes a command with more than one key-spec,
and called with a total of more than 256 keys, it'll call getKeysPrepareResult again,
but since numkeys isn't updated, getKeysPrepareResult will not bother to copy key
names from the old result (leaving these slots uninitialized). Furthermore, it did not
consider the keys it already found when allocating more space.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/db.c', 'tests/unit/introspection-2.tcl']","Command 'getKeysUsingKeySpecs' crashes when processing a command with more than one key-spec and more than 256 keys, due to uninitialized slots and improper space allocation."
bd0f06c18ccea62cb5e0fff018a5eaa876d3f90e,1563468705,"RDB: handle encoding errors with rdbExitReportCorruptRDB().

Without such change, the diskless replicas, when loading RDB files from
the socket will not abort when a broken RDB file gets loaded. This is
potentially unsafe, because right now Redis is not able to guarantee
that encoding errors are safe from the POV of memory corruptions (for
instance the LZF library may not be safe against untrusted data?) so
better to abort when the RDB file we are going to load is corrupted.

Instead I/O errors are still returned to the caller without aborting,
so that in case of short read the diskless replica can try again.
",['src/rdb.c'],"Diskless replicas currently don't abort when loading a corrupted RDB file, potentially leading to unsafe conditions related to memory corruptions."
eacccd2acbbebde3e50e14a4d0ade5b1ccc2bc07,1611998338,"fix sentinel tests error (#8422)

This commit fixes sentinel announces hostnames test error in certain linux environment
Before this commit, we only check localhost is resolved into 127.0.0.1, however in ubuntu
or some other linux environments ""localhost"" will be resolved into ::1 ipv6 address first if
the network stack is capable.",['tests/sentinel/tests/08-hostname-conf.tcl'],Test failure occurs while announcing hostnames as 'localhost' resolves to ::1 IPv6 address instead of expected 127.0.0.1 in certain Linux environments.
1ef014ee6b0ca385451cc74e3cb2da9048975c7e,1596886601,"Fix applying zero offset to null pointer when creating moduleFreeContextReusedClient (#7323)

Before this fix we where attempting to select a db before creating db the DB, see: #7323

This issue doesn't seem to have any implications, since the selected DB index is 0,
the db pointer remains NULL, and will later be correctly set before using this dummy
client for the first time.

As we know, we call 'moduleInitModulesSystem()' before 'initServer()'. We will allocate
memory for server.db in 'initServer', but we call 'createClient()' that will call 'selectDb()'
in 'moduleInitModulesSystem()', before the databases where created. Instead, we should call
'createClient()' for moduleFreeContextReusedClient after 'initServer()'.","['src/module.c', 'src/server.c', 'src/server.h']",Attempting to select a database before its creation when initializing the module system results in a null pointer getting a zero offset.
9f3b410050d998110ae91c2ac49f832ce0ba2629,1655056391,"Correctly check for vm.overcommit_memory == 0 (#10841)

A regression caused by #10636 (released in 7.0.1) causes Redis startup warning about overcommit to be missing when needed and printed when not. 

Also, using atoi() to convert the string's value to an integer cannot discern
between an actual 0 (zero) having been read, or a conversion error. 
",['src/syscheck.c'],"Redis startup warning about overcommit is incorrectly displayed due to a regression, and atoi() function fails to distinguish between actual 0 and a conversion error."
569a3f4548cc525aac8bd246da213eb48a17f0e1,1617254415,"Use chi-square for random distributivity verification in test (#8709)

Problem:
Currently, when performing random distribution verification, we determine
the probability of each element occurring in the sum, but the probability is
only an estimate, these tests had rare sporadic failures, and we cannot verify
what the probability of failure will be.

Solution:
Using the chi-square distribution instead of the original random distribution
validation makes the test more reasonable and easier to find problems.","['tests/support/util.tcl', 'tests/unit/type/hash.tcl', 'tests/unit/type/set.tcl', 'tests/unit/type/zset.tcl']","Sporadic failures in tests during random distribution verification due to reliance on estimated probabilities, uncertainty over failure probability."
d7920ff9b16b1b3cec5838581d8499d0d50dc935,1618351085,"Modules API docs: Sections and links (#8442)

* Modules API docs: Link API function names to their definitions

Occurrences of API functions are linked to their definition.

A function index with links to all functions is added on the bottom
of the page.

Comment blocks in module.c starting with a markdown h2 heading are
used as sections. A table of contents is generated from these
headings.

The functions names are changed from h2 to h3, since they are now
rendered as sub-headings within each section.

Existing sections in module.c are used with some minor changes.
Some documentation text is added or sligtly modified.

The markdown renderer will add IDs which may clash with our
generated IDs. By prefixing section IDs with ""section-"" we make
them different.

Replace double dashes with a unicode long ndash
","['src/module.c', 'src/modules/gendoc.rb']","The Modules API documentation lacks proper organization and hyperlinks, making it difficult to navigate between API function definitions and their occurrences. Additionally, potential ID clashes arise due to the markdown renderer generating IDs that might be identical to our generated IDs."
c3b9f2fbd9085f4c9ea3151cc88d3b31c0d30b91,1679465840,"Allow clients to report name and version (#11758)

This PR allows clients to send information about the client library to redis
to be displayed in CLIENT LIST and CLIENT INFO.

Currently supports:
`CLIENT [lib-name | lib-ver] <value>`
Client libraries are expected to pipeline these right after AUTH, and ignore
the failure in case they're talking to an older version of redis.

These will be shown in CLIENT LIST and CLIENT INFO as:
* `lib-name` - meant to hold the client library name.
* `lib-ver` - meant to hold the client library version.

The values cannot contain spaces, newlines and any wild ASCII characters,
but all other normal chars are accepted, e.g `.`, `=` etc (same as CLIENT NAME).

The RESET command does NOT clear these, but they can be cleared to the
default by sending a command with a blank string.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/commands.c', 'src/networking.c', 'src/server.h', 'tests/unit/introspection.tcl']","Redis client is unable to report its library name and version to the server, hindering optimized integration and detailed client information management."
d10b2f3173ebfcc673d4ec36efef512802e3a535,1597318251,"wait command optimization (#7333)

Client that issued WAIT last will most likely have the highest replication offset, so imagine a probably common case where all clients are waiting for the same number of replicas. we prefer the loop to start from the last client (one waiting for the highest offset), so that the optimization in the function will call replicationCountAcksByOffset for each client until it found a good one, and stop calling it for the rest of the clients.
the way the loop was implemented would mean that in such case it is likely to call replicationCountAcksByOffset for all clients.

Note: the change from > to >= is not directly related to the above.

Co-authored-by: 曹正斌 <caozb@jiedaibao.com>",['src/replication.c'],"When all clients are waiting for the same number of replicas, the function `replicationCountAcksByOffset` is unnecessarily called for each client in a loop, rather than being optimized to stop after a matching client is found."
d506334b678ea1f0ad2f28b1144365ac7751999a,1533212153,"Test: new sorted set skiplist order consistency.

This should be able to find new bugs and regressions about the new
sorted set update function when ZADD is used to update an element
already existing.

The test is able to find the bug fixed at 2f282aee immediately.
",['tests/unit/type/zset.tcl'],Use of ZADD to update an element in the new sorted set update function causing order inconsistencies and possible regressions.
06b577aad01d132b0792581a3f1235bc84a32026,1669633435,"Fix replication on expired key test timing issue, give it more chances (#11548)

In replica, the key expired before master's `INCR` was arrived, so INCR
creates a new key in the replica and the test failed.
```
*** [err]: Replication of an expired key does not delete the expired key in tests/integration/replication-4.tcl
Expected '0' to be equal to '1' (context: type eval line 13 cmd {assert_equal 0 [$slave exists k]} proc ::test)
```

This test is very likely to do a false positive if the `wait_for_ofs_sync`
takes longer than the expiration time, so give it a few more chances.

The test was introduced in #9572.",['tests/integration/replication-4.tcl'],"Key expiration in replica before Master's `INCR` operation leads to test failure in replication integration, with potential false positives if `wait_for_ofs_sync` exceeds expiration time."
264953871b97bcc3d663cd006578c151a13b48ca,1610437317,"Fix cluster diskless load swapdb test (#8308)

The test was trying to wait for the replica to start loading the rdb
from the master before it kills the master, but it was actually waiting
for ROLE to be in ""sync"" mode, which corresponds to REPL_STATE_TRANSFER
that starts before the actual loading starts.
now instead it waits for the loading flag to be set.

Besides, the test was dependent on the previous configuration of the
servers, relying on the fact the replica is configured to persist
(either RDB of AOF), now it is set explicitly.","['tests/cluster/cluster.tcl', 'tests/cluster/tests/17-diskless-load-swapdb.tcl']","'Replica server starts loading rdb from master prematurely due to incorrect synchronization check, additionally, the test module assumes server persistence configuration incorrectly. '"
38e284f1064256586a18d9a12fe8a22826866537,1684740434,"optimize spopwithcount propagation (#12082)

A single SPOP with command with count argument resulted in many SPOP
commands being propagated to the replica.
This is inefficient because the key name is repeated many times, and is also
being looked-up many times.
also it results in high QPS metrics on the replica.
To solve that, we flush batches of 1024 fields per SPOP command.

Co-authored-by: zhaozhao.zz <zhaozhao.zz@alibaba-inc.com>
","['src/t_set.c', 'tests/integration/replication-4.tcl']","The use of SPOP command with a count argument leads to a multitude of SPOP commands propagated to the replica, causing inefficiency due to repeated key name lookups and high QPS metrics on the replica."
45b8eea19f3e2491dec669f0745e513a4c9d7329,1681200856,"Add ZREMRANGEBYLEX basics tests to fix reply-schemas daily (#12021)

We do have ZREMRANGEBYLEX tests, but it is a stress test
marked with slow tag and then skipped in reply-schemas daily.

In the past, we were able to succeed on a daily, i guess
it was because there were some random command executions,
such as corrupt-dump-fuzzy, which might call it.

These test examples are taken from ZRANGEBYLEX basics test.",['tests/unit/type/zset.tcl'],"The ZREMRANGEBYLEX basic tests are not executing in the reply-schemas daily due to a slow tag marker, potentially missing detection of related issues."
eaa7a7bb93c1ac6dbf347e1c29ac719a12a75158,1623066216,"Fix XTRIM or XADD with LIMIT may delete more entries than Count. (#9048)

The decision to stop trimming due to LIMIT in XADD and XTRIM was after the limit was reached.
i.e. the code was deleting **at least** that count of records (from the LIMIT argument's perspective, not the MAXLEN),
instead of **up to** that count of records.
see #9046","['src/t_stream.c', 'tests/unit/type/stream.tcl']",XTRIM or XADD commands with LIMIT functionality result in deleting more entries than specified by the Count parameter.
d5935bb0a46ba397781affda794e10a307db649b,1617256865,"generalize config file check for sentinel (#8730)

The implications of this change is just that in the past when a config file was missing,
in some cases it was exiting before printing the sever startup prints and sometimes after,
and now it'll always exit before printing them.","['src/sentinel.c', 'src/server.c', 'src/server.h']","Inconsistent behavior of the system when configuration file is absent; sometimes terminates before, and sometimes after server startup prints."
a6b3ce28a8050b257659db479a3c8f1d38edbfd1,1650791790,"Fix timing issue in slowlog redact test (#10614)

* Fix timing issue in slowlog redact test

This test failed once in my daily CI (test-sanitizer-address (clang))
```
*** [err]: SLOWLOG - Some commands can redact sensitive fields in tests/unit/slowlog.tcl
Expected 'migrate 127.0.0.1 25649 key 9 5000 AUTH2 (redacted) (redacted)' to match '* key 9 5000 AUTH (redacted)' (context: type eval line 12 cmd {assert_match {* key 9 5000 AUTH (redacted)} [lindex [lindex [r slowlog get] 1] 3]} proc ::test)
```

The reason is that with slowlog-log-slower-than 10000,
slowlog get will have a chance to exceed 10ms.

Change slowlog-log-slower-than from 10000 to -1, disable it.
Also handles a same potentially problematic test above.
This is actually the same timing issue as #10432.

But also avoid repeated calls to `SLOWLOG GET`",['tests/unit/slowlog.tcl'],Slowlog redact test occasionally fails in daily continuous integration due to timing issue with `slowlog-log-slower-than` exceeding 10ms.
d0cc3de73f91ca79b2343e73e640b40709cfcaf5,1672899717,"Fix issues with listpack encoded set (#11685)

PR #11290 added listpack encoding for sets, but was missing two things:
1. Correct handling of MEMORY USAGE (leading to an assertion).
2. Had an uncontrolled scratch buffer size in SRANDMEMBER leading to
   OOM panic (reported in #11668). Fixed by copying logic from ZRANDMEMBER.

note that both issues didn't exist in any redis release.","['src/object.c', 'src/t_set.c']",Issues reported with listpack encoding for sets; it's causing assertion due to incorrect handling of MEMORY USAGE and uncontrolled scratch buffer size in SRANDMEMBER leading to OOM panic.
b59bb9b476cbcc9ed321dc5172be3ba2e0738142,1645429045,"Fix script active defrag test (#10318)

This includes two fixes:
* We forgot to count non-key reallocs in defragmentation stats.
* Fix the script defrag tests so to make dict entries less signigicant in fragmentation by making the scripts larger.
This assures active defrage will complete and reach desired results.
Some inherent fragmentation might exists in dict entries which we need to ignore.
This lead to occasional CI failures.","['src/commands.c', 'src/defrag.c', 'tests/unit/memefficiency.tcl']","Active defragmentation tests fail occasionally due to inherent fragmentation in dict entries not being considered, and non-key reallocs are missing in defragmentation stats."
dcbfcb916ca1a269b3feef86ee86835294758f84,1677590126,"String pattern matching had exponential time complexity on pathological patterns (CVE-2022-36021) (#11858)

Authenticated users can use string matching commands with a
specially crafted pattern to trigger a denial-of-service attack on Redis,
causing it to hang and consume 100% CPU time.

Co-authored-by: Tom Levy <tomlevy93@gmail.com>","['src/util.c', 'tests/unit/keyspace.tcl']",Pathological patterns in string matching commands cause a denial-of-service attack on Redis by triggering excessive CPU usage and system hang.
62b1f32062b8f688179a8262959a5b80d0ad4de7,1612709711,"Optimize HRANDFIELD and ZRANDMEMBER case 4 when ziplist encoded (#8444)

It is inefficient to repeatedly pick a single random element from a
ziplist.
For CASE4, which is when the user requested a low number of unique
random picks from the collectoin, we used thta pattern.

Now we use a different algorithm that picks unique elements from a
ziplist, and guarentee no duplicate but doesn't provide random order
(which is only needed in the non-unique random picks case)

Unrelated changes:
* change ziplist count and indexes variables to unsigned
* solve compilation warnings about uninitialized vars in gcc 10.2

Co-authored-by: xinluton <xinluton@qq.com>","['src/t_hash.c', 'src/t_zset.c', 'src/ziplist.c', 'src/ziplist.h']","Inefficiency when randomly selecting a low number of unique elements from a ziplist in HRANDFIELD and ZRANDMEMBER case 4. Also, compilation warnings about uninitialized variables in gcc 10.2."
a3778f3b0f0aacbb34ecc77541615a9eda251443,1500556655,"Make representClusterNodeFlags() more robust.

This function failed when an internal-only flag was set as an only flag
in a node: the string was trimmed expecting a final comma before
exiting the function, causing a crash. See issue #4142.
Moreover generation of flags representation only needed at DEBUG log
level was always performed: a waste of CPU time. This is fixed as well
by this commit.
",['src/cluster.c'],"The function representClusterNodeFlags() crashes when an internal-only flag is set as the only flag in a node. Also, unnecessary flag representations consume extra CPU time even at non-DEBUG log levels."
8bdcbbb085888d98b765cc7899009670e019b9d0,1598515772,"Update memory metrics for INFO during loading (#7690)

During a long AOF or RDB loading, the memory stats were not updated, and
INFO would return stale data, specifically about fragmentation and RSS.
In the past some of these were sampled directly inside the INFO command,
but were moved to cron as an optimization.

This commit introduces a concept of loadingCron which should take
some of the responsibilities of serverCron.
It attempts to limit it's rate to approximately the server Hz, but may
not be very accurate.

In order to avoid too many system call, we use the cached ustime, and
also make sure to update it in both AOF loading and RDB loading inside
processEventsWhileBlocked (it seems AOF loading was missing it).","['src/aof.c', 'src/module.c', 'src/networking.c', 'src/rdb.c', 'src/server.c', 'src/server.h']","During a lengthy AOF or RDB load, memory statistics aren't refreshed, leading to outdated information being returned by the INFO command, particularly regarding fragmentation and RSS."
0e22cb2680db9f87fd232bc54419d538629edc2d,1585323303,"Precise timeouts: cleaup the table on unblock.

Now that this mechanism is the sole one used for blocked clients
timeouts, it is more wise to cleanup the table when the client unblocks
for any reason. We use a flag: CLIENT_IN_TO_TABLE, in order to avoid a
radix tree lookup when the client was already removed from the table
because we processed it by scanning the radix tree.
","['src/blocked.c', 'src/server.c', 'src/server.h']","Blocked client timeouts solely using this mechanism fails to cleanup the table upon client unblocks, resulting in redundant radix tree lookups."
a9c06021498444b8d785066a2fae7d49693721dc,1603803858,"Disable THP if enabled (#7381)

In case redis starts and find that THP is enabled (""always""), instead
of printing a log message, which might go unnoticed, redis will try to
disable it (just for the redis process).

Note: it looks like on self-bulit kernels THP is likely be set to ""always"" by default.

Some discuss about THP side effect on Linux:
according to http://www.antirez.com/news/84, we can see that
redis latency spikes are caused by linux kernel THP feature.
I have tested on E3-2650 v3, and found that 2M huge page costs
about 0.25ms to fix COW page fault.

Add a new config 'disable-thp', the recommended setting is 'yes',
(default) the redis tries to disable THP by prctl syscall. But
users who really want THP can set it to ""no""

Thanks to Oran & Yossi for suggestions.

Signed-off-by: zhenwei pi <pizhenwei@bytedance.com>","['src/config.c', 'src/latency.c', 'src/latency.h', 'src/server.c', 'src/server.h']","Redis may face latency issues if Transparent Huge Pages (THP) feature is enabled on Linux, especially with 'always' setting in self-built kernels, resulting in significant spikes and increased time to fix Copy-On-Write page faults."
ee220599b0481cf4b10e8a5293691258fd6f35a6,1650442281,"Fixes around clients that must be obeyed. Replica report disk errors in PING. (#10603)

This PR unifies all the places that test if the current client is the
master client or AOF client, and uses a method to test that on all of
these.

Other than some refactoring, these are the actual implications:
- Replicas **don't** ignore disk error when processing commands not
  coming from their master.
  **This is important for PING to be used for health check of replicas**
- SETRANGE, APPEND, SETBIT, BITFIELD don't do proto_max_bulk_len check for AOF
- RM_Call in SCRIPT_MODE ignores disk error when coming from master /
  AOF
- RM_Call in cluster mode ignores slot check when processing AOF
- Scripts ignore disk error when processing AOF
- Scripts **don't** ignore disk error on a replica, if the command comes
  from clients other than the master
- SCRIPT KILL won't kill script coming from AOF
- Scripts **don't** skip OOM check on replica if the command comes from
  clients other than the master

Note that Script, AOF, and module clients don't reach processCommand,
which is why some of the changes don't actually have any implications.

Note, reverting the change done to processCommand in 2f4240b9d9
should be dead code due to the above mentioned fact.","['src/bitops.c', 'src/module.c', 'src/script.c', 'src/server.c', 'src/server.h', 'src/t_stream.c', 'src/t_string.c', 'src/timeout.c']","Replicas inaccurately handle disk errors, potentially causing incorrect processing of commands and PING health checks. Various commands also may not appropriately check for disk errors and memory limitations."
62adabd0e0935cbb1cdedb3cde012c50ad1356a1,1582455952,"Fix latency sensitivity of new defrag test

I saw that the new defag test for list was failing in CI recently, so i
reduce it's threshold from 12 to 60.

besides that, i add / improve the latency test for that other two defrag
tests (add a sensitive latency and digest / save checks)

and fix bad usage of debug populate (can't overrides existing keys).
this was the original intention, which creates higher fragmentation.
",['tests/unit/memefficiency.tcl'],"New defrag test for list failing in CI due to high threshold and misuse of debug populate overriding existing keys, resulting in increased fragmentation. Also, latency test needs improvement."
39b0f0dd73110989ee3ccdba3c202565dac41c7b,1623073678,"Add support for combining NX and GET flags on SET command (#8906)

Till now GET and NX were mutually exclusive.
This change make their combination mean a ""Get or Set"" command.

If the key exists it returns the old value and avoids setting,
and if it does't exist it returns nil and sets it to the new value (possibly with expiry time)","['src/t_string.c', 'tests/unit/type/string.tcl']","Mutually exclusive GET and NX flags on SET command limit flexibility, preventing a ""Get or Set"" response based on the key's existence."
bfa3931a04bddec8eb37c91c25d3a77c70e33000,1695909953,"WAITAOF: Update fsynced_reploff_pending just before starting the initial AOFRW fork (#12620)

If we set `fsynced_reploff_pending` in `startAppendOnly`, and the fork doesn't start
immediately (e.g. there's another fork active at the time), any subsequent commands
will increment `server.master_repl_offset`, but will not cause a fsync (given they were
executed before the fork started, they just ended up in the RDB part of it)
Therefore, any WAITAOF will wait on the new master_repl_offset, but it will time out
because no fsync will be executed.

Release notes:
```
WAITAOF could timeout in the absence of write traffic in case a new AOF is created and
an AOFRW can't immediately start.
This can happen by the appendonly config is changed at runtime, but also after FLUSHALL,
and replica full sync.
```","['src/aof.c', 'tests/unit/wait.tcl']","`fsynced_reploff_pending` setting in `startAppendOnly` leads WAITAOF to timeout if a new AOF is created and an AOFRW can't immediately start, as no fsync occurs for subsequent commands executed before the fork begins."
463ccf86642ae35e18cf0c84be4e8e9e7c905c70,1557158571,"Threaded IO: logging should be safe in I/O threads.

Potentially it is possible that we get interleaved writes, even if
serverLog() makes sure to write into a buffer and then use printf(), so
even this should be ok. However in general POSIX guarantees that writing
to the same file pointer object from multiple threads is safe. Anyway
currently we *reopen* the file at each call, but for the standard output
logging.

The logging functions actually also access global configuration while
performing the log (for instance in order to check the log level, the
log filename and so forth), however dunring the I/O threads execution
we cannot alter such shared state in any way.
",['src/networking.c'],Interleaved writes might be occurring even when serverLog() writes into a buffer and uses printf(). Logging functions accessing global configuration during I/O threads execution could potentially alter the shared state.
5ce64ab01011781c426074f3fa7398260cce94cc,1658201713,"Fix timing issue in cluster test (#11008)

A timing issue like this was reported in freebsd daily CI:
```
*** [err]: Sanity test push cmd after resharding in tests/unit/cluster/cli.tcl
Expected 'CLUSTERDOWN The cluster is down' to match '*MOVED*'
```

We additionally wait for each node to reach a consensus on the cluster
state in wait_for_condition to avoid the cluster down error.

The fix just like #10495, quoting madolson's comment:
Cluster check just verifies the the config state is self-consistent,
waiting for cluster_state to be okay is an independent check that all
the nodes actually believe each other are healthy.

At the same time i noticed that unit/moduleapi/cluster.tcl has an exact
same test, may have the same problem, also modified it.","['tests/unit/cluster/cli.tcl', 'tests/unit/moduleapi/cluster.tcl']","Cluster tests are experiencing timing issues resulting in 'CLUSTERDOWN The cluster is down' errors, potentially due to nodes not reaching a consensus on the cluster state. The same issue may be present in unit/moduleapi/cluster.tcl."
323019992053cda5f540dfd8a318b3dfebe79a1b,1687464916,"Modules: Unblock from within a timer coverage (#12337)

Apart from adding the missing coverage, this PR also adds `blockedBeforeSleep`
that gathers all block-related functions from `beforeSleep`

The order inside `blockedBeforeSleep` is different: now `handleClientsBlockedOnKeys`
(which may unblock clients) is called before `processUnblockedClients` (which handles
unblocked clients).
It makes sense to have this order.

There are no visible effects of the wrong ordering, except some cleanups of the now-unblocked
client would have  happen in the next `beforeSleep` (will now happen in the current one)

The reason we even got into it is because i triggers an assertion in logresreq.c (breaking
the assumption that `unblockClient` is called **before** actually flushing the reply to the socket):
`handleClientsBlockedOnKeys` is called, then it calls `moduleUnblockClientOnKey`, which calls
`moduleUnblockClient`, which adds the client to `moduleUnblockedClients` back to `beforeSleep`,
we call `handleClientsWithPendingWritesUsingThreads`, it writes the data of buf to the client, so
`client->bufpos` became 0
On the next `beforeSleep`, we call `moduleHandleBlockedClients`, which calls `unblockClient`,
which calls `reqresAppendResponse`, triggering the assert. (because the `bufpos` is 0) - see https://github.com/redis/redis/pull/12301#discussion_r1226386716","['src/blocked.c', 'src/server.c', 'src/server.h', 'tests/modules/blockedclient.c', 'tests/modules/blockonkeys.c', 'tests/unit/moduleapi/blockedclient.tcl', 'tests/unit/moduleapi/blockonkeys.tcl']","The order of functions within `blockedBeforeSleep` leads to clients being unblocked after they are processed. This can lead to an assertion being triggered in logresreq.c, considering the `unblockClient` function is called after flushing the reply to the socket."
35c2ee8716dc9b1d4edbbb409815a585af491335,1656400277,"Add sharded pubsub keychannel count to client info (#10895)

When calling CLIENT INFO/LIST, and in various debug prints, Redis is printing
the number of pubsub channels / patterns the client is subscribed to.
With the addition of sharded pubsub, it would be useful to print the number of
keychannels the client is subscribed to as well.

","['src/networking.c', 'tests/unit/introspection.tcl']",Current CLIENT INFO/LIST and debug prints in Redis don't include information on the number of keychannels a client is subscribed to in the context of sharded pubsub.
73951abe7b86a565e3123fc1b8033efdcd1c0c04,1640952484,"Fix when the master connection is disconnected, replication retry read indefinitely (#10032)

Now if redis is still loading when we receive sigterm, we will wait for the loading to reach the event
loop (once in 2mb) before actually shutting down. See #10003.

This change caused valgrind CI to fail.
See https://github.com/redis/redis/runs/4662901673?check_suite_focus=true

This pr is mainly to solve the problem that redis process cannot be exited normally.
When the master is disconnected, if repl is processing diskless loading and using `connRead` to read data from master,
it may enter an infinite retry state, which does not handle `connRead` returning 0(master connection disconnected).",['src/rio.c'],"Redis process cannot exit normally during diskless loading replication. This is due to an infinite retry state when the master connection is disconnected and `connRead` returns 0, ignoring the disconnection event."
91f4bdc9f9d80ba7431f093cb58b6c49f4021d0b,1572518107,"Modules: block on keys: use a better interface.

Using the is_key_ready() callback plus the reply callback later, creates
different issues AFAIK:

1. More complex API.
2. We need to call the reply callback() ASAP if the is_key_ready()
interface returned success, however the internals do not work in that
way, so when the reply callback is called the setup could be different.
To fix that, there is to break the current design that handles the
unblocked clients asyncrhonously, and run the list ASAP.
","['src/blocked.c', 'src/module.c', 'src/modules/hellotype.c', 'src/redismodule.h', 'src/server.h']",The current module interface with is_key_ready() callback and later reply callback leads to increased API complexity and potential anomalies due to asynchronous handling of unblocked clients.
452ad2e928524cfe42856e869effd2d8b37ae280,1512063506,"PSYNC2: just store script bodies into RDB.

Related to #4483. As suggested by @soloestoy, we can retrieve the SHA1
from the body. Given that in the new implementation using AUX fields we
ended copying around a lot to create new objects and strings, extremize
such concept and trade CPU for space inside the RDB file.
","['src/rdb.c', 'src/scripting.c']","Current implementation trades CPU for space inside the RDB file, resulting in excessive creation of new objects and strings when using AUX fields. SHA1 retrieval from the script body is also affected."
8b70cb0ef8e654d09d0d2974ad72388f46be9fde,1600254902,"bio: fix doFastMemoryTest.

If one thread got SIGSEGV, function sigsegvHandler() would be triggered,
it would call bioKillThreads(). But call pthread_cancel() to cancel itself
would make it block. Also note that if SIGSEGV is caught by bio thread, it
should kill the main thread in order to give a positive report.
","['src/bio.c', 'src/debug.c', 'src/server.c', 'src/server.h']","SIGSEGV handling in bio thread causes blocking due to attempted self-cancellation, and lacks functionality for terminating the main thread for proper error reporting."
9e1a00d663c9212e6e992feb690089a5eac686f3,1671711794,"Fix race in PSYNC2 partial resync test (#11653)

This test sometimes fails:
```
*** [err]: PSYNC2: Partial resync after Master restart using RDB aux fields with expire in tests/integration/psync2-master-restart.tcl
Expected [status ::redis::redisHandle24 sync_partial_ok] == 1 (context: type eval line 49 cmd {assert {[status $replica sync_partial_ok] == 1}} proc ::test)
```

This is because the default repl-timeout value is 10s, sometimes the test
got timeout, then it will do a reconnect, it will incr the sync_partial_ok
counter, and then cause the test to fail.In this fix, we set the repl-timeout
to a very large number to make sure we won't get the timeout.",['tests/integration/psync2-master-restart.tcl'],The PSYNC2 partial resync test occasionally fails due to a race condition caused by the default repl-timeout value of 10s triggering a reconnect operation.
f57e844b2edbb86a5df2f3436045814812c0a3ae,1595330234,"Tests: drop TCL 8.6 dependency. (#7548)

This re-implements the redis-cli --pipe test so it no longer depends on a close feature available only in TCL 8.6.

Basically what this test does is run redis-cli --pipe, generates a bunch of commands and pipes them through redis-cli, and inspects the result in both Redis and the redis-cli output.

To do that, we need to close stdin for redis-cli to indicate we're done so it can flush its buffers and exit. TCL has bi-directional channels can only offers a way to ""one-way close"" a channel with TCL 8.6. To work around that, we now generate the commands into a file and feed that file to redis-cli directly.

As we're writing to an actual file, the number of commands is now reduced.",['tests/integration/redis-cli.tcl'],'redis-cli --pipe test relies on a close feature available only in TCL 8.6 resulting in unnecessary dependency for the entire test suite.'
38ed6c600791f266cea1ba66f53674c605b7babb,1647238977,"improve string2ll() to avoid extra conversion for long integer string. (#10408)

For an integer string like ""123456789012345678901"" which could cause
overflow-failure in string2ll() conversion, we could compare its length at
the beginning to avoid extra work.

* move LONG_STR_SIZE to be in declared in util.h, next to MAX_LONG_DOUBLE_CHARS","['src/server.h', 'src/util.c', 'src/util.h']","Overflow-failure in string2ll() conversion when handling long integer strings like ""123456789012345678901"", causing unnecessary additional computation."
a176cb56a3c0235adddde33fcbaee2369a5af73e,1594747319,"diskless master disconnect replicas when rdb child failed (#7518)

in case the rdb child failed, crashed or terminated unexpectedly redis
would have marked the replica clients with repl_put_online_on_ack and
then kill them only after a minute when no ack was received.

it would not stream anything to these connections, so the only effect of
this bug is a delay of 1 minute in the replicas attempt to re-connect.",['src/replication.c'],"If the rdb child fails or terminates unexpectedly, Redis delay the disconnection of replica clients for one minute without successfully acknowledging or streaming any data to these connections."
9e56d3969a3d19ef73d7042f91ba34bef39ed189,1611676532,"Add tests for RESP3 responce of ZINTER and ZRANGE (#8391)

It was confusing as to why these don't return a map type.
the reason is that order matters, so we need to make sure the client
library knows to respect it.
Added comments in the implementation and tests to cover it.","['src/t_zset.c', 'tests/support/redis.tcl', 'tests/unit/type/zset.tcl']","Confusion exists over why ZINTER and ZRANGE are not returning a map type due to the importance of order, leading to potential mishandling in the client library."
10f94b0ab12f9315939dcccf39d64b9388c0c7fa,1609764508,"Swapdb should make transaction fail if there is any client watching keys (#8239)

This PR not only fixes the problem that swapdb does not make the
transaction fail, but also optimizes the FLUSHALL and FLUSHDB command to
set the CLIENT_DIRTY_CAS flag to avoid unnecessary traversal of clients.

FLUSHDB was changed to first iterate on all watched keys, and then on the
clients watching each key.
Instead of iterating though all clients, and for each iterate on watched keys.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/db.c', 'src/multi.c', 'src/server.h', 'tests/unit/multi.tcl']","The SWAPDB operation does not cause a transaction to fail if any key being watched by a client is involved, resulting in potential complications with FLUSHALL and FLUSHDB operations."
16258f21d1d0e84e0cc4b5b0f93c78be6384658d,1611136026,"More modules API ref formatting fixes (#8344)

Fix broken formatting in `RM_Call` and `RM_CreateDataType`,
`RM_SubscribeToServerEvent` (nested lists, etc. in list items).

Unhide docs of `RM_LoadDataTypeFromString` and
`RM_SaveDataTypeToString` by removing blank line between docs and
function.

Clarification added to `RM__Assert`: Recommentation to use the
`RedisModule_Assert` macro instead.

All names containing underscores (variable and macro names) are
wrapped in backticks (if not already wrapped in backticks). This
prevents underscore from being interpreted as italics in some
cases.

Names including a wildcard star, e.g. RM_Defrag*(), is wrapped in
backticks (and RM replaced by RedisModule in this case). This
prevents the * from being interpreted as an italics marker.

A list item with a sublist, a paragraph and another sublist is a
combination which seems impossible to achieve with RedCarped
markdown, so the one occurrence of this is rewritten.

Various trivial changes (typos, backticks, etc.).

Ruby script:

* Replace `RM_Xyz` with `RedisModule_Xyz` in docs. (RM is correct
  when refering to the C code but RedisModule is correct in the
  API docs.)
* Automatic backquotes around C functions like `malloc()`.
* Turn URLs into links. The link text is the URL itself.
* Don't add backticks inside bold (**...**)","['src/module.c', 'src/modules/gendoc.rb']","API documentation formatting issues found in multiple modules such as `RM_Call`, `RM_CreateDataType`, `RM_SubscribeToServerEvent` etc. Some sections of code are hidden, underscore included names are not interpreted correctly, and markdown issues with nested lists."
66bed3f220d3ae73449b6b9f89ae616e42eff2e4,1675922239,"When DEBUG LOADAOF fails, return an error instead of exiting (#11790)

Return an error when loadAppendOnlyFiles fails instead of
exiting. DEBUF LOADAOF command is only meant to be used by
the test suite, and only by tests that generated an AOF file
first. So this change is ok (considering that the caller is
likely to catch this error and die).

This actually revert part of the code in #9012, and now
DEBUG LOADAOF behaves the same as DEBUG RELOAD (returns an
error when the load fails).

Plus remove a `after 2000` in a test, which can save times (looks like copy paste error).","['src/debug.c', 'tests/unit/expire.tcl']","The DEBUG LOADAOF command unexpectedly terminates the program instead of properly returning an error when loadAppendOnlyFiles fails. In addition, there appears to be an erroneous delay in a test due to a ""after 2000"" clause."
074d24df1e82b5c794a5da9fb6cda5b77b60b27b,1557915403,"Narrow the effects of PR #6029 to the exact state.

CLIENT PAUSE may be used, in other contexts, for a long time making all
the slaves time out. Better for now to be more specific about what
should disable senidng PINGs.

An alternative to that would be to virtually refresh the slave
interactions when clients are paused, however for now I went for this
more conservative solution.
",['src/replication.c'],"'CLIENT PAUSE' command causes slaves to timeout due to prolonged pauses, adversely impacting the PING operation."
ebf20b83b2cfcaf326f037a8274848662a3a9603,1610710436,"Modules API reference formatting fixes

Fixes markdown formatting errors and some functions not showing
up in the generated documentation at all.

Ruby script (gendoc.rb) fixes:

* Modified automatic instertion of backquotes:
  * Don't add backquotes around names which are already preceded by a
    backquote. Fixes for example \`RedisModule_Reply\*\` which turning
    into \`\`RedisModule_Reply\`\*\` messes up the formatting.
  * Add backquotes around types such as RedisModuleString (in addition
    to function names `RedisModule_[A-z()]*` and macro names
    `REDISMODULE_[A-z]*`).
  * Require 4 spaces indentation for disabling automatic backquotes, i.e.
    code blocks. Fixes continuations of list items (indented 2 spaces).
* More permissive extraction of doc comments:
  * Allow doc comments starting with `/**`.
  * Make space before `*` on each line optional.
  * Make space after `/*` and `/**` optional (needed when appearing on
    its own line).

Markdown fixes in module.c:

* Fix code blocks not indented enough (4 spaces needed).
* Add black line before code blocks and lists where missing (needed).
* Enclose special markdown characters `_*^<>` in backticks to prevent them
  from messing up formatting.
* Lists with `1)` changed to `1.` for proper markdown lists.
* Remove excessive indentation which causes text to be unintentionally
  rendered as code blocks.
* Other minor formatting fixes.

Other fixes in module.c:

* Remove blank lines between doc comment and function definition. A blank
  line here makes the Ruby script exclude the function in docs.
","['src/module.c', 'src/modules/gendoc.rb']","The existing Modules API reference has markdown formatting errors, leading to some functions not appearing in the generated documentation and issues with the automatic insertion of backquotes by the gendoc.rb script."
3a5905fa8575ccc7b0e115dee0a80f774bb6bf85,1615704386,"Send the readiness notification when we are ready to accept connections (#8409)

On a replica we do accept connections, even though commands accessing
the database will operate in read-only mode. But the server is still
already operational and processing commands.

Not sending the readiness notification means that on a HA setup where
the nodes all start as replicas (with replicaof in the config) with
a replica that cannot connect to the master server and which might not
come back in a predictable amount of time or at all, the service
supervisor will end up timing out the service and terminating it, with
no option to promote it to be the main instance. This seems counter to
what the readiness notification is supposed to be signaling.

Instead send the readiness notification when we start accepting
commands, and then send the various server status changes as that.

Fixes: commit 641c64ada10404356fc76c0b56a69b32c76f253c
Fixes: commit dfb598cf3304818e608ceb6b5d9529a293345c4a","['src/replication.c', 'src/server.c']","In HA setup, service supervisor times out and terminates a replica server that can't connect to the master, even if it's operational and processing commands, due to delayed readiness notification."
2e0f6724e0d6b821789f8936531bab63eac1777e,1696224053,"Stabilization and improvements around aof tests (#12626)

In some tests, the code manually searches for a log message, and it
uses tail -1 with a delay of 1 second, which can miss the expected line.

Also, because the aof tests use start_server_aof and not start_server,
the test name doesn't log into the server log.

To fix the above, I made the following changes:
- Change the start_server_aof to wrap the start_server.
  This will add the created aof server to the servers list, and make
  srv() and wait_for_log_messages() available for the tests.

- Introduce a new option for start_server.
  'wait_ready' - an option to let the caller start the test code without
  waiting for the server to be ready. useful for tests on a server that
  is expected to exit on startup.

- Create a new start_server_aof_ex.
  The new proc also accept options as argument and make use of the
  new 'short_life' option for tests that are expected to exit on startup
  because of some error in the aof file(s).

Because of the above, I had to change many lines and replace every
local srv variable (a server config) usage with the srv().","['tests/integration/aof-multi-part.tcl', 'tests/integration/aof-race.tcl', 'tests/integration/aof.tcl', 'tests/support/aofmanifest.tcl', 'tests/support/server.tcl']","Certain tests are missing expected log messages due to manual search and delay, while some other tests aren't logging into server log due to the use of start_server_aof instead of start_server. Also, there's no way to start test without waiting for server to be ready."
6b475989984bb28499327e33cc79315d6264bc06,1620315285,"redis-cli when SELECT fails, we should reset dbnum to 0 (#8898)

when SELECT fails, we should reset dbnum to 0, so the prompt will not
display incorrectly.

Additionally when SELECT and HELLO fail, we output message to inform
it.

Add config.input_dbnum which means the dbnum about to select.
And config.dbnum means currently selected dbnum. When users succeed to
select db, config.dbnum and config.input_dbnum will be the same. When
users select db failed, config.input_dbnum will be kept. Next time if users
auth success, config.input_dbnum will be automatically selected.
When reconnect, we should select the origin dbnum.

Co-authored-by: Oran Agra <oran@redislabs.com>",['src/redis-cli.c'],"Failure of SELECT in redis-cli doesn't reset dbnum to 0, leading to incorrect prompt display."
a89bf734a933e45b9dd3ae85ef4c3b62bd6891d8,1592122815,"Fix RM_ScanKey module api not to return int encoded strings

The scan key module API provides the scan callback with the current
field name and value (if it exists). Those arguments are RedisModuleString*
which means it supposes to point to robj which is encoded as a string.
Using createStringObjectFromLongLong function might return robj that
points to an integer and so break a module that tries for example to
use RedisModule_StringPtrLen on the given field/value.

The PR introduces a fix that uses the createObject function and sdsfromlonglong function.
Using those function promise that the field and value pass to the to the
scan callback will be Strings.

The PR also changes the Scan test module to use RedisModule_StringPtrLen
to catch the issue. without this, the issue is hidden because
RedisModule_ReplyWithString knows to handle integer encoding of the
given robj (RedisModuleString).

The PR also introduces a new test to verify the issue is solved.
","['src/module.c', 'tests/modules/scan.c', 'tests/unit/moduleapi/scan.tcl']","The RM_ScanKey module API returns an integer- encoded string when using createStringObjectFromLongLong, causing breakage when RedisModule_StringPtrLen is used."
677d14c2137ab50fa25c8163d20b14bc563261c7,1599375559,"test infra - improve test skipping ability

- skip full units
- skip a single test (not just a list of tests)
- when skipping tag, skip spinning up servers, not just the tests
- skip tags when running against an external server too
- allow using multiple tags (split them)
","['tests/support/server.tcl', 'tests/support/test.tcl', 'tests/test_helper.tcl']","Limited test skipping capabilities in our test infrastructure, cannot skip full units or individual tests, also when skipping tags, the servers still spin up, and multiple tags can't be utilized for skipping."
52b2fbe9705739fd5f03099011f652775ab3285f,1643553563,"Improve srand entropy (and fix Sentinel failures) (#10197)

As Sentinel relies upon consensus algorithm, all sentinel instances,
randomize a time to initiate their next attempt to become the
leader of the group. But time after time, all raffled the same value.

The problem is in the line `srand(time(NULL)^getpid())` such that
all spinned up containers get same time (in seconds) and same pid
which is always 1. Added material `tv_usec` and verify that even
consecutive calls brings different values and makes the difference.",['src/server.c'],"Sentinel instances simultaneously attempt to become the leader of the group due to lack of entropy in randomization. Containers are getting same time (in seconds) and process ID, always 1."
12504105c42b65127aa016562c713af63b4cbc71,1591599006,"fix disconnectSlaves, to try to free each slave.

the recent change in that loop (iteration rather than waiting for it to
be empty) was intended to avoid an endless loop in case some slave would
refuse to be freed.

but the lookup of the first client remained, which would have caused it
to try the first one again and again instead of moving on.
",['src/networking.c'],'Change in loop iteration for disconnectSlaves function potentially causes endless loop due to repeated unsuccessful attempts in freeing the same slave client.'
143bfa1e6e65cf8be1eaad0b8169e2d95ca62f9a,1545151535,"Cluster Manager: compare key values after BUSYKEY error (migration).

If a key exists in the target node during a migration (BUSYKEY),
the value of the key on both nodes (source and target) will be compared.
If the key has the same value on both keys, the migration will be
automatically retried with the REPLACE argument in order to override
the target's key.

If the key has different values, the behaviour will depend on such
cases:
- In case of 'fix' command, the migration will stop and the user
  will be warned to manually check the key(s).
- In other cases (ie. reshard), if the user launched the command
  with the --cluster-replace option, the migration will be
  retried with the REPLACE argument, elsewhere the migration will
  stop and the user will be warned.
",['src/redis-cli.c'],"During a migration, if a key already exists in the target node (BUSYKEY), the system doesn't compare the key's value on both nodes which might prevent or erroneously allow its replacement."
a43b6922d1e37d60acf63484b7057299c9bf584d,1643536975,"Set default channel permission to resetchannels for 7.0 (#10181)

For backwards compatibility in 6.x, channels default permission was set to `allchannels` however with 7.0,
we should modify it and the default value should be `resetchannels` for better security posture.
Also, with selectors in ACL, a client doesn't have to set channel rules everytime and by default
the value will be `resetchannels`.

Before this change
```
127.0.0.1:6379> acl list
1) ""user default on nopass ~* &* +@all""
127.0.0.1:6379>  acl setuser hp on nopass +@all ~*
OK
127.0.0.1:6379> acl list
1) ""user default on nopass ~* &* +@all""
2) ""user hp on nopass ~* &* +@all""
127.0.0.1:6379>  acl setuser hp1 on nopass -@all (%R~sales*)
OK
127.0.0.1:6379> acl list
1) ""user default on nopass ~* &* +@all""
2) ""user hp on nopass ~* &* +@all""
3) ""user hp1 on nopass &* -@all (%R~sales* &* -@all)""
```

After this change
```
127.0.0.1:6379> acl list
1) ""user default on nopass ~* &* +@all""
127.0.0.1:6379> acl setuser hp on nopass +@all ~*
OK
127.0.0.1:6379> acl list
1) ""user default on nopass ~* &* +@all""
2) ""user hp on nopass ~* resetchannels +@all""
127.0.0.1:6379> acl setuser hp1 on nopass -@all (%R~sales*)
OK
127.0.0.1:6379> acl list
1) ""user default on nopass ~* &* +@all""
2) ""user hp on nopass ~* resetchannels +@all""
3) ""user hp1 on nopass resetchannels -@all (%R~sales* resetchannels -@all)""
```","['src/config.c', 'tests/unit/acl-v2.tcl', 'tests/unit/acl.tcl']","Default channel permission in version 7.0 remains set to `allchannels` of 6.x, causing an unnecessary repetition of setting channel rules, thus compromising security."
1407ac1f3ece8f7e4094f92e649e5d0b7c5e5bde,1692636510,"BITCOUNT and BITPOS with non-existing key and illegal arguments should return error, not 0 (#11734)

BITCOUNT and BITPOS with non-existing key will return 0 even the
arguments are error, before this commit:
```
> flushall
OK
> bitcount s 0
(integer) 0
> bitpos s 0 0 1 hello
(integer) 0

> set s 1
OK
> bitcount s 0
(error) ERR syntax error
> bitpos s 0 0 1 hello
(error) ERR syntax error
```

The reason is that we judged non-existing before parameter checking and
returned. This PR fixes it, and after this commit:
```
> flushall
OK
> bitcount s 0
(error) ERR syntax error
> bitpos s 0 0 1 hello
(error) ERR syntax error
```

Also BITPOS made the same fix as #12394, check for wrong argument, before
checking for key.
```
> lpush mylist a b c
(integer) 3                                                                                    
> bitpos mylist 1 a b
(error) WRONGTYPE Operation against a key holding the wrong kind of value
```
","['src/bitops.c', 'tests/unit/bitops.tcl']","Using BITCOUNT and BITPOS with a non-existing key and invalid arguments unexpectedly returns 0 instead of an error message. Similarly, using BITPOS with incorrect arguments on an incompatible key type doesn't produce a valid error."
f560531d5b8a6e6d810b62114e69a5ffda7730f7,1631184288,"Fix: client pause uses an old timeout (#9477)

A write request may be paused unexpectedly because `server.client_pause_end_time` is old.

**Recreate this:**
redis-cli -p 6379
127.0.0.1:6379> client pause 500000000 write
OK
127.0.0.1:6379> client unpause
OK
127.0.0.1:6379> client pause 10000 write
OK
127.0.0.1:6379> set key value

The write request `set key value` is paused util  the timeout of 500000000 milliseconds was reached.

**Fix:**
reset `server.client_pause_end_time` = 0 in `unpauseClients`","['src/networking.c', 'src/server.c']","Write requests get paused unexpectedly as `server.client_pause_end_time` appears to retain old values, resulting in unintentionally long timeouts."
7be7834e6573134b1e5a0b8ea90acf856b0c7774,1678643226,"Large blocks of replica client output buffer could lead to psync loops and unnecessary memory usage (#11666)

This can happen when a key almost equal or larger than the
client output buffer limit of the replica is written.

Example:
1. DB is empty
2. Backlog size is 1 MB
3. Client out put buffer limit is 2 MB
4. Client writes a 3 MB key
5. The shared replication buffer will have a single node which contains
the key written above, and it exceeds the backlog size.

At this point the client output buffer usage calculation will report the
replica buffer to be 3 MB (or more) even after sending all the data to
the replica.
The primary drops the replica connection for exceeding the limits,
the replica reconnects and successfully executes partial sync but the
primary will drop the connection again because the buffer usage is still
3 MB. This happens over and over.

To mitigate the problem, this fix limits the maximum size of a single
backlog node to be (repl_backlog_size/16). This way a single node can't
exceed the limits of the COB (the COB has to be larger than the
backlog).
It also means that if the backlog has some excessive data it can't trim,
it would be at most about 6% overuse.

other notes:
1. a loop was added in feedReplicationBuffer which caused a massive LOC
  change due to indentation, the actual changes are just the `min(max` and the loop.
3. an unrelated change in an existing test to speed up a server termination which took 10 seconds.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/replication.c', 'tests/integration/replication-buffer.tcl']",Writing a key nearly or exceeding the replica client output buffer limit leads to persistent sync loops and excessive memory usage.
a7da7473cbd791589b943f38dcb855404a872928,1661675087,"Remove the NONDETERMINISTIC_OUTPUT flag from most CLUSTER sub-commands. (#11157)

TLDR: the CLUSTER command originally had the `random` flag,
so all the sub-commands initially got that new flag, but in fact many
of them don't need it.
The only effect of this change is on the output of COMMAND INFO.",['src/commands.c'],"The `random` flag in CLUSTER command sub-commands is unnecessary, causing misleading information in COMMAND INFO output."
96bb078577ce2b0d093c873faae5d3ecca26a1de,1624262491,"redis-cli: support for REDIS_REPLY_SET in CSV and RAW output. (#7338)

Fixes #6792. Added support of REDIS_REPLY_SET in raw and csv output of `./redis-cli`

Test:

run commands to test:
  ./redis-cli -3 --csv COMMAND
  ./redis-cli -3 --raw COMMAND

Now they are returning resuts, were failing with: ""Unknown reply type: 10"" before the change.",['src/redis-cli.c'],"The redis-cli is giving an ""Unknown reply type:10"" error when running commands with CSV or RAW output formats, indicating a lack of support for REDIS_REPLY_SET."
66ee45b65cc8165ba86e98192920374dcd06d943,1601640439,"Add GET parameter to SET command (#7852)

Add optional GET parameter to SET command in order to set a new value to
a key and retrieve the old key value. With this change we can deprecate
`GETSET` command and use only the SET command with the GET parameter.","['src/t_string.c', 'tests/unit/type/string.tcl']",Lack of functionality to set a new value to a key and retrieve the old key value in a single command in the current SET command implementation.
9ca5e8c5470db52f786e1bdec61a067dd6fc289e,1626950400,"fix zslGetRank bug in dead-code (#9246)

This fixes an issue with zslGetRank which will happen only if the
skiplist data stracture is added two entries with the same element name,
this can't happen in redis zsets (we use dict), but in theory this is a
bug in the underlaying skiplist code.

Fixes #3081 and #4032

Co-authored-by: minjian.cai <cmjgithub@163.com>",['src/t_zset.c'],"The skiplist data structure in zslGetRank encounters an issue when two entries with the same element name are added, which theoretically shouldn't occur in Redis zsets."
808f3004f0de8c129b3067d8b2ce5002fa703e77,1617700176,"Fix: server will crash if rdbload or rdbsave method is not provided in module (#8670)

With this fix, module data type registration will fail if the load or save callbacks are not defined, or the optional aux load and save callbacks are not either both defined or both missing.","['src/module.c', 'tests/modules/datatype.c', 'tests/modules/defragtest.c', 'tests/modules/testrdb.c']","When registering a new module data type, missing rdbload or rdbsave methods (aux load and save callbacks) causes the server to crash."
4e8f2d6881a38397bfbf0d7d161959163a5f6e88,1595308385,"Add missing calls to raxStop (#7532)

Since the dynamic allocations in raxIterator are only used for deep walks, memory
leak due to missing call to raxStop can only happen for rax with key names longer
than 32 bytes.

Out of all the missing calls, the only ones that may lead to a leak are the rax
for consumer groups and consumers, and these were only in AOFRW and rdbSave, which
normally only happen in fork or at shutdown.","['src/aof.c', 'src/defrag.c', 'src/rdb.c', 'src/timeout.c']","Memory leaks occur in raxIterator for rax with key names longer than 32 bytes due to missing calls to raxStop in AOFRW and rdbSave, particularly in the consumer groups and consumers."
5f7789d329d08ad658021b8be67cd4c3bf4073f7,1632293324,"tune lazyfree test timeout (#9527)

i've seen this CI failure a couple of times on MacOS:

*** [err]: lazy free a stream with all types of metadata in tests/unit/lazyfree.tcl
lazyfree isn't done

only reason i can think of is that 500ms is sometimes not enough on slow systems.
",['tests/unit/lazyfree.tcl'],"The lazyfree function fails intermittently on MacOS systems during CI testing, indicating that the 500ms timeout could be insufficient on slower systems."
00a3bc435915c6f3526a135af40ebbb46a942005,1589015435,"Cluster: introduce data_received field.

We want to send pings and pongs at specific intervals, since our packets
also contain information about the configuration of the cluster and are
used for gossip. However since our cluster bus is used in a mixed way
for data (such as Pub/Sub or modules cluster messages) and metadata,
sometimes a very busy channel may delay the reception of pong packets.
So after discussing it in #7216, this commit introduces a new field that
is not exposed in the cluster, is only an internal information about
the last time we received any data from a given node: we use this field
in order to avoid detecting failures, claiming data reception of new
data from the node is a proof of liveness.
","['src/cluster.c', 'src/cluster.h']","Mixing of data and metadata on the same cluster bus is causing delays in reception of pong packets, potentially leading to incorrect failure detections."
45d83fb2d49c9d44249265b3ccb63cd39fa50501,1646653447,"Fix timing issue in rehash test (#10388)

`Expected '*table size: 4096*' to match '*table size: 8192*'`

This test failed once on daily macOS, the reason is because
the bgsave has not stopped after the kill and `after 200`.
So there is a child process and no rehash triggered.

This commit use `waitForBgsave` to wait for it to finish.",['tests/unit/other.tcl'],"The rehash test fails intermittently on daily macOS due to a timing issue where bgsave hasn't stopped after the kill command and the waiting period, preventing a rehash trigger."
834e8843de0f6d8379600f6dcedededb5e29ac06,1633610628,"obuf based eviction tests run until eviction occurs (#9611)

obuf based eviction tests run until eviction occurs instead of assuming a certain
amount of writes will fill the obuf enough for eviction to occur.
This handles the kernel buffering written data and emptying the obuf even though
no one actualy reads from it.

The tests have a new timeout of 20sec: if the test doesn't pass after 20 sec it'll fail.
Hopefully this enough for our slow CI targets.

This also eliminates the need to skip some tests in TLS.",['tests/unit/maxmemory.tcl'],"Obuf based eviction tests assume a specific volume of writes will trigger eviction. However, tests may fail due to kernel buffering and emptying the obuf, despite no actual reads occurring. Additionally, some tests are skipped in TLS."
1833d008b3af8628835b5f082c5b4b1359557893,1573035739,"Support streams in general module API functions

Fixes GitHub issue #6492
Added stream support in RM_KeyType and RM_ValueLength.
Also moduleDelKeyIfEmpty was updated, even though it has
no effect now (It will be relevant when stream type direct
API will be coded - i.e. RM_StreamAdd)
","['src/module.c', 'src/redismodule.h', 'src/stream.h', 'src/t_stream.c']","The module API functions RM_KeyType and RM_ValueLength do not support streams, which has become relevant with the impending addition of stream type direct API."
1736fa4d220266718980676855c3d3ca54d7da44,1638885951,"Don't write oom score adj to proc unless we're managing it. (#9904)

When disabling redis oom-score-adj managment we restore the
base value read before enabling oom-score-adj management.

This fixes an issue introduced in #9748 where updating
`oom-score-adj-values` while `oom-score-adj` was set to `no`
would write the base oom score adj value read on startup to `/proc`.
This is a bug since while `oom-score-adj` is disabled we should
never write to proc and let external processes manage it.

Added appropriate tests.","['src/config.c', 'src/server.c', 'src/server.h', 'tests/unit/oom-score-adj.tcl']","Disabling Redis `oom-score-adj` management results in writing of base `oom-score-adj-values` to `/proc`, interfering with external process management."
5a3cdddd2a98a270a4e09c8ccfa44079e0f2f5be,1675414276,"Fix timing issue in new ACL log test (#11781)

There is a timing issue in the new ACL log test:
```
*** [err]: ACL LOG aggregates similar errors together and assigns unique entry-id to new errors in tests/unit/acl.tcl
Expected 1675382873989 < 1675382873989 (context: type eval line 15 cmd {assert {$timestamp_last_update_original < $timestamp_last_updated_after_update}} proc ::test)
```

Looking at the test code, we will check the `timestamp-last-updated` before
and after a new ACL error occurs. Actually `WRONGPASS` errors can be executed
very quickly on fast machines. For example, in the this case, the execution is
completed within one millisecond.

The error is easy to reproduce, if we reduce the number of the for loops, for
example set to 2, and using --loop and --stop. Avoid this timing issue by adding
an `after 1` before the new errors.

The test was introduced in #11477.",['tests/unit/acl.tcl'],"The current structure of the ACL log test allows for ""WRONGPASS"" errors to be executed within the same millisecond on fast machines, disrupting the accurate checking of `timestamp-last-updated` before and after a new ACL error occurs."
b5f52bf01cd99d0bb2d51d158fd009690a36d042,1578692055,"Make error when submitting command in incorrect context more explicit

So error message `ERR only (P)SUBSCRIBE / (P)UNSUBSCRIBE / PING / QUIT allowed in this context` will become
`ERR 'get' command submitted, but only (P)SUBSCRIBE / (P)UNSUBSCRIBE / PING / QUIT allowed in this context`
",['src/server.c'],The error message when submitting a command in incorrect context is too vague and doesn't specify the submitted command causing the error.
469d6e2b37e2913ecb673f910cdb7dbd3af18a67,1493305717,"PSYNC2: fix master cleanup when caching it.

The master client cleanup was incomplete: resetClient() was missing and
the output buffer of the client was not reset, so pending commands
related to the previous connection could be still sent.

The first problem caused the client argument vector to be, at times,
half populated, so that when the correct replication stream arrived the
protcol got mixed to the arugments creating invalid commands that nobody
called.

Thanks to @yangsiran for also investigating this problem, after
already providing important design / implementation hints for the
original PSYNC2 issues (see referenced Github issue).

Note that this commit adds a new function to the list library of Redis
in order to be able to reset a list without destroying it.

Related to issue #3899.
","['src/adlist.c', 'src/adlist.h', 'src/replication.c']","Incomplete master client cleanup results in persistent commands from a previous connection and half populated client argument vector, creating invalid commands."
79f089bdd92348d5fa3965258f539752f2eb8e78,1643482829,"Fixed Sentinel support for hostnames (#10146)

Sentinel tries to resolve instances hostname to IP only during registration. 
It might be that the instance is unavailable during that time, such as 
leader crashed and failover took place. Yet, promoted replica must support:

 - Register leader, even if it fails to resolve its hostname during failover
 - Try later to resolve it, if instance is disconnected. Note that
   this condition also support ip-change of an instance.",['src/sentinel.c'],"Sentinel's hostname resolution fails if the instance is unavailable during registration, causing problems in registering leaders and handling failovers or IP changes in instances."
795ea011ba8674a9b5e05e79284f6f20a5d0cc03,1643219162,"Solve race in a BGSAVE test (#10190)

This PR attempts to solve two problems that happen sometime in valgrind:
`ERR Background save already in progress`
and
`not bgsave not aborted`

the test used to populate the database with DEBUG, which didn't
increment the dirty counter, so couldn't trigger an automatic bgsave.
then it used a manual bgsave, and aborted it (when it got aborted it
populated the dirty counter), and then it tried to do another bgsave.
that other bgsave could have failed if the automatic one already
started.",['tests/integration/rdb.tcl'],"Automatic BGSAVE may initiate during a test, conflicting with manual BGSAVE, leading to 'Background save already in progress' and 'not bgsave not aborted' errors."
0c3b8b7e90ea242640a417abb5c40f30dfc3e036,1680112139,"Overhauls command summaries and man pages. (#11942)

This is an attempt to normalize/formalize command summaries.

Main actions performed:

* Starts with the continuation of the phrase ""The XXXX command, when called, ..."" for user commands.
* Starts with ""An internal command..."", ""A container command..."", etc... when applicable.
* Always uses periods.
* Refrains from referring to other commands. If this is needed, backquotes should be used for command names.
* Tries to be very clear about the data type when applicable.
* Tries to mention additional effects, e.g. ""The key is created if it doesn't exist"" and ""The set is deleted if the last member is removed.""
* Prefers being terse over verbose.
* Tries to be consistent.","['src/commands.c', 'tests/modules/cmdintrospection.c']","Inconsistent formatting and lack of clarity in command summaries and man pages, creating difficulty in understanding and using commands."
4024bc7eee9b20b2f3bc85fb6f94e243f1a61b9d,1588150835,"fix pipelined WAIT performance issue.

If client gets blocked again in `processUnblockedClients`, redis will not send
`REPLCONF GETACK *` to slaves untill next eventloop, so the client will be
blocked for 100ms by default(10hz) if no other file event fired.

move server.get_ack_from_slaves sinppet after `processUnblockedClients`, so
that both the first WAIT command that puts client in blocked context and the
following WAIT command processed in processUnblockedClients would trigger
redis-sever to send `REPLCONF GETACK *`, so that the eventloop would get
`REPLCONG ACK <reploffset>` from slaves and unblocked ASAP.
",['src/server.c'],"Redis client gets blocked for 100ms by default due to lack of `REPLCONF GETACK *` sent to slaves until the next event loop, after re-blocking in `processUnblockedClients`."
efa162bcd7ab1477c07d8ee85537e27c0cb1524b,1651009060,"Protect any table which is reachable from globals and added globals white list.

The white list is done by setting a metatable on the global table before initializing
any library. The metatable set the `__newindex` field to a function that check
the white list before adding the field to the table. Fields which is not on the
white list are simply ignored.

After initialization phase is done we protect the global table and each table
that might be reachable from the global table. For each table we also protect
the table metatable if exists.
","['deps/lua/src/lapi.c', 'deps/lua/src/lua.h', 'src/eval.c', 'src/function_lua.c', 'src/script_lua.c', 'src/script_lua.h', 'tests/unit/scripting.tcl']","Global table and reachable tables are not sufficiently protected, additionally there is no white list check for new fields added to the global table, which may lead to unwanted modifications."
3ba2281f96098691d9d672ce8b484207379b09ee,1606851686,"Improve dbid range check for SELECT, MOVE, COPY (#8085)

SELECT used to read the index into a `long` variable, and then pass it to a function
that takes an `int`, possibly causing an overflow before the range check.

Now all these commands use better and cleaner range check, and that also results in
a slight change of the error response in case of an invalid database index.

SELECT:
in the past it would have returned either `-ERR invalid DB index` (if not a number),
or `-ERR DB index is out of range` (if not between 1..16 or alike).
now it'll return either `-ERR value is out of range` (if not a number), or
`-ERR value is out of range, value must between -2147483648 and 2147483647`
(if not in the range for an int), or `-ERR DB index is out of range`
(if not between 0..16 or alike)


MOVE:
in the past it would only fail with `-ERR index out of range` no matter the reason.
now return the same errors as the new ones for SELECT mentioned above.
(i.e. unlike for SELECT even for a value like 17 we changed the error message)

COPY:
doesn't really matter how it behaved in the past (new command), new behavior is
like the above two.","['src/db.c', 'src/object.c', 'src/server.h', 'tests/unit/keyspace.tcl']","Long converters were used in SELECT, MOVE, COPY commands can potentially cause an overflow before the range check, and error responses are inconsistent."
a7d6ca9770933b7d9fec7c137232831f08ba7ccc,1652612865,"Make the check for if script is running or not consistent (#10725)

sometimes it is using `scriptIsRunning()` and other times it is using `server.in_script`.
We should use the `scriptIsRunning()` method consistently throughout the code base.
Removed server.in_script sine it's no longer used / needed.","['src/module.c', 'src/script.c', 'src/server.c', 'src/server.h']","Inconsistent methods used for checking if the script is running, potentially causing unpredictable behavior."
36b392a0b2528ad22f016e9ca8fce93eb6dab3a0,1528126083,"XGROUP SETID implemented + consumer groups core fixes.

Now that we have SETID, the inetrnals of consumer groups should be able
to handle the case of the same message delivered multiple times just
as a side effect of calling XREADGROUP. Normally this should never
happen but if the admin manually ""XGROUP SETID mykey mygroup 0"",
messages will get re-delivered to clients waiting for the "">"" special
ID. The consumer groups internals were not able to handle the case of a
message re-delivered in this circumstances that was already assigned to
another owner.
",['src/t_stream.c'],"Consumer group's internals failing to handle cases when same message is delivered multiple times due to manual invocation of ""XGROUP SETID mykey mygroup 0"", leading to incorrect assignment to multiple owners."
66002530466a45bce85e4930364f1b153c44840b,1632667502,"Client eviction ci issues (#9549)

Fixing CI test issues introduced in #8687
- valgrind warnings in readQueryFromClient when client was freed by processInputBuffer
- adding DEBUG pause-cron for tests not to be time dependent.
- skipping a test that depends on socket buffers / events not compatible with TLS
- making sure client got subscribed by not using deferring client","['src/blocked.c', 'src/debug.c', 'src/networking.c', 'src/server.c', 'src/server.h', 'tests/support/server.tcl', 'tests/unit/client-eviction.tcl', 'tests/unit/maxmemory.tcl']","CI tests failing due to issues arising from free client in 'readQueryFromClient', time-dependent tests, incompatible socket buffers/events with TLS, and deferring client usage before subscription."
20fa156067203b93b7050cc444430d31a6cb6e1b,1687283083,"Align RM_ReplyWithErrorFormat with RM_ReplyWithError (#12321)

Introduced by https://github.com/redis/redis/pull/11923 (Redis 7.2 RC2)

It's very weird and counterintuitive that `RM_ReplyWithError` requires the error-code
**without** a hyphen while `RM_ReplyWithErrorFormat` requires either the error-code
**with** a hyphen or no error-code at all
```
RedisModule_ReplyWithError(ctx, ""BLA bla bla"");
```
vs.
```
RedisModule_ReplyWithErrorFormat(ctx, ""-BLA %s"", ""bla bla"");
```

This commit aligns RM_ReplyWithErrorFormat to behvae like RM_ReplyWithError.
it's a breaking changes but it's done before 7.2 goes GA.","['src/module.c', 'tests/unit/moduleapi/reply.tcl']","The functions `RM_ReplyWithError` and `RM_ReplyWithErrorFormat` expected differing formats for error-codes, causing inconsistent and counterintuitive usage."
f28e2ce7a4cf4a82d367c106eee4b3113b85bcbf,1653574985,"improve logging around AOF file creation and loading (#10763)

instead of printing a log when a folder or a manifest is missing (level reduced), we print:

total time it took to load all the aof files
when creating a new base or incr file
starting to write to an existing incr file on startup
","['src/aof.c', 'src/server.c']","Current logging does not provide sufficient information when creating or loading AOF files, making it difficult to troubleshoot potential issues."
8631e6477904f3d8f87662fd93a1ba294615654a,1527088424,"Sentinel: fix delay in detecting ODOWN.

See issue #2819 for details. The gist is that when we want to send INFO
because we are over the time, we used to send only INFO commands, no
longer sending PING commands. However if a master fails exactly when we
are about to send an INFO command, the PING times will result zero
because the PONG reply was already received, and we'll fail to send more
PINGs, since we try only to send INFO commands: the failure detector
will delay until the connection is closed and re-opened for ""long
timeout"".

This commit changes the logic so that we can send the three kind of
messages regardless of the fact we sent another one already in the same
code path. It could happen that we go over the message limit for the
link by a few messages, but this is not significant. However now we'll
not introduce delays in sending commands just because there was
something else to send at the same time.
",['src/sentinel.c'],"In Sentinel, master failure occurring during INFO command execution leads to PING times being zeroed, which in turn causes delay in failure detection until connection is re-established."
f0c1c730d495107c22bf722fa37d592f6c535feb,1697203732,"test suite: clean server pids after server crashed (#12639)

when a server in the test suite crashes and is restarted by redstart_server, we didn't clean it's pid from the list.
we can see that when the corrupt-dump-fuzzer hangs, it has a long list of servers to lean, but in fact they're all already dead.",['tests/support/server.tcl'],"Test suite's server crash and restart leads to a buildup of dead server pids in the list, causing the corrupt-dump-fuzzer to hang."
1a7cd2c0e25bd4488dd2c28f0ffea1b62dc0b3ad,1589530753,"Cache master without checking of deferred close flags.

The context is issue #7205: since the introduction of threaded I/O we close
clients asynchronously by default from readQueryFromClient(). So we
should no longer prevent the caching of the master client, to later
PSYNC incrementally, if such flags are set. However we also don't want
the master client to be cached with such flags (would be closed
immediately after being restored). And yet we want a way to understand
if a master was closed because of a protocol error, and in that case
prevent the caching.
","['src/networking.c', 'src/replication.c', 'src/server.h']","Threaded I/O's asynchronous client closing from readQueryFromClient() is preventing the master client from being cached for later incremental PSYNC, causing potential immediate closure upon restoration."
a418a2d3fc0250c094802d7e8ea64d96eedfda07,1625470917,"hrandfield and zrandmember with count should return emptyarray when key does not exist. (#9178)

due to a copy-paste bug, it used to reply with null response rather than empty array.
this commit includes new tests that are looking at the RESP response directly in
order to be able to tell the difference between them.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/t_hash.c', 'src/t_zset.c', 'tests/unit/type/hash.tcl', 'tests/unit/type/set.tcl', 'tests/unit/type/zset.tcl']",The commands 'hrandfield' and 'zrandmember' with count return null instead of an empty array when the key does not exist.
79fe450ebcdde3cfb69025eb845315189ba7ec7c,1669878693,"Regenerate payloads for cgroups tests using string2printable (#11560)

The test failed with ERR DUMP payload version or checksum are wrong.
And it only fails on CentOS, this is due to the fact that tcl8.5 does not correctly
parse the hexadecimal abbreviation. And in Ubuntu we are using tcl8.6.",['tests/unit/type/stream-cgroups.tcl'],"""Cgroups tests fail on CentOS due to ERR DUMP payload version or checksum discrepancies, possibly due to issues with hexadecimal abbreviation parsing in tcl8.5."""
38f6207f884f514e928513acb6560fdb375daa2e,1579527245,"Check OOM at script start to get stable lua OOM state.

Checking OOM by `getMaxMemoryState` inside script might get different result
with `freeMemoryIfNeededAndSafe` at script start, because lua stack and
arguments also consume memory.

This leads to memory `borderline` when memory grows near server.maxmemory:

- `freeMemoryIfNeededAndSafe` at script start detects no OOM, no memory freed
- `getMaxMemoryState` inside script detects OOM, script aborted

We solve this 'borderline' issue by saving OOM state at script start to get
stable lua OOM state.

related to issue #6565 and #5250.
","['src/scripting.c', 'src/server.c', 'src/server.h']",Discrepancies in Out-Of-Memory (OOM) detection at script start and within the script leading to script aborts when memory approaches server.maxmemory value.
7778f1b4b03a6a416388fe28d9bc93374384a65b,1615382716,"strip % sign from current_fork_perc info field (#8628)

`master_sync_perc` and `loading_loaded_perc` don't have that sign,
and i think the info field should be a raw floating point number (the name suggests its units).

we already have `used_memory_peak_perc` and `used_memory_dataset_perc` which do add the `%` sign, but:
1) i think it was a mistake but maybe too late to fix now, and maybe not too late to fix for `current_fork_perc`
2) it is more important to be consistent with the two other ""progress"" ""prec"" metrics, and not with the ""utilization"" metric.",['src/server.c'],"The `current_fork_perc` info field contains a '%' sign, leading to inconsistencies with other similar fields (`master_sync_perc`, `loading_loaded_perc`) that are expected to be raw floating point numbers."
40e6131ba53c001d8fa2e4ba9ddda6925a6db73b,1684391156,"Prevent repetitive backlog trimming (#12155)

When `replicationFeedSlaves()` serializes a command, it repeatedly calls
`feedReplicationBuffer()` to feed it to the replication backlog piece by piece.
It is unnecessary to call `incrementalTrimReplicationBacklog()` for every small
amount of data added with `feedReplicationBuffer()` as the chance of the conditions
being met for trimming are very low and these frequent calls add up to a notable
performance cost. Instead, we will only attempt trimming when a new block is added
to the replication backlog.

Using redis-benchmark to saturate a local redis server indicated a performance
improvement of around 3-3.5% for 100 byte SET commands with this change.",['src/replication.c'],"`replicationFeedSlaves()` is causing repetitive calls to `incrementalTrimReplicationBacklog()` for each small data addition with `feedReplicationBuffer()`, leading to a significant performance cost."
cb961d8c8e10ff3b619f1579a03336a15e9e6f45,1625997617,"Do not install a file event to send data to rewrite child when parent stop sending diff to child in aof rewrite. (#8767)

In aof rewrite, when parent stop sending data to child, if there is
new rewrite data, aofChildWriteDiffData write event will be installed.
Then this event is issued and deletes the file event without do anyting.
This will happen over and over again until aof rewrite finish.

This bug used to waste a few system calls per excessive wake-up
(epoll_ctl and epoll_wait) per cycle, each cycle triggered by receiving
a write command from a client.",['src/aof.c'],"In AOF rewrite, when parent ceases data transfer to child, unnecessary system calls are being wasted repeatedly due to installation of a file event that gets deleted without any operation."
01eb939a0661c94d22bfddb4841b4975a12dbf7e,1690290638,"update monitor client's memory and evict correctly (#12420)

A bug introduced in #11657 (7.2 RC1), causes client-eviction (#8687)
and INFO to have inaccurate memory usage metrics of MONITOR clients.

Because the type in `c->type` and the type in `getClientType()` are confusing
(in the later, `CLIENT_TYPE_NORMAL` not `CLIENT_TYPE_SLAVE`), the comment
we wrote in `updateClientMemUsageAndBucket` was wrong, and in fact that function
didn't skip monitor clients.
And since it doesn't skip monitor clients, it was wrong to delete the call for it from
`replicationFeedMonitors` (it wasn't a NOP).
That deletion could mean that the monitor client memory usage is not always up to
date (updated less frequently, but still a candidate for client eviction).","['src/networking.c', 'src/replication.c', 'src/server.c']","Inaccurate memory usage metrics for MONITOR clients are causing issues with client-eviction and INFO due to an issue with client types, potentially leading to outdated memory usage data and incorrect client evictions."
adc5df1bc3289cb1d786e0ef515ab9acde03bf52,1590168549,"Make disconnectSlaves() synchronous in the base case.

Otherwise we run into that:

Backtrace:
src/redis-server 127.0.0.1:21322(logStackTrace+0x45)[0x479035]
src/redis-server 127.0.0.1:21322(sigsegvHandler+0xb9)[0x4797f9]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fd373c5e390]
src/redis-server 127.0.0.1:21322(_serverAssert+0x6a)[0x47660a]
src/redis-server 127.0.0.1:21322(freeReplicationBacklog+0x42)[0x451282]
src/redis-server 127.0.0.1:21322[0x4552d4]
src/redis-server 127.0.0.1:21322[0x4c5593]
src/redis-server 127.0.0.1:21322(aeProcessEvents+0x2e6)[0x42e786]
src/redis-server 127.0.0.1:21322(aeMain+0x1d)[0x42eb0d]
src/redis-server 127.0.0.1:21322(main+0x4c5)[0x42b145]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7fd3738a3830]
src/redis-server 127.0.0.1:21322(_start+0x29)[0x42b409]

Since we disconnect all the replicas and free the replication backlog in
certain replication paths, and the code that will free the replication
backlog expects that no replica is connected.

However we still need to free the replicas asynchronously in certain
cases, as documented in the top comment of disconnectSlaves().
","['src/networking.c', 'src/replication.c', 'src/server.h']","Disconnecting all replicas and freeing the replication backlog in certain replication paths triggers a server assertion since it expects no replica to be connected, leading to a segmentation fault."
6b0b04f1b265c429bd19d6c99c9e7e2921723601,1638093579,"Clean Lua stack before parsing call reply to avoid crash on a call with many arguments (#9809)

This commit 0f8b634cd (CVE-2021-32626 released in 6.2.6, 6.0.16, 5.0.14)
fixes an invalid memory write issue by using `lua_checkstack` API to make
sure the Lua stack is not overflow. This fix was added on 3 places:
1. `luaReplyToRedisReply`
2. `ldbRedis`
3. `redisProtocolToLuaType`

On the first 2 functions, `lua_checkstack` is handled gracefully while the
last is handled with an assert and a statement that this situation can
not happened (only with misbehave module):

> the Redis reply might be deep enough to explode the LUA stack (notice
that currently there is no such command in Redis that returns such a nested
reply, but modules might do it)

The issue that was discovered is that user arguments is also considered part
of the stack, and so the following script (for example) make the assertion reachable:
```
local a = {}
for i=1,7999 do
    a[i] = 1
end 
return redis.call(""lpush"", ""l"", unpack(a))
```

This is a regression because such a script would have worked before and now
its crashing Redis. The solution is to clear the function arguments from the Lua
stack which makes the original assumption true and the assertion unreachable.","['src/scripting.c', 'tests/unit/scripting.tcl']","Lua stack overflows due to inclusion of user arguments in the stack count, particularly in the `redisProtocolToLuaType` function, leading to Redis crash when dealing with calls with many arguments."
2bd6802fa1bd48260022435e9a040b8233d6cdfc,1542643234,"Stream: fix XREADGROUP history reading of deleted messages.

This commit fixes #5570. It is a similar bug to one fixed a few weeks
ago and is due to the range API to be called with NULL as ""end ID""
parameter instead of repeating again the start ID, to be sure that we
selectively issue the entry with a given ID, or we get zero returned
(and we know we should emit a NULL reply).
",['src/t_stream.c'],"XREADGROUP history reading function has an issue with deleted messages, specifically when the ""end ID"" parameter is NULL instead of repeating the start ID, leading to incorrect entry selection."
b002d2b4f1415f4db805081bc8f5b85d00f30e33,1600356010,"Remove tmp rdb file in background thread (#7762)

We're already using bg_unlink in several places to delete the rdb file in the background,
and avoid paying the cost of the deletion from our main thread.
This commit uses bg_unlink to remove the temporary rdb file in the background too.

However, in case we delete that rdb file just before exiting, we don't actually wait for the
background thread or the main thread to delete it, and just let the OS clean up after us.
i.e. we open the file, unlink it and exit with the fd still open.

Furthermore, rdbRemoveTempFile can be called from a thread and was using snprintf which is
not async-signal-safe, we now use ll2string instead.","['src/rdb.c', 'src/rdb.h', 'src/server.c', 'src/server.h', 'tests/test_helper.tcl', 'tests/unit/shutdown.tcl']","The use of the synchronous file deletion function for temporary RDB files may cause unnecessary delay in the main thread, disrupting process performance. The snprintf function, used in rdbRemoveTempFile, is not async-signal-safe, causing potential thread-safety issues."
53c43fcc847d00d101c1e97563d7e3adc8b94143,1643882271,"Add check min-slave-* feature when evaluating Lua scripts and Functions (#10160)

Add check enough good slaves for write command when evaluating scripts.
This check is made before the script is executed, if we have function flags, and per redis command if we don't.

Co-authored-by: Phuc. Vo Trong <phucvt@vng.com.vn>
Co-authored-by: Oran Agra <oran@redislabs.com>
Co-authored-by: Meir Shpilraien (Spielrein) <meir@redis.com>","['src/script.c', 'tests/integration/replication-4.tcl']","There is a lack of effective slave-checking mechanism during Lua script evaluation and functions execution, which might lead to problems with write commands."
135998ed8d0fc92d7ee6e23b1a66353f5900695f,1652453749,"Update comments on command args, and a misleading error reply (#10645)

Updated the comments for:
info command
lmpopCommand and blmpopCommand
sinterGenericCommand 

Fix the missing ""key"" words in the srandmemberCommand function
For LPOS command, when rank is 0, prompt user that rank could be
positive number or negative number, and add a test for it
","['src/sentinel.c', 'src/server.c', 'src/t_list.c', 'src/t_set.c', 'tests/unit/type/list.tcl']","Comments on several commands are outdated and error message for LPOS command when rank is 0 is misleading, creating confusion for users."
595ecd5f4be39eeec71fb07f687b2d6b7cf5c20c,1609864913,"sdscatfmt call sdsMakeRoomFor, asked for more space than intended (#8286)

instead of asking for the extra new space it wanted, it asked to grow the
string by the size it already has too.
i.e. a string of 1000 bytes, needing to grow by 10 bytes, would have been
asking for an **additional** 1010 bytes.",['src/sds.c'],"Bug: `sdscatfmt` function incorrectly requests additional space equal to the current size of the string plus the new space required, leading to over-allocation."
7e5ded2ad0521600ceb57d71b0dbb19bbbc087b7,1642938890,"Fix timing issue in sentinel CKQUORUM test (#10036)

A test failure was reported in Daily CI (test-centos7-tls).
`CKQUORUM detects failover authorization cannot be reached`.

```
CKQUORUM detects failover authorization cannot be reached: FAILED:
Expected 'invalid command name ""OK 4 usable Sentinels. Quorum and failover authorization can be reached""' to match '*NOQUORUM*'
```

It seems that current sentinel does not confirm that the other
sentinels are actually `down`, and then check the quorum.
It at least take 3 seconds on my machine, and we can see there
will be a timing issue with the hard code `after 5000`.

In this commit, we check the response of `SENTINEL SENTINELS mymaster`
to ensure that other sentinels are actually `down` in the view the
current sentinel. Solve the timing issue due to sentinel monitor mechanism.",['tests/sentinel/tests/06-ckquorum.tcl'],"Sentinel CKQUORUM test failure due to timing issue, sentinel failover authorization is reached before confirming other sentinels are 'down'."
8f13ac10b46de0c6f3cb9fd5c9e86824aa1e8355,1670317971,"Fix command line startup --sentinel problem (#11591)

There is a issue with --sentinel:
```
[root]# src/redis-server sentinel.conf --sentinel --loglevel verbose

*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***
Reading the configuration file, at line 352
>>> 'sentinel ""--loglevel"" ""verbose""'
Unrecognized sentinel configuration statement
```

This is because in #10660 (Redis 7.0.1), `--` prefix change break it.
In this PR, we will handle `--sentinel` the same as we did for `--save`
in #10866. i.e. it's a pseudo config option with no value.",['src/server.c'],The `--sentinel` command line option is being unrecognized as a configuration statement causing startup failures. Likely due to `--` prefix change.
cdd925b2898ac270afdf3d72f065410a96980f80,1597728523,"Trim trailing spaces in error replies coming from rejectCommand (#7668)

65a3307bc9 added rejectCommand which takes an robj reply and passes it
through addReplyErrorSafe to addReplyErrorLength.
The robj contains newline at it's end, but addReplyErrorSafe converts it
to spaces, and passes it to addReplyErrorLength which adds the protocol
newlines.

The result was that most error replies (like OOM) had extra two trailing
spaces in them.",['src/networking.c'],Error replies coming from rejectCommand have two trailing spaces due to the conversion of newline characters into spaces.
91f3689bf5dc4ce3cf00d9d957b9677b362a205e,1622981346,"XTRIM call streamParseAddOrTrimArgsOrReply use wrong arg xadd. (#9047)

xtrimCommand call streamParseAddOrTrimArgsOrReply should use xadd==0.

When the syntax is valid, it does not cause any bugs because the params of XADD is superset of XTRIM.
Just XTRIM will not respond with error on invalid syntax. The syntax of XADD will also be accpeted by XTRIM.",['src/t_stream.c'],"The XTRIM command does not respond with an error on invalid syntax and accepts syntax of XADD wrongly, potentially causing issues during usage."
8bd01a07ae75609a36335ab34b37da41da1b1bf2,1649740577,"Allow specifying ACL reason for module log entry (#10559)

Allow specifying an ACL log reason, which is shown in the log. Right now it always shows ""unknown"", which is a little bit cryptic. This is a breaking change, but this API was added as part of 7 so it seems ok to stabilize it still.","['src/module.c', 'src/redismodule.h', 'tests/modules/aclcheck.c', 'tests/unit/moduleapi/aclcheck.tcl']","The ACL log always displays ""unknown"", providing insufficient information about reason for the log entry."
36b949438547eb5bf8555fcac2c5040528fd7854,1595339779,"testsuite may leave servers alive on error (#7549)

in cases where you have
test name {
  start_server {
    start_server {
      assert
    }
  }
}

the exception will be thrown to the test proc, and the servers are
supposed to be killed on the way out. but it seems there was always a
bug of not cleaning the server stack, and recently (#7404) we started
relying on that stack in order to kill them, so with that bug sometimes
we would have tried to kill the same server twice, and leave one alive.

luckly, in most cases the pattern is:
start_server {
  test name {
  }
}",['tests/support/server.tcl'],"In certain nested server start scenarios with assertions, an exception can lead to servers not being properly cleaned up, potentially leaving a server running."
639b73cd2ad906aad583ee404ac4b2f8b95a9c34,1604476818,"redis-cli cluster import support source and target that need auth (#7994)

Make it possible for redis-cli cluster import to work with source and
target that require AUTH.

Adding two different flags --cluster-from-user, --cluster-from-pass
and --cluster-askpass for source node authentication.
Also for target authentication, using existing --user and --pass flag.

Example:

./redis-cli --cluster import 127.0.0.1:7000 --cluster-from 127.0.0.1:6379 --pass 1234 --user default --cluster-from-user default --cluster-from-pass 123456

./redis-cli --cluster import 127.0.0.1:7000 --cluster-from 127.0.0.1:6379 --askpass --cluster-from-user default --cluster-from-askpass

",['src/redis-cli.c'],"Redis-cli cluster import currently doesn't support source and target nodes that require authentication, making it unable to handle secured connections."
2f6ed9333f082b9cd27bfab05013521b88f077c7,1546455058,"Fix broken interval and repeat bahaviour in redis-cli (incluing cluster mode)

This addresses two problems, one where infinite (negative) repeat count is broken for all types for Redis,
and another specific to cluster mode where redirection is needed.

Now allows and works correctly for negative (i.e. -1) repeat values passed with `-r` argument to redis-cli
as documented here https://redis.io/topics/rediscli#continuously-run-the-same-command which seems to have
regressed as a feature in 95b988 (though that commit removed bad integer wrap-around to `0` behaviour).

This broken behaviour exists currently (e50458), and redis-cli will just exit immediately with repeat `-r <= 0`
as opposed to send commands indefinitely as it should with `-r < 0`

Additionally prevents a repeat * interval seconds hang/time spent doing nothing at the start before issuing
commands in cluster mode (`-c`), where the command needed to redirect to a slot on another node, as commands
where failing and waiting to be reissued but this was fully repeated before being reissued. For example,

        redis-cli -c -r 10 -i 0.5 INCR test_key_not_on_6379

Would hang and show nothing for 5 seconds (10 * 0.5) before showing

        (integer) 1
        (integer) 2
        (integer) 3
        (integer) 4
        (integer) 5
        (integer) 6
        (integer) 7
        (integer) 8
        (integer) 9
        (integer) 10

at half second intervals as intended.
",['src/redis-cli.c'],"Redis-cli exits immediately when repeat `-r <= 0` should send commands indefinitely. Also, redis-cli in cluster mode involves an unnecessary delay resulting from a failed command waiting to be reissued for repeat * interval seconds."
a84c964d37a1899bf90c920efef85a1d7202d058,1641726411,"Fix crash when error [sub]command name contains | (#10082)

The following error commands will crash redis-server:
```
> get|
Error: Server closed the connection
> get|set
Error: Server closed the connection
> get|other
```

The reason is in #9504, we use `lookupCommandBySds` for find the
container command. And it split the command (argv[0]) with `|`.
If we input something like `get|other`, after the split, `get`
will become a valid command name, pass the `ERR unknown command`
check, and finally crash in `addReplySubcommandSyntaxError`

In this case we do not need to split the command name with `|`
and just look in the commands dict to find if `argv[0]` is a
container command.

So this commit introduce a new function call `isContainerCommandBySds`
that it will return true if a command name is a container command.

Also with the old code, there is a incorrect error message:
```
> config|get set
(error) ERR Unknown subcommand or wrong number of arguments for 'set'. Try CONFIG|GET HELP.
```

The crash was reported in #10070.","['src/networking.c', 'src/server.c', 'tests/unit/other.tcl']","Redis-server crashes when error subcommand name contains the '|' character. Additionally, an error message concerning command arguments appears incorrectly formatted."
cf3323dba473f3849200d294bb0349cd442f2006,1654148215,"Fix bugs in CONFIG REWRITE, omitting rename-command and include lines, and inserting comments around module and acl configs (#10761)

A regression from #10285 (redis 7.0).
CONFIG REWRITE would put lines with: `include`, `rename-command`,
`user`,  `loadmodule`, and any module specific config in a comment.

For ACL `user`, `loadmodule` and module specific configs would be
re-inserted at the end (instead of updating existing lines), so the only
implication is a messy config file full of comments.

But for `rename-command` and `include`, the implication would be that
they're now missing, so a server restart would lose them.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/config.c', 'tests/unit/acl.tcl', 'tests/unit/introspection.tcl']",CONFIG REWRITE is malfunctioning and causing the omission of `rename-command` and `include` lines. This results in the loss of these commands upon server restart.
20c33fe6a8fce2edb3e4158bc10bde2c3740a25b,1642010714,"Show subcommand full name in error log / ACL LOG (#10105)

Use `getFullCommandName` to get the full name of the command.
It can also get the full name of the subcommand, like ""script|help"".

Before:
```
> SCRIPT HELP
(error) NOPERM this user has no permissions to run the 'help' command or its subcommand

> ACL LOG
    7) ""object""
    8) ""help""
```

After:
```
> SCRIPT HELP
(error) NOPERM this user has no permissions to run the 'script|help' command

> ACL LOG
    7) ""object""
    8) ""script|help""
```

Fix #10094","['src/acl.c', 'src/server.c', 'tests/unit/acl.tcl']","Error logs and ACL LOG do not display the full subcommand name, leading to confusion when interpreting them."
e5d8a5eb85b50ee7da1bf652c7d67e8e5b757ec9,1623757579,"Fix the wrong reisze of querybuf (#9003)

The initialize memory of `querybuf` is `PROTO_IOBUF_LEN(1024*16) * 2` (due to sdsMakeRoomFor being greedy), under `jemalloc`, the allocated memory will be 40k.
This will most likely result in the `querybuf` being resized when call `clientsCronResizeQueryBuffer` unless the client requests it fast enough.

Note that this bug existed even before #7875, since the condition for resizing includes the sds headers (32k+6).

## Changes
1. Use non-greedy sdsMakeRoomFor when allocating the initial query buffer (of 16k).
1. Also use non-greedy allocation when working with BIG_ARG (we won't use that extra space anyway)
2. in case we did use a greedy allocation, read as much as we can into the buffer we got (including internal frag), to reduce system calls.
3. introduce a dedicated constant for the shrinking (same value as before)
3. Add test for querybuf.
4. improve a maxmemory test by ignoring the effect of replica query buffers (can accumulate many ACKs on slow env)
5. improve a maxmemory by disabling slowlog (it will cause slight memory growth on slow env).","['src/networking.c', 'src/sds.c', 'src/sds.h', 'src/server.c', 'src/server.h', 'tests/test_helper.tcl', 'tests/unit/maxmemory.tcl', 'tests/unit/querybuf.tcl']","The querybuf resizing mechanism is behaving inconsistently resulting in frequent and unnecessary memory reallocations, majorly caused by greedy memory initialization and unoptimized memory adjustments for BIG_ARG."
8a81ed1b5ab204fd33651bf448cd5e1230631fb7,1610531892,"Fix use of  lookupKeyRead and lookupKeyWrite in zrangeGenericCommand, zunionInterDiffGenericCommand (#8316)

* Change zunionInterDiffGenericCommand to use lookupKeyRead if dstkey is null
* Change zrangeGenericCommand to use lookupKey Write if dstkey isn't null

ZRANGESTORE and UNION, ZINTER, ZDIFF are all new commands (6.2 RC1 and RC2).
In redis 6.0 the ZRANGE was using lookupKeyRead, and ZUNIONSTORE / ZINTERSTORE were using lookupKeyWrite.
So there bugs are introduced in 6.2 and will be resolved before it is released.

the implications of this bug are also not big:
The sole difference between LookupKeyRead and LookupKeyWrite is for command executed on a replica, which are not received from its master client. (for the master, and for the master client on the replica, these two functions behave the same)!",['src/t_zset.c'],"New commands in Redis 6.2 (ZRANGESTORE, ZINTER, ZDIFF, UNION) using incorrect lookupKey function for dstkey, could behave differently on a replica not received from its master client."
32f45215c33e8b8d801c4c339e18504918082936,1685343317,"Try lazyfree temp zset in ZUNION / ZINTER / ZDIFF and optimize ZINTERCARD to avoid create temp zset (#12229)

We check lazyfree_lazy_server_del in sunionDiffGenericCommand
to see if we need to lazyfree the temp set. Now do the same in
zunionInterDiffGenericCommand to lazyfree the temp zset.

This is a minor change, follow #5903. Also improved the comments.

Additionally, avoid creating unused zset object in ZINTERCARD,
results in some 10% performance improvement. ","['src/t_set.c', 'src/t_zset.c']","ZUNION/ZINTER/ZDIFF operations in Redis are causing unnecessary memory usage due to persistence of temporary zset. Also, unsued zset objects in ZINTERCARD impacting overall performance efficiency."
f1e7df4b7c0dfb8ceaaa2e844b2b29024e8dfbcc,1552064951,"Increment delivery counter on XCLAIM unless RETRYCOUNT specified

The XCLAIM docs state the XCLAIM increments the delivery counter for
messages. This PR makes the code match the documentation - which seems
like the desired behaviour - whilst still allowing RETRYCOUNT to be
specified manually.

My understanding of the way streamPropagateXCLAIM() works is that this
change will safely propagate to replicas since retry count is pulled
directly from the streamNACK struct.

Fixes #5194
","['src/t_stream.c', 'tests/unit/type/stream-cgroups.tcl']",The XCLAIM function doesn't increment the delivery counter for messages as per documentation when RETRYCOUNT is not specified.
9e1b879f5bcbf105d5581c77f8c368b0ef1708f6,1666460477,"Make PFMERGE source key optional in docs, add tests with one input key, add tests on missing source keys (#11205)

The following two cases will create an empty destkey HLL:
1. called with no source keys, like `pfmerge destkey`
2. called with non-existing source keys, like `pfmerge destkey non-existing-source-key`

In the first case, in `PFMERGE`, the dest key is actually one of the source keys too.
So `PFMERGE k1 k2` is equivalent to `SUNIONSTORE k1 k1 k2`,
and `PFMERGE k1` is equivalent to `SUNIONSTORE k1 k1`.
So the first case is reasonable, the source key is actually optional.

And the second case, `PFMERGE` on missing keys should succeed and create an empty dest.
This is consistent with `PFCOUNT`, and also with `SUNIONSTORE`, no need to change.","['src/commands.c', 'src/help.h', 'tests/unit/hyperloglog.tcl']","`PFMERGE` command in Redis is causing ambiguities and unexpected results when used without source keys or with missing source keys, leading to the creation of an unintended empty destination HyperLogLog (HLL)."
75987431f008e0072a70e7d5c001c1724a7fb421,1519733202,"AOF: fix a bug that may prevent proper fsyncing when fsync=always.

In case the write handler is already installed, it could happen that we
serve the reply of a query in the same event loop cycle we received it,
preventing beforeSleep() from guaranteeing that we do the AOF fsync
before sending the reply to the client.

The AE_BARRIER mechanism, introduced in a previous commit, prevents this
problem. This commit makes actual use of this new feature to fix the
bug.
",['src/networking.c'],"The reply of a query may be served in the same event loop cycle it's received, preventing guaranteed AOF fsync before response is sent to the client."
e57a4db5d753e427317075071b0992fd965168b3,1638461898,"Fix CONFIG SET test failures in MacOS/FreeBSD (#9881)

After the introduction of `Multiparam config set` in #9748,
there are two tests cases failed.

```
[exception]: Executing test client: ERR Config set failed - Failed to set current oom_score_adj. Check server logs..
ERR Config set failed - Failed to set current oom_score_adj. Check server logs.
```

`CONFIG sanity` test failed on the `config set oom-score-adj-values`
which is a ""special"" config that does not catch no-op changes.
And then it will update `oom-score-adj` which not supported in
MacOs. We solve it by adding `oom-score*` to the `skip_configs` list.

```
*** [err]: CONFIG SET rollback on apply error in tests/unit/introspection.tcl
Expected an error but nothing was caught
```

`CONFIG SET rollback on apply error` test failed on the
`config set port $used_port`. In theory, it should throw the
error `Unable to listen on this port*`. But it failed on MacOs.
We solve it by adding `-myaddr 127.0.0.1` to the socket call.
",['tests/unit/introspection.tcl'],`Multiparam config set` leads to test failures in MacOS/FreeBSD due to failure in setting 'oom_score_adj' and inability to catch the 'Unable to listen on this port' error in CONFIG SET rollback.
2854637385f6f44661ebd8833d852c068039e641,1656244972,"Support conversion between `RedisModuleString` and `unsigned long long` (#10889)

Since the ranges of `unsigned long long` and `long long` are different, we cannot read an
`unsigned long long` integer from a `RedisModuleString` by `RedisModule_StringToLongLong` . 

So I added two new Redis Module APIs to support the conversion between these two types:
* `RedisModule_StringToULongLong`
* `RedisModule_CreateStringFromULongLong`

Signed-off-by: RinChanNOWWW <hzy427@gmail.com>","['src/module.c', 'src/redismodule.h', 'tests/modules/misc.c', 'tests/unit/moduleapi/misc.tcl']","`RedisModuleString` cannot convert to or from `unsigned long long` integers, as the `RedisModule_StringToLongLong` function only supports `long long` data type."
f9dacf8aacc8af7fdcb41bc9f927b42390557f7d,1610028869,"Refactory fork child related infra, Unify child pid

This is a refactory commit, isn't suppose to have any actual impact.
it does the following:
- keep just one server struct fork child pid variable instead of 3
- have one server struct variable indicating the purpose of the current fork
  child.
- redisFork is now responsible of updating the server struct with the pid,
  which means it can be the one that calls updateDictResizePolicy
- move child info pipe handling into redisFork instead of having them
  repeated outside
- there are two classes of fork purposes, mutually exclusive group (AOF, RDB,
  Module), and one that can create several forks to coexist in parallel (LDB,
  but maybe Modules some day too, Module API allows for that).
- minor fix to killRDBChild:
  unlike killAppendOnlyChild and TerminateModuleForkChild, the killRDBChild
  doesn't clear the pid variable or call wait4, so checkChildrenDone does
  the cleanup for it.
  This commit removes the explicit calls to rdbRemoveTempFile, closeChildInfoPipe,
  updateDictResizePolicy, which didn't do any harm, but where unnecessary.
","['src/aof.c', 'src/db.c', 'src/module.c', 'src/networking.c', 'src/rdb.c', 'src/replication.c', 'src/server.c', 'src/server.h']","Multiple fork child PID variables in server struct lead to redundancy and potential confusions. Additionally, child info pipe handling is repeated outside redisFork, leading to inefficiency and clutter."
8163e816fe9b1ef7f1a904d862f6e2e24bc19713,1683033638,"[redis-benchmark] Adding --seed option to seed the RNG (#11945)

Adds ability to set the random seed so that more consistent repeatable benchmarks are possible.

Example usage:

Adding 2 hash items
```
src/redis-benchmark -r 100 -n 2 --seed 250 hset myhash:__rand_int__ age __rand_int__
```

Monitor:

1st benchmark invocation:
```
1679332814.824357 [0 127.0.0.1:36686] ""hset"" ""myhash:000000000022"" ""age"" ""000000000069""
1679332814.824404 [0 127.0.0.1:36690] ""hset"" ""myhash:000000000007"" ""age"" ""000000000043""
```

2nd benchmark invocation:
```
1679332814.824357 [0 127.0.0.1:36686] ""hset"" ""myhash:000000000022"" ""age"" ""000000000069""
1679332814.824404 [0 127.0.0.1:36690] ""hset"" ""myhash:000000000007"" ""age"" ""000000000043""
```",['src/redis-benchmark.c'],"Redis benchmark tool doesn't allow for seeding the random number generator, making it hard to achieve repeatable benchmarks."
1eb4baa5b8e76adc337ae9fab49acc2585a0cdd0,1625635586,"Direct redis-cli repl prints to stderr, because --rdb can print to stdout. fflush stdout after responses  (#9136)

1. redis-cli can output --rdb data to stdout
   but redis-cli also write some messages to stdout which will mess up the rdb.

2. Make redis-cli flush stdout when printing a reply
  This was needed in order to fix a hung in redis-cli test that uses
  --replica.
   Note that printf does flush when there's a newline, but fwrite does not.

3. fix the redis-cli --replica test which used to pass previously
   because it didn't really care what it read, and because redis-cli
   used printf to print these other things to stdout.

4. improve redis-cli --replica test to run with both diskless and disk-based.

Co-authored-by: Oran Agra <oran@redislabs.com>
Co-authored-by: Viktor Söderqvist <viktor@zuiderkwast.se>","['src/redis-cli.c', 'tests/integration/redis-cli.tcl']","redis-cli's mixed usage of stdout for both --rdb data and command responses leads to muddled output, affecting the data integrity. Concurrently, non-flushing of stdout post writing a reply results in redis-cli tests failing under --replica mode."
fa675256c127963c74ea68f8bab22ef105bada02,1540987007,"Add support for Sentinel authentication.

So far it was not possible to setup Sentinel with authentication
enabled. This commit introduces this feature: every Sentinel will try to
authenticate with other sentinels using the same password it is
configured to accept clients with.

So for instance if a Sentinel has a ""requirepass"" configuration
statemnet set to ""foo"", it will use the ""foo"" password to authenticate
with every other Sentinel it connects to. So basically to add the
""requirepass"" to all the Sentinels configurations is enough in order to
make sure that:

1) Clients will require the password to access the Sentinels instances.
2) Each Sentinel will use the same password to connect and authenticate
   with every other Sentinel in the group.

Related to #3279 and #3329.
",['src/sentinel.c'],"Sentinel setup does not support authentication, making it impossible to secure communication between different Sentinels in a group or ensure clients require a password to access Sentinel instances."
bc82309ceb0cc4cbdf74faac6816b49b981199be,1681401941,"Adding missing test cases for linsert command (#12040)

Currently LINSERT command does not have the test case coverage for following scenarios.
1. When key does not exist, it is considered an empty list and no operation is performed.
2. An error is returned when key exists but does not hold a list value.

Added above two missing test cases for linsert command.
",['tests/unit/type/list.tcl'],"LINSERT command lacks test cases for scenarios: 1. Where the key does not exist and is treated as an empty list, with no operation performed. 2. When a key exists, but holds a non-list value, returning an error."
719db14ec78d9da1c8abe3577cb77019c4c61eb3,1649580091,"COMMAND DOCS shows module name, where applicable (#10544)

Add field to COMMAND DOCS response to denote the name of the module
that added that command.
COMMAND LIST can filter by module, but if you get the full commands list,
you may still wanna know which command belongs to which module.
The alternative would be to do MODULE LIST, and then multiple calls to COMMAND LIST","['src/module.c', 'src/server.c', 'src/server.h', 'tests/unit/moduleapi/cmdintrospection.tcl', 'tests/unit/moduleapi/subcommands.tcl']","The COMMAND DOCS response does not mention the name of the module that added a particular command, making it hard to filter particular commands for specific modules."
ffbe36fc3ee824407b3818c0a6f861176ce56482,1639652080,"Command table: Sorted subcommands (#9951)

Sort the sub-commands so that every time we execute the script it generates the exact same results.
This will case less merge conflicts if two PRs edit different json files.

also:
* make the script agnostic to where it is executed (more flexible).
* add documentation about commands.c and the json files in the readme.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/commands.c', 'utils/generate-command-code.py']","Subcommands are not sorted consistently which causes merge conflicts when multiple PRs edit different json files. Also, the script execution is location-dependent and lacks sufficient documentation."
01acfa71ca972a79d215160104fad5f8e6e824af,1603692299,"redis-benchmark: add tests, --version, a minor bug fixes (#7947)

- add test suite coverage for redis-benchmark
- add --version (similar to what redis-cli has)
- fix bug sending more requests than intended when pipeline > 1.
- when done sending requests, avoid freeing client in the write handler, in theory before
  responses are received (probably dead code since the read handler will call clientDone first)

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/redis-benchmark.c', 'tests/integration/redis-benchmark.tcl', 'tests/support/benchmark.tcl', 'tests/test_helper.tcl']",Redis-benchmark is executing more requests than intended when pipeline > 1 and there is also a risk of freeing client prematurely before receiving responses.
8aad2ac35201fd6feadb6f59e7cc0c2afb19129b,1660467018,"Add missing lua_pop in luaGetFromRegistry (#11097)

This pr mainly has the following four changes:

1. Add missing lua_pop in `luaGetFromRegistry`.
    This bug affects `redis.register_function`, where `luaGetFromRegistry` in
    `luaRegisterFunction` will return null when we call `redis.register_function` nested.
    .e.g
    ```
    FUNCTION LOAD ""#!lua name=mylib \n local lib=redis \n lib.register_function('f2', function(keys, args) lib.register_function('f1', function () end) end)""
    fcall f2 0
    ````
    But since we exit when luaGetFromRegistry returns null, it does not cause the stack to grow indefinitely.

3. When getting `REGISTRY_RUN_CTX_NAME` from the registry, use `serverAssert`
    instead of error return. Since none of these lua functions are registered at the time
    of function load, scriptRunCtx will never be NULL.
4. Add `serverAssert` for `luaLdbLineHook`, `luaEngineLoadHook`.
5. Remove `luaGetFromRegistry` from `redis_math_random` and
    `redis_math_randomseed`, it looks like they are redundant.","['src/eval.c', 'src/function_lua.c', 'src/script_lua.c', 'tests/unit/functions.tcl']",Nested invocations of `redis.register_function` (using `luaGetFromRegistry` in `luaRegisterFunction`) return null causing erroneous state behavior in the lua registry function operation.
233da29f4da7360f691fc491b03278c8d56fe7de,1521564825,"CG: propagate XACK by incrementing server.dirty.

This should be more than enough, even if in case of partial IDs that are
not found, we send all the IDs to the slave/AOF, but this is definitely
a corner case without bad effects if not some wasted space.
",['src/t_stream.c'],"The server does not propagate XACKs, leading to potential cases of unfound partial IDs and wasted space due to excess IDs sent to the slave/AOF."
ac2c96f5b1714b36f215fc9caec87e746a804041,1580972003,"A few non-data commands that should be allowed while loading or stale

SELECT, and HELLO are commands that may be executed by the client
as soon as it connects, there's no reason to block them, preventing the
client from doing the rest of his sequence (which might just be INFO or
CONFIG, etc).

MONITOR, DEBUG, SLOWLOG, TIME, LASTSAVE are all non-data accessing
commands, which there's no reason to block.
",['src/server.c'],"Blocking occurs when client attempts to execute SELECT, HELLO, MONITOR, DEBUG, SLOWLOG, TIME, LASTSAVE commands while loading or stale, preventing further client operations."
5075e74366c99033886bce13fcd504a2c57fa180,1650436700,"Optimized `hdr_value_at_percentile` (#10606)

`hdr_value_at_percentile()` is part of the Hdr_Histogram library
used when generating `latencystats` report. 

There's a pending optimization for this function which greatly
affects the performance of `info latencystats`.
https://github.com/HdrHistogram/HdrHistogram_c/pull/107

This PR:
1. Upgrades the sources in _deps/hdr_histogram_ to the latest Hdr_Histogram
  version 0.11.5
2. Applies the referenced optimization.
3. Adds minor documentation about the hdr_histogram dependency which was
  missing under _deps/README.md_.

benchmark on my machine:
running: `redis-benchmark -n 100000 info latencystats` on a clean build with no data.

| benchmark | RPS |
| ---- | ---- |
| before upgrade to v0.11.05  | 7,681 |
| before optimization | 12,474 |
| after optimization | 52,606 |

Co-authored-by: filipe oliveira <filipecosta.90@gmail.com>","['deps/hdr_histogram/hdr_alloc.c', 'deps/hdr_histogram/hdr_alloc.h', 'deps/hdr_histogram/hdr_histogram.c', 'deps/hdr_histogram/hdr_histogram.h', 'deps/hdr_histogram/hdr_redis_malloc.h', 'deps/hdr_histogram/hdr_tests.h', 'src/server.c']",The `hdr_value_at_percentile()` function used in the `latencystats` report is suffering from performance issues.
ae418eca24ba53a7dca07b0e7065f856e625469b,1624371760,"Adjustments to recent RM_StringTruncate fix (#3718) (#9125)

- Introduce a new sdssubstr api as a building block for sdsrange.
  The API of sdsrange is many times hard to work with and also has
  corner case that cause bugs. sdsrange is easy to work with and also
  simplifies the implementation of sdsrange.
- Revert the fix to RM_StringTruncate and just use sdssubstr instead of
  sdsrange.
- Solve valgrind warnings from the new tests introduced by the previous
  PR.","['src/module.c', 'src/sds.c', 'src/sds.h', 'tests/modules/basics.c']","Issues with RM_StringTruncate due to the complex API of sdsrange causing bugs and complications, alongside Valgrind warnings from new tests."
324070c8f6f63240629893c2ce9bcbeecf6e77fb,1609830910,"Fix rdb checksum / crc64 on bigendian (#8270)

Turns out the RDB checksum in Redis 6.0 on bigendian is broken.
It always returned 0, so the RDB files are generated as if checksum is
disabled, and will be loaded ok on littleendian, and on bigendian.
But it'll not be able to load RDB files generated on littleendian or older versions.

Similarly DUMP and RESTORE will work on the same version (0==0),
but will be unable to exchange dump payloads with littleendian or old versions.",['src/crcspeed.c'],"RDB checksum in Redis 6.0 on bigendian consistently returns 0, leading to generation of RDB files as if the checksum is disabled and preventing loading of RDB files generated on littleendian or older versions."
49876158cc2e77bba8e4658fa757891310dc37b6,1655915422,"fix redis-benchmark's bug: check if clients are created successfully in idle mode (#10891)

my maxclients config:
```
redis-cli config get maxclients
1) ""maxclients""
2) ""4064""
```

Before this bug was fixed, creating 4065 clients appeared to be successful, but only 4064 were actually created```
```
./redis-benchmark -c 4065 -I
Creating 4065 idle connections and waiting forever (Ctrl+C when done)
cients: 4065
```

now :
```
./redis-benchmark -c 4065 -I
Creating 4065 idle connections and waiting forever (Ctrl+C when done)
Error from server: ERR max number of clients reached

./redis-benchmark -c 4064 -I
Creating 4064 idle connections and waiting forever (Ctrl+C when done)
clients: 4064

```","['src/redis-benchmark.c', 'tests/integration/redis-benchmark.tcl']","Redis-benchmark falsely reports successful creation of more clients than the configured maxclients limit in idle mode, causing server error later."
ecd53518700f5b2be19c18e7a19f87c9f25a7ff5,1609760537,"Fix assertion on loading AOF with timed out script. (#8284)

If AOF file contains a long Lua script that timed out, then the `evalCommand` calls
`blockingOperationEnds` which sets `server.blocked_last_cron` to 0. later on,
the AOF `whileBlockedCron` function asserts that this value is not 0.

The fix allows nesting call to `blockingOperationStarts` and `blockingOperationEnds`.

The issue was first introduce in this commit: 9ef8d2f67 (Redis 6.2 RC1)","['src/server.c', 'src/server.h']","Loading an AOF file containing a long, timed-out Lua script causes an assertion error in the `whileBlockedCron` function due to 'server.blocked_last_cron' being set to 0."
69017fa232093728a84dfcb1befe7bc4b204c182,1647450698,"Fix redis-cli CLUSTER SETSLOT race conditions (#10381)

After migrating a slot, send CLUSTER SETSLOT NODE to the destination
node first to make sure the slot isn't left without an owner in case
the destination node crashes before it is set as new owner.

When informing the source node, it can happen that the destination
node has already informed it and if the source node has lost its
last slot, it has already turned itself into a replica. Redis-cli
should ignore this error in this case.","['src/redis-cli.c', 'tests/support/server.tcl', 'tests/unit/cluster.tcl']","After migrating a slot, CLUSTER SETSLOT NODE can leave the slot ownerless if the destination node crashes before the new ownership is set. This can lead to undesired results when the source node is informed of the change."
91f4f41665c4e9e0ad248ce0b528644de28d0acd,1617136822,"Add replica-announced config option (#8653)

The 'sentinel replicas <master>' command will ignore replicas with
`replica-announced` set to no.

The goal of disabling the config setting replica-announced is to allow ghost
replicas. The replica is in the cluster, synchronize with its master, can be
promoted to master and is not exposed to sentinel clients. This way, it is
acting as a live backup or living ghost.

In addition, to prevent the replica to be promoted as master, set
replica-priority to 0.","['src/config.c', 'src/sentinel.c', 'src/server.c', 'src/server.h', 'tests/sentinel/tests/10-replica-priority.tcl']","When 'replica-announced' is set to no, sentinel clients can't interact with replicas, thus enabling ghost replicas. This could cause issues as these replicas can still synchronize with their master and even be promoted to master."
e138698e54e97bfaababf56507026bf92dd4deb4,1616757001,"make processCommand check publish channel permissions. (#8534)

Add publish channel permissions check in processCommand.

processCommand didn't check publish channel permissions, so we can
queue a publish command in a transaction. But when exec the transaction,
it will fail with -NOPERM.

We also union keys/commands/channels permissions check togegher in
ACLCheckAllPerm. Remove pubsubCheckACLPermissionsOrReply in 
publishCommand/subscribeCommand/psubscribeCommand. Always 
check permissions in processCommand/execCommand/
luaRedisGenericCommand.","['src/acl.c', 'src/multi.c', 'src/pubsub.c', 'src/scripting.c', 'src/server.c', 'src/server.h', 'tests/unit/acl.tcl']","`processCommand` doesn't check for publish channel permissions, allowing a publish command to queue in a transaction that eventually fails with -NOPERM."
92dc5e1fa41491e0ab0744a2bab55f837db89dc2,1580987882,"Diskless-load emptyDb-related fixes

1. Call emptyDb even in case of diskless-load: We want modules
   to get the same FLUSHDB event as disk-based replication.
2. Do not fire any module events when flushing the backups array.
3. Delete redundant call to signalFlushedDb (Called from emptyDb).
","['src/db.c', 'src/replication.c', 'src/server.h']","In the diskless load scenario, modules are not receiving the same FLUSHDB event as disk-based replication, leading to inconsistency. Redundant module events being fired when flushing the backups array and multiple calls to signalFlushedDb leading to inefficiencies."
a66814fd7ac2b37cefb43f2ec03dd8684ebcccf2,1614578092,"Make dbid range check for SWAPDB command consistent with SELECT, MOVE, and COPY. (#8555)

DB ID used to be parsed as a long for SWAPDB command, now
make it into an int to be consistent with other commands that parses
the DB ID argument like SELECT, MOVE, COPY. See #8085

The implication is that the error message when the provided db index
is greater than 4M changes slightly.",['src/db.c'],"Inconsistent range check for SWAPDB command leads to different error messages when provided db index is larger than 4M compared to SELECT, MOVE, COPY commands."
fe5d325107c11ff4a0b5f40deae47c3016c9b9d7,1625991991,"Add test for ziplist (nextdiff == -4 && reqlen < 4) (#9218)

The if judgement `nextdiff == -4 && reqlen < 4` in __ziplistInsert.
It's strange, but it's useful. Without it there will be problems during
chain update.
Till now these lines didn't have coverage in the tests, and there was
a question if they are at all needed (#7170)",['src/ziplist.c'],Lack of test coverage for the conditional statement `nextdiff == -4 && reqlen < 4` in the __ziplistInsert function raises questions about its necessity and potential unidentified issues during chain updates.
c22d3684bad9bb20825a885ea41a2cc7b30b387c,1636451170,"Fix diskless load handling on broken EOF marker (#9752)

During diskless replication, the check for broken EOF mark is misplaced
and should be earlier. Now we do not swap db, we do proper cleanup and
correctly raise module events on this kind of failure.

This issue existed prior to #9323, but before, the side effect was not restoring
backup and not raising the correct module events on this failure.",['src/replication.c'],"Diskless replication fails to properly check for broken EOF mark resulting in incorrect database swap, insufficient cleanup, and misfired module events."
ef6f902372d4646b1894ec5dbd5f857dea5688d6,1619592667,"redis-benchmark: Error/Warning handling updates. (#8869)

- Immediately exit on errors that are not related to topology updates.
- Deprecates the `-e` option ( retro compatible ) and warns that we now
  exit immediately on errors that are not related to topology updates.
- Fixed wrongfully failing on config fetch error (warning only). This only affects RE.

Bottom line:
- MOVED and ASK errors will not show any warning (unlike the throttled error with `-e` before).
- CLUSTERDOWN still prints an error unconditionally and sleeps for 1 second.
- other errors are fatal.",['src/redis-benchmark.c'],"The redis-benchmark tool does not correctly handle errors unrelated to topology updates, causing unwanted termination. The -e option is also behaving unexpectedly, and there's a misclassification of configuration fetch errors."
a9897b0084919d85c64e6a41a0fea0f882550760,1618747954,"Fix timing of new replication test (#8807)

In github actions CI with valgrind, i saw that even the fast replica
(one that wasn't paused), didn't get to complete the replication fast
enough, and ended up getting disconnected by timeout.

Additionally, due to a typo in uname, we didn't get to actually run the
CPU efficiency part of the test.",['tests/integration/replication.tcl'],"In CI tests, fast replica disconnects due to timeout before completing replication. Also, a typo in uname is causing the CPU efficiency part of the test to not run."
fbff351406ec924f48898badc7a25576a2c1b7b3,1542992472,"Don't treat unsupported protocols as fatal errors

If we encounter an unsupported protocol in the ""bind"" list, don't
ipso-facto consider it a fatal error. We continue to abort startup if
there are no listening sockets at all.

This ensures that the lack of IPv6 support does not prevent Redis from
starting on Debian where we try to bind to the ::1 interface by default
(via ""bind 127.0.0.1 ::1""). A machine with IPv6 disabled (such as some
container systems) would simply fail to start Redis after the initiall
call to apt(8).

This is similar to the case for where ""bind"" is not specified:

  https://github.com/antirez/redis/issues/3894

... and was based on the corresponding PR:

  https://github.com/antirez/redis/pull/4108

... but also adds EADDRNOTAVAIL to the list of errors to catch which I
believe is missing from there.

This issue was raised in Debian as both <https://bugs.debian.org/900284>
& <https://bugs.debian.org/914354>.
",['src/server.c'],"Redis fails to start when encountering unsupported protocols in the ""bind"" list, or when IPv6 support is lacking on systems like Debian that bind to the ::1 interface by default."
a6fd2a46d101d4df23ade2e28cbc04656c721b2b,1642853380,"Improved handling of subcommands (don't allow ACL on first-arg of a sub-command) (#10147)

Recently we added extensive support for sub-commands in for redis 7.0,
this meant that the old ACL mechanism for
sub-commands wasn't needed, or actually was improved (to handle both include
and exclude control, like for commands), but only for real sub-commands.
The old mechanism in ACL was renamed to first-arg, and was able to match the
first argument of any command (including sub-commands).
We now realized that we might wanna completely delete that first-arg feature some
day, so the first step was not to give it new capabilities in 7.0 and it didn't have before.

Changes:
1. ACL: Block the first-arg mechanism on subcommands (we keep if in non-subcommands
  for backward compatibility)
2. COMMAND: When looking up a command, insist the command name doesn't contain
  extra words. Example: When a user issues `GET key` we want `lookupCommand` to return
  `getCommand` but when if COMMAND calls `lookupCommand` with `get|key` we want it to fail.

Other changes:
1. ACLSetUser: prevent a redundant command lookup","['src/acl.c', 'src/server.c', 'tests/unit/acl.tcl', 'tests/unit/introspection-2.tcl']","Sub-commands extension in redis 7.0 is not properly supported by the old ACL mechanism, particularly the first-arg feature. This causes issues with command lookups and potential backward compatibility problems."
a4a0eab52b351b5f152071dda2a993d90f99d64b,1680452384,"redis-cli - handle sensitive command redaction for variadic CONFIG SET (#11975)

In the Redis 7.0 and newer version,
config set command support multiply `<parameter> <value>` pairs, thus the previous
sensitive command condition does not apply anymore

For example:

The command:
**config set maxmemory 1GB masteruser aa** will be written to redis_cli historyfile

In this PR, we update the condition for these sensitive commands
config set masteruser <username>
config set masterauth <master-password>
config set requirepass foobared",['src/redis-cli.c'],"Redis-cli history file contains sensitive information due to variadic CONFIG SET not redacting entries for commands like 'config set masteruser', 'config set masterauth' and 'config set requirepass'."
9ab873d9d35e789a228c5281d57c9c4fdc1e4ce1,1665662634,"MIGTATE with AUTH that contains ""keys"" is getting wrong key names in migrateGetKeys, leads to ACL errors (#11253)

When using the MIGRATE, with a destination Redis that has the user name or password set to the string ""keys"",
Redis would have determine the wrong set of key names the command is gonna access.
This lead to ACL returning wrong authentication result.

Destination instance:
```
127.0.0.1:6380> acl setuser default >keys
OK
127.0.0.1:6380> acl setuser keys on nopass ~* &* +@all
OK
```

Source instance:
```
127.0.0.1:6379> set a 123
OK
127.0.0.1:6379> acl setuser cc on nopass ~a* +@all
OK
127.0.0.1:6379> auth cc 1
OK
127.0.0.1:6379> migrate 127.0.0.1 6380 """" 0 1000 auth keys keys a
(error) NOPERM this user has no permissions to access one of the keys used as arguments
127.0.0.1:6379> migrate 127.0.0.1 6380 """" 0 1000 auth2 keys pswd keys a
(error) NOPERM this user has no permissions to access one of the keys used as arguments
```

Using `acl dryrun` we know that the parameters of `auth` and `auth2` are mistaken for the `keys` option.
```
127.0.0.1:6379> acl dryrun cc migrate whatever whatever """" 0 1000 auth keys keys a
""This user has no permissions to access the 'keys' key""
127.0.0.1:6379> acl dryrun cc migrate whatever whatever """" 0 1000 auth2 keys pswd keys a
""This user has no permissions to access the 'pswd' key""
```

Fix the bug by editing db.c/migrateGetKeys function, which finds the `keys` option and all the keys following.
","['src/db.c', 'tests/unit/acl-v2.tcl']","The MIGRATE command, when used with AUTH containing 'keys', results in incorrect key names being identified in migrateGetKeys, triggering false ACL authentication errors."
f6029fb9256ee1ffa1491f6fa57bb56a9558e952,1584519727,"Fix master replica inconsistency for upgrading scenario.

Before this commit, when upgrading a replica, expired keys will not
be loaded, thus causing replica having less keys in db. To this point,
master and replica's keys is logically consistent. However, before
the keys in master and replica are physically consistent, that is,
they have the same dbsize, if master got a problem and the replica
got promoted and becomes new master of that partition, and master
updates a key which does not exist on master, but physically exists
on the old master(new replica), the old master would refuse to update
the key, thus causing master and replica data inconsistent.

How could this happen?
That's all because of the wrong judgement of roles while starting up
the server. We can not use server.masterhost to judge if the server
is master or replica, since it fails in cluster mode.

When we start the server, we load rdb and do want to load expired keys,
and do not want to have the ability to active expire keys, if it is
a replica.
","['src/rdb.c', 'src/server.c', 'src/server.h']","During the process of upgrading a replica, expired keys not loading may lead to a master-replica inconsistency. The issue is chiefly observable when a master crashes and the upgraded replica is promoted, subsequent key updates can lead to refusals and data inconsistencies."
6444214ce4d032b75b0dac7d579f385ead58850c,1675044024,"Fix master client check in expireIfNeeded() for read only replica (#11761)

Redis 7.0 introduced new logic in expireIfNeeded() where a read-only replica would never consider a key as expired when replicating commands from the master. See acf3495. This was done by checking server.current_client with server.master. However, we should instead check for CLIENT_MASTER flag for this logic to be more robust and consistent with the rest of the Redis code base.",['src/db.c'],The expireIfNeeded() function in Redis 7.0 doesn't consider keys as expired on read-only replicas when replicating commands from the master due to an inconsistent client check.
aabce8932a8ef3617301b142447e05c622b48830,1655960185,"Add bus_port argument in cluster-meet.json (#10304)

In `CLUSTER MEET`, bus-port argument was added in 11436b1.
For cluster announce ip / port implementation, part of the
4.0-RC1.

And in #9389, we add a new cluster-port config and make
cluster bus port configurable, part of the 7.0-RC1.",['src/commands.c'],"The `CLUSTER MEET` function lacks a bus-port argument for cluster announce IP / port implementation, making the cluster bus port not configurable."
ccaef5c923a14dc183c50530f52ada0fda012179,1667980938,"diskless master, avoid bgsave child hung when fork parent crashes (#11463)

During a diskless sync, if the master main process crashes, the child would
have hung in `write`. This fix closes the read fd on the child side, so that if the
parent crashes, the child will get a write error and exit.

This change also fixes disk-based replication, BGSAVE and AOFRW.
In that case the child wouldn't have been hang, it would have just kept
running until done which may be pointless.

There is a certain degree of risk here. in case there's a BGSAVE child that could
maybe succeed and the parent dies for some reason, the old code would have let
the child keep running and maybe succeed and avoid data loss.
On the other hand, if the parent is restarted, it would have loaded an old rdb file
(or none), and then the child could reach the end and rename the rdb file (data
conflicting with what the parent has), or also have a race with another BGSAVE
child that the new parent started.

Note that i removed a comment saying a write error will be ignored in the child
and handled by the parent (this comment was very old and i don't think relevant).","['src/childinfo.c', 'src/rdb.c', 'src/server.c', 'tests/integration/replication.tcl', 'tests/support/util.tcl']","The bgsave child process hangs during a diskless sync if the master main process crashes, causing potential data loss and race conditions when the parent gets restarted."
bfe50a30edff6837897964ac3374c082b0d9e5da,1677927276,"Increase the threshold of the AOF loading defrag test  (#11871)

This test is very sensitive and fragile. It often fails in Daily,
in most cases, it failed in test-ubuntu-32bit (the AOF loading one),
with the range in (31, 40):
```
[err]: Active defrag in tests/unit/memefficiency.tcl
Expected 38 <= 30 (context: type eval line 113 cmd {assert {$max_latency <= 30}} proc ::test)
```

The AOF loading part isn't tightly fixed to the cron hz. It calls
processEventsWhileBlocked once in every 1024 command calls.
```
        /* Serve the clients from time to time */
        if (!(loops++ % 1024)) {
            off_t progress_delta = ftello(fp) - last_progress_report_size;
            loadingIncrProgress(progress_delta);
            last_progress_report_size += progress_delta;
            processEventsWhileBlocked();
            processModuleLoadingProgressEvent(1);
        }
```

In this case, we can either decrease the 1024 or increase the
threshold of just the AOF part of that test. Considering the test
machines are sometimes slow, and all sort of quirks could happen
(which do not indicate a bug), and we've already set to 30, we suppose
we can set it a little bit higher, set it to 40. We can have this instead of
adding another testing config (we can add it when we really need it).

Fixes #11868",['tests/unit/memefficiency.tcl'],"AOF loading defrag test is frequently failing, especially on test-ubuntu-32bit, due to the test's sensitivity and fragility tied with the cron hz. Expected latency exceeds set constraint."
2530dc0ebd8be8d792f4673073401377cd5bdc42,1597244336,"Add oom-score-adj configuration option to control Linux OOM killer. (#1690)

Add Linux kernel OOM killer control option.

This adds the ability to control the Linux OOM killer oom_score_adj
parameter for all Redis processes, depending on the process role (i.e.
master, replica, background child).

A oom-score-adj global boolean flag control this feature. In addition,
specific values can be configured using oom-score-adj-values if
additional tuning is required.","['src/config.c', 'src/config.h', 'src/replication.c', 'src/server.c', 'src/server.h', 'tests/test_helper.tcl', 'tests/unit/oom-score-adj.tcl']","Lack of configuration options for controlling oom_score_adj parameter of Linux OOM killer for different Redis processes roles (master, replica, background child)."
3330ea1864e8a4fe3dcbb69101a2d1afd5e8401f,1664363707,"RM_CreateCommand should not set CMD_KEY_VARIABLE_FLAGS automatically (#11320)

The original idea behind auto-setting the default (first,last,step) spec was to use
the most ""open"" flags when the user didn't provide any key-spec flags information.

While the above idea is a good approach, it really makes no sense to set
CMD_KEY_VARIABLE_FLAGS if the user didn't provide the getkeys-api flag:
in this case there's not way to retrieve these variable flags, so what's the point?

Internally in redis there was code to ignore this already, so this fix doesn't change
redis's behavior, it only affects the output of COMMAND command.","['src/db.c', 'src/module.c', 'tests/modules/keyspecs.c', 'tests/unit/moduleapi/keyspecs.tcl']",Auto-setting of CMD_KEY_VARIABLE_FLAGS in RM_CreateCommand even when user didn't provide getkeys-api flag might lead to generation of unnecessary variable flags that cannot be retrieved.
7dfd7b9197bbe216912049eebecbda3f1684925e,1669724422,"Reduce eval related overhead introduced in v7.0 by evalCalcFunctionName (#11521)

As being discussed in #10981 we see a degradation in performance
between v6.2 and v7.0 of Redis on the EVAL command. 

After profiling the current unstable branch we can see that we call the
expensive function evalCalcFunctionName twice. 

The current ""fix"" is to basically avoid calling evalCalcFunctionName and
even dictFind(lua_scripts) twice for the same command.
Instead we cache the current script's dictEntry (for both Eval and Functions)
in the current client so we don't have to repeat these calls.
The exception would be when doing an EVAL on a new script that's not yet
in the script cache. in that case we will call evalCalcFunctionName (and even
evalExtractShebangFlags) twice.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/eval.c', 'src/functions.c', 'src/networking.c', 'src/server.h']",The performance degradation between v6.2 and v7.0 of Redis on the EVAL command has been tied to calling the expensive function 'evalCalcFunctionName' multiple times unnecessarily.
d27c7413a95a0a271b94376f3ec64dae06326b29,1697466109,"remove heap allocations from signal handlers. (#12655)

Using heap allocation during signal handlers is unsafe.
This PR purpose is to replace all the heap allocations done within the signal
handlers raised upon server crash and assertions.
These were added in #12453.

writeStacktraces(): allocates the stacktraces output array on the calling thread's
stack and assigns the address to a global variable.
It calls `ThreadsManager_runOnThreads()` that invokes `collect_stacktrace_data()`
by each thread: each thread writes to a different location in the above array to allow
sync writes.

get_ready_to_signal_threads_tids(): instead of allocating the `tids` array, it receives it
as a fixed size array parameter, allocated on on the stack of the calling function, and
returns the number of valid threads. The array size is hard-coded to 50.

`ThreadsManager_runOnThreads():` To avoid the outputs array allocation, the
**callback signature** was changed. Now it should return void. This function return type
has also changed to int - returns 1 if successful, and 0 otherwise.

Other unsafe calls will be handled in following PRs","['src/debug.c', 'src/threads_mngr.c', 'src/threads_mngr.h']",Heap allocations within signal handlers raised upon a server crash or assertions are causing potentially unsafe operations.
2fa077b0e93f9312590f6c7c8070eaa0db76747d,1591596992,"Avoid rejecting WATCH / UNWATCH, like MULTI/EXEC/DISCARD

Much like MULTI/EXEC/DISCARD, the WATCH and UNWATCH are not actually
operating on the database or server state, but instead operate on the
client state. the client may send them all in one long pipeline and check
all the responses only at the end, so failing them may lead to a
mismatch between the client state on the server and the one on the
client end, and execute the wrong commands (ones that were meant to be
discarded)

the watched keys are not actually stored in the client struct, but they
are in fact part of the client state. for instance, they're not cleared
or moved in SWAPDB or FLUSHDB.
",['src/server.c'],"Failure of WATCH and UNWATCH commands can lead to state inconsistency between the client and server, potentially executing wrong, possibly discarded, commands."
dd60c6c8d3f605bca2fb45431edadbf0a5da1492,1665655520,"Improve linux overcommit check and warning (#11357)

1. show the overcommit warning when overcommit is disabled (2),
   not just when it is set to heuristic (0).
2. improve warning text to mention the issue with jemalloc causing VM
   mapping fragmentation when set to 2.",['src/syscheck.c'],"Linux overcommit check is unable to display warning when overcommit is disabled, and doesn't mention the issue of VM mapping fragmentation when jemalloc is set to 2."
12c56a8e752e5f01429ebf3cd9b70d1d9ebf1a14,1567612546,"redis-cli: always report server errors on read errors.

Before this commit we may have not consumer buffers when a read error is
encountered. Such buffers may contain errors that are important clues
for the user: for instance a protocol error in the payload we send in
pipe mode will cause the server to abort the connection. If the user
does not get the protocol error, debugging what is happening can be a
nightmare.

This commit fixes issue #3756.
",['src/redis-cli.c'],"Read errors result in unretrieved buffers, hiding crucial error clues for user, making debugging of protocol errors or broken connections difficult."
216f168b2b144ecf3d082fa9df20970901479d32,1631176690,"Add INFO total_active_defrag_time and current_active_defrag_time (#9377)

Add two INFO metrics:
```
total_active_defrag_time:12345
current_active_defrag_time:456
```
`current_active_defrag_time` if greater than 0, means how much time has
passed since active defrag started running. If active defrag stops, this metric is reset to 0.
`total_active_defrag_time` means total time the fragmentation
was over the defrag threshold since the server started.

This is a followup PR for #9031","['src/defrag.c', 'src/server.c', 'src/server.h']",Lack of INFO metrics for active defragmentation time tracking resulting in difficulty in understanding the duration of active defrag operations.
4505eb18213c8da31c6dd39ba7cd36d3d01141a5,1659599257,"errno cleanup around rdbLoad (#11042)

This is an addition to #11039, which cleans up rdbLoad* related errno. Remove the
errno print from the outer message (may be invalid since errno may have been overwritten).

Our aim should be the code that detects the error and knows which system call
triggered it, is the one to print errno, and not the code way up above (in some cases
a result of a logical error and not a system one).

Remove the code to update errno in rdbLoadRioWithLoadingCtx, signature check
and the rdb version check, in these cases, we do print the error message.
The caller dose not have the specific logic for handling EINVAL.

Small fix around rdb-preamble AOF: A truncated RDB is considered a failure,
not handled the same as a truncated AOF file.","['src/aof.c', 'src/debug.c', 'src/rdb.c', 'src/replication.c', 'src/server.c']","Erroneous and inconsistent handling of 'errno' in rdbLoad related functions, leading to invalid or inaccurate error messages. Particularly issues with rdb-preamble AOF where a truncated RDB is not appropriately handled."
8d24f8b46b3c544fef33427ca1f2582d92c59dd4,1536239566,"Scripting & Streams: some commands need right flags

xadd with id * generates random stream id

xadd & xtrim with approximate maxlen count may
trim stream randomly

xinfo may get random radix-tree-keys/nodes

xpending may get random idletime

xclaim: master and slave may have different
idletime in stream
",['src/server.c'],"Commands like xadd, xtrim, xinfo, xpending, and xclaim are experiencing erratic behavior due to possibly incorrect flag settings, resulting in random stream ID generation, random trimming, and inconsistent idletime across master and slave."
6b5b3ca4148b058210f7c32096a6d1201a2121d9,1643720051,"forbid module to unload when it holds ongoing timer (#10187)

This is done to avoid a crash when the timer fires after the module was unloaded.
Or memory leaks in case we wanted to just ignore the timer.
It'll cause the MODULE UNLOAD command to return with an error

Co-authored-by: sundb <sundbcn@gmail.com>","['src/module.c', 'tests/unit/moduleapi/timer.tcl']",Unloading a module while it holds an ongoing timer can lead to a crash or memory leaks when the timer fires post the module unload.
c8052122a2af9b85124a8697488a0c44d66777b8,1672823035,"Fix potential issue with Lua argv caching, module command filter and libc realloc (#11652)

TLDR: solve a problem introduced in Redis 7.0.6 (#11541) with
RM_CommandFilterArgInsert being called from scripts, which can
lead to memory corruption.

Libc realloc can return the same pointer even if the size was changed. The code in
freeLuaRedisArgv had an assumption that if the pointer didn't change, then the
allocation didn't change, and the cache can still be reused.
However, if rewriteClientCommandArgument or RM_CommandFilterArgInsert were
used, it could be that we realloced the argv array, and the pointer didn't change, then
a consecutive command being executed from Lua can use that argv cache reaching
beyond its size.
This was actually only possible with modules, since the decision to realloc was based
on argc, rather than argv_len.","['src/module.c', 'src/script_lua.c', 'tests/unit/moduleapi/commandfilter.tcl', 'tests/unit/scripting.tcl']","The libc realloc in freeLuaRedisArgv causes memory corruption when called from scripts using RM_CommandFilterArgInsert, as it can lead to size exceeding when reusing the argv cache."
bf963253ecfd367b49081a26c1b5c410558aecfc,1558539544,"Implement `SCAN cursor [TYPE type]` modifier suggested in issue #6107.

Add tests to check basic functionality of this optional keyword, and also tested with
a module (redisgraph). Checked quickly with valgrind, no issues.

Copies name the type name canonicalisation code from `typeCommand`, perhaps this would
be better factored out to prevent the two diverging and both needing to be edited to
add new `OBJ_*` types, but this is a little fiddly with C strings.

The [redis-doc](https://github.com/antirez/redis-doc/blob/master/commands.json) repo
will need to be updated with this new arg if accepted.

A quirk to be aware of here is that the GEO commands are backed by zsets not their own
type, so they're not distinguishable from other zsets.

Additionally, for sparse types this has the same behaviour as `MATCH` in that it may
return many empty results before giving something, even for large `COUNT`s.
","['src/db.c', 'tests/unit/scan.tcl']","The 'SCAN cursor' command lacks a '[TYPE type]' modifier, resulting in difficulties distinguishing between GEO commands and zsets, and may produce many empty results for sparse types."
08c3fe8063b0a7e477dee8036ff5409a48c6f9a9,1575553031,"-        memcpy(&id,ri.key,ri.key_len);
+        memcpy(&id,ri.key,sizeof(id));

The memcpy from the key to the id reliease on the fact that this key
*should* be 8 bytes long as it was entered as such a few lines up the
code.

BUT if someone will change the code to the point this is no longer true,
current code can trash the stack which makes debugging very hard
while this fix will result in some garbage id, or even page fault.
Both are preferable to stack mangaling.
",['src/tracking.c'],"If the key length changes and isn't 8 bytes long anymore, then the existing memcpy operation could cause stack corruption and make debugging quite difficult."
74fe15b3602ed7c003b5c53e45e31f7aa6d4a86f,1625060994,"redis-cli --rdb: fix broken fsync/ftruncate for stdout (#9135)

A change in redis 6.2 caused redis-cli --rdb that's directed to stdout to fail because fsync fails.
This commit avoids doing ftruncate (fails with a warning) and fsync (fails with an error) when the
output file is `-`, and adds the missing documentation that `-` means stdout.

Co-authored-by: Oran Agra <oran@redislabs.com>
Co-authored-by: Wang Yuan <wangyuancode@163.com>",['src/redis-cli.c'],"The redis-cli --rdb command directed to stdout is failing due to ineffective fsync and ftruncate operations, introduced after changes in Redis 6.2. Additionally, the `-` option (stdout) is not documented properly."
1df8c129bc5a8ffaac28410dc1c39b4241c6078c,1623140752,"Fix some minor bugs in redis-cli (#8982)

* clusterManagerAddSlots check argv_idx error.

* clusterManagerLoadInfoFromNode remove unused param opts.

* redis-cli node->ip may be an sds or a c string. Using %s to format
is always right, %S may be wrong.

* In clusterManagerFixOpenSlot clusterManagerBumpEpoch call is redundant,
because it is already called in clusterManagerSetSlotOwner.

* redis-cli cluster help add more commands in help messages.",['src/redis-cli.c'],"Redis-cli has several minor issues namely, argv_idx error in clusterManagerAddSlots, unused param opts in clusterManagerLoadInfoFromNode, incorrect string formatting in node->ip, redundant call in clusterManagerFixOpenSlot and missing commands in help messages."
df1890ef7fc0691c0b940f00ddff14a1c395a52a,1684308362,"Allow SENTINEL CONFIG SET and SENTINEL CONFIG GET to handle multiple parameters. (#10362)

Extend SENTINEL CONFIG SET and SENTINEL CONFIG GET to be
compatible with variadic CONFIG SET and CONFIG GET and allow multiple
parameters to be modified in a single call atomically.

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/sentinel.c', 'tests/sentinel/tests/15-config-set-config-get.tcl']","The SENTINEL CONFIG SET and GET do not support setting or getting multiple parameters atomically in a single call, causing inconvenience."
7b48de68ce13701f5a15191d1a6685336ffa3b0a,1621347190,"Remove useless - from help.h on commands with no arguments (#8939)

As far as i can tell it shows up in redis-cli in both HELP, e.g.
`help client list`, and also in the command completion tips, but it is
unclear what it was needed for.
It exists since the very first commit that added this mechanism.","['src/help.h', 'utils/generate-command-help.rb']","The help command in redis-cli is incorrectly showing a ""-"" when commands have no arguments which is causing confusion for users."
91e77a0cfb5c7e4bc6473ae04353e48ad9e8697b,1637223196,"Fixes ZPOPMIN/ZPOPMAX wrong replies when count is 0 with non-zset (#9711)

Moves ZPOP ... 0 fast exit path after type check to reply with
WRONGTYPE. In the past it will return an empty array.

Also now count is not allowed to be negative.

see #9680

before:
```
127.0.0.1:6379> set zset str
OK
127.0.0.1:6379> zpopmin zset 0
(empty array)
127.0.0.1:6379> zpopmin zset -1
(empty array)
```

after:
```
127.0.0.1:6379> set zset str
OK
127.0.0.1:6379> zpopmin zset 0
(error) WRONGTYPE Operation against a key holding the wrong kind of value
127.0.0.1:6379> zpopmin zset -1
(error) ERR value is out of range, must be positive
```","['src/blocked.c', 'src/module.c', 'src/server.h', 'src/t_list.c', 'src/t_stream.c', 'src/t_zset.c', 'tests/unit/type/zset.tcl']","When count is zero or negative with non-zset, ZPOPMIN and ZPOPMAX are returning incorrect responses, either an empty array or not properly handling the invalid input."
45f318d2ffaeb7b44b3aebb803a012fef1c36f0a,1582824072,"Show Redis version when not understanding a config directive.

This makes simpler to give people help when posting such kind of errors
in the mailing list or other help forums, because sometimes the
directive looks well spelled, but the version of Redis they are using is
not able to support it.
",['src/config.c'],"When Redis can't understand a config directive, it doesn't provide a version, hindering targeted help in forums due to potential version mismatches."
5f0119ca91ad37d8b656d51bcfc46c600d177e8f,1644603352,"Fix duplicate module options define (#10284)

The bug is introduced by #9323. (released in 7.0 RC1)
The define of `REDISMODULE_OPTIONS_HANDLE_IO_ERRORS` and `REDISMODULE_OPTION_NO_IMPLICIT_SIGNAL_MODIFIED` have the same value.

This will result in skipping `signalModifiedKey()` after `RM_CloseKey()` if the module has set
`REDISMODULE_OPTIONS_HANDLE_REPL_ASYNC_LOAD` option.
The implication is missing WATCH and client side tracking invalidations.

Other changes:
- add `no-implicit-signal-modified` to the options in INFO modules

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/module.c', 'src/redismodule.h', 'tests/unit/moduleapi/testrdb.tcl']","The definition of `REDISMODULE_OPTIONS_HANDLE_IO_ERRORS` and `REDISMODULE_OPTION_NO_IMPLICIT_SIGNAL_MODIFIED` have the same value, causing `signalModifiedKey()` to be skipped after `RM_CloseKey()`. This can lead to missing WATCH and client-side tracking invalidations."
897c3d522c6814b1853b767795a18565c047133e,1677136069,"Add CLIENT NO-TOUCH for clients to run commands without affecting LRU/LFU of keys (#11483)

When no-touch mode is enabled, the client will not touch LRU/LFU of the
keys it accesses, except when executing command `TOUCH`.
This allows inspecting or modifying the key-space without affecting their eviction.

Changes:
- A command `CLIENT NO-TOUCH ON|OFF` to switch on and off this mode.
- A client flag `#define CLIENT_NOTOUCH (1ULL<<45)`, which can be shown
  with `CLIENT INFO`, by the letter ""T"" in the ""flags"" field.
- Clear `NO-TOUCH` flag in `clearClientConnectionState`, which is used by `RESET`
  command and resetting temp clients used by modules.
- Also clear `NO-EVICT` flag in `clearClientConnectionState`, this might have been an
  oversight, spotted by @madolson.
- A test using `DEBUG OBJECT` command to verify that LRU stat is not touched when
  no-touch mode is on.
 

Co-authored-by: chentianjie <chentianjie@alibaba-inc.com>
Co-authored-by: Madelyn Olson <34459052+madolson@users.noreply.github.com>
Co-authored-by: sundb <sundbcn@gmail.com>","['src/commands.c', 'src/db.c', 'src/networking.c', 'src/server.h', 'tests/unit/introspection-2.tcl']","Executing commands through the client updates the LRU/LFU of keys, affecting their eviction even when it's not desired, like during key-space observations or modifications."
b02780c41dbc5b28d265b5cf141c03c1a7383ef9,1610031965,"Add check for the MADV_FREE/fork arm64 Linux kernel bug (#8224)

Older arm64 Linux kernels have a bug that could lead to data corruption during
background save under the following scenario:

1) jemalloc uses MADV_FREE on a page,
2) jemalloc reuses and writes the page,
3) Redis forks the background save process, and
4) Linux performs page reclamation.

Under these conditions, Linux will reclaim the page wrongfully and the
background save process will read zeros when it tries to read the page.

The bug has been fixed in Linux with commit:
ff1712f953e27f0b0718762ec17d0adb15c9fd0b (""arm64: pgtable: Ensure dirty bit is
preserved across pte_wrprotect()"")

This Commit adds an ignore-warnings config, when not found, redis will
print a warning and exit on startup (default behavior).

Co-authored-by: Oran Agra <oran@redislabs.com>","['src/config.c', 'src/server.c', 'src/server.h']",Data corruption occurs during background save in older arm64 Linux kernels due to a bug in MADV_FREE/fork resulting in wrongful page reclamation when jemalloc reuses and writes the page.
89ae353748cffc50dbb0222529fa765e156f2951,1624536506,"change streamAppendItem to use raxEOF instead of raxNext (#9138)

The call to raxNext didn't really progress in the rax, since we were already on the last item.
instead, all it does is check that it is indeed a valid item, so the new code clearer.",['src/t_stream.c'],"The current implementation of streamAppendItem using raxNext doesn't progress as expected on the last item, only verifies its validity."
e48ac075c053c39d3f7ed4d86deb6554b7d97e1a,1670247904,"GEOSEARCH BYBOX: Simplified haversine distance formula when longitude diff is 0 (#11579)

This is take 2 of `GEOSEARCH BYBOX` optimizations based on haversine
distance formula when longitude diff is 0.
The first one was in #11535 . 

- Given longitude diff is 0 the asin(sqrt(a)) on the haversine is asin(sin(abs(u))).
- arcsin(sin(x)) equal to x when x ∈[−𝜋/2,𝜋/2]. 
- Given latitude is between [−𝜋/2,𝜋/2] we can simplifiy arcsin(sin(x)) to x.

On the sample dataset with 60M datapoints, we've measured 55% increase
in the achievable ops/sec.",['src/geohash_helper.c'],"In GEOSEARCH BYBOX, current haversine distance formula is inefficient when longitude difference is zero, limiting the operations per second."
ec582cc7ad0706e252bc905822226e49f4c4d0e4,1625466616,"Query buffer shrinking improvements (#5013)

when tracking the peak, don't reset the peak to 0, reset it to the
maximum of the current used, and the planned to be used by the current
arg.

when shrining, split the two separate conditions.
the idle time shrinking will remove all free space.
but the peak based shrinking will keep room for the current arg.

when we resize due to a peak (rahter than idle time), don't trim all
unused space, let the qbuf keep a size that's sufficient for the
currently process bulklen, and the current peak.

Co-authored-by: sundb <sundbcn@gmail.com>
Co-authored-by: yoav-steinberg <yoav@monfort.co.il>
","['src/sds.c', 'src/sds.h', 'src/server.c']",Query buffer peak resetting to 0 rather than to the max value in use currently or planned to be used by current argument. This causes unnecessary resizing and frees up all space when shrinking due to idle time or peak size.
d7fcb3c5a16dce188728b0c2ab3a2a0df9bb5e2a,1643967591,"Fix SENTINEL SET config rewrite test (#10232)

Change the sentinel config file to a directory in SENTINEL SET test.
So it will now fail on the `rename` in `rewriteConfigOverwriteFile`.

The test used to set the sentinel config file permissions to `000` to
simulate failure. But it fails on centos7 / freebsd / alpine. (introduced in #10151)

Other changes:
1. More error messages after the config rewrite failure.
2. Modify arg name `force_all` in `rewriteConfig` to `force_write`. (was rename in #9304)
3. Fix a typo in debug quicklist-packed-threshold, then -> than. (#9357)","['src/debug.c', 'src/sentinel.c', 'src/server.c', 'src/server.h', 'tests/sentinel/tests/03-runtime-reconf.tcl']","SENTINEL SET test fails on certain OS due to sentinel config file permissions being set to `000`, causing a failure in `rewriteConfigOverwriteFile` function."
00097bf4aa9e28477ee5d9361569f3212fb7493a,1658837593,"Change the return value of rdbLoad function to enums (#11039)

The reason we do this is because in #11036, we added error
log message when failing to open RDB file for reading.
In loadDdataFromDisk we call rdbLoad and also check errno,
now the logging corrupts errno (reported in alpine daily).

It is not safe to rely on errno as we do today, so we change
the return value of rdbLoad function to enums, like we have
when loading an AOF.","['src/debug.c', 'src/rdb.c', 'src/replication.c', 'src/server.c', 'src/server.h']","The `rdbLoad` function's reliance on `errno` for error checking is unsafe, leading to potential corruption of `errno` value when failures occur during RDB file reading."
61baabd8d5cd9178f48b6fad79ae3cb477ef0a11,1655794873,"Fix crash on RM_Call with script mode. (#10886)

The PR fixes 2 issues:

### RM_Call crash on script mode

`RM_Call` can potentially be called from a background thread where `server.current_client`
are not set. In such case we get a crash on `NULL` dereference.
The fix is to check first if `server.current_client` is `NULL`, if it does we should
verify disc errors and readonly replica as we do to any normal clients (no masters nor AOF).

### RM_Call block OOM commands when not needed

Again `RM_Call` can be executed on a background thread using a `ThreadSafeCtx`.
In such case `server.pre_command_oom_state` can be irrelevant and should not be
considered when check OOM state. This cause OOM commands to be blocked when
not necessarily needed.

In such case, check the actual used memory (and not the cached value). Notice that in
order to know if the cached value can be used, we check that the ctx that was used on
the `RM_Call` is a ThreadSafeCtx. Module writer can potentially abuse the API and use
ThreadSafeCtx on the main thread. We consider this as a API miss used.","['src/module.c', 'tests/modules/blockedclient.c', 'tests/unit/moduleapi/blockedclient.tcl']","`RM_Call` crashes when executed in script mode from a background thread due to potential `NULL` dereference on `server.current_client`. Also, it unnecessarily blocks OOM commands when executed on a background thread with `ThreadSafeCtx`."
51da5c3dde38b138c68733977e53eee789e51d10,1656479820,"Fix CLUSTER RESET command argument number issue (#10898)

Fix regression of CLUSTER RESET command in redis 7.0.

cluster reset command format is:
CLUSTER RESET [ HARD | SOFT]

According to the cluster reset command doc and codes, the third argument is optional, so
the arity in json file should be -2 instead of 3.

Add test to verify future regressions with RESET and RESET SOFT that were not covered.

Co-authored-by: Ubuntu <lucas.guang.yang1@huawei.com>
Co-authored-by: Oran Agra <oran@redislabs.com>
Co-authored-by: Binbin <binloveplay1314@qq.com>","['src/commands.c', 'tests/cluster/tests/00-base.tcl']","Regression in Redis 7.0 caused CLUSTER RESET command to inaccurately require third argument, diverging from documentation and past implementation."
5725088ff22065d6d6ce39ed79ed7ad63ff43f92,1633509549,"Avoid argv memcpy when queuing a multi command. (#9602)

When queuing a multi command we duplicated the argv (meaning an alloc
and a memcpy). This isn't needed since we can use the previously allocated
argv and just reset the client objects argv to NULL. This should saves some
memory and is a minor optimization in heavy MULTI/EXEC traffic, especially
if there are lots of arguments.

","['src/multi.c', 'src/server.h']","Queuing a multi command unnecessarily duplicates argv, resulting in alloc and memcpy. This wastes memory and decreases performance during heavy MULTI/EXEC traffic, especially with many arguments."
