commit_id,commit_date,commit_message,actual_files_modified,transformed_message_gpt3
07a31599c3f5b23158f83988ebc05381a1e86b00,1664217249,"KAFKA-10199: Fix switching to updating standbys if standby is removed (#12687)

When the state updater only contains standby tasks and then a
standby task is removed, an IllegalStateException is thrown
because the changelog reader does not allow to switch to standby
updating mode more than once in a row.

This commit fixes this bug by checking that the removed task is
an active one before trying to switch to standby updating mode.
If the task to remove is a standby task then either we are already
in standby updating mode and we should not switch to it again or
we are not in standby updating mode which implies that there are
still active tasks that would prevent us to switch to standby
updating mode.

Reviewer: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java']","When the state updater only contains standby tasks and one of these standby tasks is removed, an ""IllegalStateException"" is thrown. This happens because the changelog reader does not allow switching to standby updating mode more than once consecutively."
fb77da941ac2a34513cf2cd5d11137ba9b275575,1631102228,"KAFKA-12766 - Disabling WAL-related Options in RocksDB (#11250)

Description
Streams disables the write-ahead log (WAL) provided by RocksDB since it replicates the data in changelog topics. Hence, it does not make much sense to set WAL-related configs for RocksDB.

Proposed solution
Ignore any WAL-related configuration and state in the log that we are ignoring them.

Co-authored-by: Tomer Wizman <tomer.wizman@personetics.com>
Co-authored-by: Bruno Cadonna <cadonna@apache.org>

Reviewers: Boyang Chen <boyang@apache.org>, Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapterTest.java']","Streams currently disable the write-ahead log (WAL) offered by RocksDB as it mirrors the data in changelog topics, making it redundant to set WAL-related configurations for RocksDB."
1d496a26c998cefc77223a7575980208620dfe2d,1575593843,"KAFKA-9179; Fix flaky test due to race condition when fetching reassignment state (#7786)

This patch fixes a race condition on reassignment completion. The previous code fetched metadata first and then fetched the reassignment state. It is possible in between those times for the reassignment to complete, which leads to spurious URPs being reported. The fix here is to change the order of these checks and to explicitly check for reassignment completion. 

Note this patch fixes the flaky test `TopicCommandWithAdminClientTest.testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress`.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['core/src/main/scala/kafka/admin/TopicCommand.scala', 'core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala']","A race condition is observed when fetching reassignment state that leads to the failing of the test `TopicCommandWithAdminClientTest.testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress`. This issue arises due to the ordering of how metadata and reassignment states are fetched. If a reassignment completes in the time between fetching metadata and fetching reassignment state, spurious URPs get reported."
acf39fe94ad6a666e7ddd88dbcc6ddf5173fe56c,1612558664,"MINOR: Allow KafkaApis to be configured for Raft controller quorums (#10045)

`KafkaApis` is configured differently when it is used in a broker with a Raft-based controller quorum vs. a ZooKeeper quorum.  For example, when using Raft, `ForwardingManager` is required rather than optional, and there is no `AdminManager`, `KafkaController`, or `KafkaZkClient`.  This PR introduces `MetadataSupport` to abstract the two possibilities: `ZkSupport` and `RaftSupport`.  This provides a fluent way to decide what to do based on the type of support that `KafkaApis` has been configured with.  Certain types of requests are not supported when using raft (`AlterIsrRequest`, `UpdateMetadataRequest`, etc.), and `MetadataSupport` gives us an intuitive way to identify the constraints and requirements associated with the different configurations and react accordingly.

Existing tests are sufficient to detect bugs and regressions.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/main/scala/kafka/server/MetadataSupport.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/metadata/MetadataRequestBenchmark.java']","When `KafkaApis` is being used in a broker with a Raft-based controller quorum as compared to a ZooKeeper quorum, it has different configuration requirements. It calls for a required `ForwardingManager` and lacks an `AdminManager`, `KafkaController`, or `KafkaZkClient`. Additionally, certain requests are not supported when using Raft such as `AlterIsrRequest` and `UpdateMetadataRequest`. The system needs a clear way to identify the constraints and requirements associated with the different configurations and react accordingly, as it currently struggles to distinguish between these two possibilities."
7e573001484427dc73d821cc232a4c3bb3b5f5bb,1632873016,"KAFKA-12486: Enforce Rebalance when a TaskCorruptedException is throw… (#11076)

This PR aims to utilize HighAvailabilityTaskAssignor to avoid downtime on corrupted tasks. The idea is that, when we hit TaskCorruptedException on an active task, a rebalance is triggered after we've wiped out the corrupted state stores. This will allow the assignor to temporarily redirect this task to another client who can resume work on the task while the original owner works on restoring the state from scratch.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']","When a TaskCorruptedException is thrown on an active task, there is no mechanism in place to avoid downtime. The corrupted state stores need to be wiped and current system does not automatically trigger a rebalance. Without a rebalance, the task cannot be temporarily redirected to another client for uninterrupted work while the original task owner restores the state."
b35ca4349dabb199411cb6bc4c80ef89f19d9328,1613770567,"KAFKA-9274: Throw TaskCorruptedException instead of TimeoutException when TX commit times out (#10072)

Part of KIP-572: follow up work to PR #9800. It's not save to retry a TX commit after a timeout, because it's unclear if the commit was successful or not, and thus on retry we might get an IllegalStateException. Instead, we will throw a TaskCorruptedException to retry the TX if the commit failed.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/errors/TaskCorruptedException.java', 'streams/src/main/java/org/apache/kafka/streams/errors/TaskTimeoutExceptions.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","'Retrying a transaction commit after a timeout can potentially be unsafe. There's a lack of clarity whether the commit was successful or not, which may result in an IllegalStateException when a retry is attempted. The current behavior is that a TimeoutException is thrown.'"
a03bda61e068d72823af47e5f25ffd12c3319541,1631567722,"KAFKA-13249: Always update changelog offsets before writing the checkpoint file (#11283)

When using EOS checkpointed offsets are not updated to the latest offsets from the changelog because the maybeWriteCheckpoint method is only ever called when commitNeeded=false. This change will force the update if enforceCheckpoint=true .

I have also added a test which verifies that both the state store and the checkpoint file are completely up to date with the changelog after the app has shutdown.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java']","'When using exactly-once semantics (EOS), checkpointed offsets are not updated to the latest offsets from the changelog as the maybeWriteCheckpoint method is only called when commitNeeded=false. This could potentially result in out-of-sync state stores and checkpoint files, affecting reliable state recovery after the app has shut down.'"
43bcc5682da82a602a4c0a000dc7433d0507b450,1636033445,"KAFKA-13396: Allow create topic without partition/replicaFactor (#11429)

[KIP-464](https://cwiki.apache.org/confluence/display/KAFKA/KIP-464%3A+Defaults+for+AdminClient%23createTopic) (PR: https://github.com/apache/kafka/pull/6728)
made it possible to create topics without passing partition count and/or replica factor when using
the admin client. We incorrectly disallowed this via https://github.com/apache/kafka/pull/10457 while
trying to ensure validation was consistent between ZK and the admin client (in this case the
inconsistency was intentional).

Fix this regression and add tests for the command lines in quick start (i.e. create topic and describe
topic) to make sure it won't be broken in the future.

Reviewers: Lee Dongjin <dongjin@apache.org>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/admin/TopicCommand.scala', 'core/src/test/scala/unit/kafka/admin/TopicCommandTest.scala']",Creating a topic without passing the partition count and/or the replica factor using the admin client has been incorrectly disallowed due to an attempt to ensure validation consistency between ZK and the admin client. This inconsistency was originally intentional.
67e99a42361b8f0febc3e691488c668ebfe257e9,1646160917,"MINOR: Ensure LocalLog.flush is thread safe to recoveryPoint changes (#11814)

Issue:
Imagine a scenario where two threads T1 and T2 are inside UnifiedLog.flush() concurrently:

KafkaScheduler thread T1 -> The periodic work calls LogManager.flushDirtyLogs() which in turn calls UnifiedLog.flush(). For example, this can happen due to log.flush.scheduler.interval.ms here.
KafkaScheduler thread T2 -> A UnifiedLog.flush() call is triggered asynchronously during segment roll here.
Supposing if thread T1 advances the recovery point beyond the flush offset of thread T2, then this could trip the check within LogSegments.values() here for thread T2, when it is called from LocalLog.flush() here. The exception causes the KafkaScheduler thread to die, which is not desirable.

Fix:
We fix this by ensuring that LocalLog.flush() is immune to the case where the recoveryPoint advances beyond the flush offset.

Reviewers: Jun Rao <junrao@gmail.com>",['core/src/main/scala/kafka/log/LocalLog.scala'],"When two threads are inside UnifiedLog.flush() concurrently, an issue arises. For example, this could happen if a KafkaScheduler thread advances the recovery point beyond the flush offset of another thread. This could cause a check within LogSegments.values() to trip when it is called from LocalLog.flush(), resulting in an exception that causes the KafkaScheduler thread to die."
66d81a0e50680916f541703049dd4e911542b5ed,1565269282,"MINOR: Update dependencies for Kafka 2.4 (#7126)

Scala 2.12.9 brings another 5% ~ 10% improvement in compiler performance,
improved compatibility with JDK 11/12/13, and experimental infrastructure for
build pipelining.

zstd update includes performance improvements, among which the
primary improvement is that decompression is ~7% faster.

Level | v1.4.0 | v1.4.1 | Delta
-- | -- | -- | --
1 | 1390 MB/s | 1453 MB/s | +4.5%
3 | 1208 MB/s | 1301 MB/s | +7.6%
5 | 1129 MB/s | 1233 MB/s | +9.2%
7 | 1224 MB/s | 1347 MB/s | +10.0%
16 | 1278 MB/s | 1430 MB/s | +11.8%

Jetty 9.4.19 includes a number of bug fixes:
https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.19.v20190610

Mockito 3.0.0 switched the Java requirement from 7 to 8.

Several updates to owaspDepCheckPlugin (4.0.2 -> 5.2.1).

The rest are patch updates.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>","['bin/kafka-run-class.sh', 'bin/windows/kafka-run-class.bat']","The current dependencies for Kafka are outdated, causing sub-optimal compiler performance for Scala, slower than possible decompression speeds for zstd, potential bug occurrences due to using an older version of Jetty, inability to take advantage of features available in more recent versions of Mockito (which now requires Java 8), and missing updates and patches for the owaspDepCheckPlugin and other dependencies."
81b71c06f300daf353cef04653d59385db561b38,1662672089,"KAFKA-14204: QuorumController must correctly handle overly large batches (#12595)

Originally, the QuorumController did not try to limit the number of records in a batch that it sent
to the Raft layer.  This caused two problems. Firstly, we were not correctly handling the exception
that was thrown by the Raft layer when a batch of records was too large to apply atomically. This
happened because the Raft layer threw an exception which was a subclass of ApiException. Secondly,
by letting the Raft layer split non-atomic batches, we were not able to create snapshots at each of
the splits. This led to O(N) behavior during controller failovers.

This PR fixes both of these issues by limiting the number of records in a batch. Atomic batches
that are too large will fail with a RuntimeException which will cause the active controller to
become inactive and revert to the last committed state. Non-atomic batches will be split into
multiple batches with a fixed number of records in each.

Reviewers: Luke Chen <showuon@gmail.com>, José Armando García Sancio <jsancio@gmail.com>","['metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java']","The QuorumController does not limit the number of records in a batch that it sends to the Raft layer. This results in two issues. First, the exception thrown by the Raft layer when a batch of records is too large to apply atomically isn't correctly handled. Second, by allowing the Raft layer to split non-atomic batches, snapshots cannot be created at each of the splits, leading to O(N) behavior during controller failovers. Overly large atomic batches also fail, causing the active controller to become inactive and revert to the last committed state."
1574b9f16df83c6a392502a9134d4b04647511cf,1690568934,"MINOR: Code cleanups in group-coordinator module (#14117)

This patch does a few code cleanups in the group-coordinator module.

It renames Coordinator to CoordinatorShard;
It renames ReplicatedGroupCoordinator to GroupCoordinatorShard. I was never really happy with this name. The new name makes more sense to me;
It removes TopicPartition from the GroupMetadataManager. It was only used in log messages. The log context already includes it so we don't have to log it again.
It renames assignors to consumerGroupAssignors.

Reviewers: Jeff Kim <jeff.kim@confluent.io>, Justine Olshan <jolshan@confluent.io>","['group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorShard.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorShard.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorShardBuilder.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorShardBuilderSupplier.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorShardTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntimeTest.java']","Several elements within the group-coordinator module are causing confusion or redundancy. They may not have intuitive names or may only be used for unnecessary additional logging. For instance, 'Coordinator' and 'ReplicatedGroupCoordinator' have misleading names, while 'TopicPartition' from the GroupMetadataManger is only used in log messages. Additionally, 'assignors' are more specific to consumer groups, so the terminology could be improved."
c38825ab976e5187d14c10503a5b403fc41c8d5c,1595332931,"KAFKA-9432:(follow-up) Set `configKeys` to null in `describeConfigs()` to make it backward compatible with older Kafka versions.

- After #8312, older brokers are returning empty configs,  with latest `adminClient.describeConfigs`.  Old brokers  are receiving empty configNames in `AdminManageer.describeConfigs()` method. Older brokers does not handle empty configKeys. Due to this old brokers are filtering all the configs.
- Update ClientCompatibilityTest to verify describe configs
- Add test case to test describe configs with empty configuration Keys

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #9046 from omkreddy/KAFKA-9432
","['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'core/src/test/scala/unit/kafka/server/AdminManagerTest.scala', 'tests/kafkatest/tests/client/client_compatibility_features_test.py', 'tools/src/main/java/org/apache/kafka/tools/ClientCompatibilityTest.java']","Older versions of Kafka are encountering issues when handling requests from the latest adminClient.describeConfigs due to receiving empty configNames in the AdminManager.describeConfigs() method. This is causing old brokers to inappropriately filter out all configurations, as they do not handle empty configKeys correctly."
b94c7f479b917d4ec602c31a24f11390627c479b,1574489103,"MINOR: Add ignorable field check to `toStruct` and fix usage (#7710)

If a field is not marked as ignorable, we should raise an exception if it has been set to a non-default value. This check already exists in `Message.write`, so this patch adds it to `Message.toStruct`. Additionally, we fix several fields which should have been marked ignorable and we fix some related test assertions.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Colin Patrick McCabe <cmccabe@apache.org>","['clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsResponse.java', 'clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java', 'clients/src/test/java/org/apache/kafka/common/message/SimpleExampleMessageTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/JoinGroupRequestTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/integration/kafka/api/BaseAdminIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/DescribeAuthorizedOperationsTest.scala', 'core/src/test/scala/unit/kafka/server/MetadataRequestTest.scala', 'generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java']","Fields set to non-default values can lead to unexpected errors if they are not marked as ignorable. This issue is particularly noticeable when those fields were not originally marked as ignorable, nor were their related test assertions correctly set up."
45ecae6a28fe820eb2698c8a375c83ee15036f5c,1679505966,"KAFKA-14491: [15/N] Add integration tests for versioned stores (#13340)

Adds integration tests for the new versioned stores introduced in KIP-889.

This PR also contains a small bugfix for the restore record converter, required to get the tests above to pass: even though versioned stores are timestamped stores, we do not want to use the record converter for prepending timestamps when restoring a versioned store.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedStateStore.java', 'streams/src/test/java/org/apache/kafka/streams/integration/VersionedKeyValueStoreIntegrationTest.java']","The new versioned stores introduced in KIP-889 are lacking integration tests. Additionally, there might be an issue with the restore record converter for these versioned stores, as it is currently using a record converter for prepending timestamps when restoring a versioned store, which may not be appropriate in this context."
f97f36b650ef75841164ba01d42e02955141655a,1626198876,"KAFKA-13051; Require principal builders implement `KafkaPrincipalSerde` and set default (#11011)

This patch adds a check to ensure that principal builder implementations implement `KafkaPrincipalSerde` as specified in KIP-590: https://cwiki.apache.org/confluence/display/KAFKA/KIP-590%3A+Redirect+Zookeeper+Mutation+Protocols+to+The+Controller. This patch also changes the default value of `principal.builder.class` to `DefaultKafkaPrincipalBuilder`, which was already the implicit behavior when no principal builder was specified.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/GroupAuthorizerIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/GroupEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/PlaintextEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/SaslPlainSslEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/SslEndToEndAuthorizationTest.scala', 'core/src/test/scala/unit/kafka/server/AlterUserScramCredentialsRequestTest.scala', 'core/src/test/scala/unit/kafka/server/ControllerMutationQuotaTest.scala', 'core/src/test/scala/unit/kafka/server/DescribeUserScramCredentialsRequestTest.scala']","Principal builder implementations do not necessarily implement `KafkaPrincipalSerde` as specified in KIP-590, potentially leading to inconsistency issues. Moreover, the default value of `principal.builder.class` is not explicitly set to `DefaultKafkaPrincipalBuilder`, making the default behavior unclear when no principal builder is specified.
"
40d5c9fac92f019c46b1de15ee428e6f47544551,1557530439,"KAFKA-8352 : Fix Connect System test failure 404 Not Found (#6713)

Corrects the system tests to check for either a 404 or a 409 error and sleeping until the Connect REST API becomes available. This corrects a previous change to how REST extensions are initialized (#6651), which added the ability of Connect throwing a 404 if the resources are not yet started. The integration tests were already looking for 409.

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",['tests/kafkatest/services/connect.py'],The Connect REST API may throw a 404 error if the resources are not yet started due to a change in how REST extensions are initialized. This can cause system tests to fail as they currently only check for a 409 error.
6acf69d7a257b83fbf3103772ca8d68093718274,1696247960,"MINOR: Remove the client side assignor from the ConsumerGroupHeartbeat API (#14469)

As a first step, we plan to release a preview of the new consumer group rebalance protocol without the client side assignor. This patch removes all the related fields from the ConsumerGroupHeartbeat API for now. Removing fields is fine here because this API is not released yet and not exposed by default. We will add them back while bumping the version of the request when we release this part in the future.

Reviewers: Justine Olshan <jolshan@confluent.io>","['clients/src/test/java/org/apache/kafka/clients/consumer/internals/MembershipManagerImplTest.java', 'clients/src/test/java/org/apache/kafka/common/protocol/ProtoUtilsTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'core/src/test/scala/unit/kafka/server/ConsumerGroupHeartbeatRequestTest.scala', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java']","'The ConsumerGroupHeartbeat API currently includes fields related to the client side assignor. However, a new consumer group rebalance protocol is planned to be released as a preview without the client side assignor, posing potential inconsistencies or conflicts with the current API structure.'"
294b62963b563f65f64a60ed673276c0c36af101,1582843615,"throttle consumer timeout increase (#8188)

The test_throttled_reassignment test fails because the consumer that is used to validate reassignment does not start on time to consume all messages. This does not seem like an issue with the throttling of the reassignment, since increasing the timeout allowed the test to pass multiple consecutive runs locally.

This test seemed to rely on the default JmxTool for the console consumer that was removed in this commit: 179d0d7
The console consumer would check to see if it had partitions assigned to it before beginning to consume. Although the test occasionally failed with the JmxTool, it began to fail much more after the removal.

Error messages of failures followed the below format with varying numbers of missed messages. They are the first messages by the producer.

535 acked message did not make it to the Consumer. They are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19...plus 515 more. Total Acked: 192792, Total Consumed: 192259. We validated that the first 535 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.
In the scope of the test, this error suggests that the test is falling into the race condition described in produce_consume_validate.py, which has the timeout to prevent the consumer from missing initial messages.

This can serve as a temporary fix until the logic of consumer startup is addressed further.

Reviewers: Jason Gustafson <jason@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",['tests/kafkatest/tests/core/throttling_test.py'],"The test_throttled_reassignment test faces frequent failures due to a delayed start of the consumer which fails to consume all messages. This issue escalates primarily when default JmxTool for the console consumer is removed. After its removal, the test began to fail more frequently. The error messages indicate a count of missed messages, primarily the first few messages by the producer. These missed messages correctly make it into Kafka's data files suggesting a loss enroute to the consumer. This implies the existence of a race condition as described in produce_consume_validate.py, leading to the consumer missing initial messages."
8cabd57612189d633ca2aed40b63f20e1f668fa4,1607651812,"MINOR: Update jmh to 1.27 for async profiler support (#9129)

Also updated the jmh readme to make it easier for new people to know
what's possible and best practices.

There were some changes in the generated benchmarking code that
required adjusting `spotbugs-exclude.xml` and for a `javac` warning
to be suppressed for the benchmarking module. I took the chance
to make the spotbugs exclusion mode maintainable via a regex
pattern.

Tested the commands on Linux and macOS with zsh.

JMH highlights:

* async-profiler integration. Can be used with -prof async,
pass -prof async:help to look for the accepted options.
* perf c2c [2] integration. Can be used with -prof perfc2c,
if available.
* JFR profiler integration. Can be used with -prof jfr, pass
-prof jfr:help to look for the accepted options.

Full details:
* 1.24: https://mail.openjdk.java.net/pipermail/jmh-dev/2020-August/002982.html
* 1.25: https://mail.openjdk.java.net/pipermail/jmh-dev/2020-August/002987.html
* 1.26: https://mail.openjdk.java.net/pipermail/jmh-dev/2020-October/003024.html
* 1.27: https://mail.openjdk.java.net/pipermail/jmh-dev/2020-December/003096.html

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Lucas Bradstreet <lucasbradstreet@gmail.com>
",['jmh-benchmarks/jmh.sh'],"'JMH version 1.27 is not being used, which limits access to async profiler support. This can limit the ability to identify performance problems. Aside from that, newcomers to the platform can face difficulties understanding what's possible and best practices due to an outdated readme. Additionally, changes in the generated benchmarking code can lead to issues with `spotbugs-exclude.xml`'s maintainability and a `javac` warning for the benchmarking module.'"
558bb1d0692b3ba6cf9c46d7170be6d31656024b,1565562702,"KAFKA-8782: Close metrics in QuotaManagerTests (#7191)

Since `Metrics` was constructed with `enableExpiration=false`, this was
not a source of flakiness given the current implementation. This could
change in the future, so good to follow the class contract.

Included a few clean-ups with regards to redundant casts and type parameters
as well as usage of try with resources for inline usage of `Metrics`.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>","['clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java', 'clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java', 'clients/src/test/java/org/apache/kafka/common/metrics/SensorTest.java', 'clients/src/test/java/org/apache/kafka/common/metrics/stats/FrequenciesTest.java', 'core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicationQuotaManagerTest.scala']","The `Metrics` class in `QuotaManagerTests` can potentially cause issues in the future. Although `Metrics` is currently constructed with `enableExpiration=false`, which isn't causing flakiness in the present implementation, it's possible that this could change and cause issues in the future if left unchecked. Additionally, there may be unnecessary redundant casts, type parameters, and improper use of try with resources for inline usage of `Metrics`."
7c5c739b7f020ae40cc435a5f31f4c72bdfa1242,1617234520,"Initial commit (#10454)

This PR is a precursor to the recovery logic refactor work (KAFKA-12553).

I've renamed the file: core/src/test/scala/unit/kafka/log/LogUtils.scala to core/src/test/scala/unit/kafka/log/LogTestUtils.scala. Also I've renamed the underlying lass from LogUtils to LogTestUtils. This is going to help avoid a naming conflict with a new file called LogUtils.scala that I plan to introduce in core/src/main/scala/kafka/log/ as part of the recovery logic refactor. The new file will also contain a bunch of static functions.

Tests:
Relying on existing tests to catch regressions (if any) since this is a simple change.

Reviewers: Satish Duggana <satishd@apache.org>, Dhruvil Shah <dhruvil@confluent.io>, Jun Rao <junrao@gmail.com>","['core/src/test/scala/unit/kafka/log/LogSegmentTest.scala', 'core/src/test/scala/unit/kafka/log/LogSegmentsTest.scala', 'core/src/test/scala/unit/kafka/log/LogTestUtils.scala']",The current naming of the file 'LogUtils.scala' and the class 'LogUtils' in the test directory could potentially conflict with the introduction of a new file with the same name in the main directory as part of the recovery logic refactor. This issue might lead to confusion and errors in code implementation and operation.
762d11c13f1f4dc8e5a82f9bbed88582dcb045f4,1626478089,"MINOR: ducktape should start brokers in parallel and support co-located kraft

This patch adds a sanity-check bounce system test for the case where we have 3
co-located KRaft controllers and fixes the system test code so that this case
will pass by starting brokers in parallel by default instead of serially. We
now also send SIGKILL to any running KRaft broker or controller nodes for the
co-located case when a majority of co-located controllers have been stopped --
otherwise they do not shutdown, and we spin for the 60 second timeout. Finally,
this patch adds the ability to specify that certain brokers should not be
started when starting the cluster, and then we can start those nodes at a later
time via the add_broker() method call; this is going to be helpful for KRaft
snapshot system testing.

We were not testing the 3 co-located KRaft controller case previously, and it
would not pass because the first Kafka node would never be considered started.
We were starting the Kafka nodes serially, and we decide that a node has
successfully started when it logs a particular message. This message is not
logged until the broker has identified the controller (i.e. the leader of the
KRaft quorum). There cannot be a leader until a majority of the KRaft quorum
has started, so with 3 co-located controllers the first node could never be
considered ""started"" by the system test.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['tests/kafkatest/sanity_checks/test_bounce.py', 'tests/kafkatest/services/kafka/kafka.py']","The current system test fails in the case where we have 3 co-located KRaft controllers. The issue seems to arise from the fact that Kafka nodes are started serially, and a node is considered started only when it logs a particular message. This message is not logged until the broker identifies the controller (i.e., the leader of the Kraft quorum). Since there can be no leader until a majority of the KRaft quorum has started, if there are three co-located controllers, the first node could never be considered as ""started"" by the system test. Additionally, running KRaft broker or controller nodes for the co-located case do not shut down when a majority of co-located controllers have been stopped, leading to spinning for a 60-second timeout."
28d5a059438634db6fdecdbb816e2584715884d6,1661903763,"KAFKA-14187: kafka-features.sh: add support for --metadata (#12571)

This PR adds support to kafka-features.sh for the --metadata flag, as specified in KIP-778.  This
flag makes it possible to upgrade to a new metadata version without consulting a table mapping
version names to short integers. Change --feature to use a key=value format.

FeatureCommandTest.scala: make most tests here true unit tests (that don't start brokers) in order
to improve test run time, and allow us to test more cases. For the integration test part, test both
KRaft and ZK-based clusters. Add support for mocking feature operations in MockAdminClient.java.

upgrade.html: add a section describing how the metadata.version should be upgraded in KRaft
clusters.

Add kraft_upgrade_test.py to test upgrades between KRaft versions.

Reviewers: David Arthur <mumrah@gmail.com>, dengziming <dengziming1993@gmail.com>, José Armando García Sancio <jsancio@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java', 'clients/src/main/java/org/apache/kafka/clients/admin/SupportedVersionRange.java', 'clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java', 'core/src/main/scala/kafka/admin/FeatureCommand.scala', 'core/src/test/scala/unit/kafka/admin/FeatureCommandTest.scala', 'tests/kafkatest/services/kafka/kafka.py', 'tests/kafkatest/tests/core/kraft_upgrade_test.py', 'tests/kafkatest/version.py']","The kafka-features.sh script currently doesn't support the --metadata flag as specified in KIP-778, making it impossible to upgrade to a new metadata version without resorting to consulting a table mapping version names to short integers. In addition, issues with the --feature command using a key=value format are present. Another problem lies in the test cases residing in FeatureCommandTest.scala, as most aren't functionally unit tests and rather require starting brokers, leading to longer test run times and fewer possible test cases. Lack of support for mocking feature operations in MockAdminClient.java is also noted. The upgrade.html is missing a section describing how to upgrade the metadata.version in KRaft clusters. Finally, kraft_upgrade_test.py doesn't currently support testing upgrades between KRaft versions."
9d17bf98b6eae2d5a91a9a719e5c447b55ded3a5,1580950267,"KAFKA-9447: Add new customized EOS model example (#8031)

With the improvement of 447, we are now offering developers a better experience on writing their customized EOS apps with group subscription, instead of manual assignments. With the demo, user should be able to get started more quickly on writing their own EOS app, and understand the processing logic much better.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsResult.java', 'clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsResult.java', 'examples/bin/exactly-once-demo.sh', 'examples/src/main/java/kafka/examples/Consumer.java', 'examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java', 'examples/src/main/java/kafka/examples/KafkaConsumerProducerDemo.java', 'examples/src/main/java/kafka/examples/KafkaExactlyOnceDemo.java', 'examples/src/main/java/kafka/examples/Producer.java']","The current lack of sample applications leveraging the improvements introduced by issue 447 makes it difficult for developers to quickly grasp how to write their custom Exactly-Once Semantics (EOS) apps utilizing group subscriptions, rather than using manual assignments. This lack of readily available demos could be deemed as a barrier to understanding the processing logic more effectively and writing EOS applications more efficiently."
dcbd28da53bfc6edb78fd1735993163fc6f4c7c7,1605811938,"KAFKA-10723: Fix LogManager shutdown error handling (#9596)

The asynchronous shutdown in LogManager has the shortcoming that if during shutdown any of the internal futures fail, then we do not always ensure that all futures are completed before LogManager.shutdown returns. This is because, this line in the finally clause shuts down the thread pools asynchronously. As a result, despite the shut down completed message from KafkaServer is seen in the error logs, some futures continue to run from inside LogManager attempting to close some logs. This is misleading during debugging. Also sometimes it introduces an avoidable post-shutdown activity where resources (such as file handles) are released or persistent state is checkpointed in the Broker.

In this PR, we fix the above behavior such that we prevent leakage of threads. If any of the futures throw an error, we skip creating of checkpoint and clean shutdown file only for the affected log directory. We continue to wait for all futures to complete for all the directories.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/LogManager.scala', 'core/src/test/scala/unit/kafka/log/LogManagerTest.scala']","During the asynchronous shutdown in LogManager, an issue occurs where, if any internal futures fail, the system does not always ensure all futures are completed before LogManager.shutdown returns. This could be misleading during debugging as the KafkaServer shutdown completed message may appear in the error logs, yet certain futures continue attempting to close logs from within LogManager. This also occasionally leads to superfluous post-shutdown activity such as the releasing of resources or persistent state checkpointing in the Broker."
87bef1d1b0fe1502be2335524990aad94bfd4bae,1591232010,"KAFKA-9788; Use distinct names for transaction and group load time sensors (#8784)

Sensor objects are stored in the Kafka metrics registry and keyed by name. If a new sensor is created with the same name as an existing one, the existing one is returned rather than a new object being created. The partition load time sensors for the transaction and group coordinators used the same name, so data recorded to either was stored in the same object. This meant that the metrics values for both metrics were identical and consisted of the combined data. This patch changes the names to be distinct so that the data will be stored in separate Sensor objects.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala']","The transaction and group coordinator's partition load time sensors are using the same name. This is causing data recorded to either to be stored in the same Sensor object, resulting in metrics values for both being identical and a combination of respective data."
40ad4fe0ae9c687ed3c4d35fb5f5830cb1a867b8,1602613145,"KAFKA-10494: Eager handling of sending old values (#9415)

Nodes that are materialized should not forward requests to `enableSendingOldValues` to parent nodes, as they themselves can handle fulfilling this request. However, some instances of `KTableProcessorSupplier` were still forwarding requests to parent nodes, which was causing unnecessary materialization of table sources.

The following instances of `KTableProcessorSupplier` have been updated to not forward `enableSendingOldValues` to parent nodes if they themselves are materialized and can handle sending old values downstream:

 * `KTableFilter`
 * `KTableMapValues`
 * `KTableTransformValues`

Other instances of `KTableProcessorSupplier` have not be modified for reasons given below:
 * `KTableSuppressProcessorSupplier`: though it has a `storeName` field, it didn't seem right for this to handle sending old values itself. Its only job is to suppress output.
 * `KTableKTableAbstractJoin`: doesn't have a store name, i.e. it is never materialized, so can't handle the call itself.
 * `KTableKTableJoinMerger`: table-table joins already have materialized sources, which are sending old values. It would be an unnecessary performance hit to have this class do a lookup to retrieve the old value from its store.
 * `KTableReduce`: is always materialized and already handling the call without forwarding
 * `KTableAggregate`: is always materialized and already handling the call without forwarding

Reviewer: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableFilter.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMapValues.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableTransformValues.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableFilterTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableLeftJoinTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableMapValuesTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableTransformValuesTest.java']","Materialized nodes forwarding requests to 'enableSendingOldValues' to parent nodes can cause unnecessary materialization of table sources. However, some instances of 'KTableProcessorSupplier' have been identified as still forwarding requests to parent nodes even when they are materialized and capable of handling sending old values downstream. These instances include 'KTableFilter', 'KTableMapValues', and 'KTableTransformValues', amongst others. This unnecessary forwarding can lead to performance issues and unexpected behaviors in the application."
f978d0551be5ffaec282cf8c3643bacbe90ad4b9,1605215355,"MINOR: Increase the amount of time available to the `test_verifiable_producer` (#9201)

Increase the amount of time available to the `test_verifiable_producer` test to login and get the process name for the verifiable producer from 5 seconds to 10 seconds.

We were seeing some test failures due to the assertion failing because the verifiable producer would complete before we could login, list the processes, and parse out the producer version. Previously, we were giving this operation 5 seconds to run, this PR bumps it up to 10 seconds. 

I verified locally that this does not flake, but even at 5 seconds I wasn't seeing any flakes. Ultimately we should find a better strategy than racing to query the producer process (as outlined in the existing comments). 

Reviewers: Jason Gustafson <jason@confluent.io>",['tests/kafkatest/sanity_checks/test_verifiable_producer.py'],"The `test_verifiable_producer` test has occasionally failed due to an assertion failing. The failure occurs because the producer completes before the system can login, list the processes, and parse out the producer version. With the current setup, this operation is only given 5 seconds to run, which can lead to these sporadic test failures."
fc6e91e19920a41a56ff60c65e3b9719f4506977,1657361162,"KAFKA-13474: Allow reconfiguration of SSL certs for broker to controller connection (#12381)

What:
When a certificate is rotated on a broker via dynamic configuration and the previous certificate expires, the broker to controller connection starts failing with SSL Handshake failed.

Why:
A similar fix was earlier performed in #6721 but when BrokerToControllerChannelManager was introduced in v2.7, we didn't enable dynamic reconfiguration for it's channel.

Summary of testing strategy (including rationale)
Add a test which fails prior to the fix done in the PR and succeeds afterwards. The bug wasn't caught earlier because there was no test coverage to validate the scenario.

Reviewers: Luke Chen <showuon@gmail.com>","['core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala', 'core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala', 'core/src/test/scala/kafka/server/BrokerToControllerRequestThreadTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","When a certificate is rotated on a broker via dynamic configuration and the previous certificate expires, the broker to controller connection starts failing with SSL Handshake failed. This specifically happens when BrokerToControllerChannelManager gets introduced, as it doesn't enable dynamic reconfiguration for its channel. The issue arises due to the lack of dynamic reconfiguration when a certificate is expired after rotation, causing broker to controller SSL connection drop."
ce883892270a02a72e91afbdb1fabdd50d7da474,1648176010,"KAFKA-13770: Restore compatibility with KafkaBasedLog using older Kafka brokers (#11946)

The `retryEndOffsets(…)` method in `TopicAdmin` recently added (KAFKA-12879, #11797) to allow the `KafkaBasedLog.start()` method to retry any failures reading the last offsets for a topic. However, this introduce a regression when talking to older brokers (0.10.x or earlier).

The `KafkaBasedLog` already had logic that expected an `UnsupportedVersionException` thrown by the admin client when a Kafka API is not available on an older broker, but the new retry logic in `TopicAdmin` did not account for this and wrapped the exception, thereby breaking the `KafkaBasedLog` logic and preventing startup.

The fix is to propagate this `UnsupportedVersionException` from the `TopicAdmin.retryEndOffsets(…)` method. Added a new unit test that first replicated the problem before the fix, and verified the fix corrects the problem.","['connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java']","The newly added `retryEndOffsets(…)` method in `TopicAdmin` to allow retrying any failures when reading the last offsets for a topic has caused a regression when interacting with older Kafka brokers (0.10.x or earlier). The existing logic in `KafkaBasedLog` was designed to expect an `UnsupportedVersionException` when a Kafka API is not supported on an old broker; however, the `retryEndOffsets(…)` method doesn't account for this exception and instead wraps it up, breaking the `KafkaBasedLog` logic and hindering startup."
4deb80676e66311dc919f97dad1cac6131267885,1573248073,"KAFKA-9098: When users name repartition topic, use the name for the repartition filter, source and sink node. (#7598)

When users specify a name for a repartition topic, we should use the same name for the repartition filter, source, and sink nodes. With the addition of KIP-307 if users go to the effort of naming every node in the topology having processor nodes with generated names is inconsistent behavior.

Updated tests in the streams test suite.

Reviewers: John Roesler <john@confluent.io>, Christopher Pettitt <cpettitt@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/RepartitionTopicNamingTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressTopologyTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java']","When users specify a name for a repartition topic, the given name is not being applied to the associated repartition filter, source, and sink nodes. This leads to inconsistency in node naming, especially with the addition of KIP-307, which allows users to put effort into naming every node in the topology. Consequently, processor nodes with generated names remain inconsistent."
6b6b36cec043eefc830c5209d5a18bbcb47a31db,1577145348,"KAFKA-9232; Coordinator new member timeout does not work for JoinGroup v3 and below (#7753)

The v3 JoinGroup logic does not properly complete the initial heartbeat for new members, which then expires after the static 5 minute timeout if the member does not rejoin. The core issue is in the `shouldKeepAlive` method, which returns false when it should return true because of an inconsistent timeout check.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/coordinator/group/MemberMetadata.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala']",The v3 JoinGroup logic appears to have an issue with the initial heartbeat for new members. The new members trigger a static 5-minute timeout if they do not rejoin. The problem seems to occur specifically in the `shouldKeepAlive` method where an inconsistent timeout check results in the method returning false when it should return true.
c6d2778a8d1383e76695ec82710208153e83b2c2,1624981776,"KAFKA-12996; Return OFFSET_OUT_OF_RANGE for fetchOffset < startOffset even for diverging epochs (#10930)

If fetchOffset < startOffset, we currently throw OffsetOutOfRangeException when attempting to read from the log in the regular case. But for diverging epochs, we return Errors.NONE with the new leader start offset, hwm etc.. ReplicaFetcherThread throws OffsetOutOfRangeException when processing responses with Errors.NONE if the leader's offsets in the response are out of range and this moves the partition to failed state. The PR adds a check for this case when processing fetch requests and throws OffsetOutOfRangeException regardless of epoch.

Reviewers: Luke Chen <showuon@gmail.com>, Nikhil Bhatia <rite2nikhil@gmail.com>, Guozhang Wang <wangguoz@gmail.com>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala']","In the current state, if fetchOffset < startOffset, an OffsetOutOfRangeException is thrown when attempting to read from the log in standard scenarios. However, when dealing with diverging epochs, Errors.NONE is returned alongside the new leader start offset, high water mark etc. As a result, the ReplicaFetcherThread throws OffsetOutOfRangeException when processing responses with Errors.NONE if the leader's offsets in the response are out of range, causing the partition to move into a failed state. This issue appears to be an inconsistency in how different scenarios are handled."
1a3e23a5798373fb40f31af2eb7c4348270ac437,1633459058,"MINOR: TopicIdPartition improvements (#11374)

1. It should not require a TopicPartition during construction and normal
usage.
2. Simplify `equals` since `topicId` and `topicPartition` are never
null.
3. Inline `Objects.hash` to avoid array allocation.
4. Make `toString` more concise using a similar approach as
`TopicPartition` since this `TopicIdPartition` will replace
`TopicPartition` in many places in the future.
5. Add unit tests for `TopicIdPartition`, it seems like we had none.
6. Minor clean-up in calling/called classes.

Reviewers: David Jacot <djacot@confluent.io>, Satish Duggana <satishd@apache.org>","['clients/src/main/java/org/apache/kafka/common/TopicIdPartition.java', 'clients/src/main/java/org/apache/kafka/common/TopicPartition.java', 'clients/src/test/java/org/apache/kafka/common/TopicIdPartitionTest.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataTopicPartitioner.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogSegmentMetadataTransform.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogSegmentMetadataUpdateTransform.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemotePartitionDeleteMetadataTransform.java']","'The TopicIdPartition presents several issues, such as requiring a TopicPartition during construction and routine use, a needlessly complex equals method, unnecessary array allocation in Objects.hash, and a verbose toString method. Additionally, no unit tests are currently in place for TopicIdPartition and there are several areas that could benefit from minor clean-up.'"
ecfccb480bd57f5195ea67d23702710014ec0e1a,1625764403,"KAFKA-12660; Do not update offset commit sensor after append failure (#10560)

Do not update the commit-sensor if the commit failed and add test logic. The patch also adds 2 unit tests, the first for `OFFSET_METADATA_TOO_LARGE` error, the second is to cover circumstance when one offset is committed and the other is failed with `OFFSET_METADATA_TOO_LARGE`. Both of these cases were uncovered previously.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","When an offset commit fails, the commit-sensor is still being updated, which does not correctly reflect the state of the commit. This happens even in cases where the error is `OFFSET_METADATA_TOO_LARGE`, or when one offset is successfully committed and another fails with the same error. Both of these scenarios do not have suitable test coverage."
7f90eda04720274a0e4f7ec32a7340245f493e9e,1614301360,"KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while building SslTransportLayer (#10059)

This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/common/network/SaslChannelBuilder.java', 'clients/src/main/java/org/apache/kafka/common/network/SslChannelBuilder.java', 'clients/src/main/java/org/apache/kafka/common/security/ssl/SslFactory.java', 'clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java', 'clients/src/test/java/org/apache/kafka/common/network/SslTransportLayerTest.java', 'clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorFailureDelayTest.java', 'clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java']","While building the `SslTransportLayer` in `SaslChannelBuilder`, a reverse DNS lookup is being performed in `getHostName`. This could cause performance issues, especially in environments where DNS lookup is slow or unreliable."
3c4472d701a7e9d9b8714a0b9d87ae190d1679fb,1680271267,"KAFKA-14867: Trigger rebalance when replica racks change if client.rack is configured (KIP-881) (#13474)

When `client.rack` is configured for consumers, we perform rack-aware consumer partition assignment to improve locality. After/during reassignments, replica racks may change, so to ensure optimal consumer assignment, trigger rebalance from the leader when set of racks of any partition changes.

Reviewers: David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java', 'core/src/test/scala/integration/kafka/server/FetchFromFollowerIntegrationTest.scala']","'When `client.rack` is configured for consumers, rack-aware consumer partition assignment is performed to enhance locality. However, issues arise as replica racks may change after/during reassignments. In order to ensure optimal consumer assignment, it is necessary to have a mechanism that can detect and respond when the set of racks of any partition changes.'"
cfdd567955588e134770a9145ba57800ca88313c,1655522222,"KAFKA-13880: Remove DefaultPartitioner from StreamPartitioner (#12304)

There are some considerata embedded in this seemingly straight-forward PR that I'd like to explain here. The StreamPartitioner is used to send records to three types of topics:

1) repartition topics, where key should never be null.
2) changelog topics, where key should never be null.
3) sink topics, where only non-windowed key could be null and windowed key should still never be null.
Also, the StreamPartitioner is used as part of the IQ to determine which host contains a certain key, as determined by the case 2) above.

This PR's main goal is to remove the deprecated producer's default partitioner, while with those things in mind such that:

We want to make sure for not-null keys, the default murmur2 hash behavior of the streams' partitioner stays consistent with producer's new built-in partitioner.
For null-keys (which is only possible for non-window default stream partition, and is never used for IQ), we would fix the issue that we may never rotate to a new partitioner by setting the partition as null hence relying on the newly introduced built-in partitioner.

Reviewers: Artem Livshits <84364232+artemlivshits@users.noreply.github.com>, Matthias J. Sax <matthias@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/BuiltInPartitioner.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowedStreamPartitioner.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStreamPartitioner.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java']","The Kafka system uses a StreamPartitioner to send records to topics, and it's important that for certain topics, the key should never be null. However, the StreamPartitioner depends on the deprecated DefaultPartitioner from the producer. This potentially leads to inconsistencies in the default murmur2 hash behavior of the stream's partitioner with the producer's new built-in partitioner, especially for not-null keys. Additionally, there's an issue with null-keys in non-window default stream partitions where the system may never rotate to a new partitioner."
fbf6a76fc40fc5fecd679ef6484a0b92a4ab3971,1561695159,"KAFKA-8356: add static membership info to round robin assignor (#6815)

The purpose here is to leverage static membership information during round robin consumer assignment, because persistent member id could help make the assignment remain the same during rebalance.
The comparison logic is changed to:

1. If member A and member B both have group.instance.id, then compare their group.instance.id
2. If member A has group.instance.id, while member B doesn't, then A < B
3. If both member A and B don't have group.instance.id, compare their member.id

In round robin assignor, we use ephemeral member.id to sort the members in order for assignment. This semantic is not stable and could trigger unnecessary shuffle of tasks. By leveraging group.instance.id the static member assignment shall be persist when satisfying following conditions:

1. number of members remain the same across generation
2. static members' identities persist across generation

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java']","During round robin consumer assignment, the use of ephemeral member.id to sort the members for assignment can lead to instability and possibly trigger an unnecessary shuffle of tasks. There also appears to be no leveraged use of group.instance.id for static member assignment, which could potentially maintain the assignment during rebalance if certain conditions are met. These conditions are when the number of members remain the same across generation and when static members' identities persist across the generation."
3348fc49d8824155e737b866f633e14684da5fe9,1593050258,"KAFKA-10198: guard against recycling dirty state (#8924)

We just needed to add the check in StreamTask#closeClean to closeAndRecycleState as well. I also renamed closeAndRecycleState to closeCleanAndRecycleState to drive this point home: it needs to be clean.

This should be cherry-picked back to the 2.6 branch

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, ","['streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","The state recycling mechanism in StreamTask#closeClean lacks necessary checks, causing problems when trying to recycle dirty state. This results in potential inconsistency or corruption within state."
b1830e4aa2d83d2f213faf5f7432a119742df80c,1681465434,"KAFKA-14834: [9/N] Disable versioned-stores for unsupported operations (#13565)

Using versioned-stores for global-KTables is not allowed, because a
global-table is bootstrapped on startup, and a stream-globalTable join
does not support temporal semantics.

Furthermore, `suppress()` does not support temporal semantics and thus
cannot be applied to an versioned-KTable.

This PR disallows both use-cases explicitely.

Part of KIP-914.

Reviewers: Bill Bejeck <bbejeck@gmail.com>, Victoria Xia <victoria.xia@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/KTable.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableSuppressNode.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/VersionedKeyValueStoreIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableJoinTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableLeftJoinTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressTopologyTest.java']","The use of versioned-stores for global-KTables can cause a problem, as a global-table is rebooted during startup and a stream-globalTable cannot support temporal semantics. Additionally, `suppress()` is unable to support temporal semantics, thus making it unsuitable to apply to a versioned-KTable. Consequently, these use-cases pose serious issues."
6d649f503a964ed9612e79ef2d9e55e26240fbc3,1553004769,"KAFKA-8062: Do not remore StateListener when shutting down stream thread (#6468)

In a previous commit #6091, we've fixed a couple of edge cases and hence do not need to remove state listener anymore (before that we removed the state listener intentionally to avoid some race conditions, which has been gone for now).

Reviewers: Matthias J. Sax <mjsax@apache.org>,   Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java']","'Due to previously existing race conditions, state listeners were intentionally removed when shutting down a stream thread. However, this procedure may no longer be needed as the mentioned race conditions have been eliminated in recent updates.'"
9eface2e731ac53037bfa29b191171dae8c44d41,1579300051,"KAFKA-9449; Adds support for closing the producer's BufferPool. (#7967)

The producer's BufferPool may block allocations if its memory limit has hit capacity. If the producer is closed, it's possible for the allocation waiters to wait for max.block.ms if progress cannot be made, even when force-closed (immediate), which can cause indefinite blocking if max.block.ms is particularly high. 

This patch fixes the problem by adding a `close()` method to `BufferPool`, which wakes up any waiters that have pending allocations and throws an exception.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/BufferPool.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/BufferPoolTest.java']","When the producer's BufferPool hits its memory limit, it may block allocations. This can lead to a possible situation where allocation waiters get blocked indefinitely, especially when the producer is being force-closed and max.block.ms is high. This indefinite blocking occurs even if progress cannot be made."
1b36e11967e7a459d6401ba5ea740a659fc1994b,1585069823,"MINOR: Restore and global consumers should never have group.instance.id (#8322)

And hence restore / global consumers should never expect FencedInstanceIdException.

When such exception is thrown, it means there's another instance with the same instance.id taken over, and hence we should treat it as fatal and let this instance to close out instead of handling as task-migrated.

Reviewers: Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java']","'Restore and global consumers currently expect a FencedInstanceIdException, which is problematic. This exception arises when another instance with the same instance.id takes over. Instead of handling it as task-migrated, it would be more appropriate to treat it as a fatal issue and close out the instance.'"
b166ac43cb646d9589623abfe283874d59ce923b,1663421450,"KAFKA-14238;  KRaft metadata log should not delete segment past the latest snapshot (#12655)

Disable segment deletion based on size and time by setting the KRaft metadata log's `RetentionMsProp` and `RetentionBytesProp` to `-1`. This will cause `UnifiedLog.deleteRetentionMsBreachedSegments` and `UnifiedLog.deleteRetentionSizeBreachedSegments` to short circuit instead of deleting segments.

Without this changes the included test would fail. This happens because `deleteRetentionMsBreachedSegments` is able to delete past the `logStartOffset`. Deleting past the `logStartOffset` would violate the invariant that if the `logStartOffset` is greater than 0 then there is a snapshot with an end offset greater than or equal to the log start offset.

Reviewers: Luke Chen <showuon@gmail.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/raft/KafkaMetadataLog.scala', 'core/src/test/scala/kafka/raft/KafkaMetadataLogTest.scala']","The KRaft metadata log's ability to delete segments based on size and time can potentially delete past the `logStartOffset`. This deletion violates the invariant that if the `logStartOffset` is greater than 0, there should be a snapshot with an end offset greater than or equal to the log start offset. This could subsequently cause system malfunction or data loss."
9b409de1e2719f72e3c17663ce131e40dc459361,1669948469,"KAFKA-14358; Disallow creation of cluster metadata topic (#12885)

With KRaft the cluster metadata topic (__cluster_metadata) has a different implementation compared to regular topic. The user should not be allowed to create this topic. This can cause issues if the metadata log dir is the same as one of the log dirs.

This change returns an authorization error if the user tries to create the cluster metadata topic.

Reviewers: David Arthur <mumrah@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/main/java/org/apache/kafka/common/internals/Topic.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java', 'core/src/main/scala/kafka/server/ControllerApis.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/main/scala/kafka/server/KafkaRaftServer.scala', 'core/src/test/scala/unit/kafka/server/AbstractCreateTopicsRequestTest.scala', 'core/src/test/scala/unit/kafka/server/ControllerApisTest.scala', 'core/src/test/scala/unit/kafka/server/CreateTopicsRequestTest.scala']","Allowing user creation of the cluster metadata topic (__cluster_metadata) under KRaft can lead to conflicts. Particularly, issues may occur if the metadata log directory is the same as one of the log directories."
9c7d8577134559437c67cc2d24154023993a901f,1646872242,"KAFKA-12648: fix #getMinThreadVersion and include IOException + topologyName in StreamsException when topology dir cleanup fails (#11867)

Quick fix to make sure we log the actual source of the failure both in the actual log message as well as the StreamsException that we bubble up to the user's exception handler, and also to report the offending topology by filling in the StreamsException's taskId field.

Also prevents a NoSuchElementException from being thrown when trying to compute the minimum topology version across all threads when the last thread is being unregistered during shutdown.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TopologyMetadata.java']","There are two issues that contribute to this problem: 

1. In cases where topology directory cleanup fails, the actual source of the error is not appropriately logged in both the actual log message and the StreamsException reported to the user's exception handler, and the offending topology is not reported. 

2. A NoSuchElementException is being thrown when attempting to compute the minimum topology version among all threads just as the last thread is being deregistered during shutdown."
d9b139220ee253da673af44d58dc87bd184188f1,1670626956,"KAFKA-14318: KIP-878, Introduce partition autoscaling configs (#12962)

First PR for KIP-878: Internal Topic Autoscaling for Kafka Streams

Introduces two new configs related to autoscaling in Streams: a feature flag and retry timeout. This PR just adds the configs and gets them passed through to the Streams assignor where they'll ultimately be needed/used

Reviewers: Bill Bejeck <bill@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfigurationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientTagAwareStandbyTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StandbyTaskAssignorFactoryTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java']","Kafka Streams has no feature for internal topic autoscaling. This lack of autoscaling could affect the efficiency and speed of partition handling in Kafka streams. Additionally, there is no retry timeout configuration for attempts at autoscaling, which could lead to excessive, uncontrolled retry attempts."
aebb0e33943bd8d410de1aa8160a01d9581c8681,1607447612,"KAFKA-10264; Fix Flaky Test TransactionsTest.testBumpTransactionalEpoch (#9291)

The test case sends two records before killing broker. The failure is caused when both records are NOT sent in a single batch. The failure of first record can abort second batch and then produces `KafkaException` rather than `TimeoutException`. The patch removes the second record send.

Reviewers: Jason Gustafson <jason@confluent.io>",['core/src/test/scala/integration/kafka/api/TransactionsTest.scala'],"The test case 'TransactionsTest.testBumpTransactionalEpoch' becomes flaky when two records, sent before killing the broker, are NOT sent in a single batch. Failure of the first record can cause the second batch to abort, leading to a `KafkaException` instead of the expected `TimeoutException`."
de4183485b0be534738010b19944edf3388cf49b,1602176729,"KAFKA-10028: Minor fixes to describeFeatures and updateFeatures apis (#9393)

In this PR, I have addressed the review comments from @chia7712 in #9001 which were provided after #9001 was merged. The changes are made mainly to KafkaAdminClient:

Improve error message in updateFeatures api when feature name is empty.
Propagate top-level error message in updateFeatures api.
Add an empty-parameter variety for describeFeatures api.
Minor documentation updates to @param and @return to make these resemble other apis.

Reviewers: Chia-Ping Tsai chia7712@gmail.com, Jun Rao junrao@gmail.com","['clients/src/main/java/org/apache/kafka/clients/admin/Admin.java', 'clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java']","The `updateFeatures` API has difficulty handling scenarios when a feature name is empty, leading to less-than-useful error messaging. Additionally, top-level error messages do not propagate properly. Also, there is an absence of an empty-parameter variety for the `describeFeatures` API. Lastly, documentation related to `@param` and `@return` needs modifications for its consistency with other APIs."
0bc8da7aec8dfb7c61321b22e717a94565f4855d,1657473399,"KAFKA-14055; Txn markers should not be removed by matching records in the offset map (#12390)

When cleaning a topic with transactional data, if the keys used in the user data happen to conflict with the keys in the transaction markers, it is possible for the markers to get removed before the corresponding data from the transaction is removed. This results in a hanging transaction or the loss of the transaction's atomicity since it would effectively get bundled into the next transaction in the log. Currently control records are excluded when building the offset map, but not when doing the cleaning. This patch fixes the problem by checking for control batches in the `shouldRetainRecord` callback.

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerTest.scala']","During the cleaning process of a topic with transactional data, any keys in the user data that conflict with keys in the transaction markers may cause the markers to be removed before the corresponding transaction data is removed. This can result in hanging transactions or the loss of a transaction's atomicity, as the transaction may get bundled into the next transaction in the log. This occurs because control records are currently excluded when building the offset map, but not during the cleaning."
b60f4464acaa540de75ef2fc83a911d4ff6a8786,1647994769,"Revert ""KAFKA-7077: Use default producer settings in Connect Worker (#11475)"" (#11932)

This reverts commit 76cf7a5793702b55e2cfd98a375f8f1708ff32c3.

Connect already allows users to enable idempotent producers for connectors and the Connect workers. Although Kafka producers enabled idempotency by default in 3.0, due to compatibility requirements and the fact that [KIP-318](https://cwiki.apache.org/confluence/display/KAFKA/KIP-318%3A+Make+Kafka+Connect+Source+idempotent) hasn't been explicitly approved, the changes here are reverted. A separate commit will explicitly disable idempotency in producers instantiated by Connect by default until KIP-318 is approved and scheduled for release. ","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java']","The default settings of the Kafka producer in Connect Worker were reverted due to idempotency concerns. Although the Kafka producers enabled idempotency by default in 3.0, there could be compatibility issues. Furthermore, KIP-318, which proposes making Kafka Connect Source idempotent, hasn't been explicitly approved yet. These factors could cause potential operational or data consistency complications for users."
7201976378aeb206563c949dd0e0c62d411e57d6,1630081701,"KAFKA-13079: Forgotten Topics in Fetch Requests may incorrectly use topic IDs (#11104)

The FetchSessionHandler had a small bug in the session build method where we did not consider building a session where no partitions were added and the session previously did not use topic IDs. (ie, it was relying on at least one partition being added to signify whether topic IDs were present)

Due to this, we could send forgotten partitions with the zero UUID. This would always result in an exception and closed session.

This patch fixes the logic to check that any forgotten partitions have topic IDs. There is also a test added for the empty session situation when topic IDs are used and when topic names are used.

Reviewers: David Jacot <djacot@confluent.io>, Jun Rao <junrao@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/FetchSessionHandler.java', 'clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java']","The FetchSessionHandler may encounter an issue when building sessions. Particularly, when no partitions are added and the previous session did not utilize topic IDs, it may lead to disregarded partitions being sent with the zero UUID. Every instance of this occurrence then results in an exception and terminated session."
0a5097323bce270440697201d359bc67bf646f28,1586968244,"KAFKA-9864: Avoid expensive QuotaViolationException usage (#8477)

QuotaViolationException generates an exception message via String.format in the constructor
even though the message is often not used, e.g. https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ClientQuotaManager.scala#L258. We now override `toString` instead.

It also generates an unnecessary stack trace, which is now avoided using the same pattern as in ApiException.

I have also avoided use of QuotaViolationException for control flow in
ReplicationQuotaManager which is another hotspot that we have seen in practice.

Reviewers: Gwen Shapira <gwen@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Ismael Juma <ismael@juma.me.uk>","['clients/src/main/java/org/apache/kafka/common/metrics/QuotaViolationException.java', 'core/src/main/scala/kafka/server/ReplicationQuotaManager.scala']","The use of QuotaViolationException is leading to performance issues. It generates an expensive exception message via String.format in the constructor even when the message is generally not used. This exception also generates an unnecessary stack trace. Furthermore, the application of QuotaViolationException for control flow in ReplicationQuotaManager is another observed hotspot."
d2c06c9c3c35803b9f5f0b6060b242789657f008,1617903750,"KAFKA-12619; Raft leader should expose hw only after committing LeaderChange (#10481)

KIP-595 describes an extra condition on commitment here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum#KIP595:ARaftProtocolfortheMetadataQuorum-Fetch. In order to ensure that a newly elected leader's committed entries cannot get lost, it must commit one record from its own epoch. This guarantees that its latest entry is larger (in terms of epoch/offset) than any previously written record which ensures that any future leader must also include it. This is the purpose of the `LeaderChange` record which is written to the log as soon as the leader gets elected.

Although we had this check implemented, it was off by one. We only ensured that replication reached the epoch start offset, which does not reflect the appended `LeaderChange` record. This patch fixes the check and clarifies the point of the check. The rest of the patch is just fixing up test cases.

Reviewers: dengziming <swzmdeng@163.com>, Guozhang Wang <wangguoz@gmail.com>","['raft/src/main/java/org/apache/kafka/raft/LeaderState.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java', 'raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java']","There is an issue with the Raft leader's handling of committed entries after leadership change. Following KIP-595, a newly elected leader should commit one record from its own epoch to prevent its committed entries from getting lost. However, it has been found that the currently implemented check is off by one. It only guarantees replication reaching the epoch start offset, neglecting to account for the appended `LeaderChange` record. This could potentially lead to data loss of the leader's committed entries."
28393be6d7416f51eb51f7fe2b075570b45ef09f,1646851168,"KAFKA-12879: Revert changes from KAFKA-12339 and instead add retry capability to KafkaBasedLog (#11797)

Fixes the compatibility issue regarding KAFKA-12879 by reverting the changes to the admin client from KAFKA-12339 (#10152) that retry admin client operations, and instead perform the retries within Connect's `KafkaBasedLog` during startup via a new `TopicAdmin.retryEndOffsets(..)` method. This method delegates to the existing `TopicAdmin.endOffsets(...)` method, but will retry on `RetriableException` until the retry timeout elapses.

This change should be backward compatible to the KAFKA-12339 so that when Connect's `KafkaBasedLog` starts up it will retry attempts to read the end offsets for the log's topic. The `KafkaBasedLog` existing thread already has its own retry logic, and this is not changed.

Added more unit tests, and thoroughly tested the new `RetryUtil` used to encapsulate the parameterized retry logic around any supplied function.","['clients/src/main/java/org/apache/kafka/clients/admin/internals/MetadataOperationContext.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/RetryUtil.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/KafkaBasedLogTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/RetryUtilTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java']","Changes to the admin client under KAFKA-12339 that retried admin client operations encountered a compatibility issue with KAFKA-12879. The problem sector lies within Connect's `KafkaBasedLog` during startup, specifically attempts to read the end offsets for the log's topic. Current retry logic is inadequate, which leads to the need for an improved retry mechanism during the startup of `KafkaBasedLog`."
5c0fd36ee53052fa8a92dac1266fee3aaf16d7a1,1587669902,"KAFKA-9823: Remember the sent generation for the coordinator request (#8445)

For join / sync / commit / heartbeat request, we would remember the sent generation in the created handler object, and then upon getting the error code, we could check whether the sent generation still matches the current generation. If not, it means that the member has already reset its generation or has participated in a new rebalance already. This means:

1. For join / sync-group request, we do not need to call reset-generation any more for illegal-generation / unknown-member. But we would still set the error since at a given time only one join/sync round-trip would be in flight, and hence we should not be participating in a new rebalance. Also for fenced instance error we still treat it as fatal since we should not be participating in a new rebalance, so this is still not expected.

2. For commit request, we do not set the corresponding error for illegal-generation / unknown-member / fenced-instance but raise rebalance-in-progress. For commit-sync it would be still thrown to user, while for commit-async it would be logged and swallowed.

3. For heartbeat request, we do not treat illegal-generation / unknown-member / fenced-instance errors and just consider it as succeeded since this should be a stale heartbeat which can be ignored.

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Boyang Chen <boyang@confluent.io>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupResponse.java', 'clients/src/main/java/org/apache/kafka/common/requests/SyncGroupResponse.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java', 'core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala']","In the current setup, for join/sync/commit/heartbeat requests, the sent generation is stored in the created handler object. When receiving an error code, the system checks if the sent and current generation match. Issues arise when there is a mismatch, indicating the member has reset its generation or already participated in new rebalance. This brings about the following challenges:

1. For join/sync-group requests, illegal-generation or unknown-member errors, the system redundantly calls reset-generation. Despite only a single join/sync round-trip being possible at a time, these errors are still set and fenced instance errors are still considered fatal.

2. For commit requests, specific errors such as illegal-generation, unknown-member, fenced–instance errors are not set to trigger rebalance in progress. For commit-sync, this error is thrown to the user, while for commit-async it's just logged and swallowed.

3. Errors such as illegal-generation, unknown-member, fenced–instance errors are not treated in heartbeat requests. It's assumed these are stale heartbeats, leading to them being ignored, which could lead to miscommunication or misinterpretation of the member state."
079e5d647ce39cf2ab5b5f37c5ce28b59fb6db13,1695745043,"KAFKA-15326: [8/N] Move consumer interaction out of processing methods (#14226)

The process method inside the tasks needs to be called from within
the processing threads. However, it currently interacts with the
consumer in two ways:

* It resumes processing when the PartitionGroup buffers are empty
* It fetches the lag from the consumer

We introduce updateLags() and 
resumePollingForPartitionsWithAvailableSpace() methods that call into
the task from the polling thread, in order to set up the consumer
correctly for the next poll, and extract metadata from the consumer
after the poll.

Reviewer: Bruno Cadonna <bruno@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ReadOnlyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/integration/PauseResumeIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java']","The process method within tasks, which must be called from processing threads, currently interacts directly with the consumer in two ways: resumption of processing when PartitionGroup buffers are empty, and fetching of lag from consumer. This might lead to issues related to correct consumer setup for the next poll and extracting metadata from the consumer post-poll."
33df5c633f68cb3726f053fe51109bbed897cbb4,1621898098,"MINOR: clarify message ordering with max in-flight requests and idempotent producer (#10690)

The docs for the max.in.flight.requests.per.connection and enable.idempotence configs currently imply that setting the max in-flight request greater than 1 will break the message ordering guarantee, but that is only true if enable.idempotence is false. When using an idempotent producer, the max in-flight request can be up to 5 without re-ordering messages.

Reviewers: Matthias J. Sax <mjsax@confluent.io>, Ismael Juma <mlists@juma.me.uk>, Luke Chen <showuon@gmail.com>",['clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java'],"'The documentation for the ""max.in.flight.requests.per.connection"" and ""enable.idempotence"" configurations may lead to confusion. It implies that if the max in-flight request is set to more than 1, the message ordering guarantee will not hold. However, this is only the case if the ""enable.idempotence"" is set to false. This could lead to misunderstandings about the order of messages when using an idempotent producer.'"
0aea498b9a2e70bdd92859159c0a868e9b8900d9,1654194083,"MINOR: Pin ducktape version to < 0.9 (#12242)

With newer ducktape versions than < 0.9 system tests
may run into authentication issues with the AK system test
infrastructure.

The version will be bumped up once we have infrastructure
in place for newer paramiko versions brought in by ducktape
0.9.

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Matthias J. Sax <mjsax@apache.org>, Kvicii <Karonazaba@gmail.com>",['tests/setup.py'],"'Running system tests with newer ducktape versions greater than 0.9 may cause authentication issues with the AK system test infrastructure. The issue needs to be resolved before we can bump up to the newer versions of ducktape, which are compatible with more recent versions of paramiko.'"
7dc17908de540b461b67b71652cef652adc488b0,1666130960,"KAFKA-14300; Generate snapshot after repeated controller resign (#12747)

Setting the `committedBytesSinceLastSnapshot` to 0 when resigning can cause the controller to not generate a snapshot after `snapshotMaxNewRecordBytes` committed bytes have been replayed.

This change fixes that by simply not resetting the counter during resignation. This is correct because the counter tracks the number of committed bytes replayed but not included in the latest snapshot. In other words, reverting the last committed state does not invalidate this value.

Reviewers: Colin Patrick McCabe <cmccabe@apache.org>","['metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/metalog/LocalLogManager.java']","Setting the `committedBytesSinceLastSnapshot` to 0 when the controller resigns potentially prevents the controller from generating a snapshot after `snapshotMaxNewRecordBytes` committed bytes have been replayed. Consequently, the mechanism to count the amount of committed bytes replayed but not included in the latest snapshot can lose its integrity, potentially causing issues."
6625214c52961fc40b7992a031d60c6805db4cec,1675273323,"KAFKA-14658: Do not open broker ports until we are ready to accept traffic (#13169)

When we are listening on fixed ports, we should defer opening ports until we're ready to accept
traffic. If we open the broker port too early, it can confuse monitoring and deployment systems.
This is a particular concern when in KRaft mode, since in that mode, we create the SocketServer
object earlier in the startup process than when in ZK mode.

The approach taken in this PR is to defer opening the acceptor port until Acceptor.start is called.
Note that when we are listening on a random port, we continue to open the port ""early,"" in the
SocketServer constructor. The reason for doing this is that there is no other way to find the
random port number the kernel has selected. Since random port assignment is not used in production
deployments, this should be reasonable.

FutureUtils.java: add chainFuture and tests.

SocketServerTest.scala: add timeouts to cases where we call get() on futures.

Reviewers: David Arthur <mumrah@gmail.com>, Alexandre Dupriez <hangleton@users.noreply.github.com>","['core/src/main/scala/kafka/network/SocketServer.scala', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/ControllerServer.scala', 'core/src/test/scala/unit/kafka/network/SocketServerTest.scala', 'core/src/test/scala/unit/kafka/server/ServerStartupTest.scala', 'server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java', 'server-common/src/test/java/org/apache/kafka/server/util/FutureUtilsTest.java']","When the system is listening on fixed ports, opening these broker ports too early can result in confusion for monitoring and deployment systems. This issue particularly arises in KRaft mode, where the SocketServer object creation happens earlier in the startup process in comparison to ZK mode. It's also problematic when listening on a random port, since the port needs to be opened ""early"" in the SocketServer constructor, in order to find out the random port number selected by the kernel."
8b1eca1c58aef822082dd9954bc41179ecf3eb06,1627082561,"KAFKA-13126: guard against overflow when computing `joinGroupTimeoutMs` (#11111)

Setting the max.poll.interval.ms to MAX_VALUE causes overflow when computing the joinGroupTimeoutMs and results in the JoinGroup timeout being set to the request.timeout.ms instead, which is much lower.

This can easily make consumers drop out of the group, since they must rejoin now within 30s (by default) yet have no obligation to almost ever call poll() given the high max.poll.interval.ms, especially when each record takes a long time to process or the `max.poll.records` is also very large. We just need to check for overflow and fix it to Integer.MAX_VALUE when it occurs.

Reviewers: Luke Chen <showuon@gmail.com>, John Roesler <vvcephei@apache.org>","['clients/src/main/java/org/apache/kafka/clients/NetworkClientUtils.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java', 'clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosLogin.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSinkTask.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSourceTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java']","Setting the max.poll.interval.ms to MAX_VALUE can cause an overflow when calculating the joinGroupTimeoutMs. This consequently reduces the JoinGroup timeout to the request.timeout.ms value, which is significantly lower. This discrepancy can cause consumers to unexpectedly drop out of the group, especially in scenarios where record processing time is high or the `max.poll.records` value is notably large."
31a5f92b9f657d3b0f65ad2abfc2a085958621c9,1567543637,"Changed for updatedTasks, avoids stopping and starting of unnecessary tasks (#7097)

Corrected the `KafkaConfigBackingStore` logic to notify of only the changed tasks, rather than all tasks. This was not noticed before because Connect always stopped and restarted all tasks during a rebalanced, but since 2.3 the incremental rebalance logic exposed this bug.

Author: Luying Liu <lyliu@lyliu-mac.freewheelmedia.net>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",['connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java'],"The `KafkaConfigBackingStore` logic is currently notifying of all tasks rather than just the ones that have changed. This inefficiency was not noticeable in previous versions because Connect always stopped and restarted all tasks during a rebalance. However, this issue has become apparent with the introduction of incremental rebalance logic in version 2.3."
d2521855b8eb8a0dbb6f94cd9bb5093276fb7db2,1599594216,"KAFKA-10432; LeaderEpochCache is incorrectly recovered for leader epoch 0 (#9219)

The leader epoch cache is incorrectly recovered for epoch 0 as the
assignment is skipped when epoch == 0. This check was likely intended to
prevent negative epochs from being applied or there was an assumption
that epochs started at 1.

A test has been added to LogSegmentTest to show the LogSegment
recovery path works for the epoch cache. This was a test gap as none of the 
recover calls supply a leader epoch cache to recover.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/log/LogSegment.scala', 'core/src/test/scala/unit/kafka/log/LogSegmentTest.scala']","The LeaderEpochCache recovery process is facing issues for leader epoch 0. This seems to happen because the assignment gets bypassed when epoch == 0, possibly due to precautions to avoid negative epochs or an initial assumption that epochs start at 1. There's also a gap in testing in LogSegmentTest, as none of the recovery calls supply a leader epoch cache to recover."
ba1e16f0c02aeeaba54600c1a828f6cd13acf1a4,1610742523,"MINOR: Upstream ApisUtils from kip-500 (#9715)

In the [KIP-500 development branch](https://github.com/confluentinc/kafka/tree/kip-500),
we have a separate ControllerApis that shares a lot of functionality with KafkaApis. We
introduced a utility class ApisUtils to pull out the common code. Some things were moved
to RequestChannel as well.

We'd like to upstream this work now so we don't continue to diverge (since KafkaApis is
a frequently modified class). There should be no logical changes in this PR, only shuffling
code around.

Reviewers: Jason Gustafson <jason@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Jose Sancio <jsancio@users.noreply.github.com>, Ismael Juma <ismael@juma.me.uk>
","['core/src/main/scala/kafka/server/AuthHelper.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/main/scala/kafka/server/RequestHandlerHelper.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala']","The current ControllerApis shares a lot of functionality with KafkaApis leading to duplicated code. Furthermore, KafkaApis is frequently modified, increasing the risk of these two similar functionalities to diverge. As a result, maintaining and updating these functionalities could become complicated over a period of time."
a3305c4b8b29c9fb4eede11aea4dd6fd60e445dc,1612543860,"MINOR: Remove ZK dependency for coordinator topics' partition counts (#10008)

The group coordinator and the transaction state manager query ZooKeeper 
to retrieve the partition count for the topics they manager. Since ZooKeeper
won't be available when the broker is using a Raft-based metadata quorum,
this PR changes the startup function to provide an accessor function instead.
This will allow the ZK-based broker to continue using ZK, while the kip-500
broker will query the metadata provided by the metadata log.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <david.arthur@confluent.io>

Co-authored-by: Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala', 'core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala']",The group coordinator and the transaction state manager's reliance on querying ZooKeeper to retrieve the partition count for the topics they manage poses a problem when ZooKeeper is unavailable. This is particularly problematic when the broker is using a Raft-based metadata quorum.
5359b2e3bc1cf13a301f32490a6630802afc4974,1582575153,"MINOR: Improve AuthorizerIntegrationTest (#7926)

This patch improves the authorizer integration tests in the following ways:

1. We use a separate principal for inter-broker communications. This ensures that ACLs set in the test cases do not interfere with inter-broker communication. We had two test cases (`testCreateTopicAuthorizationWithClusterCreate` and `testAuthorizationWithTopicExisting`) which depend on topic creation and were timing out because of inter-broker metadata propagation failures. The timeouts were treated as successfully satisfying the expectation of authorization. So the tests passed, but not because of the intended reason.
2. Previously `GroupAuthorizerIntegrationTest` was inheriting _all_ of the tests from `AuthorizerIntegrationTest`. This seemed like overkill since the ACL evaluation logic is essentially the same. 

Totally this should take about 5-10 minutes off the total build time and make the authorizer integration tests a little more resilient to problems with inter-broker communication.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>","['core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/DelegationTokenEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/GroupAuthorizerIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/GroupEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/PlaintextEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/PlaintextProducerSendTest.scala', 'core/src/test/scala/integration/kafka/api/SaslGssapiSslEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/SaslOAuthBearerSslEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/SaslPlainSslEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/SaslScramSslEndToEndAuthorizationTest.scala', 'core/src/test/scala/integration/kafka/api/SslEndToEndAuthorizationTest.scala', 'core/src/test/scala/unit/kafka/integration/KafkaServerTestHarness.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","The authorizer integration tests have a couple of issues. Firstly, the ACLs set in the test cases are interfering with inter-broker communication. This problem caused two test cases, `testCreateTopicAuthorizationWithClusterCreate` and `testAuthorizationWithTopicExisting`, to time out due to inter-broker metadata propagation failures. However, the timeouts were misinterpreted as successful authorization expectations causing the tests pass for incorrect reasons. Secondly, `GroupAuthorizerIntegrationTest` inherits all of the tests from `AuthorizerIntegrationTest` which could be redundant considering the ACL evaluation logic is essentially the same. These issues might contribute to an extended total build time and reduce the resilience of the authorizer integration tests to problems with inter-broker communication."
23cade850678eeae1bc1227a2a2a4bc09b02f2fc,1599233671,"KAFKA-10314: KafkaStorageException on reassignment when offline log directories exist (#9122)

Make sure that we set the isNew field in LeaderAndIsrRequest correctly for brokers
that gets added to the replica set on reassignment.

This is tested by creating a variant of ControllerIntergationTest.testPartitionReassignment()
that makes one of the log directories on the target broker offline before initiating the
reassignment. Without the change to the way isNew is set, this fails after a timeout. With
the change, it succeeds.

To facilitate calling causeLogDirFailure() both from ControllerIntegrationTest and
LogDirFailureTest, the method was moved to TestUtils along with the other helper
methods that deals with interacting with KafkaServer instances for test cases.

Reviewers: Mickael Maison <mickael.maison@gmail.com>","['core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/LogDirFailureTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","During reassignment, when offline log directories exist, there is an issue with setting the isNew field in the LeaderAndIsrRequest correctly for brokers that get added to the replica set. The problem manifests as a KafkaStorageException and occurs specifically when one of the log directories on the target broker is made offline before initiating the reassignment. There needs to be a reliable methodology to handle this scenario without causing a timeout failure."
56ef910358e2bde42aba4aaafd7f5c899055ad92,1626824480,"KAFKA-13104: Controller should notify raft client when it resigns #11082

When the active controller encounters an event exception it attempts to renounce leadership.
Unfortunately, this doesn't tell the RaftClient that it should attempt to give up leadership. This
will result in inconsistent state with the RaftClient as leader but with the controller as
inactive.  This PR changes the implementation so that the active controller asks the RaftClient
to resign.

Reviewers: Jose Sancio <jsancio@gmail.com>, Colin P. McCabe <cmccabe@apache.org>","['metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'raft/src/main/java/org/apache/kafka/raft/RaftClient.java']","When the active controller encounters an event exception and tries to renounce leadership, it doesn't relay this information to the RaftClient to give up its leadership. This could lead to inconsistent states, with the RaftClient remaining as a leader but the controller being inactive."
82dff1db5486647b27e5297e4c839bd2a905c1d9,1586212229,"KAFKA-9753: A few more metrics to add (#8371)

Instance-level:
* number of alive stream threads

Thread-level:
* avg / max number of records polled from the consumer per runOnce, INFO
* avg / max number of records processed by the task manager (i.e. across all tasks) per runOnce, INFO

Task-level:
* number of current buffered records at the moment (i.e. it is just a dynamic gauge), DEBUG.

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <john@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/internals/metrics/ClientMetrics.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/internals/metrics/ClientMetricsTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/metrics/TaskMetricsTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetricsTest.java']","'There is currently a lack of certain metrics at Instance, Thread, and Task levels within Kafka. Specially, the system is missing metrics for the number of alive stream threads on the Instance level, the average/maximum number of records polled from the consumer per runOnce, and the average/maximum number of records processed by the task manager per runOnce at the Thread level. Additionally, Task-level lacks a current gauge of the number of buffered records at any given moment.'"
2df7ea5a4a43920ae1ac8a259f6f69846011eda7,1582481792,"KAFKA-9530; Fix flaky test `testDescribeGroupWithShortInitializationTimeout` (#8154)

With a short timeout, a call in KafkaAdminClient may timeout and the client might disconnect. Currently this can be exposed to the user as either a TimeoutException or a DisconnectException. To be consistent, rather than exposing the underlying retriable error, we handle both cases with a TimeoutException.

Reviewers: Boyang Chen <boyang@confluent.io>, Ismael Juma <ismael@juma.me.uk>",['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java'],"In KafkaAdminClient, using a short timeout may cause inconsistency with the type of exception exposed to the user. When a call times out and the client disconnects, this is sometimes shown as a TimeoutException and other times as a DisconnectException. This inconsistency can lead to confusion and ambiguity about the underlying error."
a126e3a622f2b7142f3543b9dbee54b6412ba9d8,1655310015,"KAFKA-13888; Addition of Information in DescribeQuorumResponse about Voter Lag (#12206)

This commit adds an Admin API handler for DescribeQuorum Request and also
adds in two new fields LastFetchTimestamp and LastCaughtUpTimestamp to
the DescribeQuorumResponse as described by KIP-836.

This commit does not implement the newly added fields. Those will be
added in a subsequent commit.

Reviewers: dengziming <dengziming1993@gmail.com>, David Jacot <djacot@confluent.io>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/admin/Admin.java', 'clients/src/main/java/org/apache/kafka/clients/admin/DescribeMetadataQuorumOptions.java', 'clients/src/main/java/org/apache/kafka/clients/admin/DescribeMetadataQuorumResult.java', 'clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/main/java/org/apache/kafka/clients/admin/QuorumInfo.java', 'clients/src/main/java/org/apache/kafka/common/internals/Topic.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java', 'clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java', 'core/src/main/scala/kafka/server/KafkaRaftServer.scala', 'core/src/test/scala/unit/kafka/server/DescribeQuorumRequestTest.scala']","The DescribeQuorumResponse lacks crucial information regarding the LastFetchTimestamp and LastCaughtUpTimestamp. Without these fields, it is difficult to monitor and keep track of fetch status and synchronization details which is critical for maintaining system health and performance."
7f640f13b4d486477035c3edb28466734f053beb,1585889317,"KAFKA-9776: Downgrade TxnCommit API v3 when broker doesn't support (#8375)

Revert the decision for the sendOffsetsToTransaction(groupMetadata) API to fail with old version of brokers for the sake of making the application easier to adapt between versions. This PR silently downgrade the TxnOffsetCommit API when the build version is small than 3.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/TxnOffsetCommitRequestTest.java', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala', 'streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java', 'tests/kafkatest/tests/streams/streams_broker_compatibility_test.py']","'The sendOffsetsToTransaction(groupMetadata) API may fail with older versions of brokers, creating difficulties for application adaptation between different versions. This problem is particularly noticeable when the build version is less than 3, leading to compatibility issues with the TxnOffsetCommit API.'"
c462a657ecbc4761e247c28159fa4055546f1be2,1651151603,"KAFKA-13794: Fix comparator of inflightBatchesBySequence in TransactionsManager (round 3) (#12096)

Conceptually, the ordering is defined by the producer id, producer epoch
and the sequence number. This set should generally only have entries
for the same producer id and epoch, but there is one case where
we can have conflicting `remove` calls and hence we add this as
a temporary safe fix.

We'll follow-up with a fix that ensures the original intended invariant.

Reviewers: Jason Gustafson <jason@confluent.io>, David Jacot <djacot@confluent.io>, Luke Chen <showuon@gmail.com>",['clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java'],"In the TransactionsManager, the ordering set defined by the producer id, producer epoch, and the sequence number can have conflicting entries. This set would generally only have entries for the same producer id and epoch, however, in some cases conflicting 'remove' calls can disrupt this. This issue requires a temporary safety fix."
47a9871ef65a13f9d58d5ea216de340f7e123da5,1555372408,"KAFKA-7471: Multiple Consumer Group Management Feature (#5726)

* Describe/Delete/Reset offsets on multiple consumer groups at a time (including each group by repeating `--group` parameter)
* Describe/Delete/Reset offsets on ALL consumer groups at a time (add new `--all-groups` option similar to `--all-topics`)
* Reset plan CSV file generation reworked: structure updated to support multiple consumer groups and make sure that CSV file generation is done properly since there are no restrictions on consumer group names and symbols like commas and quotes are allowed.
* Extending data output table format by adding `GROUP` column for all `--describe` queries","['core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala', 'core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/admin/DeleteConsumerGroupsTest.scala', 'core/src/test/scala/unit/kafka/admin/DescribeConsumerGroupTest.scala', 'core/src/test/scala/unit/kafka/admin/ResetConsumerGroupOffsetTest.scala']","Managing offsets on consumer groups is limited, as users can only manage one group at a time. Moreover, there are no available options to manage offsets for all consumer groups at once. This makes operation on consumer groups less efficient. On top of that, the structure of the reset plan CSV file does not support multiple consumer groups. Consequently, handling consumer groups with commas and quotes in their names might lead to improper CSV file generation. Also, the data output table for describe queries lacks a `GROUP` column, offering incomplete information about consumer groups."
e3ccf2079463d0be6141d3a86c0499ceba8a3afa,1584113009,"KAFKA-9685: Solve Set concatenation perf issue in AclAuthorizer

To dismiss the usage of operation ++ against Set which is slow when Set has many entries. This pr introduces a new class 'AclSets' which takes multiple Sets as parameters and do 'find' against them one by one. For more details about perf and benchmark, refer to [KAFKA-9685](https://issues.apache.org/jira/browse/KAFKA-9685)

Author: jiao <jiao.zhang@linecorp.com>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #8261 from jiao-zhangS/jira-9685
","['core/src/main/scala/kafka/security/authorizer/AclAuthorizer.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/acl/AclAuthorizerBenchmark.java']","The current implementation of Set concatenation in the AclAuthorizer exhibits performance issues. Using the operation ++ against Set, particularly when the Set has numerous entries, leads to a slow response. The performance challenge also extends to the 'find' operation."
aa399a335f75122d258c5cef1ffc3604b42f4052,1695906277,"KAFKA-15499: Fix the flaky DeleteSegmentsDueToLogStartOffsetBreach test (#14439)

DeleteSegmentsDueToLogStartOffsetBreach configures the segment such that it can hold at-most 2 record-batches. And, it asserts that the local-log-start-offset based on the assumption that each segment will contain exactly two messages.

During leader switch, the segment can get rotated and may not always contain two records. Previously, we were checking whether the expected local-log-start-offset is equal to the base-offset-of-the-first-local-log-segment. With this patch, we will scan the first local-log-segment for the expected offset.

Reviewers: Divij Vaidya <diviv@amazon.com>","['storage/src/test/java/org/apache/kafka/server/log/remote/storage/LocalTieredStorageTest.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/utils/BrokerLocalStorage.java']","The test DeleteSegmentsDueToLogStartOffsetBreach appears to be flaky due to how it configures and asserts segments. The test assumes that each segment will contain exactly two messages. However, during a leader switch, the segment can rotate and may not always contain the two expected records. Additionally, the check for expected local-log-start-offset might not match with the base-offset-of-the-first-local-log-segment. This discrepancy is causing the test to fail intermittently."
aaa976a3409f61e1e65ed49742567c08a502d9e1,1678402360,"MINOR: Some metadata publishing fixes and refactors (#13337)

This PR refactors MetadataPublisher's interface a bit. There is now an onControllerChange
callback. This is something that some publishers might want. A good example is ZkMigrationClient.
Instead of two different publish functions (one for snapshots, one for log deltas), we now have a single onMetadataUpdate function. Most publishers didn't want to do anything different in those two cases.
The ones that do want to do something different for snapshots can always check the manifest type.
The close function now has a default empty implementation, since most publishers didn't need to do
anything there.

Move the SCRAM logic out of BrokerMetadataPublisher and run it on the controller as well.

On the broker, simply use dynamicClientQuotaPublisher to handle dynamic client quotas changes.
That is what the controller already does, and the code is exactly the same in both cases.

Fix the logging in FutureUtils.waitWithLogging a bit. Previously, when invoked from BrokerServer
or ControllerServer, it did not include the standard ""[Controller 123] "" style prefix indicating server
name and ID. This was confusing, especially when debugging junit tests.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, David Arthur <mumrah@gmail.com>","['core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/ControllerServer.scala', 'core/src/main/scala/kafka/server/KafkaRaftServer.scala', 'core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala', 'core/src/main/scala/kafka/server/metadata/DynamicClientQuotaPublisher.scala', 'core/src/main/scala/kafka/server/metadata/DynamicConfigPublisher.scala', 'core/src/main/scala/kafka/server/metadata/ScramPublisher.scala', 'core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala', 'metadata/src/main/java/org/apache/kafka/image/loader/LoaderManifest.java', 'metadata/src/main/java/org/apache/kafka/image/loader/LoaderManifestType.java', 'metadata/src/main/java/org/apache/kafka/image/loader/LogDeltaManifest.java', 'metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java', 'metadata/src/main/java/org/apache/kafka/image/loader/SnapshotManifest.java', 'metadata/src/main/java/org/apache/kafka/image/publisher/MetadataPublisher.java', 'metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java', 'metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java', 'metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java', 'server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java', 'server-common/src/test/java/org/apache/kafka/server/util/FutureUtilsTest.java']","The previous structure of the MetadataPublisher's interface led to unnecessary complications. Publishers were forced to use two separate publish functions for snapshots and log deltas, although most use cases didn't require different operations for the two. Additionally, the absence of an onControllerChange callback led to inefficiencies for some publishers. Moreover, the logging in FutureUtils.waitWithLogging did not include the standard prefix indicating server name and ID, which led to confusion during debugging. Also, the SCRAM logic had to be moved out of BrokerMetadataPublisher and run on the controller. Finally, the absence of a default empty implementation for the close function forces most publishers to unnecessarily define this function."
a024e679c7207aee2242d72aef53dc441c60ca27,1578529520,"MINOR: Update dependencies for Kafka 2.5 (#7909)

Noteworthy:
* zstd decompression speed improvement of ~10%:
https://github.com/facebook/zstd/releases/tag/v1.4.4
* EasyMock, PowerMock and Mockito: improved support for Java 13.
* Replace usage of method deprecated by Mockito.
* Gradle plugins updated to versions that require Gradle 5.x, this is
fine since we no longer depend on the installed Gradle version.
* Fixed build not to depend on methods deprecated in Gradle 5.x
(fixes KAFKA-8786).
* Reflections 0.9.12 no longer depends on Guava (fixes KAFKA-3061).
* Updated `OptimizedKTableIntegrationTest` to pass with new version
of Hamcrest.
* Several Jetty improvements and bug fixes:
   - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.21.v20190926
   - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.22.v20191022
   - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.23.v20191118
   - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.24.v20191120
   - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.25.v20191220

Note that I did not upgrade lz4 due to https://github.com/lz4/lz4-java/issues/156.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Co-authored-by: Ismael Juma <ismael@juma.me.uk>
Co-authored-by: Aljoscha <aljoscha.poertner@posteo.de>","['clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginModuleTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java']","The existing dependencies in the Kafka 2.5 project require attention as some of them do not function properly with Java 13. Additionally, Gradle plugins are outdated and dependent on an older installed Gradle version. The build also relies on methods deprecated in Gradle 5.x which could cause potential issues. Moreover, the Reflections 0.9.12 depends on Guava, causing further problems. The `OptimizedKTableIntegrationTest` is not compatible with the new version of Hamcrest while the lz4 hasn't been upgraded due to existing issues. Lastly, the project employs several Jetty versions with known problems and fixes."
d3130f2e911473fcf1265dec14016c824499d9a7,1657616299,"KAFKA-14062: OAuth client token refresh fails with SASL extensions (#12398)

- Different objects should be considered unique even with same content to support logout
- Added comments for SaslExtension re: removal of equals and hashCode
- Also swapped out the use of mocks in exchange for *real* SaslExtensions so that we exercise the use of default equals() and hashCode() methods.
- Updates to implement equals and hashCode and add tests in SaslExtensionsTest to confirm

Co-authored-by: Purshotam Chauhan <pchauhan@confluent.io>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>","['clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java', 'clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java', 'clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java', 'clients/src/test/java/org/apache/kafka/common/security/SaslExtensionsTest.java', 'clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginModuleTest.java']","'Issues arise with the OAuth client token refresh in relation to SASL extensions. Despite possessing identical content, different objects should be treated as distinct to effectively support logout functionality. Current equals and hashCode implementation in SaslExtensions might be causing this issue.'"
0586f544ef5a3e2ffa263babc811568ce672dfa1,1648225802,"KAFKA-10405: Set purge interval explicitly in PurgeRepartitionTopicIntegrationTest (#11948)

In KIP-811, we added a new config repartition.purge.interval.ms to set repartition purge interval. In this flaky test, we expected the purge interval is the same as commit interval, which is not correct anymore (default is 30 sec). Set the purge interval explicitly to fix this issue.

Reviewers: Bruno Cadonna <cadonna@apache.org>, Guozhang Wang <wangguoz@gmail.com>",['streams/src/test/java/org/apache/kafka/streams/integration/PurgeRepartitionTopicIntegrationTest.java'],"The issue arises from incorrectly expecting the purge interval to be the same as the commit interval in the PurgeRepartitionTopicIntegrationTest. This is causing instability in the test as the default purge interval has been set to 30 seconds, which could lead to unexpected results."
cd1e46c8bb46f1e5303c51f476c74e33b522fce8,1586265978,"MINOR: Pass one action per unique resource name in KafkaApis.filterAuthorized (#8432)

90bbeedf52f introduced a regression resulting in passing an action per resource
name to the `Authorizer` instead of passing one per unique resource name. Refactor
the signatures of both `filterAuthorized` and `authorize` to make them easier to test
and add a test for each.

Reviewers: Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala']","'There is a regression that causes an action to be passed per resource name to the 'Authorizer', instead of passing one per unique resource name. This issue impacts the 'filterAuthorized' and 'authorize' functions, making them more difficult to test thoroughly.'"
c249ea8e5d9dad5e74b804a6f062b059517d6a8d,1584743173,"KAFKA-9727: cleanup the state store for standby task dirty close and check null for changelogs (#8307)

This PR fixes three things:

* the state should be closed when standby task is restoring as well
* the EOS standby task should also wipe out state under dirty close
* the changelog reader should check for null as well

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java']","'State store issues arise when the standby task is restoring, and the change log reader fails in situations it encounters null values. Additionally, the EOS standby task does not clean up state under dirty close, leading to unnecessary build-up and potential errors.'"
ff48edbed46f23600ab434deee57e3ef2da8ed28,1604620513,"KAFKA-10624: For FeatureZNodeStatus, use sealed trait instead of Enumeration (#9561)

This is a follow-up to initial KIP-584 development. In this PR, I've switched the FeatureZNodeStatus enum to be a sealed trait. In Scala, we prefer sealed traits over Enumeration since the former gives you exhaustiveness checking. With Scala enumeration, you don't get a warning if you add a new value that is not handled in a given pattern match.

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/zk/ZkData.scala', 'core/src/test/scala/kafka/zk/FeatureZNodeTest.scala']","In the initial development of KIP-584, the use of Scala enumeration for FeatureZNodeStatus can lead to issues. Lack of exhaustiveness checking in Scala enumeration means that no warnings will be given if a new value is added that is not addressed in a given pattern match."
0a1c26934757afae4dce49ff3ee038311ca6dd4a,1550039800,"KAFKA-7652: Part III; Put to underlying before Flush (#6191)

1. In the caching layer's flush listener call, we should always write to the underlying store, before flushing (see #4331 's point 4) for detailed explanation). When fixing 4331, it only touches on KV stores, but it turns out that we should fix for window and session store as well.

2. Also apply the optimization that was in session-store already: when the new value bytes and old value bytes are all null (this is possible e.g. if there is a put(K, V) followed by a remove(K) or put(K, null) and these two operations only hit the cache), upon flushing this mean the underlying store does not have this value at all and also no intermediate value has been sent to downstream as well. We can skip both putting a null to the underlying store as well as calling the flush listener sending `null -> null` in this case.

Modifies corresponding unit tests.

Reviewers: John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java', 'streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableFilterTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractKeyValueStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/CachingKeyValueStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/CachingWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java', 'streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java']","Currently, a problem exists in the caching layer's flush listener call. The issue may arise when we do not write to the underlying store before flushing. While this was previously fixed for the KV stores, the solution does not seem to cover window and session stores.

There's also a situation where the new value bytes and old value bytes are all null (which could happen for example if there is a put(K, V) followed by a remove(K) or put(K, null) and these two operations only get processed in the cache). Upon flushing, this means that the underlying store does not have this value at all, and also no intermediate value has been sent downstream either. The existing system does not cater to this scenario, and a proper handling mechanism for this need to be implemented."
bda5c34b030207f542c7987a5e0f9bcb23406c18,1647373853,"MINOR: refactor how ConfigurationControl checks for resource existence (#11835)

ConfigurationControl methods should take a boolean indicating whether the resource is newly
created, rather than taking an existence checker object. The boolean is easier to understand. Also
add a unit test of existing checking failing (and succeeding).

Reviewers: Kirk True <kirk@mustardgrain.com>, José Armando García Sancio <jsancio@users.noreply.github.com>","['metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/ConfigurationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java']","The current approach ConfigurationControl methods use to check for resource existence might be difficult to understand. The methods take an existence checker object instead of a boolean to indicate whether the resource is newly created. Additionally, a unit test for checking the existence failure (and success) appears to be missing."
44b3177a087ff809a9d95a27b63b10e00aa4da7d,1671562514,"KAFKA-14457; Controller metrics should only expose committed data (#12994)

The controller metrics in the controllers has three problems. 1) the active controller exposes uncommitted data in the metrics. 2) the active controller doesn't update the metrics when the uncommitted data gets aborted. 3) the controller doesn't update the metrics when the entire state gets reset.

We fix these issues by only updating the metrics when processing committed metadata records and reset the metrics when the metadata state is reset.

This change adds a new type `ControllerMetricsManager` which processes committed metadata records and updates the metrics accordingly. This change also removes metrics updating responsibilities from the rest of the controller managers. 

Reviewers: Ron Dagostino <rdagostino@confluent.io>","['metadata/src/main/java/org/apache/kafka/controller/BrokersToIsrs.java', 'metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ControllerMetrics.java', 'metadata/src/main/java/org/apache/kafka/controller/ControllerMetricsManager.java', 'metadata/src/main/java/org/apache/kafka/controller/PartitionChangeBuilder.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumControllerMetrics.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ControllerMetricsManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/MockControllerMetrics.java', 'metadata/src/test/java/org/apache/kafka/controller/ProducerIdControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java', 'server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java']","The controller metrics within the controllers currently have several identified issues. Firstly, the active controller is exposing uncommitted data in the metrics. Secondly, this active controller is not updating the metrics when this uncommitted data gets aborted. Lastly, there lacks an update to the metrics when the entire state is reset. This could lead to inaccurate or misleading metrics being displayed."
e2847e8603fe19a87ff03584fb38954e4bd3a59e,1558158345,"KAFKA-8365; Consumer and protocol support for follower fetching (#6731)

This patch includes API changes for follower fetching per [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) as well as the consumer implementation. After this patch, consumers will continue to fetch only from the leader, since the broker implementation to select an alternate read replica is not included here.

Adds new `client.rack` consumer configuration property is added which allows the consumer to indicate its rack. This is just an arbitrary string to indicate some relative location, it doesn't have to actually represent a physical rack. We are keeping the naming consistent with the broker property (`broker.rack`).

FetchRequest now includes `rack_id` which can optionally be specified by the consumer. FetchResponse includes an optional `preferred_read_replica` field for each partition in the response. OffsetForLeaderEpochRequest also adds new `replica_id` field which is similar to the same field in FetchRequest.

When the consumer sees a `preferred_read_replica` in a fetch response, it will use the Node with that ID for the next fetch.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/CommonClientConfigs.java', 'clients/src/main/java/org/apache/kafka/clients/Metadata.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java', 'clients/src/main/java/org/apache/kafka/common/Cluster.java', 'clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java', 'clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java', 'clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java', 'clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java', 'clients/src/test/java/org/apache/kafka/clients/MetadataTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'core/src/main/scala/kafka/api/ApiVersion.scala', 'core/src/main/scala/kafka/server/ReplicaFetcherThread.scala', 'core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala']","The existing Kafka consumer and protocol architecture only allows consumers to fetch from the leader, which might not always be the optimal choice in terms of network efficiency and server load balancing. There is no way to indicate a preferred replica to fetch from, and the consumer is not able to specify its rack, limiting the potential for location-aware fetching configurations and causing inefficiency in fetching operations."
1317f3f77a9e1e432e7a81de2dcb88365feeac43,1648682641,"MINOR: log warning when topology override for cache size is non-zero (#11959)

Since the topology-level cache size config only controls whether we disable the caching layer entirely for that topology, setting it to anything other than 0 has no effect. The actual cache memory is still just split evenly between the threads, and shared by all topologies.

It's possible we'll want to change this in the future, but for now we should make sure to log a warning so that users who do try to set this override to some nonzero value are made aware that it doesn't work like this.

Also includes some minor refactoring plus a fix for an off-by-one error in #11796

Reviewers: Luke Chen <showuon@gmail.com>, Walker Carlson <wcarlson@confluent.io>, Sagar Rao <sagarmeansocean@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/TopologyConfig.java', 'streams/src/main/java/org/apache/kafka/streams/internals/StreamsConfigUtils.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java']","Setting the topology-level cache size config to any non-zero value is not having the expected effect of controlling cache size. Instead, the cache memory is still being evenly distributed among the threads and shared by all topologies. Therefore, changing this setting to non-zero is not serving any purpose and may be leading to confusion or misunderstandings for users. There's also an off-by-one error that needs addressing."
caaa4c55fee68c5893d54ffe84287f3b5205fff1,1693870996,"KAFKA-15410: Expand partitions, segment deletion by retention and enable remote log on topic integration tests (1/4) (#14307)

Added the below integration tests with tiered storage
 - PartitionsExpandTest
 - DeleteSegmentsByRetentionSizeTest
 - DeleteSegmentsByRetentionTimeTest and
 - EnableRemoteLogOnTopicTest
 - Enabled the test for both ZK and Kraft modes.

These are enabled for both ZK and Kraft modes.

Reviewers: Satish Duggana <satishd@apache.org>, Luke Chen <showuon@gmail.com>, Christo Lolov <lolovc@amazon.com>, Divij Vaidya <diviv@amazon.com>","['core/src/main/java/kafka/log/remote/RemoteLogManager.java', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'storage/src/test/java/org/apache/kafka/tiered/storage/TieredStorageTestContext.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/TieredStorageTestHarness.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/actions/ExpectEmptyRemoteStorageAction.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/integration/BaseDeleteSegmentsTest.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/integration/DeleteSegmentsByRetentionSizeTest.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/integration/DeleteSegmentsByRetentionTimeTest.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/integration/EnableRemoteLogOnTopicTest.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/integration/PartitionsExpandTest.java', 'storage/src/test/java/org/apache/kafka/tiered/storage/utils/BrokerLocalStorage.java']","'The project is currently missing integration tests for tiered storage capabilities. These should include tests for expanding partitions, deleting segments by retention size or retention time, and enabling remote log on topics. This is also needed to ensure functionality in both ZK and Kraft modes.'"
9f5a69a4c2d6ac812ab6134e64839602a0840b87,1554923447,"[MINOR] Guard against crashing on invalid key range queries (#6521)

Due to KAFKA-8159, Streams will throw an unchecked exception when a caching layer or in-memory underlying store is queried over a range of keys from negative to positive. We should add a check for this and log it then return an empty iterator (as the RocksDB stores happen to do) rather than crash

Reviewers: Bruno Cadonna <bruno@confluent.io> Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/MemoryNavigableLRUCache.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractKeyValueStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/CachingWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java']","When a caching layer or in-memory underlying store in Kafka Streams is queried over a range of keys from negative to positive, an unchecked exception is thrown. This exception crashing the code indicates a possible lack of error handling in case of an invalid key range query."
ee8e7578789e02e369ce933341501a383b62d5dd,1674516275,"temporarily disable the 'false' parameter (#13147)

Need to get a clean build for 3.4 and this test has been extremely flaky. I'm looking into the failure as well, and want to pinpoint whether it's the true build that's broken or it's the parameterization itself causing this -- thus, let's start by temporarily disabling the false parameter first.

See KAFKA-14533 for more details

Reviewers: Lucas Brutschy <lbrutschy@confluent.io>",['streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java'],"'The 'false' parameter in our test appears to be causing build issues in version 3.4, making it extremely flaky. This leads to uncertainty whether the build itself is problematic, or the parameterization is the source of the problem.'"
131d4753cfed65ed6dee0a8c754765c97c3d513f,1612438963,"KAFKA-12193: Re-resolve IPs after a client disconnects (#9902)

This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Satish Duggana <satishd@apache.org>, David Jacot <djacot@confluent.io>
","['clients/src/main/java/org/apache/kafka/clients/ClientUtils.java', 'clients/src/main/java/org/apache/kafka/clients/ClusterConnectionStates.java', 'clients/src/main/java/org/apache/kafka/clients/DefaultHostResolver.java', 'clients/src/main/java/org/apache/kafka/clients/HostResolver.java', 'clients/src/main/java/org/apache/kafka/clients/NetworkClient.java', 'clients/src/test/java/org/apache/kafka/clients/AddressChangeHostResolver.java', 'clients/src/test/java/org/apache/kafka/clients/ClientUtilsTest.java', 'clients/src/test/java/org/apache/kafka/clients/ClusterConnectionStatesTest.java', 'clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java', 'clients/src/test/java/org/apache/kafka/test/MockSelector.java']","The NetworkClient does not re-resolve the target node's hostname after disconnecting from an established connection. This can result in a scenario where the node's IP address changes during the lifetime of the connection, leaving the client trying to connect to invalidated IP addresses until it has tried each one."
62dcfa196e7819f7fb77aa181d96767988cbe908,1585939864,"KAFKA-9750; Fix race condition with log dir reassign completion (#8412)

There is a race on receiving a LeaderAndIsr request for a replica with an active log dir reassignment. If the reassignment completes just before the LeaderAndIsr handler updates epoch information, it can lead to an illegal state error since no future log dir exists. This patch fixes the problem by ensuring that the future log dir exists when the fetcher is started. Removal cannot happen concurrently because it requires access the same partition state lock.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Arthur <mumrah@gmail.com>

Co-authored-by: Chia-Ping Tsai <chia7712@gmail.com>","['core/src/main/scala/kafka/server/AbstractFetcherManager.scala', 'core/src/main/scala/kafka/server/AbstractFetcherThread.scala', 'core/src/main/scala/kafka/server/ReplicaAlterLogDirsManager.scala', 'core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherManagerTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala']","There's a race condition occurring when receiving a LeaderAndIsr request for a replica with an active log dir reassignment. If the reassignment finishes right before the LeaderAndIsr handler updates the epoch information, it can result in an illegal state error due to the absence of a future log dir. Additionally, concurrent removal cannot happen, as it requires access to the same partition state lock."
530224e4fe853df734e7bf7c15661fe6b1ab34fe,1623597277,"KAFKA-12940: Enable JDK 16 builds in Jenkins (#10702)

JDK 15 no longer receives updates, so we want to switch from JDK 15 to JDK 16.
However, we have a number of tests that don't yet pass with JDK 16.

Instead of replacing JDK 15 with JDK 16, we have both for now and we either
disable (via annotations) or exclude (via gradle) the tests that don't pass with
JDK 16 yet. The annotations approach is better, but it doesn't work for tests
that rely on the PowerMock JUnit 4 runner.

Also add `--illegal-access=permit` when building with JDK 16 to make MiniKdc
work for now. This has been removed in JDK 17, so we'll have to figure out
another solution when we migrate to that.

Relevant JIRAs for the disabled tests: KAFKA-12790, KAFKA-12941, KAFKA-12942.

Moved some assertions from `testTlsDefaults` to `testUnsupportedTlsVersion`
since the former claims to test the success case while the former tests the failure case.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",['clients/src/test/java/org/apache/kafka/common/network/SslTransportLayerTest.java'],"The project is not currently building constantly on JDK 16 because of a number of tests that fail with this version. Moreover, the project need to ensure it remains up-to-date with latest Java versions, and the previous JDK 15 is no longer receiving updates. Additionally, there are issues with the MiniKdc working with JDK 16 due to changes in handling illegal access. Finally, there seems to be a discrepancy with the assertions for `testTlsDefaults` and `testUnsupportedTlsVersion` where they're not aligned with their respective use cases."
9ee5f920d5e4b837c3240dff948e120aaef7cd23,1551726495,"KAFKA-7312: Change broker port used in testMinimumRequestTimeouts and testForceClose

Port 22 is used by ssh, which causes the AdminClient to throw an OOM:

> java.lang.OutOfMemoryError: Java heap space
> 	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
> 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
> 	at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30)
> 	at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:112)
> 	at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:424)
> 	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:385)
> 	at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:640)
> 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:561)
> 	at org.apache.kafka.common.network.Selector.poll(Selector.java:472)
> 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:535)
> 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1140)
> 	at java.lang.Thread.run(Thread.java:748)
>
>

Author: Manikumar Reddy <manikumar.reddy@gmail.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #6360 from omkreddy/KAFKA-7312
","['core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","Port 22, which is being used in testMinimumRequestTimeouts and testForceClose, is conflicting with ssh usage. This conflict is leading to an OutOfMemoryError within the AdminClient when java tries to allocate a ByteBuffer."
dbbe0fcf9299b9625dadcf21aa183987539b5877,1578597083,"KAFKA-9287; Fix unneeded delay before failing pending transaction commit (#7799)

It is possible for the user to call `commitTransaction` before a pending `AddPartitionsToTxn` call has returned. If the `AddPartitionsToTxn` call returns an abortable error, we need to cancel any pending batches in the accumulator and we need to fail the pending commit. The problem in this case is that `Sender.runOnce`, after failing the batches, enters `NetworkClient.poll` before it has a chance to cancel the commit. Since there are no pending requests at this time, this will block unnecessarily and prevent completion of the doomed commit. This patch fixes the problem by returning from `runOnce` if any batches have been aborted, which allows the commit to also fail without the delay.

Note that this was the cause of the delay executing `AuthorizerIntegrationTest.testTransactionalProducerTopicAuthorizationExceptionInCommit` compared to other tests in this class. After fixing the bug, the delay is gone and the test time is similar to other test cases in the class.

Reviewers:  Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java']","'When a user calls `commitTransaction` before a pending `AddPartitionsToTxn` call has returned, potential issues arise if the `AddPartitionsToTxn` call returns an abortable error. After failing the batches, `Sender.runOnce` enters `NetworkClient.poll` before a chance to cancel the commit arises. If there are no pending requests at this time, unnecessary blocking occurs, delaying the completion of the commit.'"
5234ddff5025501be2b4fca3ecba4e4eb584bbc5,1692026248,"KAFKA-15326: [5/N] Processing thread punctuation (#14001)

Implements punctuation inside processing threads. The scheduler
algorithm checks if a task that is not assigned currently can
be punctuated, and returns it when a worker thread asks for the
next task to be processed. Then, the processing thread runs all
punctuations in the punctionation queue.

Piggy-backed: take TaskExecutionMetadata into account when
processing records.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/PunctuationQueue.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskExecutor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/TaskExecutorCreator.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/PunctuationQueueTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskExecutorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskManagerTest.java']","'The processing threads lack a functionality for punctuation which might be leading to a less efficient scheduling algorithm. This could result in tasks that could have been punctuated not being taken into account when assigning them for processing. In addition, TaskExecutionMetadata might not be sufficiently considered when processing records.'"
