commit_id,commit_date,commit_message,actual_files_modified,transformed_message_gpt4
cca05cace4105c829f303c13eed8ace2efd7fa0c,1559940732,"KAFKA-8331: stream static membership system test (#6877)

As title suggested, we boost 3 stream instances stream job with one minute session timeout, and once the group is stable, doing couple of rolling bounces for the entire cluster. Every rejoin based on restart should have no generation bump on the client side.

Reviewers: Guozhang Wang <wangguoz@gmail.com>,  Bill Bejeck <bbejeck@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java', 'streams/src/test/java/org/apache/kafka/streams/tests/StaticMemberTestClient.java', 'streams/src/test/java/org/apache/kafka/streams/tests/StreamsNamedRepartitionTest.java', 'tests/kafkatest/services/consumer_property.py', 'tests/kafkatest/services/streams.py', 'tests/kafkatest/services/streams_property.py', 'tests/kafkatest/tests/streams/streams_named_repartition_topic_test.py', 'tests/kafkatest/tests/streams/streams_optimized_test.py', 'tests/kafkatest/tests/streams/streams_static_membership_test.py', 'tests/kafkatest/tests/streams/streams_upgrade_test.py', 'tests/kafkatest/tests/streams/utils/__init__.py', 'tests/kafkatest/tests/streams/utils/util.py']",Rolling bounces on a stable group in a stream static membership system test leads to an unexpected generation bump on the client side.
aa4ba8eee8e6f52a9d80a98fb2530b5bcc1b9a11,1566077048,"KAFKA-8041: Enable producer retries in log dir failure test to address flakiness (#7200)

`testProduceAfterLogDirFailureOnLeader` currently disables producer retries in
order to catch and validate the exception thrown by a failure, and then tries to
produce successfully once the leadership changes. This second produce can
intermittently fail, causing test flakiness. This patch splits these validations
into two tests in order to allow retries for the produce request after the
leadership change.

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Ismael Juma <ismael@juma.me.uk>",['core/src/test/scala/unit/kafka/server/LogDirFailureTest.scala'],"The `testProduceAfterLogDirFailureOnLeader` intermittently fails due to disallowed producer retries after a leadership change, causing flaky tests."
07c10024890c0761ba47ccd4d6301e8101d8c8de,1665625198,"KAFKA-12960: Enforcing strict retention time for WindowStore and Sess… (#11211)

WindowedStore and SessionStore do not implement a strict retention time in general. We should consider to make retention time strict: even if we still have some record in the store (due to the segmented implementation), we might want to filter expired records on-read. This might benefit PAPI users.

This PR, adds the filtering behaviour in the Metered store so that, it gets automatically applied for cases when a custom state store is implemented

Reviewer: Luke Chen <showuon@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <mjsax@apache.org>","['streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractDualSchemaRocksDBSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBTimeOrderedSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedSegmentedBytesStore.java', 'streams/src/test/java/org/apache/kafka/streams/integration/TimeWindowedKStreamIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregateTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowedKStreamImplTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImplTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractDualSchemaRocksDBSegmentedBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractWindowBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java']","WindowedStore and SessionStore don't strictly enforce retention time, causing potential issues with expired records for PAPI users, and hindering filtering on-read operations."
e903f2cd9646639cbada753a705b49fb903e1add,1675950790,"KAFKA-7109: Close fetch sessions on close of consumer (#12590)

## Problem
When consumer is closed, fetch sessions associated with the consumer should notify the server about it's intention to close using a Fetch call with epoch = -1 (identified by `FINAL_EPOCH` in `FetchMetadata.java`). However, we are not sending this final fetch request in the current flow which leads to unnecessary fetch sessions on the server which are closed only after timeout.

## Changes
1. Change `close()` in `Fetcher` to add a logic to send the final Fetch request notifying close to the server.
2. Change `close()` in `Consumer` to respect the timeout duration passed to it. Prior to this change, the timeout parameter was being ignored.
3. Change tests to close with `Duration.zero` to reduce the execution time of the tests. Otherwise the tests will wait for default timeout to exit (close() in the tests is expected to be unsuccessful because there is no server to send the request to).
4. Distinguish between the case of ""close existing session and create new session"" and ""close existing session"" by renaming the `nextCloseExisting` function to `nextCloseExistingAttemptNew`.

## Testing
Added unit test which validates that the correct close request is sent to the server.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Kirk True <kirk@mustardgrain.com>, Philip Nee <philipnee@gmail.com>, Luke Chen <showuon@gmail.com>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/FetchSessionHandler.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/common/requests/FetchMetadata.java', 'clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java', 'clients/src/main/java/org/apache/kafka/common/utils/Utils.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'core/src/test/java/kafka/testkit/KafkaClusterTestKit.java']","'Closing the Kafka consumer doesn't send a 'close' fetch request to the server, resulting in orphaned fetch sessions which only close after hitting the timeout.'"
7f4fc76e968a6b2cf4a73364c93bfdea03f81af3,1591864163,"KAFKA-9374: Make connector interactions asynchronous (#8069)

These changes allow herders to continue to function even when a connector they are running hangs in its start, stop, initialize, validate, and/or config methods.

The main idea is to make these connector interactions asynchronous and accept a callback that can be invoked upon the completion (successful or otherwise) of these interactions. The distributed herder handles any follow-up logic by adding a new herder request to its queue in that callback, which helps preserve some synchronization and ordering guarantees provided by the current tick model.

If any connector refuses to shut down within a graceful timeout period, the framework will abandon it and potentially start a new connector in its place (in cases such as connector restart or reconfiguration).

Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.

Reviewers: Greg Harris <gregh@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/CloseableConnectorContext.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderConnectorContext.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConnector.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/ConnectUtils.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConnectorTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResourceTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectCluster.java']","Herders fail to function when a connector hangs in its start, stop, initialize, validate, and/or config methods, resulting in potential shutdown issues and disrupted synchronization."
3d45e1f09ee91a3233eec9adc416ff9c15c2b5c8,1591996640,"KAFKA-10157: Fix broken tests due to InterruptedException from FinalizedFeatureChangeListener (#8857)

This PR fixes the cause of failing tests mentioned in the jira:

kafka.network.DynamicConnectionQuotaTest
kafka.api.CustomQuotaCallbackTest
kafka.server.DynamicBrokerReconfigurationTest

Issue:
The call to ChangeNotificationProcessorThread.queue.take() could throw an InterruptedException. While the queue is empty and the thread is blocking on taking an item from the queue, a concurrent call to FinalizedFeatureChangeListener.close() could interrupt the thread and cause an InterruptedException to be raised from queue.take(). In such a case, it is safe to ignore the exception since the thread is being shutdown.
Definitely ignoring the InterruptedException for the above reason was the intent of the code that used the ignoring clause for the same. But it seems unfortunately the ignoring clause does not ignore InterruptedException, so that doesn't work for us. To confirm this theory, I found the following code in scala.util.control.Exception.scala: https://github.com/scala/scala/blob/v2.12.0/src/library/scala/util/control/Exception.scala#L167-L176.

Fix:
The fix in this PR is to just not use the ignoring clause. We rely on existing mechanism in ShutdownableThread that ignores the exception during shutdown.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chan <boyang@confluent.io>, Anna Povzner <anna@confluent.io>, Jun Rao <junrao@gmail.com>",['core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala'],"Tests are failing due to an InterruptedException raised during the shutdown of ChangeNotificationProcessorThread in FinalizedFeatureChangeListener, despite the intent to safely ignore this exception."
9972297e510d74bd5dedbffe5dfb7a9f1c0a123f,1692845956,"KAFKA-14780: Fix flaky test 'testSecondaryRefreshAfterElapsedDelay' (#14078)

""The test RefreshingHttpsJwksTest#testSecondaryRefreshAfterElapsedDelay relies on the actual system clock, which makes it frequently fail. The fix adds a second constructor that allows for passing a ScheduledExecutorService to manually execute the scheduled tasks before refreshing. The fixed task is much more robust and stable.

Co-authored-by: Fei Xie <feixie@MacBook-Pro.attlocal.net>

Reviewers: Divij Vaidya <diviv@amazon.com>, Luke Chen <showuon@gmail.com>","['clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/RefreshingHttpsJwks.java', 'clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/secured/RefreshingHttpsJwksTest.java']",'Test case RefreshingHttpsJwksTest#testSecondaryRefreshAfterElapsedDelay exhibits flakiness due to reliance on actual system clock.'
b916cb40bd7bfe7077ce9fd1c2b4270b1b97b5ed,1647297730,"KAFKA-13690: Fix flaky test in EosIntegrationTest (#11887)

I found a couple of flakiness with the integration test.

IQv1 on stores failed although getting the store itself is covered with timeouts, since the InvalidStoreException is upon the query (store.all()). I changed to the util function with IQv2 whose timeout/retry covers the whole procedure. Example of such failure is: https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11802/11/tests/

With ALOS we should not check that the output, as well as the state store content is exactly as of processed once, since it is possible that during processing we got spurious task-migrate exceptions and re-processed with duplicates. I actually cannot reproduce this error locally, but from the jenkins errors it seems possible indeed. Example of such failure is: https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11433/4/tests/

Some minor cleanups.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",['streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java'],"EosIntegrationTest is flaky, showing sporadic failures with IQv1 on stores and possible re-processing with duplicates during ALOS. Links to such failures provided."
3e38038278e8c6bb9c59432bedc81a06bc10d1de,1626467419,"HOTFIX: Init stream-stream left/outer join emit interval correctly (#11055)

Follow up to #10917

The fix from #10917 intended to reduce the emit frequency to save the creation cost of RocksDB iterators. However, we incorrectly initialized the ""timer"" with timestamp zero, and thus, the timer was always in the past and we did try to emit left/outer join result too often.

This PR fixes the initialization of the emit interval timer to current wall-clock time to effectively 'enable' the fix from #10917.

Reviewers: Sergio Peña <sergio@confluent.io>, Guozhang Wang <guozhang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamLeftJoinTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamOuterJoinTest.java']","The timer in stream-stream left/outer join is incorrectly initialized with timestamp zero, causing the join result to be emitted too frequently and impacting performance."
36b273370dd840cc2bb6307f311f8952b886b323,1596161516,"KAFKA-10282; Remove Log metrics immediately when deleting log

Currently, we remove the Log metrics when asynchronous deletion of the log is triggered. However, we attempt to register the metrics immediately upon log creation. If a Log object is re-created for a partition that is pending deletion (because a topic was quickly re-created or because a partition was moved off and back onto a broker), the registration of the new metrics can happen before the asyncrhonous deletion. In this case, the metrics are removed after the second registration, leading to missing Log metrics.

To fix this, this patch changes the log deletion behavior to remove the metrics when the log is first marked for deletion, rather than when the files are deleted. This removes the window in which metrics registration can occur before metrics removal. This is justifiable because the log should be logically deleted when a delete request or partition movement finishes, rather than when the files are actually removed. Tested with unit tests.

Author: Bob Barrett <bob.barrett@confluent.io>

Reviewers: David Jacot, Dhruvil Shah, Vikas Singh, Gwen Shapira

Closes #9054 from bob-barrett/KAFKA-10282
","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogManager.scala', 'core/src/test/scala/unit/kafka/log/LogManagerTest.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']","Log metrics get removed after a second registration when a Log object for a partition pending deletion is re-created, leading to missing Log metrics."
058d8d530b0cda41c4f709b57621043ad858cd47,1673633198,"KAFKA-14618; Fix off by one error in snapshot id (#13108)

The KRaft client expects the offset of the snapshot id to be an end offset. End offsets are
exclusive. The MetadataProvenance type was createing a snapshot id using the last contained offset
which is inclusive. This change fixes that and renames some of the fields to make this difference
more obvious.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/server/metadata/BrokerServerMetrics.scala', 'core/src/test/scala/kafka/server/metadata/BrokerServerMetricsTest.scala', 'metadata/src/main/java/org/apache/kafka/image/MetadataImage.java', 'metadata/src/main/java/org/apache/kafka/image/MetadataProvenance.java', 'metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java', 'metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotEmitter.java', 'metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java', 'metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java', 'metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotEmitterTest.java', 'raft/src/main/java/org/apache/kafka/snapshot/Snapshots.java']","The KRaft client misinterprets snapshot id offsets because the MetadataProvenance type creates a snapshot id using the last contained offset which is inclusive, causing off by one errors."
7eafea0d57522ca391a5c3c91bd251c59f839c3c,1574030520,"KAFKA-9196; Update high watermark metadata after segment roll (#7695)

When we roll a new segment, the log offset metadata tied to the high watermark may
need to be updated. This is needed when the high watermark is equal to the log end
offset at the time of the roll. Otherwise, we risk exposing uncommitted data early.

Reviewers: Dhruvil Shah <dhruvil@confluent.io>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']","Upon segment roll, high watermark metadata tied to the log offset can potentially fail to update, possibly leading to exposure of uncommitted data."
59efa12d0c0c098e6f57848bf5a43b459c0563e0,1590030328,"KAFKA-9409: Supplement immutability of ClusterConfigState class in Connect (#7942)

The class claims to be immutable, but there are some mutable features of this class.
Increase the immutability of it and add a little cleanup:

* Pre-initialize size of ArrayList
* Remove superfluous syntax
* Use ArrayList instead of LinkedList since the list is created once

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>",['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ClusterConfigState.java'],"ClusterConfigState class in Connect, purported to be immutable, exhibits mutable features."
706f5097b70ce140e6a0473ad55342b514b6db04,1612404086,"KAFKA-10716: persist UUID in state directory for stable processId across restarts (#9978)

To stabilize the task assignment across restarts of the JVM we need some way to persist the process-specific UUID. We can just write it to a file in the state directory, and initialize it from there or create a new one if no prior UUID exists.

Reviewers: Walker Carlson <wcarlson@confluent.io>, Leah Thomas <lthomas@confluent.io>, Guozhang Wang <guozhang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinMultiIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StoreUpgradeIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java']",Task assignment instability across JVM restarts due to non-persistent process-specific UUID.
0ce16406e0abbc5d27a0be064f64da74b2e404c5,1690936404,"KAFKA-15022: [4/N] use client tag assignor for rack aware standby task assignment (#14097)

Part of KIP-925.

For rack aware standby task assignment, we can either use the already existing ""rack tags"" or as a fall-back the newly added ""rack.id"". This PR unifies both without the need to change the actual standby task assignment logic.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientTagAwareStandbyTaskAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/RackAwareTaskAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StandbyTaskAssignorFactory.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientTagAwareStandbyTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/RackAwareTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StandbyTaskAssignorFactoryTest.java']","Rack aware standby task assignment not unified for using either existing ""rack tags"" or new ""rack.id"", causing inconsistencies in task assignment logic."
2bef80e3604e9a50660d423daaf8e66dd7c4e989,1662035379,"KAFKA-10199: Remove changelog unregister from state updater (#12573)

Changelogs are already unregistered when tasks are closed.
There is no need to also unregister them in the state
updater.

In future, when we will only have the state updater without
the old code path, we should consider registering and
unregistering the changelogs within the state updater.

Reviewer: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java']",Redundant unregistering of changelogs in state updater is happening even though it is already done when tasks are closed.
76d287c96771159d9b86dcf2fa193ff69198dd5b,1647372383,"KAFKA-13727; Preserve txn markers after partial segment cleaning (#11891)

It is possible to clean a segment partially if the offset map is filled before reaching the end of the segment. The highest offset that is reached becomes the new dirty offset after the cleaning completes. The data above this offset is nevertheless copied over to the new partially cleaned segment. Hence we need to ensure that the transaction index reflects aborted transactions from both the cleaned and uncleaned portion of the segment. Prior to this patch, this was not the case. We only collected the aborted transactions from the cleaned portion, which means that the reconstructed index could be incomplete. This can cause the aborted data to become effectively committed. It can also cause the deletion of the abort marker before the corresponding data has been removed (i.e. the aborted transaction becomes hanging).

Reviewers: Jun Rao <junrao@gmail.com>","['clients/src/main/java/org/apache/kafka/common/record/RecordBatchIterator.java', 'core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerTest.scala']","Partial cleaning of a segment could lead to incomplete transaction index reconstruction, resulting in aborted data effectively becoming committed and the potential creation of hanging aborted transactions."
9783b85fdd947fe55fa80c724c8b4ae9a7eb63c3,1585929884,"KAFKA-9739: Fixes null key changing child node (#8400)

For some context, when building a streams application, the optimizer keeps track of the key-changing operations and any repartition nodes that are descendants of the key-changer. During the optimization phase (if enabled), any repartition nodes are logically collapsed into one. The optimizer updates the graph by inserting the single repartition node between the key-changing node and its first child node. This graph update process is done by searching for a node that has the key-changing node as one of its direct parents, and the search starts from the repartition node, going up in the parent hierarchy.

The one exception to this rule is if there is a merge node that is a descendant of the key-changing node, then during the optimization phase, the map tracking key-changers to repartition nodes is updated to have the merge node as the key. Then the optimization process updates the graph to place the single repartition node between the merge node and its first child node.

The error in KAFKA-9739 occurred because there was an assumption that the repartition nodes are children of the merge node. But in the topology from KAFKA-9739, the repartition node was a parent of the merge node. So when attempting to find the first child of the merge node, nothing was found (obviously) resulting in StreamException(Found a null keyChangingChild node for..)

This PR fixes this bug by first checking that all repartition nodes for optimization are children of the merge node.

This PR includes a test with the topology from KAFKA-9739.

Reviewers: John Roesler <john@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/StreamsGraphTest.java']","Streams application optimization phase assumes repartition nodes to be children of the merge node, causing StreamException when a repartition node is a parent of the merge node."
4d43abf1e09e01fc5e7af52f65e3fbae02cf9771,1678182061,"KAFKA-14770: Allow dynamic keystore update for brokers if string representation of DN matches even if canonical DNs don't match (#13346)

To avoid mistakes during dynamic broker config updates that could potentially affect clients, we restrict changes that can be performed dynamically without broker restart. For broker keystore updates, we require the DN to be the same for the old and new certificates since this could potentially contain host names used for host name verification by clients. DNs are compared using standard Java implementation of X500Principal.equals() which compares canonical names. If tags of fields change from one with a printable string representation and one without or vice-versa, canonical name check fails even if the actual name is the same since canonical representation converts to hex for some tags only. We can relax the verification to allow dynamic updates in this case by enabling dynamic update if either the canonical name or the RFC2253 string representation of the DN matches.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Kalpesh Patel <kpatel@confluent.io>","['clients/src/main/java/org/apache/kafka/common/security/ssl/SslFactory.java', 'clients/src/test/java/org/apache/kafka/common/security/ssl/SslFactoryTest.java', 'clients/src/test/java/org/apache/kafka/test/TestSslUtils.java']","Dynamic broker config updates in Kafka fail to update keystore if X500Principal.equals() fails, even if the actual names are the same. Causes broker restarts unnecessarily."
d5e216d6183a2aae109324652d92a3355690b9d6,1680883769,"KAFKA-14617: Fill broker epochs to the AlterPartitionRequest (#13489)

As the third part of the KIP-903, it fills the broker epochs from the Fetch request into the AlterPartitionRequest. Also, before generating the alterPartitionRequest, the partition will check whether the broker epoch from the FetchRequest matches with the broker epoch recorded in the metadata cache. If not, the ISR change will be delayed.

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/api/LeaderAndIsr.scala', 'core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/cluster/Replica.scala', 'core/src/main/scala/kafka/server/AlterPartitionManager.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/main/scala/kafka/server/metadata/KRaftMetadataCache.scala', 'core/src/main/scala/kafka/zk/ZkMigrationClient.scala', 'core/src/test/scala/unit/kafka/cluster/AbstractPartitionTest.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/cluster/ReplicaTest.scala', 'core/src/test/scala/unit/kafka/server/AlterPartitionManagerTest.scala', 'core/src/test/scala/unit/kafka/server/IsrExpirationTest.scala', 'core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/zk/ZkMigrationClientTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java']",Mismatch between broker epoch from FetchRequest and the one recorded in metadata cache results in unnecessary delay in ISR change.
5c2492bca71200806ccf776ea31639a90290d43e,1688394934,"KAFKA-10199: Consider tasks in state updater when computing offset sums (#13925)

With the state updater, the task manager needs also to look into the
tasks owned by the state updater when computing the sum of offsets
of the state. This sum of offsets is used by the high availability
assignor to assign warm-up replicas.
If the task manager does not take into account tasks in the
state updater, a warm-up replica will never report back that
the state for the corresponding task has caught up. Consequently,
the warm-up replica will never be dismissed and probing rebalances
will never end.

Reviewers: Lucas Brutschy <lbrutschy@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","Task manager not accounting for tasks owned by the state updater when computing the sum of offsets, which causes warm-up replicas never to be dismissed leading to unending probing rebalances."
0d461e4ea0a8353c358ae661837f471995943bb0,1550861455,"KAFKA-7672: Restoring tasks need to be closed upon task suspension (#6113)

* In activeTasks.suspend, we should also close all restoring tasks as well. Closing restoring tasks would not require `task.close` as in `closeNonRunningTasks `, since the topology is not initialized yet, instead only state stores are initialized. So we only need to call `task.closeStateManager`.
* Also add @linyli001 's fix.
* Unit tests updated accordingly.

Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","Restoring tasks are not appropriately closed upon task suspension in activeTasks.suspend, causing potential state store initialization issues."
704cabf10d2e5daeddb3ebfbecf7cd831e6c856e,1616728172,"MINOR: Use Java 11 for generating aggregated javadoc in release.py (#10399)

Java 11 generates html5 pages with search support, which provides a better user experience.

Fixed `get_jdk` bugs found during testing and updated `release.py` blurb to indicate that
both JDK 8 and JDK 11 are required to perform a release.

Tested by running `python2 release.py stage-docs`, which triggers the `aggregateJavadoc`
path without some of the undesired (for testing) release steps.

Reviewers: John Roesler <vvcephei@apache.org>",['release.py'],"The current JDK being used for generating aggregated javadoc does not support html5 pages with search functionality, yielding a sub-optimal user experience. Bugs were encountered in `get_jdk` during testing."
d3c067f35d184ca75e8cc59bedd56689cbc8269b,1583537935,"MINOR: Check store directory empty to decide whether throw task corrupted exception with EOS (#8180)

Before we register the stores (and hence create the store dirs), we check if the task dir is empty except the lock / checkpoint files. Then later when loading the checkpoint files if we do not find the offsets AND the store dirs are not empty, meaning that the stores may be not empty, we treat it as task corrupted.

Reviewers: John Roesler <vvcephei@apache.org>","['streams/src/main/java/org/apache/kafka/streams/errors/TaskCorruptedException.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerUtilTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java']",Task corruption may occur when offsets are not found in checkpoint files and store directories are not empty during the registration of stores.
c8684d883b4c7db13272e8e28f55e2a02bf8cab7,1624371131,"KAFKA-12483: Enable client overrides in connector configs by default (KIP-722) (#10336)

Changes the default value for the `connector.client.config.override.policy` worker configuration property from `None` to `All`. Modified unit tests to verify all policies still work, and that by default connectors can override all client policies.
See https://cwiki.apache.org/confluence/display/KAFKA/KIP-722%3A+Enable+connector+client+overrides+by+default

Updated the documentation for the worker's client overrides policy to mention the new default.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorClientPolicyIntegrationTest.java']",The `connector.client.config.override.policy` worker configuration in the Kafka connector configs does not allow client overrides by default which may limit flexibility for users.
21b79635474209fcd67d3bf33a70c25f6827c6ef,1551153554,"KAFKA-7918: Inline generic parameters Pt. I: in-memory key-value store (#6293)

First PR in series to inline the generic parameters of the following bytes stores:

[x] InMemoryKeyValueStore
[ ] RocksDBWindowStore
[ ] RocksDBSessionStore
[ ] MemoryLRUCache
[ ] MemoryNavigableLRUCache
[ ] (awaiting merge) InMemoryWindowStore

A number of tests took advantage of the generic InMemoryKeyValueStore and had to be reworked somewhat -- this PR covers everything related to the in-memory key-value store.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/state/Stores.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/DelegatingPeekingKeyValueIterator.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableReduceTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/KeyValueStoreMaterializerTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/CachingKeyValueStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/DelegatingPeekingKeyValueIteratorTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/FilteredCacheIteratorTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java', 'streams/src/test/java/org/apache/kafka/test/GenericInMemoryKeyValueStore.java', 'streams/test-utils/src/test/java/org/apache/kafka/streams/MockProcessorContextTest.java']","The InMemoryKeyValueStore's generic parameters are not being inlined, leading to difficulty in testing and potential performance issues."
3ec2ca5e334ef42683263ed50c718c62917046af,1552605532,"KAFKA-7730; Limit number of active connections per listener in brokers (KIP-402)

Adds a new listener config `max.connections` to limit the number of active connections on each listener. The config may be prefixed with listener prefix. This limit may be dynamically reconfigured without restarting the broker.

This is one of the PRs for KIP-402 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-402%3A+Improve+fairness+in+SocketServer+processors). Note that this is currently built on top of PR #6022

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Gwen Shapira <cshapi@gmail.com>

Closes #6034 from rajinisivaram/KAFKA-7730-max-connections
","['clients/src/main/java/org/apache/kafka/common/network/Selector.java', 'clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java', 'core/src/main/scala/kafka/network/SocketServer.scala', 'core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/integration/kafka/network/DynamicConnectionQuotaTest.scala', 'core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala']","The Kafka broker lacks a limit on the number of active connections per listener, potentially leading to resource exhaustion."
6d7723f073ad0b935b02792be9aa4fd49b581b50,1651017072,"MINOR: fix html generation syntax errors (#12094)

The html document generation has some errors in it, specifically related to protocols. The two issues identified and resolved are:

* Missing </tbody> closing tags added
* Invalid usage of a <p> tag as a wrapper element for <table> elements. Changed the <p> tag to be a <div>.

Tested by running ./gradlew siteDocsTar and observing that the output was properly formed.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/common/protocol/ApiKeys.java', 'clients/src/main/java/org/apache/kafka/common/protocol/Errors.java', 'clients/src/main/java/org/apache/kafka/common/protocol/Protocol.java', 'clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java']",HTML generation is producing incorrectly structured documents due to missing closing tags and invalid usage of tag as a wrapper for other elements.
bf55afecdab9fe00c9defcec04adb27856df93cc,1609956416,"KAFKA-10778; Fence appends after write failure (#9676)

This patch improves append fencing after an IO error. Previously there was a window between the time of an IO error and the time the log is taken offline in which additional appends can be attempted. This is due to the asynchronous propagation of the IO error. This patch tightens the fencing so that no additional appends will be accepted after a previous append failed with an IO error.

Reviewers: Guozhang Wang <guozhang@apache.org>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']",Additional appends can be attempted in the gap after IO error occurs and before the log is taken offline due to asynchronous propagation of the IO error.
cfd0503006127b58bb181076bffaccb947fdd2bd,1681493948,"MINOR: fix some flaky KRaft-related tests (#13543) (#13543)

In SharedServer, fix some cases where a volatile variable could change to null while we were using
it, during shutdown. This is mainly a junit test issue, although it could also cause ugly error
messages during shutdown when running the server in a production context.

Fix a race in KafkaEventQueueTest.testSize.

Reviewers: David Arthur <mumrah@gmail.com>","['core/src/main/scala/kafka/server/SharedServer.scala', 'server-common/src/test/java/org/apache/kafka/queue/KafkaEventQueueTest.java']","Volatile variable in SharedServer changes to null during shutdown causing junit test issues and potentially problematic error messages in production. Additionally, there's a race condition discovered in KafkaEventQueueTest.testSize."
a1f3c6d16061566a4f53c72a95e2679b8ee229e0,1663311911,"KAFKA-10199: Register and unregister changelog topics in state updater (#12638)

Registering and unregistering the changelog topics in the
changelog reader outside of the state updater leads to
race conditions between the stream thread and the state
updater thread. Thus, this PR moves registering and
unregistering of changelog topics in the changelog
reader into the state updater if the state updater
is enabled.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Hao Li <1127478+lihaosky@users.noreply.github.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogRegister.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ReadOnlyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Tasks.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreatorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/MockChangelogReader.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java', 'streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java']",Changelog topics registration and unregistration outside of the state updater is causing race conditions between the stream thread and the state updater thread.
102490ea0caba93631c4dc7ea05a64c0117331b7,1562627477,"MINOR: improve RocksDBConfigSetter docs (#7009)

Users often use the RocksDBConfigSetter to modify parameters such as cache or block size, which must be set through the BlockBasedTableConfig object. Rather than creating a new object in the config setter, however, users should most likely retrieve a reference to the existing one so as to not lose the other defaults (eg the BloomFilter)

There have been notes from the community that it is not obvious this should be done, nor is it immediately clear how to do so. This PR updates the RocksDBConfigSetter docs to hopefully improve things.

I also piggybacked a few minor cleanups in the docs

Reviewers: Kamal Chandraprakash, Jim Galasyn <jim.galasyn@confluent.io>,  Bruno Cadonna <bruno@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",['streams/src/main/java/org/apache/kafka/streams/state/RocksDBConfigSetter.java'],"The current RocksDBConfigSetter documentation is unclear, leading users to inadvertently create new BlockBasedTableConfig objects and lose existing defaults rather than modifying parameters on the existing object."
9285820df066af002cc5b009a6829baade8f4bd6,1637255676,"MINOR: Set mock correctly in RocksDBMetricsRecorderTest (#11462)

With a nice mock in RocksDBMetricsRecorderTest#shouldCorrectlyHandleHitRatioRecordingsWithZeroHitsAndMisses() and RocksDBMetricsRecorderTest#shouldCorrectlyHandleAvgRecordingsWithZeroSumAndCount() were green although getTickerCount() was never called. The tests were green because EasyMock returns 0 for a numerical return value by default if no expectation is specified. Thus, commenting out the expectation for getTickerCount() did not change the result of the test.

This commit changes the mock to a default mock and fixes the expectation to expect getAndResetTickerCount(). Now, commenting out the expectation leads to a test failure.

Reviewers: Luizfrf3 <lf.fonseca@hotmail.com>, Guozhang Wang <wangguoz@gmail.com>",['streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorderTest.java'],"The RocksDBMetricsRecorderTest is green even when getTickerCount() is never called due to incorrect mock settings in EasyMock, which defaults to zero for unspecified numerical expectations."
c199840f0add1c191c4333dc58baae49594734a8,1692127799,"MINOR: Fix the ZkMigrationState metric in KafkaController

This patch fixes an issue for ZK controllers where we were emitting the ZkMigrationState enum
rather than a value. This can lead to downstream failures with JMX metrics since the RMI protocol
will marshal the ZkMigrationState object returned by the gauge. Any downstream consumer of this
metric (like jconsole or a metrics exporter) will not be able to unmarshal the value since the
ZkMigrationState class will not be present.

The fix is simply to emit the byte value of this enum.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Alok Thatikunta <athatikunta@confluent.io>
","['core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/test/scala/unit/kafka/metrics/MetricsTest.scala']","ZkMigrationState enum is emitted instead of a value in ZK controllers, causing failures with JMX metrics due to unmarshal errors in any downstream consumers."
97fc6a52216a3da1c76771ddc37e25fed102551c,1583868658,"KAFKA-9658; Fix user quota removal (#8232)

Adding (add-config) default user, user, or <user, client-id> quota and then removing it via delete-config does not update quota bound in ClientQuotaManager.Metrics for existing users or <user,client-id>. This causes brokers to continue to throttle with the previously set quotas until brokers restart (or <user,client> stops sending traffic for sometime and sensor expires). This happens only when removing the user or user,client-id where there are no more quotas  to fall back to. Common example where the issue happens: Initial no quota state --> add default user quota --> remove default user quota. 

The cause of the issue was `DefaultQuotaCallback.quotaLimit` was returning `null` when no default user quota set, which caused `ClientQuotaManager.updateQuotaMetricConfigs` to skip updating the appropriate sensor, which left it unchanged with the previous quota. Since `null` is an acceptable return value for `ClientQuotaCallback.quotaLimit`, which is already treated as unlimited quota in other parts of the code, this PR ensures that `ClientQuotaManager.updateQuotaMetricConfigs` updates the quotas for which  `ClientQuotaCallback.quotaLimit` returns `null` to unlimited quota.

Reviewers: Jason Gustafson <jason@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>","['core/src/main/scala/kafka/server/ClientQuotaManager.scala', 'core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala']","Removing user/user-client quotas via delete-config doesn't update quotas in ClientQuotaManager.Metrics for existing users, causing brokers to throttle with previous quotas until they restart or cease traffic.
"
e6057e5038c2b681152d2608d551db675960d12a,1558992159,"KAFKA-8437; Await node api versions before checking if offset validation is possible (#6823)

The consumer should await api version information before determining whether the broker supports offset validation. In KAFKA-8422, we skip the validation if we don't have api version information, which means we always skip validation the first time we connect to a node. This bug was detected by the failing system test `tests/client/truncation_test.py`. The test passes again with this fix.

Reviewers: Ismael Juma <ismael@juma.me.uk>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/test/java/org/apache/kafka/clients/MockClient.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java']","Consumer is skipping offset validation on first connection to a node due to premature api version information check, as detected by failing system test `tests/client/truncation_test.py`."
d0800b3f7c135c97ec8632c247fc02946527c0a2,1597292453,"KAFKA-10391: Overwrite checkpoint in task corruption to remove corrupted partitions (#9170)

In order to do this, I also removed the optimization such that once enforced checkpoint is set to true, we always checkpoint unless the state stores are not initialized at all (i.e. the snapshot is null).

Reviewers: Boyang Chen <boyang@confluent.io>, A. Sophie Blee-Goldman <ableegoldman@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","Task corruption in Kafka unexpectedly overwrites checkpoints, removing corrupted partitions. Enforced checkpointing continues even when state stores are uninitialized."
4a5155c934b9d2045e1f7af09a0298bc4b413027,1572627835,"KAFKA-8868: Generate SubscriptionInfo protocol message (#7248)

Rather than maintain hand coded protocol serialization code, Streams could use the same code-generation framework as Clients/Core.

There isn't a perfect match, since the code generation framework includes an assumption that you're generating ""protocol messages"", rather than just arbitrary blobs, but I think it's close enough to justify using it, and improving it over time.

Using the code generation allows us to drop a lot of detail-oriented, brittle, and hard-to-maintain serialization logic in favor of a schema spec.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Boyang Chen <boyang@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java', 'generator/src/main/java/org/apache/kafka/message/MessageSpecType.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StreamsUpgradeTestIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/LegacySubscriptionInfoSerde.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java', 'streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java']","Streams component is currently relying on hand-coded, brittle protocol serialization code, which is hard to maintain and differs from the code-generation framework used by Clients/Core."
cd5f6c60b5bf191ec2aaff64ed2aceb105a3d33e,1667835985,"KAFKA-14299: Avoid busy polling in state updater (#12772)

The state updater can enter a busy polling loop if it
only updates standby tasks. We need to use the user-provided
poll-time to update always when using the state updater, since
the only other place where the state update blocks
(inside `waitIfAllChangelogsCompletelyRead`) is also
not blocking if there is at least one standby task.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java']","State updater enters a busy polling loop while only updating standby tasks, not utilizing user-provided poll-time effectively."
fecb977b257888e2022c1b1e04dd7bf03e18720c,1573695653,"KAFKA-8710; Allow transactional producers to bump producer epoch [KIP-360] (#7115)

This patch implements the broker-side changes for KIP-360. It adds two new fields to InitProducerId: lastEpoch and producerId. Passing these values allows the TransactionCoordinator to safely bump a producer's epoch after some failures (such as UNKNOWN_PRODUCER_ID and INVALID_PRODUCER_ID_MAPPING). When a producer calls InitProducerId after a failure, the coordinator first checks the producer ID from the request to make sure no other producer has been started using the same transactional ID. If it is safe to continue, the coordinator checks the epoch from the request; if it matches the existing epoch, the epoch is bumped and the producer can safely continue. If it matches the previous epoch, the the current epoch is returned without bumping. Otherwise, the producer is fenced.

Reviewers: Boyang Chen <boyang@confluent.io>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/main/java/org/apache/kafka/common/utils/ProducerIdAndEpoch.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java', 'core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionLog.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionMetadata.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala', 'core/src/main/scala/kafka/log/ProducerStateManager.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerChannelManagerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerRequestCompletionHandlerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMetadataTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala', 'core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala']",Problems might occur due to the inability of transactional producers to elevate their producer's epoch after certain failures like UNKNOWN_PRODUCER_ID and INVALID_PRODUCER_ID_MAPPING.
f91d592a270dc4254978fbdb51e9cd479426b931,1616813727,"KAFKA-12537: fix application shutdown corner case with only one thread (#10387)

When in EOS the run loop terminates on that thread before the shutdown can be called. This is a problem for EOS single thread applications using the application shutdown feature.

I changed it so in all cases with a single thread, the dying thread will spin up a new thread to communicate the shutdown and terminate the dying thread. Also @ableegoldman refactored the catch blocks in runloop.

Co-authored-by: A. Sophie Blee-Goldman <ableegoldman@gmail.com>

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']","Single thread applications using EOS encounter termination before a shutdown can be executed, causing issues in the application shutdown feature."
319732dbeb08aa7926175cbaf4dfa8a05c52ac18,1643846612,"KAFKA-12841: Fix producer callback handling when partition is missing (#11689)

Sometimes, the Kafka producer encounters an error prior to selecting a topic partition. In this case, we
would like to acknowledge the failure in the producer interceptors, if any are configured. We should also
pass a non-null Metadata object to the producer callback, if there is one. This PR implements that
behavior. It also updates the JavaDoc to clarify that if a partition cannot be selected, we will pass
back a partition id of -1 in the metadata. This is in keeping with KAFKA-3303.

Co-authors: Kirk True <kirk@mustardgrain.com>
Reviewers: Colin P. McCabe <cmccabe@apache.org>","['clients/src/main/java/org/apache/kafka/clients/producer/Callback.java', 'clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerInterceptors.java', 'clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java']","If Kafka producer encounters an error prior to selecting a topic partition, failure is not acknowledged in the producer interceptors and a non-null Metadata object is not passed to the producer callback."
937f1f741c026767b6b71ce0b0e63ae7ebe6e936,1581835630,"KAFKA-8805; Bump producer epoch on recoverable errors (#7389)

This change is the client-side part of KIP-360. It identifies cases where it is safe to abort a transaction, bump the producer epoch, and allow the application to continue without closing the producer. In these cases, when KafkaProducer.abortTransaction() is called, the producer sends an InitProducerId following the transaction abort, which causes the producer epoch to be bumped. The application can then start a new transaction and continue processing.

For recoverable errors in the idempotent producer, the epoch is bumped locally. In-flight requests for partitions with an error are rewritten to reflect the new epoch, and in-flights of all other partitions are allowed to complete using the old epoch. 

Reviewers: Boyang Chen <boyang@confluent.io>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java', 'core/src/test/scala/integration/kafka/api/TransactionsBounceTest.scala', 'core/src/test/scala/integration/kafka/api/TransactionsExpirationTest.scala', 'core/src/test/scala/integration/kafka/api/TransactionsTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","Recoverable errors in the idempotent Kafka producer and transactions abort do not increase the producer epoch, disrupting the application's ability to continue without closing the producer."
bfd15299b1f31b1f5ba0f2c9145d4abc3042405c,1679443690,"KAFKA-14491: [14/N] Set changelog topic configs for versioned stores (#13292)

Sets the correct topic configs for changelog topics for versioned stores introduced in KIP-889. Changelog topics for versioned stores differ from those for non-versioned stores only in that min.compaction.lag.ms needs to be set in order to prevent version history from being compacted prematurely.

The value for min.compaction.lag.ms is equal to the store's history retention plus some buffer to account for the broker's use of wall-clock time in performing compactions. This buffer is analogous to the windowstore.changelog.additional.retention.ms value for window store changelog topic retention time, and uses the same default of 24 hours. In the future, we can propose a KIP to expose a config such as versionedstore.changelog.additional.compaction.lag.ms to allow users to tune this value.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/UnwindowedUnversionedChangelogTopicConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/VersionedChangelogTopicConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/WindowedChangelogTopicConfig.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/VersionedKeyValueStoreBuilder.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ChangelogTopicsTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicConfigTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java']","Changelog topics for versioned stores are not getting the correct topic configs, specifically min.compaction.lag.ms, leading to potential premature compaction of version history."
fc0963a2367bbb47fa369fbfad89989f148b7e1d,1571825786,"KAFKA-9078: Fix Connect system test after adding MM2 connector classes

MM2 added a few connector classes in Connect's classpath and given that the assertion in the Connect REST system tests need to be adjusted to account for these additions.

This fix makes sure that the loaded Connect plugins are a superset of the expected by the test connectors.

Testing: The change is straightforward. The fix was tested with local system test runs.

Author: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #7578 from kkonstantine/minor-fix-connect-test-after-mm2-classes
",['tests/kafkatest/tests/connect/connect_rest_test.py'],"Connect's classpath has been expanded with new MM2 connector classes, leading to mismatched assertions in REST system tests."
b100f1efac77bf795683ab5e68ecf87845372089,1687272646,"KAFKA-15087 Move/rewrite InterBrokerSendThread to server-commons (#13856)

The Java rewrite is kept relatively close to the Scala original
to minimize potential newly introduced bugs and to make reviewing
simpler. The following details might be of note:
- The `Logging` trait moved to InterBrokerSendThread with the
rewrite of ShutdownableThread has been similarly moved to any
subclasses that currently use it. InterBrokerSendThread's own
logging has been made to use ShutdownableThread's logger which
mimics the prefix/log identifier that the trait provided.
- The case RequestAndCompletionHandler class has been made a
separate POJO class and the internal-use UnsentRequests class
has been kept as a static nested class.
- The relatively commonly used but internal (not part of the
public API) clients classes that InterBrokerSendThread relies on
have been allowlisted in the server-common import control.
- The accompanying test class has also been moved and rewritten
with one new test added and most of the pre-existing tests made
stricter.

Reviewers: David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/common/InterBrokerSendThread.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala', 'core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala', 'core/src/main/scala/kafka/server/AddPartitionsToTxnManager.scala', 'core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala', 'core/src/test/scala/kafka/common/InterBrokerSendThreadTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerChannelManagerTest.scala', 'core/src/test/scala/unit/kafka/server/AddPartitionsToTxnManagerTest.scala', 'server-common/src/main/java/org/apache/kafka/server/util/InterBrokerSendThread.java', 'server-common/src/main/java/org/apache/kafka/server/util/RequestAndCompletionHandler.java', 'server-common/src/main/java/org/apache/kafka/server/util/ShutdownableThread.java', 'server-common/src/test/java/org/apache/kafka/server/util/InterBrokerSendThreadTest.java']",InterBrokerSendThread implementation in Scala creates difficulties in debugging and reviewing due to unstructured logging and use of non-public API client classes. It may potentially also introduce new bugs.
fc5245d8c37a6c9d585c5792940a8f9501bedbe1,1623880970,"KAFKA-12955: Fix LogLoader to pass materialized view of segments for deletion (#10888)

Within LogLoader.removeAndDeleteSegmentsAsync(), we should force materialization of the segmentsToDelete iterable, to make sure the results of the iteration remain valid and deterministic. We should also pass only the materialized view to the logic that deletes the segments, as otherwise we could end up deleting the wrong segments.

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogLoader.scala', 'core/src/test/scala/unit/kafka/log/LogLoaderTest.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']","Non-materialized view of segments being deleted in LogLoader.removeAndDeleteSegmentsAsync() could potentially generate incorrect and non-deterministic results, possibly deleting wrong segments.
"
26daa8d610ca92ff8d4dd37420b61c960af478e1,1671626285,"MINOR: Fix various memory leaks in tests (#12959)

Various tests in the streams park were leaking native memory.

Most tests were fixed by closing the corresponding rocksdb resource.

I tested that the corresponding leak is gone by using a previous rocksdb
release with finalizers and checking if the finalizers would be called at some
point.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionStoreReceiveProcessorSupplierTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/BlockBasedTableConfigWithAccessibleCacheTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapterTest.java']",Several tests in the streams park are leaking native memory because rocksdb resources are not closed.
07f492eb862f99e6ba53843afa2f927fdd57be1e,1561495444,"MINOR: Fix flaky test case for compact/delete topics (#6975)

We recently observed a transient failure of this test case:
```
java.lang.AssertionError: Contents of the map shouldn't change expected:<Map(0 -> (340,340), 5 -> (345,345), 10 -> (350,350), 14 -> (354,354), 1 -> (341,341), 6 -> (346,346), 9 -> (349,349), 13 -> (353,353), 2 -> (342,342), 17 -> (357,357), 12 -> (352,352), 7 -> (347,347), 3 -> (343,343), 18 -> (358,358), 16 -> (356,356), 11 -> (351,351), 8 -> (348,348), 19 -> (359,359), 4 -> (344,344), 15 -> (355,355))> but was:<Map(0 -> (340,340), 5 -> (345,345), 10 -> (350,350), 14 -> (354,354), 1 -> (341,341), 6 -> (346,346), 97 -> (297,297), 9 -> (349,349), 96 -> (296,296), 13 -> (353,353), 2 -> (342,342), 17 -> (357,357), 12 -> (352,352), 7 -> (347,347), 98 -> (298,298), 3 -> (343,343), 18 -> (358,358), 95 -> (295,295), 16 -> (356,356), 11 -> (351,351), 99 -> (299,299), 8 -> (348,348), 19 -> (359,359), 4 -> (344,344), 15 -> (355,355))>
```
The presence of old keys implies that not all old segments had been deleted. We believe the retention check (which runs asynchronously) is executing after the last modified time of all but one of the segments has been changed. This leaves one old segment behind which ultimately causes the above assertion error.

The fix here is to improve the wait condition.Rather than waiting for the number of segments to be 1, we wait for the log start offset to reach the end offset.

Reviewers: David Arthur <mumrah@gmail.com>",['core/src/test/scala/unit/kafka/log/LogCleanerParameterizedIntegrationTest.scala'],An asynchronous retention check in compact/delete topics test case fails due to outdated segments; resulting in an assertion error due to mismatch in expected and actual map contents.
074ab2ebf6856628219d4c4ee95d57ff69d29bf3,1588806146,"KAFKA-9419: Fix possible integer overflow in CircularIterator (#7950)

The CircularIterator class uses a wrapping index-based approach to iterate over a list. This can be a performance problem O(n^2) for a LinkedList. Also, the index counter itself is never reset, a modulo is applied to it for every list access. At some point, it may be possible that the index counter overflows to a negative value and therefore may cause a negative index read and an ArrayIndexOutOfBoundsException.

This fix changes the implementation to avoid these two scenarios. Uses the Collection Iterator classes to avoid using an index counter and it avoids having to seek to the correct index every time, this avoiding the LinkedList performance issue.

I have added unit tests to validate the new implementation.

* KAFKA-9419: Integer Overflow Possible with CircularIterator
* Added JavaDoc. Support null values in the underlying collection
* Always return true for hasNext(). Add more JavaDoc
* Use an advance method to load next value and always return true in hasNext()
* Simplify test suite
* Use assertThrows in tests and remove redundant 'this' identifier

Co-authored-by: David Mollitor <dmollitor@apache.org>
Co-authored-by: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>","['clients/src/main/java/org/apache/kafka/common/utils/CircularIterator.java', 'clients/src/test/java/org/apache/kafka/common/utils/CircularIteratorTest.java']",The CircularIterator class's index-based approach can cause a performance issue (O(n^2)) for LinkedLists and potential integer overflow leading to a negative index read and an ArrayIndexOutOfBoundsException.
22434e6535c6471b8ac3e9cff1919e5ac15a50be,1569611295,"KAFKA-8319: Make KafkaStreamsTest a non-integration test class (#7382)

Previous KafkaStreamsTest takes 2min20s on my local laptop, because lots of its integration test which is producing / consuming records, and checking state directory file system takes lots of time. On the other hand, these tests should be well simplified with mocks.

This test reduces the test from a clumsy integration test class into a unit tests with mocks of its internal modules. And some other test functions should not be in KafkaStreamsTest actually and have been moved to other modular test classes. Now it takes 2s.

Also it helps removing the potential flakiness of the following (some of them are claimed resolved only because we have not seen them recently, but after looking at the test code I can verify they are still flaky):

* KAFKA-5818 (the original JIRA ticket indeed exposed a real issue that has been fixed, but the test itself remains flaky)
* KAFKA-6215
* KAFKA-7921
* KAFKA-7990
* KAFKA-8319
* KAFKA-8427

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Bruno Cadonna <bruno@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java', 'streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/test/MockClientSupplier.java']",KafkaStreamsTest is time-consuming and potentially flaky due to its structure as an integration test involving production/consumption of records and filesystem-based state directory checks.
b006205edb57117c5a399647b4352b06953e9e84,1571084103,"KAFKA-9020: Streams sub-topologies should be sorted by sink -> source relationship (#7495)

Subtopologies are currently ordered alphabetically by source node, which prior to KIP-307 happened to always result in the ""correct"" (ie topological) order. Now that users may name their nodes anything they want, we must explicitly order them so that upstream node groups/subtopologies come first and the downstream ones come after.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/RepartitionTopicNamingTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java']","User-defined naming in node groups/subtopologies results in incorrect topological order, incorrectly placing upstream nodes after downstream ones."
43d43e6c7bbfbc87d0288f7b934d5b6e0ebf1913,1594687753,"KAFKA-10002; Improve performances of StopReplicaRequest with large number of partitions to be deleted (#8672)

Update checkpoint files once for all deleted partitions instead of updating them for each deleted partitions. With this, a stop replica requests with 2000 partitions to be deleted takes ~2 secs instead of ~40 secs previously.

Refactor the checkpointing methods to not compute the logsByDir all the time. It is now reused as much as possible.

Refactor the exception handling. Some checkpointing methods were handling IOException but the underlying write process already catches them and throws KafkaStorageException instead.

Reduce the logging in the log cleaner manager. It does not log anymore when a partition is deleted as it is not a useful information.

Reviewers:  Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>
","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/main/scala/kafka/log/LogCleanerManager.scala', 'core/src/main/scala/kafka/log/LogManager.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/log/LogManagerTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala']","StopReplicaRequest encounters performance issues with large number of partitions to be deleted, taking significantly longer to update and reuse checkpoint files. Unnecessary logging in the log cleaner manager also detected."
0b464419e2e4f28444fca653ae5aa8dd7feae9ba,1618935640,"KAFKA-12553: Refactor recovery logic to introduce LogLoader (#10478)

In this PR, I have refactored the recovery logic code introducing a new class kafka.log.LogLoader responsible for all activities related with recovery of log segments from disk. With this change, the recovery logic has been moved out of the Log class and into the new LogLoader class.

Advantages:
This refactor has the following advantages over the existing code:

As such, the recovery logic is invoked once only during Log instantiation. Some parts of the recovery logic are fairly independent from the rest of the Log class. By moving the independent private logic to a separate LogLoader class, the existing Log class has become more modular, and the constructor behavior is a lot simpler now. Therefore, this makes the code more maintainable.
This PR takes us a step closer towards the Log layer reactor work (KAFKA-12554). The Log recovery logic reads and writes to LeaderEpochFileCache and ProducerStateManager instances, so as such the logic does not fit very well into the definition of a ""local log"". By extracting it out of the Log class, in the future this will make it much easier to clearly define the separation of concerns between LocalLog and UnifiedLog.

Reviewers: Satish Duggana <satishd@apache.org>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/main/scala/kafka/log/LogConfig.scala', 'core/src/main/scala/kafka/log/LogLoader.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerTest.scala', 'core/src/test/scala/unit/kafka/log/LogLoaderTest.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala', 'core/src/test/scala/unit/kafka/log/LogTestUtils.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/utils/SchedulerTest.scala']","The current log recovery logic in Kafka is intertwined with the Log class, leading to complex constructor behavior, making it less modular, and difficult to maintain. The logic is invoked more than once during Log instantiation, contrasting with its reasonable independence from the rest of the class operations.
"
0c01ab67a034a9454fdd1e0c791c56c5466d9ff4,1639162353,"MINOR; Update merge script to work against Python3

Made the following changes so that it works in Python3:

1. Use `print()` instead of `print`
2. Use urllib instead of urllib2
3. Decode all of the cmd outputs from binary to string using UTF-8
4. Use `input` instead of `raw_input`
5. Use `next()` on the iterator returned by `filter()` instead `__get_item__()`
6. Fix any error returned by `pylint`.

Author: José Armando García Sancio <jsancio@gmail.com>

Reviewers: David Jacot <djacot@confluent.io>

Closes #11536 from jsancio/minor-merge-pr-script
",['kafka-merge-pr.py'],Merge script is not compatible with Python3 causing run-time errors and failures.
2f71708955b293658cec3b27e9a5588d39c38d7e,1687968037,"KAFKA-15028: AddPartitionsToTxnManager metrics (#13798)

Adding the following metrics as per kip-890:

VerificationTimeMs – number of milliseconds from adding partition info to the manager to the time the response is sent. This will include the round trip to the transaction coordinator if it is called. This will also account for verifications that fail before the coordinator is called.

VerificationFailureRate – rate of verifications that returned in failure either from the AddPartitionsToTxn response or through errors in the manager.

AddPartitionsToTxnVerification metrics – separating the verification request metrics from the typical add partitions ones similar to how fetch replication and fetch consumer metrics are separated.

Reviewers: Divij Vaidya <diviv@amazon.com>","['clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java', 'core/src/main/scala/kafka/network/RequestChannel.scala', 'core/src/main/scala/kafka/server/AddPartitionsToTxnManager.scala', 'core/src/test/scala/unit/kafka/server/AddPartitionsToTxnManagerTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala']","Current implementation lacks necessary diagnostic metrics for AddPartitionsToTxnManager like VerificationTimeMs, VerificationFailureRate and AddPartionsToTxnVerification. This hinders efficient monitoring and troubleshooting."
c32d2338a7e0079e539b74eb16f0095380a1ce85,1696420724,"KAFKA-10199: Enable state updater by default (#13927)

Now that the implementation for the state updater is done, we can enable it by default.

This PR enables the state updater by default and fixes code that made assumptions that are not true when the state updater is enabled (mainly tests).

Reviewers: Lucas Brutschy <lbrutschy@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ReadOnlyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskExecutor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EosV2UpgradeIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/NamedTopologyIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/PurgeRepartitionTopicIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ReadOnlyTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskExecutorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","State updater remains disabled by default causing dependent parts of the code, especially test cases, to make incorrect assumptions."
e14dd8024adae746c50c8b7d9cd268e859669576,1681961367,"KAFKA-14821 Implement the listOffsets API with AdminApiDriver (#13432)

We are handling complex workflows ListOffsets by chaining together MetadataCall instances and ListOffsetsCall instances, there are many complex and error-prone logic. In this PR we rewrote it with the `AdminApiDriver` infra, notable changes better than old logic:
1. Retry lookup stage on receiving `NOT_LEADER_OR_FOLLOWER` and `LEADER_NOT_AVAILABLE`, whereas in the past we failed the partition directly without retry.
2. Removing class field `supportsMaxTimestamp` and calculating it on the fly to avoid the mutable state, this won't change any behavior of  the client.
3. Retry fulfillment stage on `RetriableException`, whereas in the past we just retry fulfillment stage on `InvalidMetadataException`, this means we will retry on `TimeoutException` and other `RetriableException`.

We also `handleUnsupportedVersionException` to `AdminApiHandler` and `AdminApiLookupStrategy`, they are used to keep consistency with old logic, and we can continue improvise them. 

Reviewers: Ziming Deng <dengziming1993@gmail.com>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiDriver.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiHandler.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiLookupStrategy.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/ListOffsetsHandler.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/MetadataOperationContext.java', 'clients/src/test/java/org/apache/kafka/clients/admin/AdminClientTestUtils.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java', 'clients/src/test/java/org/apache/kafka/clients/admin/internals/AdminApiDriverTest.java', 'clients/src/test/java/org/apache/kafka/clients/admin/internals/ListOffsetsHandlerTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/admin/GetListOffsetsCallsBenchmark.java']","Complex workflows in ListOffsets, implemented by chaining MetadataCall and ListOffsetsCall instances, lead to complex and error-prone logic, failing partitions directly without retry on receiving 'NOT_LEADER_OR_FOLLOWER' and 'LEADER_NOT_AVAILABLE' exceptions.
"
62431dca700fb2c7c3afe1a7c9eb07fe336f9b04,1677282158,"KAFKA-14468: Implement CommitRequestManager to manage the commit and autocommit requests (#13021)

This pull request introduces a CommitRequestManager to efficiently manage commit requests from clients and the autocommit state. The manager utilizes a ""staged"" commit queue to store commit requests made by clients. A background thread regularly polls the CommitRequestManager, which then checks the queue for any outstanding commit requests. When permitted, the CommitRequestManager generates a PollResult which contains a list of UnsentRequests that are subsequently processed by the NetworkClientDelegate.

In addition, a RequestManagerRegistry has been implemented to hold all request managers, including the new CommitRequestManager and the CoordinatorRequestManager. The registry is regularly polled by a background thread in each event loop, ensuring that all request managers are kept up to date and able to handle incoming requests

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/DefaultBackgroundThread.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/GroupState.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/NetworkClientDelegate.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/PrototypeAsyncConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestManager.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEvent.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEventProcessor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/CommitApplicationEvent.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/PollApplicationEvent.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/CommitRequestManagerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/DefaultBackgroundThreadTest.java']",Inability to efficiently manage commit requests from clients and autocommit state resulting in suboptimal handling of commit requests and event loops.
bab3e082dc48bc3db68692694bd114a39b41fa68,1568695705,"KAFKA-8859: Expose built-in streams metrics version in `StreamsMetricsImpl` (#7323)

The streams config built.in.metrics.version is needed to add metrics in
a backward-compatible way. However, not in every location where metrics are
added a streams config is available to check built.in.metrics.version. Thus,
the config value needs to be exposed through the StreamsMetricsImpl object.

Reviewers: John Roesler <vvcephei@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/MockStreamsMetrics.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImplTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/GlobalStateStoreProviderTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/processor/MockProcessorContext.java']",Backwards-compatibility issues arise when adding metrics due to the unavailability of a streams config to check the built.in.metrics.version in every location.
dcabc295ec86b61c6d1420c5c54626eb4514bd93,1690548588,"KAFKA-14048; CoordinatorContext should be protected by a lock (#14090)

Accessing the `CoordinatorContext` in the `CoordinatorRuntime` should be protected by a lock. The runtime guarantees that the context is never access concurrently however it is accessed by multiple threads. The lock is here to ensure that we have a proper memory barrier. The patch does the following:
1) Adds a lock to `CoordinatorContext`;
2) Adds helper methods to get the context and acquire/release the lock.
3) Allow transition from Failed to Loading. Previously, the context was recreated in this case.

Reviewers: Justine Olshan <jolshan@confluent.io>",['group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java'],`CoordinatorContext` in `CoordinatorRuntime` is potentially accessed by multiple threads simultaneously causing issues due to lack of proper memory barrier. Failed to Loading transition attempts to recreate context resulting in problems.
cdd5ef3a5063a83f67f49b51fadffa524ab9cbaf,1583777115,"MINOR: Remove throttling logic from RecordAccumulator (#7195)

This is redundant since `Sender` and `NetworkClient` handle throttling. It's
also confusing since the `RecordAccumulator` logic only applies when
`max.in.flight.requests.per.connection=1`.

In `Sender.sendProducerData`, the following code handles throttling:

```java
while (iter.hasNext()) {
    Node node = iter.next();
    if (!this.client.ready(node, now)) {
        iter.remove();
        notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));
    }
}
```

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java']",Redundant throttling logic in `RecordAccumulator` causing confusion when `max.in.flight.requests.per.connection=1` due to conflicting handling in `Sender` and `NetworkClient`.
7f6bf95c1e4875b1042746e3d73240496073f081,1551946043,"Fix for KAFKA-7974: Avoid zombie AdminClient when node host isn't resolvable (#6305)

* Fix for KAFKA-7974: Avoid calling disconnect() when not connecting

* Resolve host only when currentAddress() is called

Moves away from automatically resolving the host when the connection entry is constructed, which can leave ClusterConnectionStates in a confused state.
Instead, resolution is done on demand, ensuring that the entry in the connection list is present even if the resolution failed.

* Add Javadoc to ClusterConnectionStates.connecting()","['clients/src/main/java/org/apache/kafka/clients/ClusterConnectionStates.java', 'clients/src/main/java/org/apache/kafka/clients/NetworkClient.java', 'clients/src/test/java/org/apache/kafka/clients/ClusterConnectionStatesTest.java', 'clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java']","AdminClient becomes unresponsive (""zombie"") when the node host is unresolvable, and host resolution during connection entry construction can cause confusion in ClusterConnectionStates."
440bed2391338dc10fe4d36ab17dc104b61b85e8,1683871397,"MINOR:code optimization in QuorumController (#13697)

1. add hint in switch item ""BROKER_LOGGER"" in ConfigResourceExistenceChecker, otherwise, it will be classified as default break and deleted directly. I don’t know if adding hint is better than deleting directly.
2. delete some unused variables and methods.
3. add the ""@test"" mark to a method in unit test that is forgotten.

Reviewers: dengziming <dengziming1993@gmail.com>","['metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java']","Unmarked unit test methods, extraneous variables and methods in the QuorumController code, and a misclassified break item in ConfigResourceExistenceChecker require cleaning up."
dcfb641add650073f46fe8d8370c72f0284e62d7,1583801324,"KAFKA-9176: Do not update limit offset if we are in RESTORE_ACTIVE mode (#8235)

Previously we may be updating the standby's limit offset as committed offsets to those source changelogs, and then inside the inner method we check if the state is in RESTORE_ACTIVE or not, which is a bug.

We should, instead, just check on the caller that we can skip restoring if:

1. we are in RESTORE_ACTIVE mode.
2. there're no source changelog partitions.
3. those partitions do not have any buffered records.

Also updated the unit test for this coverage.

Reviewers: Boyang Chen <boyang@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java']","In RESTORE_ACTIVE mode, limit offset updates for standbys may be improperly assigned to committed offsets for source changelogs, even when there are no source changelog partitions or they do not have any buffered records."
c5df2082811af3cd3d7bc20974dddd5780a5836e,1572039645,"KAFKA-9105; Add back truncateHead method to ProducerStateManager (#7599)

The truncateHead method was removed from ProducerStateManager by github.com/apache/kafka/commit/c49775b. This meant that snapshots were no longer removed when the log start offset increased, even though the intent of that change was to remove snapshots but preserve the in-memory mapping. This patch adds the required functionality back.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/ProducerStateManager.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']",The removal of truncateHead method in ProducerStateManager is causing snapshots to not be removed as intended when log start offset increases.
2604d39b9fbad209be6e89d395dd4c97582d9c52,1574350351,"MINOR: Fix Streams EOS system tests by adding clean-up of state dir (#7693)

Recently, system tests test_rebalance_[simple|complex] failed
repeatedly with a verfication error. The cause was most probably
the missing clean-up of a state directory of one of the processors.

A node is cleaned up when a service on that node is started and when
a test is torn down.

If the clean-up flag clean_node_enabled of a EOS Streams service is
unset, the clean-up of the node is skipped.

The clean-up flag of processor1 in the EOS tests should stay set before
its first start, so that the node is cleaned before the service is started.
Afterwards for the multiple restarts of processor1 the cleans-up flag should
be unset to re-use the local state.

After the multiple restarts are done, the clean-up flag of processor1 should
again be set to trigger node clean-up during the test teardown.

A dirty node can lead to test failures when tests from Streams EOS tests are
scheduled on the same node, because the state store would not start empty
since it reads the local state that was not cleaned up.

Reviewers: Matthias J. Sax <mjsax@apache.org>, Andrew Choi <andchoi@linkedin.com>, Bill Bejeck <bbejeck@gmail.com>",['tests/kafkatest/tests/streams/streams_eos_test.py'],The absence of state directory clean-up after system tests 'test_rebalance_[simple|complex]' is likely causing a verification error due to reusing non-empty local state stores in subsequent Streams EOS tests.
80635e4255561882509493af2c6a6faebd54da35,1631041563,"KAFKA-13262: Remove final from `MockConsumer.close()` and delegate implementation (#11307)

I added the final via 2f3600198722 to catch overriding mistakes
since the implementation was moved from the deprecated and
overloaded `close` with two parameters to the no-arg
`close`.

I didn't realize then that `MockConsumer` is a public
API (seems like a bit of a mistake since we tweak the
implementation and sometimes adds methods without a KIP).

Given that this is a public API, I have also moved the implementation
of `close` to the one arg overload. This makes it easier for a
subclass to have specific overriding behavior depending on the
timeout.

Reviewers: David Jacot <djacot@confluent.io>",['clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java'],"`MockConsumer.close()` being marked as final is restricting custom overriding behaviours, specifically when dealing with method timeout configurations."
a47dae4622a25d7ca094c32ef36cf234a6c61cca,1643621651,"MINOR: ConsoleConsumer should not always exit when Consumer::poll returns an empty record batch (#11718)

With https://github.com/apache/kafka/commit/ddb6959c6272d2039ed8c9f595634c3c9573f85e, `Consumer::poll` will return an empty record batch when position advances due to aborted transactions or control records. This makes the `ConsoleConsumer` exists because it assumes that `poll` returns due to the timeout being reached. This patch fixes this by explicitly tracking the timeout.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/tools/ConsoleConsumer.scala', 'core/src/test/scala/unit/kafka/tools/ConsoleConsumerTest.scala']","`ConsoleConsumer` exits unexpectedly when `Consumer::poll` returns an empty record batch due to advanced position from aborted transactions or control records, instead of a timeout situation."
1f9aa01a5b3b59d90499a059d719af03483d5130,1550900968," KAFKA-7672 : force write checkpoint during StreamTask #suspend (#6115)

This fix is aiming for #2 issue pointed out within https://issues.apache.org/jira/browse/KAFKA-7672
In the current setup, we do offset checkpoint file write when EOS is turned on during #suspend, which introduces the potential race condition during StateManager #closeSuspend call. To mitigate the problem, we attempt to always write checkpoint file in #suspend call.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerStub.java', 'streams/src/test/java/org/apache/kafka/test/GlobalStateManagerStub.java']",Current offset checkpoint file writing during StreamTask #suspend introduces potential race condition in StateManager #closeSuspend call when EOS is turned on.
8cf781ef0196995ef99d875612b6b7e9adcb912b,1585194822,"MINOR: Improve performance of checkpointHighWatermarks, patch 1/2 (#6741)

This PR works to improve high watermark checkpointing performance.

`ReplicaManager.checkpointHighWatermarks()` was found to be a major contributor to GC pressure, especially on Kafka clusters with high partition counts and low throughput.

Added a JMH benchmark for `checkpointHighWatermarks` which establishes a
performance baseline. The parameterized benchmark was run with 100, 1000 and
2000 topics. 

Modified `ReplicaManager.checkpointHighWatermarks()` to avoid extra copies and cached
the Log parent directory Sting to avoid frequent allocations when calculating
`File.getParent()`.

A few clean-ups:
* Changed all usages of Log.dir.getParent to Log.parentDir and Log.dir.getParentFile to
Log.parentDirFile.
* Only expose public accessor for `Log.dir` (consistent with `Log.parentDir`)
* Removed unused parameters in `Partition.makeLeader`, `Partition.makeFollower` and `Partition.createLogIfNotExists`.

Benchmark results:

| Topic Count | Ops/ms | MB/sec allocated |
|-------------|---------|------------------|
| 100               | + 51%    |  - 91% |
| 1000             | + 143% |  - 49% |
| 2000            | + 149% |   - 50% |

Reviewers: Lucas Bradstreet <lucas@confluent.io>. Ismael Juma <ismael@juma.me.uk>

Co-authored-by: Gardner Vickers <gardner@vickers.me>
Co-authored-by: Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/main/scala/kafka/log/LogCleanerManager.scala', 'core/src/main/scala/kafka/log/LogManager.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/cluster/AssignmentStateTest.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/HighwatermarkCheckpointBench.java']","High watermark checkpointing in `ReplicaManager.checkpointHighWatermarks()` is causing significant GC pressure, especially in Kafka clusters with high partition counts and low throughput."
4c85171a1f1219bb735be3ca034640e298fda0dc,1557597937,"KAFKA-7633: Allow Kafka Connect to access internal topics without cluster ACLs (#5918)

When Kafka Connect does not have cluster ACLs to create topics,
it fails to even access its internal topics which already exist.
This was originally fixed in KAFKA-6250 by ignoring the cluster
authorization error, but now Kafka 2.0 returns a different response
code that corresponds to a different error. Add a patch to ignore this
new error as well.

Reviewers: Jason Gustafson <jason@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java']","Kafka Connect fails to access existing internal topics due to lack of cluster ACLs for topic creation, with Kafka 2.0 returning a new error response code."
246a8afb63313907921407d92f9243cfcb4bce66,1627340010,"MINOR: factor state checks into descriptive methods and clarify javadocs (#11123)

Just a bit of minor cleanup that (a) does some prepwork for another PR I'm working on, (b) updates the javadocs & exception messages to report a more useful error to the user and describe what they actually need to do, and (c) hopefully makes these state checks more future-proof by defining methods for each kind of check in one place that can be easily updated instead of tracking down every individual check.

Reviewers: Walker Carlson <wcarlson@confluent.io>, Luke Chen <showuon@gmail.com>",['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java'],"State checks distributed throughout the codebase may lead to tracking difficulties and potential inconsistency, along with unclear javadocs & exception messages for users regarding errors and required actions."
317089663cc7ff4fdfcba6ee434f455e8ae13acd,1571378914,"KAFKA-8962; Use least loaded node for AdminClient#describeTopics (#7421)

Allow routing of `AdminClient#describeTopics` to any broker in the cluster than just the controller, so that we don't create a hotspot for this API call. `AdminClient#describeTopics` uses the broker's metadata cache which is asynchronously maintained, so routing to brokers other than the controller is not expected to have a significant difference in terms of metadata consistency; all metadata requests are eventually consistent.

This patch also fixes a few flaky test failures.

Reviewers: Ismael Juma <ismael@juma.me.uk>, José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala', 'core/src/test/scala/unit/kafka/admin/LeaderElectionCommandTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","`AdminClient#describeTopics` is creating a hotspot by only routing to the controller, potentially impacting metadata consistency and causing test failures."
22bcd9fac3c988c15862d0b6c01930814b676253,1598315869,"KAFKA-10054: KIP-613, add TRACE-level e2e latency metrics (#9094)

Adds avg, min, and max e2e latency metrics at the new TRACE level. Also adds the missing avg task-level metric at the INFO level.

I think where we left off with the KIP, the TRACE-level metrics were still defined to be ""stateful-processor-level"". I realized this doesn't really make sense and would be pretty much impossible to define given the DFS processing approach of Streams, and felt that store-level metrics made more sense to begin with. I haven't updated the KIP yet so I could get some initial feedback on this

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java', 'clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java', 'clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java', 'clients/src/test/java/org/apache/kafka/common/metrics/SensorTest.java', 'streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java', 'streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImplTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java']","The Kafka processing stream lacks effective TRACE-level e2e latency metrics along with a missing average task-level metric at the INFO level, limiting comprehensive performance monitoring."
922a95a18d6cf7ead03642bd422409fe922cccfc,1582694748,"KAFKA-9594: Add a separate lock to pause the follower log append while checking if the log dir could be replaced.

This PR adds new lock is used to prevent the follower replica from being updated while ReplicaAlterDirThread is executing maybeReplaceCurrentWithFutureReplica() to replace follower replica with the future replica.

Now doAppendRecordsToFollowerOrFutureReplica() doesn't need to hold the lock on leaderIsrUpdateLock for local replica updation and ongoing log appends on the follower will not delay the makeFollower() call.

**Benchmark results for Partition.makeFollower() **
Old:
```
Benchmark                                        Mode  Cnt     Score    Error  Units
PartitionMakeFollowerBenchmark.testMakeFollower  avgt    15  2046.967 ? 22.842  ns/op
```

New:
```
Benchmark                                        Mode  Cnt     Score   Error  Units
PartitionMakeFollowerBenchmark.testMakeFollower  avgt    15  1278.525 ? 5.354  ns/op
```

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #8153 from omkreddy/KAFKA-9594-LAISR
","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java']","Follower replica updates can occur simultaneously with ReplicaAlterDirThread's execution of maybeReplaceCurrentWithFutureReplica(), potentially complicating the replacement of the follower replica with the future replica."
aae8b5f54f716312d413a76fb4228e68e6df7df2,1594086624,"KAFKA-9930: Adjust ReplicaFetcherThread logging when processing UNKNOWN_TOPIC_OR_PARTITION error (#8579)

Log it as a warning and without a stacktrace (instead of error with stacktrace). This error can be seen in the
following cases:

 * Topic creation, a follower broker of a new partition starts replica fetcher before the prospective leader broker
of the new partition receives the leadership information from the controller (see KAFKA-6221).
 * Topic deletion, a follower broker of a to-be-deleted partition starts replica fetcher after the leader broker of the
to-be-deleted partition processes the deletion information from the controller.
 
As expected, clusters with frequent topic creation and deletion report UnknownTopicOrPartitionException with
relatively higher frequency.

Despite typically being a transient issue, UnknownTopicOrPartitionException may also indicate real issues if it 
doesn't fix itself after a short period of time. To ensure detection of such scenarios, we set the log level to warn
instead of info.

Reviewers: Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",['core/src/main/scala/kafka/server/AbstractFetcherThread.scala'],"ReplicaFetcherThread logs UNKNOWN_TOPIC_OR_PARTITION errors incorrectly, causing potential confusion in cases of topic creation and deletion, may also mask real issues if not transient."
d9253fed5c022c53a43e986c7d738ca25ef7d92b,1689371040,"MINOR Improve logging during the ZK to KRaft migration (#14008)

* Adds an exponential backoff to 1m while the controller is waiting for brokers to show up
* Increases one-time logs to INFO
* Adds a summary of the migration records
* Use RecordRedactor for summary of migration batches (TRACE only)

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/server/ControllerServer.scala', 'core/src/test/scala/integration/kafka/server/KafkaServerKRaftRegistrationTest.scala', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationManifest.java', 'metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java']","During the ZooKeeper to KRaft migration, the logging system lacks detailed information and does not handle waits optimally when waiting for brokers to appear."
9aaa32b64d70646c429cd657980b330f7b85d542,1552329826,"KAFKA-8091; Wait for processor shutdown before testing removed listeners (#6425)

DynamicBrokerReconfigurationTest.testAddRemoveSaslListeners removes a listener, waits for the config to be propagated to all brokers and then validates that connections to the removed listener fail. But there is a small timing window between config update and Processor shutdown. Before validating that connections to a removed listener fail, this commit waits for all metrics of the removed listener to be deleted, ensuring that the Processors of the listener have been shutdown.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",['core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala'],"DynamicBrokerReconfigurationTest.testAddRemoveSaslListeners does not account for timing window between config update and Processor shutdown, leading to inconsistent validation of connection failures to a removed listener."
074a3dacca27f3ee313b5185adcf83cfa2c6605d,1631913151,"MINOR: Make ReplicaManager, LogManager, KafkaApis easier to construct (#11320)

The ReplicaManager, LogManager, and KafkaApis class all have many
constructor parameters. It is often difficult to add or remove a
parameter, since there are so many locations that need to be updated. In
order to address this problem, we should use named parameters when
constructing these objects from Scala code. This will make it easy to
add new optional parameters without modifying many test cases.  It will
also make it easier to read git diffs and PRs, since the parameters will
have names next to them. Since Java does not support named paramters,
this PR adds several Builder classes which can be used to achieve the
same effect.

ReplicaManager also had a secondary constructor, which this PR removes.
The function of the secondary constructor was just to provide some
default parameters for the main constructor. However, it is simpler just
to actually use default parameters.

Reviewers: David Arthur <mumrah@gmail.com>","['core/src/main/java/kafka/server/builders/KafkaApisBuilder.java', 'core/src/main/java/kafka/server/builders/LogManagerBuilder.java', 'core/src/main/java/kafka/server/builders/ReplicaManagerBuilder.java', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/log/LogLoaderTest.scala', 'core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala', 'core/src/test/scala/unit/kafka/server/IsrExpirationTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/metadata/MetadataRequestBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/CheckpointBench.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/PartitionCreationBench.java']","Adding or removing constructor parameters in ReplicaManager, LogManager, and KafkaApis is cumbersome due to the many dependent locations that also need updating. Difficult to read PRs and git diffs due to lack of named parameters."
e8ce93bd5310229ffd49cc42af49ee3a4368d014,1626710746,"KAFKA-9555 Added default RLMM implementation based on internal topic storage. (#10579)

KAFKA-9555 Added default RLMM implementation based on internal topic storage.

This is the initial version of the default RLMM implementation.
This includes changes containing default RLMM configs, RLMM implementation, producer/consumer managers.
Introduced TopicBasedRemoteLogMetadataManagerHarness which takes care of bringing up a Kafka cluster and create remote log metadata topic and initializes TopicBasedRemoteLogMetadataManager.
Refactored existing RemoteLogMetadataCacheTest to RemoteLogSegmentLifecycleTest to have parameterized tests to run both RemoteLogMetadataCache and also TopicBasedRemoteLogMetadataManager.
Refactored existing InmemoryRemoteLogMetadataManagerTest, RemoteLogMetadataManagerTest to have parameterized tests to run both InmemoryRemoteLogMetadataManager and also TopicBasedRemoteLogMetadataManager.

This is part of tiered storage KIP-405 efforts.

Reviewers: Kowshik Prakasam <kprakasam@confluent.io>, Cong Ding <cong@ccding.com>, Jun Rao <junrao@gmail.com>","['storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadata.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadata.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadataUpdate.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/ConsumerManager.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/ConsumerTask.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/ProducerManager.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataTopicPartitioner.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataEventHandler.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataStore.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/TopicBasedRemoteLogMetadataManager.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/TopicBasedRemoteLogMetadataManagerConfig.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataCacheTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataSerdeTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogSegmentLifecycleManager.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogSegmentLifecycleTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/TopicBasedRemoteLogMetadataManagerConfigTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/TopicBasedRemoteLogMetadataManagerHarness.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/TopicBasedRemoteLogMetadataManagerTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/TopicBasedRemoteLogMetadataManagerWrapperWithHarness.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataManagerTest.java']","Kafka lack of a default RLMM (Remote Log Metadata Manager) implementation causes an inconsistent system for managing remote log metadata. Current issues persist with RLMM configs, their usage and struggling Kafka cluster with improper log metadata topic initializations."
2accf14ccf9b1f96c9dd8cfb94530c56378fae80,1575467549,"KAFKA-9265: Fix kafka.log.Log instance leak on log deletion (#7773)

KAFKA-8448 fixes problem with similar leak. The Log objects are being
held in ScheduledExecutor PeriodicProducerExpirationCheck callback. The
fix in KAFKA-8448 was to change the policy of ScheduledExecutor to
remove the scheduled task when it gets canceled (by calling
setRemoveOnCancelPolicy(true)).

This works when a log is closed using close() method. But when a log is
deleted either when the topic gets deleted or when the rebalancing
operation moves the replica away from broker, the delete() operation is
invoked. Log.delete() doesn't close the pending scheduled task and that
leaks Log instance.

Fix is to close the scheduled task in the Log.delete() method too.

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/utils/KafkaScheduler.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']","Log instance leaks occur upon log deletion due to scheduled tasks not being closed during delete operations, specifically when a log is deleted due to a topic deletion or rebalancing operation."
4da515da94623e54cab359f1b3df9dae8b4ab7a7,1644515714,"MINOR: Fix storage meta properties comparison (#11546)

This patch adds missing `equals` and `hashCode` implements for `RawMetaProperties`. This is relied on by the storage tool for detecting when two log directories have different `meta.properties` files. 

Reproduce current issue:

```shell
$ sed -i 's|log.dirs=/tmp/kraft-combined-logs|+log.dirs=/tmp/kraft-combined-logs,/tmp/kraft-combined-logs2' ./config/kraft/server.properties

$ ./bin/kafka-storage.sh format -t R19xNyxMQvqQRGlkGDi2cg -c ./config/kraft/server.properties
Formatting /tmp/kraft-combined-logs
Formatting /tmp/kraft-combined-logs2

$ ./bin/kafka-storage.sh info -c ./config/kraft/server.properties
Found log directories:
  /tmp/kraft-combined-logs
  /tmp/kraft-combined-logs2

Found metadata: {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1}

Found problem:
  Metadata for /tmp/kraft-combined-logs2/meta.properties was {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1}, but other directories featured {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1}
```

It's reporting that same metadata are not the same...

With this fix:

```shell
$ ./bin/kafka-storage.sh info -c ./config/kraft/server.properties
Found log directories:
  /tmp/kraft-combined-logs
  /tmp/kraft-combined-logs2

Found metadata: {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1}
```

Reviewers: Igor Soarez <soarez@apple.com>, Jason Gustafson <jason@confluent.io>",['core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala'],The storage tool incorrectly reports differing `meta.properties` files between log directories when they have the same metadata.
c1bb307a361b7b6e17261cc84ea2b108bacca84d,1666976933,"KAFKA-14337; Correctly remove topicsWithCollisionChars after topic deletion (#12790)

In https://github.com/apache/kafka/pull/11910 , we added a feature to prevent topics with conflicting metrics names from being created. We added a map to store the normalized topic name to the topic names, but we didn't remove it correctly while deleting topics. This PR fixes this bug and add a test.

Reviewers: Igor Soarez <i@soarez.me>, dengziming <dengziming1993@gmail.com>, Jason Gustafson <jason@confluent.io>","['core/src/test/scala/integration/kafka/admin/TopicCommandIntegrationTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java']","The deletion of Kafka topics doesn't correctly remove associated entries from the map storing topic name to normalized name mappings, leading to possible topic name conflicts."
6d486eddb5adc1e00d0fbf5b6e690df56628fbd0,1577576585,"KAFKA-9202: serde in ConsoleConsumer with access to headers (#7736)

The Deserializer interface has two methods, one that gives access to the headers and one that does not. ConsoleConsumer.scala only calls the latter method. It would be nice if it were to call the default method that provides header access, so that custom serde that depends on headers becomes possible.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['core/src/main/scala/kafka/tools/ConsoleConsumer.scala', 'core/src/test/scala/kafka/tools/CustomDeserializerTest.scala']","ConsoleConsumer.scala only calls the Deserializer interface method without header access, preventing custom serde that depend on headers."
70d1bb40d958918ac691ba906a35c4a158815d78,1569428583,"KAFKA-7273: Extend Connect Converter to support headers (#6362)

Implemented KIP-440 to allow Connect converters to use record headers when serializing or deserializing keys and values. This change is backward compatible in that the new methods default to calling the older existing methods, so existing Converter implementations need not be changed. This changes the WorkerSinkTask and WorkerSourceTask to use the new converter methods, but Connect's existing Converter implementations and the use of converters for internal topics are intentionally not modified. Added unit tests.

Author: Yaroslav Tkachenko <sapiensy@gmail.com>
Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Randall Hauch <rhauch@gmail.com>","['connect/api/src/main/java/org/apache/kafka/connect/storage/Converter.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TestConverterWithHeaders.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java']",Connect Converters currently lack support for using record headers when serializing or deserializing keys and values. This is causing issues for tasks utilizing these converters.
6fbe4d85a22872b97ec45614cc219d3a1da42f20,1678234559,"KAFKA-14761 Adding integration test for the prototype consumer (#13303)

The goal of this PR is to add more tests to the PrototypeAsyncConsumer to test

* Successful startup and shutdown.
* Commit.

I also added integration tests:

* Test commitAsync()
* Test commitSync()
Note that I still need to implement committed() to test if commitSync() has been successfully committed.
Additional things:

Change KafkaConsumer<K, V> to Consumer<K, V> to use different implementations

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/DefaultEventHandler.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/NetworkClientDelegate.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/PrototypeAsyncConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEventProcessor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/EventHandler.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/PrototypeAsyncConsumerTest.java', 'core/src/test/scala/integration/kafka/api/AbstractConsumerTest.scala', 'core/src/test/scala/integration/kafka/api/BaseAsyncConsumerTest.scala', 'core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala', 'core/src/test/scala/integration/kafka/api/IntegrationTestHarness.scala', 'core/src/test/scala/integration/kafka/api/PlaintextConsumerTest.scala']","PrototypeAsyncConsumer lacks sufficient testing for startup, shutdown, and commit operations. There are also no integration tests for commitAsync() and commitSync() functions."
89f331eac3aaeab53a3b36bc437eba5f6213ca91,1558594248,"KAFKA-8229; Reset WorkerSinkTask offset commit interval after task commit (#6579)

Prior to this change, the next commit time advances
_each_ time a commit happens -- including when a commit happens
because it was requested by the `Task`. When a `Task` requests a
commit several times, the clock advances far into the future
which prevents expected periodic commits from happening.

This commit changes the behavior, we reset `nextCommit` relative
to the time of the commit.

Reviewers: Jason Gustafson <jason@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java']","Repeated task commit requests cause an advance in the next commit time, preventing periodic commits from occurring as expected."
ff77b3ad041c1a4c80119f960e1f87c07b9e93dd,1686156494,"KAFKA-14278: Fix InvalidProducerEpochException and InvalidTxnStateException handling in producer clients (#13811)

This PR fixes three issues:

InvalidProducerEpochException was not handled consistently. InvalidProducerEpochException used to be able to be return via both transactional response and produce response, but as of KIP-588 (2.7+), transactional responses should not return InvalidProducerEpochException anymore, only produce responses can. It can happen that older brokers may still return InvalidProducerEpochException for transactional responses; these must be converted to the newer ProducerFencedException. This conversion wasn't done for TxnOffsetCommit (sent to the group coordinator).

InvalidTxnStateException was double-wrapped in KafkaException, whereas other exceptions are usually wrapped only once. Furthermore, InvalidTxnStateException was not handled at all for in AddOffsetsToTxn response, where it should be a possible error as well, according to API documentation.

According to API documentation, UNSUPPORTED_FOR_MESSAGE_FORMAT is not possible for TxnOffsetCommit, but it looks like it is, and it is being handled there, so I updated the API documentation.

Reviewers: Justine Olshan <jolshan@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitResponse.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java']","Inconsistent handling of InvalidProducerEpochException in transactional and produce responses causing inconsistency, InvalidTxnStateException being double-wrapped and not properly handled, and incorrect API documentation for UNSUPPORTED_FOR_MESSAGE_FORMAT error."
c6664e1d081155f6a24ac940fa07c2a061e71cff,1566596072,"MINOR: Move the resetting from revoked to the thread loop (#7243)

Move the error code resetting logic from the onPartitionsRevoked callback into the streamthread directly after we've decided to rejoin the group, since onPartitionsRevoked are not guaranteed to be triggered.

Ran system tests on the originally failed StreamsUpgradeTest 10 times and passed.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Jun Rao <junrao@gmail.com>",['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java'],"Error code resetting logic in onPartitionsRevoked callback not guaranteed to trigger, leading to potential failures in group rejoining operations."
6263197a62389c9dce0210cd9d65a6e601345edc,1696254081,"KAFKA-15326: [9/N] Start and stop executors and cornercases (#14281)

* Implements start and stop of task executors
* Introduce flush operation to keep consumer operations out of the processing threads
* Fixes corner case: handle requested unassignment during shutdown
* Fixes corner case: handle race between voluntary unassignment and requested unassigment
* Fixes corner case: task locking future completes for the empty set
* Fixes corner case: we should not reassign a task with an uncaught exception to a task executor
* Improved logging
* Number of threads controlled from outside, of the TaskManager

Reviewers: Bruno Cadonna <bruno@confluent.io>
","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskExecutor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/TaskExecutor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskExecutorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskManagerTest.java']","Task executors are not properly starting and stopping, and several edge cases (e.g., handling of unassignment requests during shutdown) are not being addressed correctly, creating issues in task reassignment and locking. There's a lack of adequate logging and control over the number of threads from outside the TaskManager."
bbe170af701609180387ad4abbfaa2712936266d,1622050512,"MINOR: deprecate TaskMetadata constructor and add KIP-740 notes to upgrade guide (#10755)

Quick followup to KIP-740 to actually deprecate this constructor, and update the upgrade guide with what we changed in KIP-740. I also noticed the TaskId#parse method had been modified previously, and should be re-added to the public TaskId class. It had no tests, so now it does

Reviewers: Matthias J. Sax <mjsax@confluent.io>, Luke Chen <showuon@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/TaskId.java', 'streams/src/main/java/org/apache/kafka/streams/processor/TaskMetadata.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java']","The TaskMetadata constructor has not been properly deprecated in line with KIP-740, and TaskId#parse method is missing from the public TaskId class, which also lacks sufficient testing."
0b1dc1ca7be3566a4367d7d3aae2c691cf94f48e,1564891246,"KAFKA-6263; Expose metrics for group and transaction metadata loading duration

[JIRA](https://issues.apache.org/jira/browse/KAFKA-6263)

- Add metrics to provide visibility for how long group metadata and transaction metadata take to load in order to understand some inactivity seen in the consumer groups
- Tests include mocking load times by creating a delay after each are loaded and ensuring the measured JMX metric is as it should be

Author: anatasiavela <anastasiavela@berkeley.edu>

Reviewers: Gwen Shapira, Jason Gustafson

Closes #7045 from anatasiavela/KAFKA-6263
","['core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala', 'core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala']","There is no insight into the duration of group metadata and transaction metadata loading, possibly leading to unexplained inactivity in consumer groups."
a05eaaa8f41db9fca86020845a3336acc105ee19,1555530106,"KAFKA-7965; Fix testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup (#6557)

Most of the time, the group coordinator runs on broker 1. Occasionally the group coordinator will be placed on broker 2. If that's the case, the loop starting at line 320 have no chance to check and update `kickedOutConsumerIdx`. A quick fix is to safely do another round of loop to ensure `kickedOutConsumerIdx` always be checked after the last broker restart.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",['core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala'],"In Kafka, the group coordinator occasionally being placed on broker 2 instead of broker 1 leads to `kickedOutConsumerIdx` not being checked and updated after the last broker restart."
6e1723b4834b75bc0e16749afe31e5b690bec698,1617828770,"MINOR Moved tiered storage API classes from clients module to a new storage-api wmodule. (#10489)

Moved tiered storage API classes from clients module to a new storage-api module.
Created storage and storage-api modules. All the remote storage API classes are moved to storage-api module. All the remote storage implementation classes will be added to storage module.

Reviewers: Jun Rao <junrao@gmail.com>","['storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/LogSegmentData.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataManager.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentId.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadata.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadataUpdate.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteMetadata.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteResourceNotFoundException.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageException.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageManager.java']",Tiered storage API classes in the client module causes design complication; requires creation of distinct modules for better management of API classes and implementation classes.
4d169f5a859ac1d4a34134ef5b6def574769aa43,1565801763,"KAFKA-7335; Store clusterId locally to ensure broker joins the right cluster (#7189)

This patch stores `clusterId` in the `meta.properties` file. During startup, the broker checks that it joins the correct cluster and fails fast otherwise.

The `meta.properties' is versioned. I have decided to not bump the version because 1) the clusterId is null anyway if not present in the file; and 2) bumping it means that rolling back to a previous version won't work.

I have refactored the way the metadata is read and written as it was strongly coupled with the brokerId bits. Now, the metadata is read independently during the startup and used to 1) check the clusterId and 2) get or generate the brokerId (as before).

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/common/InconsistentBrokerMetadataException.scala', 'core/src/main/scala/kafka/common/InconsistentClusterIdException.scala', 'core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/test/scala/unit/kafka/server/ServerGenerateClusterIdTest.scala']","There's no method to store and verify the `clusterId` during a broker's startup, resulting in potential misconnection to the wrong cluster."
b5f90daf13b4945305951ca0eecdb454a4dcafc2,1596730315,"KAFKA-10162; Use Token Bucket algorithm for controller mutation quota (KIP-599, Part III) (#9114)

Based on the discussion in #9072, I have put together an alternative way. This one does the following:

Instead of changing the implementation of the Rate to behave like a Token Bucket, it actually use two different metrics: the regular Rate and a new Token Bucket. The latter is used to enforce the quota.
The Token Bucket algorithm uses the rate of the quota as the refill rate for the credits and compute the burst based on the number of samples and their length (# samples * sample length * quota).
The Token Bucket algorithm used can go under zero in order to handle unlimited burst (e.g. create topic with a number of partitions higher than the burst). Throttling kicks in when the number of credits is under zero.
The throttle time is computed as credits under zero / refill rate (or quota).
Only the controller mutation uses it for now.
The remaining number of credits in the bucket is exposed with the tokens metrics per user/clientId.

Reviewers: Anna Povzner <anna@confluent.io>, Jun Rao <junrao@gmail.com>","['clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java', 'clients/src/main/java/org/apache/kafka/common/metrics/internals/MetricsUtils.java', 'clients/src/main/java/org/apache/kafka/common/metrics/stats/Rate.java', 'clients/src/main/java/org/apache/kafka/common/metrics/stats/TokenBucket.java', 'clients/src/test/java/org/apache/kafka/common/metrics/SensorTest.java', 'clients/src/test/java/org/apache/kafka/common/metrics/TokenBucketTest.java', 'core/src/main/scala/kafka/server/ClientQuotaManager.scala', 'core/src/main/scala/kafka/server/ControllerMutationQuotaManager.scala', 'core/src/main/scala/kafka/server/ReplicationQuotaManager.scala', 'core/src/main/scala/kafka/server/SensorAccess.scala', 'core/src/test/scala/unit/kafka/server/ControllerMutationQuotaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/ControllerMutationQuotaTest.scala']","Controller mutation quota implementation leads to unlimited bursts, e.g., creating topics with a number of partitions higher than the burst, causing improper throttling."
7c7d88339bf36a0a4c046f5dfd57e83a35a8e1e1,1589925775,"KAFKA-10010: Should make state store registration idempotent (#8681)

Standby task could also at risk of getting into illegal state when not being closed during HandleLostAll:

1. The standby task was initializing as CREATED state, and task corrupted exception was thrown from registerStateStores
2. The task corrupted exception was caught, and do a non-affected task commit
3. The task commit failed due to task migrated exception
4. The handleLostAll didn't close the standby task, leaving it as CREATED state
5. Next rebalance complete, the same task was assigned back as standby task.
6. Illegal Argument exception caught as state store already registered

Reviewers: A. Sophie Blee-Goldman <ableegoldman@gmail.com>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerUtilTest.java']","'Standby task remains in CREATED state after task migration, causing an Illegal Argument exception to be caught because the state store is already registered.'"
43db8ac86ac3e609d9e2a993444b8f1b22f7693b,1623100386,"KAFKA-12897: KRaft multi-partition placement on single broker (#10823)

#10494 introduced a bug in the KRaft controller where the controller will loop forever in StripedReplicaPlacer trying to identify the racks on which to place partition replicas if there is a single unfenced broker in the cluster and the number of requested partitions in a CREATE_TOPICS request is greater than 1.

This patch refactors out some argument sanity checks and invokes those checks in both RackList and StripedReplicaPlacer, and it adds tests for this as well as the single broker placement issue.

Reviewers: Jun Rao <junrao@gmail.com>","['metadata/src/main/java/org/apache/kafka/controller/StripedReplicaPlacer.java', 'metadata/src/test/java/org/apache/kafka/controller/StripedReplicaPlacerTest.java']","When there is a single unfenced broker in the cluster and the number of requested partitions in a CREATE_TOPICS request is greater than 1, the KRaft controller goes into an infinite loop trying to identify racks for partition replicas placement."
430f9c99012d1585aa544d4dadf449963296c1fd,1648730759,"KAFKA-13772: Partitions are not correctly re-partitioned when the fetcher thread pool is resized (#11953)

Partitions are assigned to fetcher threads based on their hash modulo the number of fetcher threads. When we resize the fetcher thread pool, we basically re-distribute all the partitions based on the new fetcher thread pool size. The issue is that the logic that resizes the fetcher thread pool updates the `fetcherThreadMap` while iterating over it. The `Map` does not give any guarantee in this case - especially when the underlying map is re-hashed - and that led to not iterating over all the fetcher threads during the process and thus in leaving some partitions in the wrong fetcher threads.

Reviewers: Luke Chen <showuon@gmail.com>, David Jacot <djacot@confluent.io>

","['core/src/main/scala/kafka/server/AbstractFetcherManager.scala', 'core/src/main/scala/kafka/server/AbstractFetcherThread.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherManagerTest.scala']",Resizing of the fetcher thread pool causes an incorrect re-partitioning with some partitions remaining in the wrong fetcher threads due to unguaranteed map iteration.
0601fa0935b03318ed08bfc578685fc4155ee948,1674431878,"MINOR: fix flaky integrations tests by using 60s default timeout for startup (#13141)

The timeouts used for starting up Streams and waiting for the RUNNING state are all over the place across our integration tests, with some as low as 15s (which are unsurprisingly rather flaky). We use 60s as the default timeout for other APIs in the IntegrationTestUtils so we should do the same for #startApplicationAndWaitUntilRunning

I also noticed that we have several versions of that exact API in StreamsTestUtils, so I migrated everyone over to the IntegrationTestUtils#startApplicationAndWaitUntilRunning and added a few overloads for ease of use, including one for single KafkaStreams apps and one for using the default timeout

Reviewers: Matthias J. Sax <mjsax@apache.org>","['streams/src/test/java/org/apache/kafka/streams/integration/ConsistencyVectorIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EmitOnChangeIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/InternalTopicIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KafkaStreamsCloseOptionsIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/NamedTopologyIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RangeQueryIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RegexSourceIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StoreQueryIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StreamsUncaughtExceptionHandlerIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/TaskMetadataIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java']","Integration tests are not consistent in startup timeouts for Streams, with some as low as 15s, making them flaky. Multiple versions of the same API exist in StreamsTestUtils."
6cf27c9c771900baf43cc47f9b010dbf7a86fa22,1584816034,"KAFKA-6145: Pt 2.5 Compute overall task lag per client (#8252)

Once we have encoded the offset sums per task for each client, we can compute the overall lag during assign by fetching the end offsets for all changelog and subtracting.

If the listOffsets request fails, we simply return a ""completely sticky"" assignment, ie all active tasks are given to previous owners regardless of balance.

Builds (but does not yet use) the statefulTasksToRankedCandidates map with the ranking:
Rank -1: active running task
Rank 0: standby or restoring task whose overall lag is within acceptableRecoveryLag
Rank 1: tasks whose lag is unknown (eg during version probing)
Rank 1+: all other tasks are ranked according to their actual total lag

Implements: KIP-441
Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>","['clients/src/main/java/org/apache/kafka/common/utils/Utils.java', 'streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignor.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java']","During task assignment in Kafka, the overall lag per client isn't computed leading to potentially less optimal task distribution. Additionally, there's no fail-safe for listOffsets request failures."
3fe2f8c4427123814fdeb8e6f620fb9c79274878,1677118398,"MINOR: after reading BYTES type it's possible to access data beyond its size (#13261)

After reading data of type BYTES, COMPACT_BYTES, NULLABLE_BYTES or COMPACT_NULLABLE_BYTES returned ByteBuffer might have a capacity that is larger than its limit, thus these data types may access data that lies beyond its size by increasing limit of the returned ByteBuffer. I guess this is not very critical but I think it would be good to restrict increasing limit of the returned ByteBuffer by making its capacity strictly equal to its limit. I think someone might unintentionally mishandle these data types and accidentally mess up data in the ByteBuffer from which they were read.

Reviewers: Luke Chen <showuon@gmail.com>","['clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java', 'clients/src/test/java/org/apache/kafka/common/protocol/types/ProtocolSerializationTest.java']","Data types like BYTES, COMPACT_BYTES, NULLABLE_BYTES, or COMPACT_NULLABLE_BYTES might access data beyond their size due to returned ByteBuffer's capacity being larger than its limit, potentially leading to data mishandling."
a900794ace4dcf1f9dadee27fbd8b63979532a18,1690390499,"KAFKA-15196 Additional ZK migration metrics (#14028)

This patch adds several metrics defined in KIP-866:

* MigratingZkBrokerCount: the number of zk brokers registered with KRaft
* ZkWriteDeltaTimeMs: time spent writing MetadataDelta to ZK
* ZkWriteSnapshotTimeMs: time spent writing MetadataImage to ZK
* Adds value 4 for ""ZK"" to ZkMigrationState

Also fixes a typo in the metric name introduced in #14009 (ZKWriteBehindLag -> ZkWriteBehindLag)

Reviewers: Luke Chen <showuon@gmail.com>, Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/controller/KafkaController.scala', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java', 'metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java', 'metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java', 'metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java', 'metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java', 'metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java', 'metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java']","Lack of metrics for Zookeeper to KRaft migration process; no information to capture the number of registered brokers, time spent in writing MetadataDelta, MetadataImage to Zookeeper and inconsistent metric naming.
"
9c9a79b459d2b34218b46da4c54113c500255bbb,1593184649,"KAFKA-9076: support consumer sync across clusters in MM 2.0 (#7577)

In order to make the Kafka consumer and stream application migrate from source to target cluster
transparently and conveniently, e.g. in event of source cluster failure, a background job is proposed
to periodically sync the consumer offsets from the source to target cluster, so that when the
consumer and stream applications switche to the target cluster, they will resume to consume from
where they left off at source cluster.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>, Thiago Pinto, Srinivas Boga","['connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java', 'connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java', 'connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorCheckpointTaskTest.java', 'connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java']","Absence of sync mechanism for Kafka consumer offsets across clusters causes inconvenience during migration from source to target cluster, particularly in event of source cluster failure."
da6871943f911bb38a98bd4551f1faf824dec890,1602731784,"KAFKA-10611: Merge log error to avoid double error (#9407)

When using an error tracking system, two error log messages result into two different alerts.
It's best to group the logs and have one error with all the information.

For example when using with Sentry, this double line of log.error will create 2 different Issues. One can merge the issues but it will be simpler to have a single error log line.

Signed-off-by: Benoit Maggi <benoit.maggi@gmail.com>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Konstantine Karantasis <k.karantasis@gmail.com>",['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java'],"Double error log messages are being generated for the same error, causing issues when using error tracking systems like Sentry, where two separate issues are unnecessarily created."
4ca8e40e2f4ab415657d1e07ee868448802f4565,1552614127,"KAFKA-7502: Cleanup KTable materialization logic in a single place (#6174)

This is a draft cleanup for KAFKA-7502. Here is the details:

* Make KTableKTableJoinNode abstract, and define its child classes ([NonMaterialized,Materialized]KTableKTableJoinNode) instead: now, all materialization-related routines are separated into the other classes.

* KTableKTableJoinNodeBuilder#build now instantiates [NonMaterialized,Materialized]KTableKTableJoinNode classes instead of KTableKTableJoinNode.

Reviewers: Guozhang Wang <wangguoz@gmail.com>,  Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableJoinMerger.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/KTableKTableJoinNode.java']",Inconsistencies and scattered logic in KTable materialization leading to code complexity and maintenance issues.
a448ddbecb5036baafea039325e644e8bf866507,1637144246,"KAFKA-13443: Kafka broker exits when OAuth enabled and certain configuration not specified (#11484)

The sasl.oauthbearer.jwks.endpoint.retry.backoff.ms and sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms configuration options were added to the SaslConfig class but their default values were not added to KafkaConfig. As a result, when the OAuth validation feature is enabled in the broker and those two configuration values aren't explicitly provided by the user, the broker exits. This patch fixes the issue by defining them in the KafkaConfig class.

Reviewers: David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala']","When OAuth is enabled, and certain SASL configurations aren't explicitly provided, the Kafka broker unexpectedly exits."
703e1d9faafbf07795261b3233ab985583f17fcb,1693412364,"KAFKA-15375: fix broken clean shutdown detection logic in LogManager

When running in kraft mode, LogManager.startup is called in a different thread than the main broker (#14239)
startup thread (by BrokerMetadataPublisher when the first metadata update is received.) If a fatal
error happens during broker startup, before LogManager.startup is completed, LogManager.shutdown may
 mark log dirs as clean shutdown improperly.

This PR includes following change:
1. During LogManager startup time:
  - track hadCleanShutdwon info for each log dir
  - track loadLogsCompleted status for each log dir
2. During LogManager shutdown time:
  - do not write clean shutdown marker file for log dirs which have hadCleanShutdown==false and loadLogsCompleted==false

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/log/LogManager.scala', 'core/src/test/scala/unit/kafka/log/LogManagerTest.scala']","In kraft mode, if a fatal error occurred before LogManager.startup completed, improper log directories are marked as clean shutdown."
d915ce58d2adcfd6113f961ecbf337770dbe760b,1620235303,"KAFKA-10847: Set shared outer store to an in-memory store when in-memory stores are supplied (#10613)

When users supply in-memory stores for left/outer joins, then the internal shared outer store must be switch to in-memory store too. This will allow users who want to keep all stores in memory to continue doing so.

Added unit tests to validate topology and left/outer joins work fine with an in-memory shared store.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java', 'streams/src/test/java/org/apache/kafka/streams/TopologyTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamLeftJoinTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamOuterJoinTest.java']","When users supply in-memory stores for left/outer joins, the internal shared outer store does not correspondingly switch to in-memory, disrupting users' intent to keep all stores in memory."
da56c1cf2992a868be02b94120ce22a3978aecd0,1672252448,"MINOR: Use `LogConfig.validate` instead of `validateValues` in `KafkaMetadataLog` (#13051)

`LogConfig.validateValues` may fail or incorrectly succeed if the properties don't include defaults.

During the conversion of `LogConfig` to Java (#13049), it became clear that the `asInstanceOf[Long]`
calls in `LogConfig.validateValues` were converting `null` to `0` when this method was invoked
from `KafkaMetadataLog`. This means that it would be possible for it to validate successfully
in cases where it should not.

Reviewers: José Armando García Sancio <jsancio@apache.org>",['core/src/main/scala/kafka/raft/KafkaMetadataLog.scala'],"`LogConfig.validateValues` incorrectly converts `null` to `0` during validation in `KafkaMetadataLog`, potentially validating properties incorrectly."
9c2e5daf606aebb343224f943b8e6bac4351c193,1695718181,"MINOR: Revert log level changes in LogCaptureAppender (#14436)

LogCaptureAppender sets the log level in various tests to check if a certain log message is produced. The log level is however never reverted, changing the log level across the board and introducing flakiness due to non-determinism since the log level depends on execution order. Some log messages change the timing inside tests significantly.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['clients/src/test/java/org/apache/kafka/common/utils/LogCaptureAppender.java', 'connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorSourceConnectorTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/SourceTaskOffsetCommitterTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","LogCaptureAppender in tests sets the log level which remains unaltered, causing unexpected changes in overall log level and non-deterministic behaviour, potentially affecting timings in tests."
6df058ec150b9c3c8938740ce5ce95d256f4b012,1573232554,"KAFKA-8677: Simplify the best-effort network client poll to never throw exception (#7613)

Within KafkaConsumer.poll, we have an optimization to try to send the next fetch request before returning the data in order to pipelining the fetch requests; however, this pollNoWakeup should NOT throw any exceptions, since at this point the fetch position has been updated. If an exception is thrown and the callers decide to capture and continue, those records would never be returned again, causing data loss.

Also fix the flaky test itself.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java', 'core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala']","In KafkaConsumer.poll, potential exception during next fetch request can cause data loss as the fetch position has already been updated."
12aa595e17c495a1650d8da349c8ef69f787ebe8,1624828519,"KAFKA-12790: Remove SslTransportLayerTest.testUnsupportedTlsVersion (#10922)

Support for TLS 1.0/1.1 was disabled in recent versions of Java 8/11
and all versions of 16 causing this test to fail.

It is possible to make it work by updating the relevant security property,
but it has to be done before the affected classes are loaded and it can
not be disabled after that. Given the low value of the test, we remove
it.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Luke Chen <showuon@gmail.com>",['clients/src/test/java/org/apache/kafka/common/network/SslTransportLayerTest.java'],"The test SslTransportLayerTest.testUnsupportedTlsVersion fails due to disabled support for TLS 1.0/1.1 in certain versions of Java, making it difficult to conduct appropriate testing."
1adf0ee889f6f221bc41da41b25a653db5dcda60,1578437967,"KAFKA-9335: Fix StreamPartitionAssignor regression in repartition topics counts (#7904)

This PR fixes the regression introduced in 2.4 from 2 refactoring PRs:
#7249
#7419

The bug was introduced by having a logical path leading numPartitionsCandidate to be 0, which is assigned to numPartitions and later being checked by setNumPartitions. In the subsequent check we will throw illegal argument if the numPartitions is 0.

This bug is both impacting new 2.4 application and upgrades to 2.4 in certain types of topology. The example in original JIRA was imported as a new integration test to guard against such regression. We also verify that without the bug fix application will still fail by running this integration test.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/test/java/org/apache/kafka/streams/integration/BranchedMultiLevelRepartitionConnectedTopologyTest.java']","A regression in StreamPartitionAssignor is leading to numPartitionsCandidate being 0, causing an illegal argument exception in certain types of topology for new 2.4 application and upgrades to 2.4."
2eeae09ca3e5a9af89e1f15c0d772413ddc489ec,1594847766,"KAFKA-9666; Don't increase producer epoch when trying to fence if the log append fails (#8239)

When fencing producers, we currently blindly bump the epoch by 1 and write an abort marker to the transaction log. If the log is unavailable (for example, because the number of in-sync replicas is less than min.in.sync.replicas), we will roll back the attempted write of the abort marker, but still increment the epoch in the transaction metadata cache. During periods of prolonged log unavailability, producer retires of InitProducerId calls can cause the epoch to be increased to the point of exhaustion, at which point further InitProducerId calls fail because the producer can no longer be fenced. With this patch, we track whenever we have failed to write the bumped epoch, and when that has happened, we don't bump the epoch any further when attempting to fence. This is safe because the in-memory epoch is still causes old producers to be fenced.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionMetadata.scala', 'core/src/test/scala/integration/kafka/api/TransactionsTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorTest.scala']","During prolonged log unavailability, producer epoch gets exhausted due to retries of InitProducerId calls, leading to failure of further InitProducerId calls as producers can no longer be fenced."
f92cc85c9f018f353da69a24d950bc281f721781,1588267990,"KAFKA-9633: Ensure ConfigProviders are closed (#8204)

ConfigProvider extends Closeable, but were not closed in the following contexts:
* AbstractConfig
* WorkerConfigTransformer
* Worker

This commit ensures that ConfigProviders are close in the above contexts. 

It also adds MockFileConfigProvider.assertClosed()
Gradle executes test classes concurrently, so MockFileConfigProvider
can't simply use a static field to hold its closure state.
Instead use a protocol whereby the MockFileConfigProvider is configured
with some unique ket identifying the test which also used when calling
assertClosed().

Reviewers: Konstantine Karantasis <konstantine@confluent.io>","['clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java', 'clients/src/test/java/org/apache/kafka/common/config/AbstractConfigTest.java', 'clients/src/test/java/org/apache/kafka/common/config/provider/MockFileConfigProvider.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java']","ConfigProviders, despite being extend Closeable, aren't getting closed in certain contexts such as AbstractConfig, WorkerConfigTransformer and Worker, which could lead to potential resource leaks.
"
2a41beb0f49f947cfa7dfd99101c8b1ba89842cb,1694674437,"MINOR: Check the existence of AppInfo for the given ID before creating a new mbean of the same name (#14287)

When using kafka consumer in apache pinot , we did see couple of WARN as we are trying to create kafka consumer class with the same name . We currently have to use a added suffix to create a new mBean as each new kafka consumer in same process creates a mBean . Adding support here to skip creation of mBean if its already existing

Reviewers: Satish Duggana <satishd@apache.org>, Luke Chen <showuon@gmail.com>","['clients/src/main/java/org/apache/kafka/common/utils/AppInfoParser.java', 'clients/src/test/java/org/apache/kafka/common/utils/AppInfoParserTest.java']","When using kafka consumer in Apache Pinot, WARN messages occur due to attempts at creating kafka consumer classes with identical names, despite existing mBeans."
9b3db6d50a9bb892032fa0c09692241fe070103a,1685555345,"KAFKA-15019: Improve handling of broker heartbeat timeouts (#13759)

When the active KRaft controller is overloaded, it will not be able to process broker heartbeat
requests. Instead, they will be timed out. When using the default configuration, this will happen
if the time needed to process a broker heartbeat climbs above a second for a sustained period.
This, in turn, could lead to brokers being improperly fenced when they are still alive.

With this PR, timed out heartbeats will still update the lastContactNs and metadataOffset of the
broker in the BrokerHeartbeatManager. While we don't generate any records, this should still be
adequate to prevent spurious fencing. We also log a message at ERROR level so that this condition
will be more obvious.

Other small changes in this PR: fix grammar issue in log4j of BrokerHeartbeatManager. Add JavaDoc
for ClusterControlManager#zkMigrationEnabled field. Add builder for ReplicationControlTestContext
to avoid having tons of constructors. Update ClusterControlManager.DEFAULT_SESSION_TIMEOUT_NS to
match the default in KafkaConfig.

Reviewers: Ismael Juma <ijuma@apache.org>, Ron Dagostino <rdagostino@confluent.io>","['core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/errors/ControllerExceptions.java', 'metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/errors/ControllerExceptionsTest.java']","Overloaded active KRaft controller causes broker heartbeat requests to timeout, potentially leading to improper fencing of live brokers."
ff9e95cb09907739c17b4f4681b11c525515b995,1565724785,"MINOR: Add fetch from follower system test (#7166)

This adds a basic system test that enables rack-aware brokers with the rack-aware replica selector for fetch from followers (KIP-392). The test asserts that the follower was read from at least once and that all the messages that were produced were successfully consumed. 

Reviewers: Jason Gustafson <jason@confluent.io>",['tests/kafkatest/tests/core/fetch_from_follower_test.py'],Rack-aware brokers with rack-aware replica selector for fetch from followers are not being tested for message consumption and reading operations.
6afb0ca735cd5c1a559d496247559d21bcadcb6c,1558764656,"KAFKA-8351; Cleaner should handle transactions spanning multiple segments (#6722)

When cleaning transactional data, we need to keep track of which transactions still have data associated with them so that we do not remove the markers. We had logic to do this, but the state was not being carried over when beginning cleaning for a new set of segments. This could cause the cleaner to incorrectly believe a transaction marker was no longer needed. The fix here carries the transactional state between groups of segments to be cleaned.

Reviewers: Dhruvil Shah <dhruvil@confluent.io>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerTest.scala']","Transactional data cleaning fails to maintain state between different segment groups, leading to potential removal of necessary transaction markers."
586c587b3db0bd622d74a5d4da5734c50c698061,1570212752,"KAFKA-8974: Trim whitespaces in topic names in sink connector configs (#7442)

Trim whitespaces in topic names specified in sink connector configs before subscribing to the consumer. Topic names don't allow whitespace characters, so trimming only will eliminate potential problems and will not place additional limits on topics specified in sink connectors.

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com>
Reviewers: Arjun Satish <arjun@confluent.io>, Randall Hauch <rhauch@gmail.com>
",['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java'],"Whitespaces in topic names specified in sink connector configurations are not eliminated before subscribing to the consumer, leading to potential problems as topic names don't allow whitespace characters."
40ee580ed23c876dec45eca4770e834d9aed76da,1590273343,"KAFKA-9888: Copy connector configs before passing to REST extensions (#8511)

The changes made in KIP-454 involved adding a `connectorConfig` method to the ConnectClusterState interface that REST extensions could use to query the worker for the configuration of a given connector. The implementation for this method returns the Java `Map` that's stored in the worker's view of the config topic (when running in distributed mode). No copying is performed, which causes mutations of that `Map` to persist across invocations of `connectorConfig` and, even worse, propagate to the worker when, e.g., starting a connector.

In this commit the map is copied before it's returned to REST extensions.

An existing unit test is modified to ensure that REST extensions receive a copy of the connector config, not the original.

Reviewers: Nigel Liang <nigel@nigelliang.com>, Konstantine Karantasis <konstantine@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/health/ConnectClusterStateImpl.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/health/ConnectClusterStateImplTest.java']","Invoking `connectorConfig` method in REST extensions can potentially mutate the worker's config topic due to lack of copying, with changes persisting across invocations and propagating to workers."
29a1a16668d76a1cc04ec9e39ea13026f2dce1de,1678209616,"KAFKA-14402: Update AddPartitionsToTxn protocol to batch and handle verifyOnly requests (#13231)

Part 1 of KIP-890

I've updated the API spec and related classes.

Clients should only be able to send up to version 3 requests and that is enforced by using a client builder.

Requests > 4 only require cluster permissions as they are initiated from other brokers. API version 4 is marked as unstable for now.

I've added tests for the batched requests and for the verifyOnly mode.

Also -- minor change to the KafkaApis method to properly match the request name.

Reviewers: Jason Gustafson <jason@confluent.io>, Jeff Kim <jeff.kim@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java', 'clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponse.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java', 'clients/src/test/java/org/apache/kafka/common/message/MessageTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequestTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponseTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/server/AddPartitionsToTxnRequestServerTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","AddPartitionsToTxn protocol not equipped to handle batch requests and verifyOnly mode, and there seems to be a restriction on the client API version, thus requiring an update."
6d7a7859568eb5b25931a3be81cda9f4e0e3ec0d,1632964783,"MINOR: expand logging and improve error message during partition count resolution (#11364)

Recently a user hit this TaskAssignmentException due to a bug in their regex that meant no topics matched the pattern subscription, which in turn meant that it was impossible to resolve the number of partitions of the downstream repartition since there was no upstream topic to get the partition count for. Debugging this was pretty difficult and ultimately came down to stepping through the code line by line, since even with TRACE logging we only got a partial picture.

We should expand the logging to make sure the TRACE logging hits both conditional branches, and improve the error message with a suggestion for what to look for should someone hit this in the future

Reviewers: Walker Carlson <wcarlson@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/RepartitionTopics.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionTopicsTest.java']","Bug in regex does not match any topics in pattern subscription, resulting in an TaskAssignmentException making it impossible to resolve the number of partitions for repartition. Lack of sufficient TRACE logging complicates debugging."
e422a67d3f0a30335a68e50df69690819dd9f511,1668415426,"KAFKA-14294: check whether a transaction is in flight before skipping a commit (#12835)

Add a new #transactionInFlight API to the StreamsProducer to expose the flag of the same name, then check whether there is an open transaction when we determine whether or not to perform a commit in TaskExecutor. This is to avoid unnecessarily dropping out of the group on transaction timeout in the case a transaction was begun outside of regular processing, eg when a punctuator forwards records but there are no newly consumer records and thus no new offsets to commit.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskExecutor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskExecutorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","The Kafka Streams commit process is skipping commits when a transaction is in progress externally, leading to unnecessary group dropouts on transaction timeouts."
df137752542c005c6998c37c03222ffbeca0f349,1682066303,"KAFKA-14828: Remove R/W locks using persistent data structures (#13437)

Currently, StandardAuthorizer uses a R/W lock for maintaining the consistency of data. For the clusters with very high traffic, we will typically see an increase in latencies whenever a write operation comes. The intent of this PR is to get rid of the R/W lock with the help of immutable or persistent collections. Basically, new object references are used to hold the intermediate state of the write operation. After the completion of the operation, the main reference to the cache is changed to point to the new object. Also, for the read operation, the code is changed such that all accesses to the cache for a single read operation are done to a particular cache object only.

In the PR description, you can find the performance of various libraries at the time of both read and write. Read performance is checked with the existing AuthorizerBenchmark. For write performance, a new AuthorizerUpdateBenchmark has been added which evaluates the performance of the addAcl operation.


Reviewers:  Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>,  Divij Vaidya <diviv@amazon.com>","['jmh-benchmarks/src/main/java/org/apache/kafka/jmh/acl/StandardAuthorizerUpdateBenchmark.java', 'metadata/src/main/java/org/apache/kafka/metadata/authorizer/AclCache.java', 'metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizer.java', 'metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerData.java', 'server-common/src/main/java/org/apache/kafka/server/immutable/ImmutableNavigableSet.java', 'server-common/src/main/java/org/apache/kafka/server/immutable/pcollections/PCollectionsImmutableNavigableSet.java', 'server-common/src/main/java/org/apache/kafka/server/immutable/pcollections/PCollectionsImmutableSet.java', 'server-common/src/test/java/org/apache/kafka/server/immutable/DelegationChecker.java', 'server-common/src/test/java/org/apache/kafka/server/immutable/pcollections/PCollectionsImmutableMapTest.java', 'server-common/src/test/java/org/apache/kafka/server/immutable/pcollections/PCollectionsImmutableNavigableSetTest.java', 'server-common/src/test/java/org/apache/kafka/server/immutable/pcollections/PCollectionsImmutableSetTest.java']",High traffic clusters experience an increase in latencies during write operations due to the use of R/W lock for maintaining data consistency in StandardAuthorizer.
6a5946282cbc6b24b185a1a1297110198fd9e97c,1588098821,"KAFKA-9921: disable caching on stores configured to retain duplicates (#8564)

These two options are essentially incompatible, as caching will do nothing to reduce downstream traffic and writes when it has to allow non-unique keys (skipping records where the value is also the same is a separate issue, see KIP-557). But enabling caching on a store that's configured to retain duplicates is actually more than just ineffective, and currently causes incorrect results.

We should just log a warning and disable caching whenever a store is retaining duplicates to avoid introducing a regression. Maybe when 3.0 comes around we should consider throwing an exception instead to alert the user more aggressively.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, John Roesler <john@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/state/Stores.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedWindowStoreBuilder.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/WindowStoreBuilder.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/WindowStoreBuilderTest.java']",Enabling caching on stores configured to retain duplicates leads to incorrect results despite the intention of reducing downstream traffic and writes.
0a5dec0b3a3e1b027230ba766fee0c08b70cc63c,1581536706,"MINOR: Fix unnecessary metadata fetch before group assignment (#8095)

The recent increase in the flakiness of one of the offset reset tests (KAFKA-9538) traces back to https://github.com/apache/kafka/pull/7941. After investigation, we found that following this patch, the consumer was sending an additional metadata request prior to performing the group assignment. This slight timing difference was enough to trigger the test failures. The problem turned out to be due to a bug in `SubscriptionState.groupSubscribe`, which no longer counted the local subscription when determining if there were new topics to fetch metadata for. Hence the extra metadata update. This patch restores the old logic.

Without the fix, we saw 30-50% test failures locally. With it, I could no longer reproduce the failure. However, #6561 is probably still needed to improve the resilience of this test.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerPartitionAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java', 'core/src/test/scala/unit/kafka/admin/ConsumerGroupCommandTest.scala', 'core/src/test/scala/unit/kafka/admin/ResetConsumerGroupOffsetTest.scala']","The Kafka offset reset tests, specifically KAFKA-9538, have become unpredictable due to a triggered additional metadata request prior to performing the group assignment. It appears that the local subscription isn't taken into account when determining new topics for metadata fetch."
39962eeeb3c91287fee49e7dc78383e4f2921503,1676013960,"KAFKA-14513; Add broker side PartitionAssignor interface (#13202)

This patch adds the broker side `PartitionAssignor` interface as detailed in KIP-848. The interfaces differs a bit from the KIP in the following ways:
* The POJOs are not defined within the interface because the interface is to heavy like this.
* The interface is kept in the `group-coordinator` module for now. We don't want to have it out there until KIP-848 is ready to be released. We will move it to its final destination later.

Reviewers: Jeff Kim <jeff.kim@confluent.io>, Justine Olshan <jolshan@confluent.io>, Christo Lolov <lolovc@amazon.com>, Guozhang Wang <wangguoz@gmail.com>","['group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentMemberSpec.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentSpec.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentTopicMetadata.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/GroupAssignment.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/MemberAssignment.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/PartitionAssignor.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/PartitionAssignorException.java']","The current Kafka interface lacks a broker side PartitionAssignor, disrupting coordination as detailed in KIP-848."
4de5f1267164d59c741632c870f2a7aa5153e3a7,1619068324,"KAFKA-12704: Improve cache access during connector class instantiation in config validations (#10580)

Concurrent requests to validate endpoint for the same connector type calls AbstractHerder::getConnector to get the cached connector instances and if the connector hasn't been cached yet then there is a race condition in the AbstractHerder::getConnector method that potentially fails to detect that an instance of the connector has already been created and, as a result, can create another instance

Existing tests are present with enough coverage so no new tests are added.

Reviewers: Chris Egerton <chrise@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>",['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java'],"Concurrent requests to validate endpoint for the same connector type can lead to race conditions in AbstractHerder::getConnector method, causing potentially multiple instances of the same connector."
c16711cb8e0d1c03f123e3e9d7e3d810796bf315,1623762586,"KAFKA-12701: NPE in MetadataRequest when using topic IDs (#10584)

We prevent handling MetadataRequests where the topic name is null (to prevent NPE) as
well as prevent requests that set topic IDs since this functionality has not yet been
implemented. When we do implement it  in https://github.com/apache/kafka/pull/9769,
we should bump the request/response version.

Added tests to ensure the error is thrown.

Reviewers: dengziming <swzmdeng@163.com>, Ismael Juma <ismael@juma.me.uk>","['clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java', 'clients/src/test/java/org/apache/kafka/common/requests/MetadataRequestTest.java', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala']",MetadataRequests are encountering a Null Pointer Exception (NPE) issue when the topic name is null or when requests attempt to set topic IDs that haven't been implemented yet.
e8e9fe82daa4b0604d039be6f408256b66f97072,1591882280,"KAFKA-10086: Integration test for ensuring warmups are effective (#8818)

Add an integration test for the task assignor.
* ensure we see proper scale-out behavior with warmups
* ensure in-memory stores are properly recycled and not restored through the scale-out process

Fix two bugs revealed by the test:

Bug 1: we can't remove active tasks in the cooperative algorithm, because this causes their state to get discarded (definitely for in-memory stores, and maybe for persistent ones, depending on the state cleaner). Instead, we convert them to standbys so they can keep warm.

Bug 2: tasks with only in-memory stores weren't reporting their offset positions

Reviewers: Matthias J. Sax <matthias@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java', 'streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java']","Task assignor's scale-out behavior does not handle active tasks appropriately, resulting in discarded states for in-memory stores. Additionally, tasks with only in-memory stores are failing to report their offset positions."
d998cbcc886c82d5fc4081cda0908aa386e856c5,1627057855,"KAFKA-8529; Flakey test ConsumerBounceTest#testCloseDuringRebalance (#11097)

When the replica fetcher receives a top-level error in the fetch response, it marks all partitions are failed and adds a backoff delay before resuming fetching from them. In addition to this, there is an additional backoff enforced after the top-level error is handled, so we end up waiting twice the configured backoff time before resuming. This patch removes this extra backoff.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/AbstractFetcherThread.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala']",The ConsumerBounceTest#testCloseDuringRebalance is failing intermittently due to a backoff time issue. Current handling of top-level errors in fetch responses results in double the configured backoff time being applied before resuming.
c75a73862aa4bf752179e83c7fbeab771a2aef57,1613756996,"KAFKA-12340: Fix potential resource leak in Kafka*BackingStore (#10153)

These Kafka*BackingStore classes used in Connect have a recently-added deprecated constructor, which is not used within AK. However, this commit corrects a AdminClient resource leak if those deprecated constructors are used outside of AK. The fix simply ensures that the AdminClient created by the “default” supplier is always closed when the Kafka*BackingStore is stopped.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaStatusBackingStore.java']",Using deprecated constructors in Kafka*BackingStore classes might lead to a potential resource leak with the AdminClient not being closed properly when the Kafka*BackingStore is stopped.
05c329e61fbfddc1f972403fbec0a24c45d4e173,1692464423,"KAFKA-10199: Change to RUNNING if no pending task to init exist (#14249)

A stream thread should only change to RUNNING if there are no
active tasks in restoration in the state updater and if there
are no pending tasks to recycle and to init.

Usually all pending tasks to init are added to the state updater
in the same poll iteration that handles the assignment. However,
if during an initialization of a task a LockException the task
is re-added to the tasks to init and initialization is retried
in the next poll iteration.

A LockException might occur when a state directory is still locked
by another thread, when the rebalance just happened.

Reviewers: Lucas Brutschy <lbrutschy@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Tasks.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TasksRegistry.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TasksTest.java']","Stream thread transitions to RUNNING even when there are active tasks in restoration or pending tasks to initiate, potentially causing lock exceptions due to state directory access conflicts post-rebalance."
981ef5166d2c95cacb6cdc1d52ed0c866b473868,1590242432,"KAFKA-9931: Implement KIP-605 to expand support for Connect worker internal topic configurations (#8654)

Added support for -1 replication factor and partitions for distributed worker internal topics by expanding the allowed values for the internal topics’ replication factor and partitions from positive values to also include -1 to signify that the broker defaults should be used.

The Kafka storage classes were already constructing a `NewTopic` object (always with a replication factor and partitions) and sending it to Kafka when required. This change will avoid setting the replication factor and/or number of partitions on this `NewTopic` if the worker configuration uses -1 for the corresponding configuration value.

Also added support for extra settings for internal topics on distributed config, status, and offset internal topics.

Quite a few new tests were added to verify that the `TopicAdmin` utility class is correctly using the AdminClient, and that the `DistributedConfig` validators for these configurations are correct. Also added integration tests for internal topic creation, covering preexisting functionality plus the new functionality.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>","['clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaStatusBackingStore.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedConfigTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStoreTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectClusterAssertions.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java']","Connect worker's internal topic configurations limit to positive values only, without offering support for -1 replication factor and partitions. Additionally, lack of extra settings for distributed config, status, and offset internal topics."
d6057a9fe44b5e9521dd679b0dc1e6ccfec866c2,1558137355,"MINOR: Remove spammy log message during topic deletion

Deletion of a large number of topics can cause a ton of log spam. In a test case on 2.2, deletion of 50 topics with 100 partitions each caused about 158 Mb of data in the controller log. With the improvements to batch StopReplica and the patch here, we reduce that to about 1.5 Mb.

Kudos to gwenshap for spotting these spammy messages.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Gwen Shapira

Closes #6738 from hachikuji/remove-verbose-topic-deletion-log-message
","['core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/main/scala/kafka/controller/TopicDeletionManager.scala']","Deletion of a large number of topics in Kafka generates a significant amount of unnecessary logs, leading to potential storage and performance issues."
7867c24a40ab0a8d1e7b0d9bcb7776d1335ad3ea,1591317078,"KAFKA-10040; Make computing the PreferredReplicaImbalanceCount metric more efficient (#8724)

This PR changes the way `PreferredReplicaImbalanceCount` is computed. It moves from re-computing after the processing of each event in the controller, which requires a full pass over all partitions, to incrementally maintaining the count as assignments and leaders are changing.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/controller/ControllerChannelManager.scala', 'core/src/main/scala/kafka/controller/ControllerContext.scala', 'core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/main/scala/kafka/controller/PartitionStateMachine.scala', 'core/src/main/scala/kafka/controller/ReplicaStateMachine.scala', 'core/src/main/scala/kafka/controller/TopicDeletionManager.scala', 'core/src/test/scala/unit/kafka/controller/ControllerChannelManagerTest.scala', 'core/src/test/scala/unit/kafka/controller/ControllerContextTest.scala', 'core/src/test/scala/unit/kafka/controller/MockPartitionStateMachine.scala', 'core/src/test/scala/unit/kafka/controller/PartitionStateMachineTest.scala', 'core/src/test/scala/unit/kafka/controller/ReplicaStateMachineTest.scala']","`PreferredReplicaImbalanceCount` metric computation required a full pass over all partitions after processing each event, leading to inefficiencies."
634c9175054cc69d10b6da22ea1e95edff6a4747,1600202296,"KAFKA-10435; Fetch protocol changes for KIP-595 (#9275)

This patch bumps the `Fetch` protocol as specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. The main differences are the following:

- Truncation detection 
- Leader discovery through the response
- Flexible version support

The most notable change is truncation detection. This patch adds logic in the request handling path to detect truncation, but it does not change the replica fetchers to make use of this capability. This will be done separately.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java', 'clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java', 'core/src/main/scala/kafka/api/ApiVersion.scala', 'core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/server/DelayedFetch.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/main/scala/kafka/server/ReplicaFetcherThread.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/main/scala/kafka/server/checkpoints/LeaderEpochCheckpointFile.scala', 'core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala', 'core/src/test/scala/integration/kafka/server/DelayedFetchTest.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/log/LogSegmentTest.scala', 'core/src/test/scala/unit/kafka/server/FetchRequestTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala', 'core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala']","The current Fetch protocol lacks truncation detection and flexible version support, and does not enable leader discovery through the response."
6b89672b5e5d527cf26207d3985a24025afedb1a,1677621547,"MINOR: some ZK migration code cleanups.

Some minor improvements to the JavaDoc for ZkMigrationState.

Rename MigrationState to MigrationDriverState to avoid confusion with ZkMigrationState.

Remove ClusterImage#zkBrokers. This costs O(num_brokers) time to calculate, but is only ever used
when in migration state. It should just be calculated in the migration code. (Additionally, the
function ClusterImage.zkBrokers() returns something other than ClusterImage#zkBrokers, which is
confusing.)

Also remove ClusterDelta#liveZkBrokerIdChanges. This is only used in one place, and it's easy to
calculate it there. In general we should avoid providing expensive accessors unless absolutely
necessary. Expensive code should look expensive: if people want to iterate over all brokers, they
can write a loop to do that rather than hiding it inside an accessor.
","['core/src/main/scala/kafka/migration/MigrationPropagator.scala', 'metadata/src/main/java/org/apache/kafka/image/ClusterDelta.java', 'metadata/src/main/java/org/apache/kafka/image/ClusterImage.java', 'metadata/src/main/java/org/apache/kafka/image/TopicsImage.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationDriverState.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java', 'metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java']","The usage of `ClusterImage#zkBrokers` and `ClusterDelta#liveZkBrokerIdChanges` is non-optimal, causing unnecessary computation and potential confusion due to their names and behavior."
844c1259a9e2ea9caf2635213c4c0a144f6b758f,1630013854,"MINOR: Optimize the OrderedBytes#upperRange for not all query cases (#11181)

Currently in OrderedBytes#upperRange method, we'll check key bytes 1 by 1, to see if there's a byte value >= first timestamp byte value, so that we can skip the following key bytes, because we know compareTo will always return 0 or 1. However, in most cases, the first timestamp byte is always 0, more specifically the upperRange is called for both window store and session store. For former, the suffix is in timestamp, Long.MAX_VALUE and for latter the suffix is in Long.MAX_VALUE, timestamp. For Long.MAX_VALUE the first digit is not 0, for timestamp it could be 0 or not, but as long as it is up to ""now"" (i.e. Aug. 23rd) then the first byte should be 0 since the value is far smaller than what a long typed value could have. So in practice for window stores, that suffix's first byte has a large chance to be 0, and hence worth optimizing for.

This PR optimizes the not all query cases by not checking the key byte 1 by 1 (because we know the unsigned integer will always be >= 0), instead, put all bytes and timestamp directly. So, we won't have byte array copy in the end either.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/state/internals/OrderedBytes.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/WindowKeySchemaTest.java']","OrderedBytes#upperRange method's current key byte check strategy doesn't efficiently handle most cases, as it checks key bytes one by one where the first timestamp byte is often always 0, leading to unnecessary checks and potential performance issues."
bfeef298041add5d10f4279119aff2209d51d284,1676512940,"KAFKA-14491: [7/N] Enforce strict grace period for versioned stores (#13243)

Changes the versioned store semantics to define an explicit ""grace period"" property. Grace period will always be equal to the history retention, though in the future we could introduce a new KIP to expose options to configure grace period separately.

Part of KIP-889.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/state/VersionedKeyValueStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBVersionedStore.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBVersionedStoreTest.java']","Versioned store semantics lack an explicit ""grace period"" property, causing issues related to history retention. Potential need in the future to configure grace period separately."
e29942347acc70aa85d47e84e2021f9c24cd7c80,1682949364,"KAFKA-14859: SCRAM ZK to KRaft migration with dual write (#13628)

Handle migrating SCRAM records in ZK when migrating from ZK to KRaft.

This includes handling writing back SCRAM records to ZK while in dual write mode where metadata updates are written to both the KRaft metadata log and to ZK. This allows for rollback of migration to include SCRAM metadata changes.

Reviewers: David Arthur <mumrah@gmail.com>","['core/src/main/scala/kafka/zk/ZkMigrationClient.scala', 'core/src/test/scala/unit/kafka/zk/ZkMigrationClientTest.scala', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java', 'metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java']","During ZK to KRaft migration while in dual write mode, SCRAM records are not properly migrated, causing potential metadata loss upon rollback."
5c0e4aa67642ac0df1bbb79be84ef579b04194c2,1680131392,"KAFKA-14468: Committed API (#13380)

In this PR, I implemented the committed API. Here are the specifics:

* the CommitRequestManager handles committed() request.
* I implemented a UnsentOffsetFetchRequestState to handle deduping the request: because we don't want to send the exact requests repeatedly.
* I implemented the retry mechanism: Some retriable errors will be retried automatically
* ClientResponse errors are handled in the handlers.
* Some of the top-level APIs were refactored lightly.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/DefaultBackgroundThread.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/PrototypeAsyncConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEvent.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEventProcessor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/OffsetFetchApplicationEvent.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/CommitRequestManagerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/PrototypeAsyncConsumerTest.java', 'core/src/test/scala/integration/kafka/api/BaseAsyncConsumerTest.scala']",Duplication of committed() requests and unhandled ClientResponse errors caused by top-level API designs resulting in unnecessary retries.
65efb981347d6f81fb2713cd27cdfdfa9d8781b9,1695734730,"KAFKA-10199: Do not process when in PARTITIONS_REVOKED (#14265)

When a Streams application is subscribed with a pattern to
input topics and an input topic is deleted, the stream thread
transists to PARTITIONS_REVOKED and a rebalance is triggered.
This happens inside the poll call. Sometimes, the poll call
returns before a new assignment is received. That means, Streams
executes the poll loop in state PARTITIONS_REVOKED.

With the state updater enabled processing is also executed in states
other than RUNNING and so processing is also executed when the
stream thread is in state PARTITION_REVOKED. However, that triggers
an IllegalStateException with error message:
No current assignment for partition TEST-TOPIC-A-0
which is a fatal error.

This commit prevents processing when the stream thread is in state
PARTITIONS_REVOKED.

Reviewer: Lucas Brutschy <lbrutschy@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']","Streams application transitioning to PARTITIONS_REVOKED state during a rebalance triggers an IllegalStateException due to processing execution, causing fatal error: No current assignment for partition."
c39123d83d996edfdc23cdd50be8e51853b6cf1d,1676987475,"KAKFA-14733: Added a few missing checks for Kraft Authorizer and updated AclAuthorizerTest to run tests for both zk and kraft (#13282)

Added the following checks - 
* In StandardAuthorizerData.authorize() to fail if `patternType` other than `LITERAL` is passed.
* In AclControlManager.addAcl() to fail if Resource Name is null or empty.

Also, updated `AclAuthorizerTest` includes a lot of tests covering various scenarios that are missing in `StandardAuthorizerTest`. This PR changes the AclAuthorizerTest to run tests for both `zk` and `kraft` modes - 
* Rename AclAuthorizerTest -> AuthorizerTest
* Parameterize relevant tests to run for both modes

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>","['core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java', 'metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerData.java', 'metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java', 'metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java', 'metadata/src/test/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerTest.java']","The StandardAuthorizerData.authorize() function doesn't fail when a non-literal patternType is passed. Also, the AclControlManager.addAcl() doesn't check for null or empty Resource Name. Current AclAuthorizerTest does not cover both zk and kraft modes."
bc96a8feb5dcd7f034a2c8f30282485a1fde5c67,1605979554,"KAFKA-10706; Ensure leader epoch cache is cleaned after truncation to end offset (#9633)

This patch fixes a liveness bug which prevents follower truncation from completing after a leader election. If there are consecutive leader elections without writing any data entries, then the leader and follower may have conflicting epoch entries at the end of the log.

The problem is the shortcut return in `Log.truncateTo` when the truncation offset is larger than or equal to the end offset, which prevents the conflicting entries from being resolved. Here we change this case to ensure `LeaderEpochFileCache.truncateFromEnd` is still called.

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']",Consecutive leader elections without data entries can result in leader and follower with conflicting epoch entries. Truncation offset being larger than or equal to the end offset disrupts resolution of these conflicts.
a043edb559a20501660ffe593b4c14bc572dc16e,1568167270,"KAFKA-8817: Remove timeout for the whole test (#7313)

I bumped into this flaky test while working on another PR. It's a bit different from the reported PR, where it actually timed out at parsing localhost:port already. I think what we should really test is that the closing call can complete, so I removed the whole test timeout and add the timeout for the shutdown latch instead.

Reviewers: Jason Gustafson <jason@confluent.io>, cpettitt-confluent <53191309+cpettitt-confluent@users.noreply.github.com>",['clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java'],"Flaky test issues arising due to unnecessary timeout at parsing localhost:port, leading to uncompleted closing calls."
8d38189eddd66d8ae53749f15bfff557f102a936,1619720430,"MINOR: clean up some replication code (#10564)

Centralize leader and ISR changes in generateLeaderAndIsrUpdates.
Consolidate handleNodeDeactivated and handleNodeActivated into this
function.

Rename BrokersToIsrs#noLeaderIterator to BrokersToIsrs#partitionsWithNoLeader.
Create BrokersToIsrs#partitionsLedByBroker, BrokersToIsrs#partitionsWithBrokerInIsr

In ReplicationControlManagerTest, createTestTopic should be a member
function of ReplicationControlTestContext.  It should invoke
ReplicationControlTestContext#replay so that records are applied to all
parts of the test context.

Reviewers: Jun Rao <junrao@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>","['metadata/src/main/java/org/apache/kafka/controller/BrokersToIsrs.java', 'metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/BrokersToIsrsTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java']","Replication control code lacks centralization for leader and ISR changes, leading to inefficiencies. Various related functions and methods are also spread out and not properly named or placed, making the codebase difficult to understand and manage."
1c31176ae1ef49071b0d3e717e5f3037492b5b30,1613767412,"KAFKA-12343: Handle exceptions better in TopicAdmin, including UnsupportedVersionException (#10158)

Refactored the KafkaBasedLog logic to read end offsets into a separate method to make it easier to test. Also changed the TopicAdmin.endOffsets method to throw the original UnsupportedVersionException, LeaderNotAvailableException, and TimeoutException rather than wrapping, to better conform with the consumer method and how the KafkaBasedLog retries those exceptions.

Added new tests to verify various scenarios and errors.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/KafkaBasedLogTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java']","TopicAdmin.endOffsets method incorrectly wraps UnsupportedVersionException, LeaderNotAvailableException, and TimeoutException, and the KafkaBasedLog's logic for reading end offsets is challenging to test."
674360f5b3a7e5e05b626fd55c277f7c03f27b9e,1583507944,"KAFKA-6145: Encode task positions in SubscriptionInfo (#8121)

* Replace Prev/Standby task lists with a representation of the current poasition
  of all tasks, where each task is encoded as the sum of the positions of all the
  changelogs in that task.
* Only the protocol change is implemented, not actual positions, and the
  assignor is updated to translate the new protocol back to lists of Prev/Standby
  tasks so that the current assignment protocol still functions without modification.

Implements: KIP-441

Reviewers: John Roesler <vvcephei@apache.org>, Bruno Cadonna <bruno@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StreamsAssignmentProtocolVersions.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/LegacySubscriptionInfoSerde.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java', 'streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java', 'tests/kafkatest/tests/streams/streams_upgrade_test.py']","SubscriptionInfo only holds Prev/Standby task lists, lacking proper representation of the current position of all tasks, potentially causing assignment protocol discrepancies."
d8f358facc2a5405d08977f922bc0b1dae8f114e,1695660181,"[KAFKA-15117] In TestSslUtils set SubjectAlternativeNames to null if there are no hostnames (#14440)

We are currently encoding an empty hostNames array to subjectAltName in the keystore. While parsing the certificates in the test this causes the issue - Unparseable SubjectAlternativeName extension due to java.io.IOException: No data available in passed DER encoded value. Up to Java 17, this parsing error was ignored. This PR assigns subjectAltName to null if hostnames are empty.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>
Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/test/java/org/apache/kafka/common/network/SslTransportLayerTest.java', 'clients/src/test/java/org/apache/kafka/test/TestSslUtils.java']",Empty hostNames array encoded to subjectAltName in the keystore leads to Unparseable SubjectAlternativeName extension error during certificate parsing in tests.
6f8ca661270c18f92304976dc8266ef2af3c16e5,1611446245,"MINOR: Tag `RaftEventSimulationTest` as `integration` and tweak it (#9925)

The test takes over 1 minute to run, so it should not be considered a
unit test.

Also:
* Replace `test` prefix with `check` prefix for helper methods. A common
mistake is to forget to add the @Test annotation, so it's good to use a
different naming convention for methods that should have the annotation
versus methods that should not.
* Replace `Action` functional interface with built-in `Runnable`.
* Remove unnecessary `assumeTrue`.
* Remove `@FunctionalInterface` from `Invariant` since it's not used
in that way.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",['raft/src/test/java/org/apache/kafka/raft/RaftEventSimulationTest.java'],"`RaftEventSimulationTest` is taking over a minute to run, making it incorrectly categorized as a unit test, and is observably prone to errors due to resemblance of helper method names with test methods."
5fc3cd61fcb73da8b52f34b72fe6bb7457f46ce2,1584511353,"KAFKA-9625: Fix altering and describing dynamic broker configurations (#8260)

* Broker throttles were incorrectly marked as sensitive configurations.  Fix this, so that their values can be returned via DescribeConfigs as expected.

* Previously, changes to broker configs that consisted only of deletions were ignored by the brokers because of faulty delta calculation logic that didn't consider deletions as changes, only alterations as changes.  Fix this and add a regression test.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/main/scala/kafka/server/DynamicConfig.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala']","Broker throttle values are incorrectly classified as sensitive and are not returned on DescribeConfigs as expected. Also, deletions in broker configurations are being ignored due to faulty delta calculation logic."
9a32ea9e5248172bb67723165afda462fd6c53d0,1600513104,"KAFKA-8098: fix the flaky test by disabling the auto commit to avoid member rejoining

In the test, we first test removing 1 member from group, and then test removing the other 2 members from group, and it failed sometimes at the 2nd member number assert. After investigation, I found it's because we enabled auto commit for the consumers(default setting), and the removed consumer offset commit will get the `UNKNOWN_MEMBER_ID` error, which will then make the member rejoin. (check ConsumerCoordinator#OffsetCommitResponseHandler) So, that's why after the 2nd members removing, the members will sometimes be not empty.

I set the consumer config to disable the auto commit to fix this issue. Thanks.

Author: Luke Chen <showuon@gmail.com>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #9062 from showuon/KAFKA-8098
",['core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala'],"In the test scenario, removal of group members results in unexpected member rejoining due to `UNKNOWN_MEMBER_ID` error, triggered by enabled auto commit for consumers. This causes intermittent failures at 2nd member number assert."
679e9e0cee67e7d3d2ece204a421ea7da31d73e9,1658458815,"KAFKA-13919: expose log recovery metrics (#12347)

Implementation for KIP-831.
1. add remainingLogsToRecover metric for the number of remaining logs for each log.dir to be recovered
2.  add remainingSegmentsToRecover metric for the number of remaining segments for the current log assigned to the recovery thread.
3. remove these metrics after log loaded completely
4. add tests 

Reviewers: Jun Rao <jun@confluent.io>, Tom Bentley <tbentley@redhat.com>","['core/src/main/scala/kafka/log/LogLoader.scala', 'core/src/main/scala/kafka/log/LogManager.scala', 'core/src/main/scala/kafka/log/UnifiedLog.scala', 'core/src/test/scala/unit/kafka/log/LogLoaderTest.scala', 'core/src/test/scala/unit/kafka/log/LogManagerTest.scala', 'core/src/test/scala/unit/kafka/log/LogTestUtils.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/CheckpointBench.java']","No visibility into the number of remaining logs and segments that need to be recovered for each log directory, making log recovery progress tracking difficult."
700a931ff55293bce1825a96913f059ff1dec2be,1578655404,"KAFKA-9188; Fix flaky test SslAdminClientIntegrationTest.testSynchronousAuthorizerAclUpdatesBlockRequestThreads (#7918)

The test blocks requests threads while sending the ACL update requests and occasionally hits request timeout. Updated the test to tolerate timeouts and retry the request for that case. Added an additional check to verify that the requests threads are unblocked when the semaphore is released, ensuring that the timeout is not due to blocked threads.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",['core/src/test/scala/integration/kafka/api/SslAdminIntegrationTest.scala'],"The SslAdminClientIntegrationTest.testSynchronousAuthorizerAclUpdatesBlockRequestThreads is flaky, occasionally timing out due to ACL update requests blocking request threads."
b53f84402890636e4990b102ba9f832a98b4e5df,1550400898,"MINOR: Make info logs for KafkaConsumer a bit more verbose (#6279)

When debugging KafkaConsumer production issues, it's pretty
useful to have log entries related to seeking and committed
offset retrieval enabled by default. These are currently present,
but only when debug logging is enabled. Change them to `info`.

Also included a minor code simplication and a slight improvement
to an exception message.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java']","In production, debugging KafkaConsumer issues is difficult due to lack of sufficient log entries related to seeking and committed offset retrieval by default."
61530d68ce83467de6190a52da37b3c0af84f0ef,1681825024,"KAFKA-14869: Bump coordinator value records to flexible versions (KIP-915, Part-2) (#13526)

This patch implemented the second part of KIP-915. It bumps the versions of the value records used by the group coordinator and the transaction coordinator to make them flexible versions. The new versions are not used when writing to the partitions but only when reading from the partitions. This allows downgrades from future versions that will include tagged fields.

Reviewers: David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionLog.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionLogTest.scala']","Group coordinator and transaction coordinator are using outdated versions of value records, making it difficult for downgrade compatibility from future versions with tagged fields."
fcc7c2de391cadad8edecb108d978ea50afd428d,1602838681,"MINOR: Handle lastFetchedEpoch/divergingEpoch in FetchSession and DelayedFetch (#9434)

In 2.7, we added lastFetchedEpoch to fetch requests and divergingEpoch to fetch responses. We are not using these for truncation yet, but in order to use these for truncation with IBP 2.7 onwards in the next release, we should make sure that we handle these in all the supporting classes even in 2.7.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/DelayedFetch.scala', 'core/src/main/scala/kafka/server/FetchSession.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/integration/kafka/server/DelayedFetchTest.scala', 'core/src/test/scala/unit/kafka/server/FetchSessionTest.scala']","The FetchSession and DelayedFetch classes currently don't handle 'lastFetchedEpoch' in fetch requests and 'divergingEpoch' in fetch responses, which are needed for truncation with IBP 2.7 onwards."
98d84b17f74b0bfe65163d0ddf88976746de5f7e,1677704495,"KAFKA-14451: Rack-aware consumer partition assignment for RangeAssignor (KIP-881) (#12990)

Best-effort rack alignment for range assignor when both consumer racks and partition racks are available with the protocol changes introduced in KIP-881. Rack-aware assignment is enabled by configuring client.rack for consumers. Balanced assignment per topic is prioritized over rack-alignment. For topics with equal partitions and the same set of subscribers, co-partitioning is prioritized over rack-alignment.

Reviewers: David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Utils.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignorTest.java', 'core/src/test/scala/integration/kafka/api/PlaintextConsumerTest.scala', 'core/src/test/scala/integration/kafka/server/FetchFromFollowerIntegrationTest.scala']","Rack-aware consumer partition assignment for RangeAssignor is not working properly, even when client.rack is configured and both consumer racks and partition racks are available. Furthermore, co-partitioning and balanced assignment are not being prioritized correctly."
ae3a6ed990f91708686d27c6023bac050c422248,1608314926,"KAKFA-10619: Idempotent producer will get authorized once it has a WRITE access to at least one topic (KIP-679) (#9485)

Includes:
- New API to authorize by resource type
- Default implementation for the method that supports super users and ACLs
- Optimized implementation in AclAuthorizer that supports ACLs, super users and allow.everyone.if.no.acl.found
- Benchmarks and tests
- InitProducerIdRequest authorized for Cluster:IdempotentWrite or WRITE to any topic, ProduceRequest authorized only for topic even if idempotent

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>
","['clients/src/main/java/org/apache/kafka/common/utils/SecurityUtils.java', 'clients/src/main/java/org/apache/kafka/server/authorizer/Authorizer.java', 'core/src/main/scala/kafka/security/authorizer/AclAuthorizer.scala', 'core/src/main/scala/kafka/security/authorizer/AuthorizerWrapper.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/security/authorizer/AclAuthorizerTest.scala', 'core/src/test/scala/unit/kafka/security/authorizer/AuthorizerInterfaceDefaultTest.scala', 'core/src/test/scala/unit/kafka/security/authorizer/AuthorizerWrapperTest.scala', 'core/src/test/scala/unit/kafka/security/authorizer/BaseAuthorizerTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/acl/AclAuthorizerBenchmark.java']","The idempotent producer is being authorized incorrectly, gaining access with WRITE permissions to any topic instead of specific ones. This permit is breaking the isolation and security properties of the producer."
627cad1f7b6dd70caf88ec314daf1719ac377f90,1637031423,"KAFKA-13406: skip assignment validation for built-in cooperativeStickyAssignor (#11439)

This fix is trying to skip the assignment validation for built-in cooperative sticky assignor, since (a) we know the assignment is valid since we do essentially this same check already in the cooperative sticky assignor, and (b) the check is broken anyways due to potential for claimed `ownedPartitions` to be incorrect 

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java']","Assignment validation check for cooperative sticky assignor is redundant and potentially broken due to incorrect `ownedPartitions`.
"
9dbf2226cdae4a30e44340a552613446c5053d87,1619724648,"MINOR: clean up some remaining locking stuff in StateDirectory (#10608)

Minor followup to #10342 that I noticed while working on the NamedTopology stuff. Cleans up a few things:

We no longer need locking for the global state directory either, since it's contained within the top-level state directory lock. Definitely less critical than the task directory locking, since it's less vulnerable to IOExceptions given that it's just locked and unlocked once during the application lifetime, but nice to have nonetheless
Clears out misc. usages of the LOCK_FILE_NAME that no longer apply. This has the awesome side effect of finally being able to actually delete obsolete task directories, whereas previously we had to leave behind the empty directory due to a ridiculous Windows bug (though I'm sure they would claim ""it's not a bug it's a feature"" 😉 )
Lazily delete old-and-now-unused lock files in the StateDirectory#taskDirIsEmpty method to clean up the state directory for applications that upgraded from an older version that still used task locking

Reviewers: Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java']","Redundant locking present in StateDirectory and obsolete task directories cannot be deleted due to leftover lock files, causing unnecessary clutter and potential upgrade issues."
5351efe48ac71318fbbca21ac280c43921267744,1558639279,"KAFKA-8407: Fix validation of class and list configs in connector client overrides (#6789)

Because of how config values are converted into strings in the `AbstractHerder.validateClientOverrides()` method after being validated by the client override policy, an exception is thrown if the value returned by the policy isn't already parsed as the type expected by the client `ConfigDef`. The fix here involves parsing client override properties before passing them to the override policy.

A unit test is added to ensure that several different types of configs are validated properly by the herder.

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Magesh Nandakumar <magesh.n.kumar@gmail.com>, Randall Hauch <rhauch@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java']",Exception is thrown during the validation of class and list configs in connector client overrides due to inappropriate string conversion of config values.
ae6725740651cc76280840af3a657f9bb9522e37,1677395811,"Kafka-14743: update request metrics after callback (#13297)

Currently, the kafka.network:type=RequestMetrics,name=MessageConversionsTimeMs,request=Fetch will not get updated because the request metrics is recorded BEFORE the messageConversions metrics value updated. That means, even if we updated the messageConversions metrics value, the request metrics will never reflect the update. This patch fixes it by updating the request metric after callback completed, so that the messageConversions metric value can be updated correctly.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Divij Vaidya <diviv@amazon.com>","['core/src/main/scala/kafka/network/SocketServer.scala', 'core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","Request metrics in Kafka does not update in accordance with messageConversions metrics value changes, resulting in inaccurate metric assessment."
2e3bbe63c1d7a5484d0475c57e82011fada9cfa5,1676660764,"KAFKA-14491: [9/N] Add versioned bytes store and supplier (#13250)

As part of introducing versioned key-value stores in KIP-889, we'd like a way to represent a versioned key-value store (VersionedKeyValueStore<Bytes, byte[]>) as a regular key-value store (KeyValueStore<Bytes, byte[]>) in order to be compatible with existing DSL methods for passing key-value stores, e.g., StreamsBuilder#table() and KTable methods, which are explicitly typed to accept Materialized<K, V, KeyValueStore<Bytes, byte[]>. This way, we do not need to introduce new versions of all relevant StreamsBuilder and KTable methods to relax the Materialized type to accept versioned stores.

This PR introduces the new VersionedBytesStore extends KeyValueStore<Bytes, byte[]> interface for this purpose, along with the corresponding supplier (VersionedBytesStoreSupplier) and implementation (RocksDbVersionedKeyValueBytesStoreSupplier). The RocksDbVersionedKeyValueBytesStoreSupplier implementation leverages an adapter (VersionedKeyValueToBytesStoreAdapter) to assist in converting from VersionedKeyValueStore to VersionedBytesStore.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/state/VersionedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/VersionedBytesStoreSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbVersionedKeyValueBytesStoreSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/VersionedKeyValueToBytesStoreAdapter.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDbVersionedKeyValueBytesStoreSupplierTest.java']",Existing DSL methods for passing key-value stores in KIP-889 are not compatible with versioned key-value stores leading to an inability to represent a versioned key-value store as a regular key-value store.
35142d43e6aa55bd11484224b8b09a83800d3a22,1676500800,"KAFKA-14664; Fix inaccurate raft idle ratio metric (#13207)

The raft idle ratio is currently computed as the average of all recorded poll durations. This tends to underestimate the actual idle ratio since it treats all measurements equally regardless how much time was spent. For example, say we poll twice with the following durations:

Poll 1: 2s
Poll 2: 0s

Assume that the busy time is negligible, so 2s passes overall.

In the first measurement, 2s is spent waiting, so we compute and record a ratio of 1.0. In the second measurement, no time passes, and we record 0.0. The idle ratio is then computed as the average of these two values (1.0 + 0.0 / 2 = 0.5), which suggests that the process was busy for 1s, which overestimates the true busy time.

In this patch, we create a new `TimeRatio` class which tracks the total duration of a periodic event over a full interval of time measurement.

Reviewers: José Armando García Sancio <jsancio@apache.org>
","['raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/internals/KafkaRaftMetrics.java', 'raft/src/main/java/org/apache/kafka/raft/internals/TimeRatio.java', 'raft/src/test/java/org/apache/kafka/raft/internals/KafkaRaftMetricsTest.java', 'raft/src/test/java/org/apache/kafka/raft/internals/TimeRatioTest.java']","The current method of computing the raft idle ratio in Kafka, by averaging all recorded poll durations, leads to an underestimation of the actual idle ratio and overestimation of the process busy time."
09286669877abaf17a1d9d52fa34c294e050f835,1655809836,"MINOR: change Streams topic-level metrics tag from 'topic-name' to 'topic' (#12310)

Changes the tag name from topic-name to just topic to conform to the way this tag is named elsewhere (ie in the clients)
Also:
    - fixes a comment about dynamic topic routing
    - fixes some indentation in MockRecordCollector
    - Undoes the changes to KStreamSplitTest.scala and TestTopicsTest which are no longer necessary after this hotfix

Reviewers: Bruno Cadonna <cadonna@apache.org>
","['streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/TopicMetricsTest.java', 'streams/src/test/java/org/apache/kafka/test/MockRecordCollector.java', 'streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KStreamSplitTest.scala', 'streams/test-utils/src/test/java/org/apache/kafka/streams/TestTopicsTest.java']","Inconsistent tag naming for Streams topic-level metrics; 'topic-name' doesn't align with the conventional 'topic' used elsewhere. Also, some comments about dynamic topic routing are misleading and need correction."
b161358998cdb6cee1ec184621836fa035fa16c6,1587626143,"MINOR: Downgrade test should wait for ISR rejoin between rolls (#8495)

I added a change to the upgrade test a while back that would make it wait for
ISR rejoin before rolls. This prevents incompatible brokers charging through a
bad roll and disguising a downgrade problem.

We now also check for protocol errors in the broker logs.

Reviewers: Boyang Chen <boyang@confluent.io>, Ismael Juma <ismael@juma.me.uk>",['tests/kafkatest/tests/core/downgrade_test.py'],"The upgrade test doesn't wait for ISR rejoin before rolls, potentially allowing incompatible brokers to obscure a downgrade problem."
c56fec2199398641a8a0a21392bf8135ac637988,1672279089,"MINOR: make sure all partition info is propagated (#13037)

Recently, we have some flaky tests failed with: (ex: build #1457, #1459, #1460

java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at kafka.utils.TestUtils$.describeTopic(TestUtils.scala:475)
	at kafka.utils.TestUtils$.topicHasSameNumPartitionsAndReplicationFactor(TestUtils.scala:484)

The line failed is when we tried to create a topic but got TopicExistsException, so we want to make sure this topic has the expected partition num and replication factor. It failed while trying to describe this topic, and got UnknownTopicOrPartitionException. That will happen when topic is created but the metadata haven't propagated to all brokers. So, fix it by explicitly waiting for partition info propagated to all brokers before verifying topicHasSameNumPartitionsAndReplicationFactor.

Reviewers: Ismael Juma <ismael@juma.me.uk>, dengziming <dengziming1993@gmail.com>, Justine Olshan <jolshan@confluent.io >",['core/src/test/scala/unit/kafka/utils/TestUtils.scala'],Flaky tests are failing due to UnknownTopicOrPartitionException when attempting to create a topic that exists. It's suspected that the issue originates from incomplete propagation of metadata to all brokers.
d04699486d4203cf53afcc6aeb2abbffdd5e1b9c,1574127360,"KAFKA-8981 Add rate limiting to NetworkDegradeSpec (#7446)

* Add rate limiting to tc

* Feedback from PR

* Add a sanity test for tc

* Add iperf to vagrant scripts

* Dynamically determine the network interface

* Add some temp code for testing on AWS

* Temp: use hostname instead of external IP

* Temp: more AWS debugging

* More AWS WIP

* More AWS temp

* Lower latency some

* AWS wip

* Trying this again now that ping should work

* Add cluster decorator to tests

* Fix broken import

* Fix device name

* Fix decorator arg

* Remove errant import

* Increase timeouts

* Fix tbf command, relax assertion on latency test

* Fix log line

* Final bit of cleanup

* Newline

* Revert Trogdor retry count

* PR feedback

* More PR feedback

* Feedback from PR

* Remove unused argument
","['tests/kafkatest/services/trogdor/degraded_network_fault_spec.py', 'tests/kafkatest/services/trogdor/trogdor.py', 'tests/kafkatest/tests/core/network_degrade_test.py', 'tests/kafkatest/tests/core/round_trip_fault_test.py', 'tests/kafkatest/utils/remote_account.py', 'tools/src/main/java/org/apache/kafka/trogdor/fault/DegradedNetworkFaultSpec.java', 'tools/src/main/java/org/apache/kafka/trogdor/fault/DegradedNetworkFaultWorker.java', 'vagrant/aws/aws-init.sh', 'vagrant/base.sh']","NetworkDegradeSpec lacks rate limiting which could possibly lead to network degradation or latency, and there's inadequate test coverage for these scenarios."
0461382adbf8b8dfddc547f28c8d7cfa801e585b,1648228610,"KAFKA-4801: don't verify assignment during broker up and down in testConsumptionWithBrokerFailures (#11949)

In this test, we have another thread to let broker down and up, to test if consumer can still work as expected. During the broker down and up, we tried to verify the assignment is as what we expected. But the rebalance will keep triggering while broker down and up. It doesn't make sense to verify the assignment here. Remove it to make the test reliable.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala'],"The current test setup for broker failures in testConsumptionWithBrokerFailures triggers unnecessary rebalances, making the assignment verification unreliable and the test itself unstable."
11a401d51d835c32d365f6a089016bdb871e42e0,1571067704,"delete (#7504)

This system test was marked @Ignore around a year and a half ago pending the version probing work, but never turned on again.

These days, it is made redundant by the suite of system tests in streams_upgrade_test, which cover rolling upgrades (including version probing and metadata change).

Reviewers: Boyang Chen <boyang@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",['tests/kafkatest/tests/streams/streams_multiple_rolling_upgrade_test.py'],"An outdated system test that has been ignored for some time is now redundant due to the comprehensive suite of system tests in streams_upgrade_test, providing better coverage including version probing and metadata change."
5a6f19b2a1ff72c52ad627230ffdf464456104ee,1632241095,"KAFKA-13246: StoreQueryIntegrationTest#shouldQueryStoresAfterAddingAndRemovingStreamThread now waits for the client state to go to REBALANCING/RUNNING after adding/removing a thread and waits for state RUNNING before querying the state store. (#11334)

KAFKA-13246: StoreQueryIntegrationTest#shouldQueryStoresAfterAddingAndRemovingStreamThread does not gate on stream state well

The test now waits for the client to transition to REBALANCING/RUNNING after adding/removing a thread as well as to transition to RUNNING before querying the state store.

Reviewers: singingMan <@3schwartz>, Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>",['streams/src/test/java/org/apache/kafka/streams/integration/StoreQueryIntegrationTest.java'],"In StoreQueryIntegrationTest, querying the state store occurs without ensuring that the client is in the RUNNING state after thread additions/removals."
7f496744399adfbe5cd283ae3f49dec7c55b88a1,1573859612,"KAFKA-8746: Kibosh must handle an empty JSON string from Trogdor (#7155)

When Trogdor wants to clear all the faults injected to Kibosh, it sends the empty JSON object {}. However, Kibosh expects {""faults"":[]} instead.  Kibosh should handle the empty JSON object, since that's consistent with how Trogdor handles empty JSON fields in general (if they're empty, they can be omitted). We should also have a test for this.

Reviewers: David Arthur <mumrah@gmail.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>","['tests/kafkatest/services/trogdor/kibosh.py', 'tests/kafkatest/tests/tools/kibosh_test.py', 'vagrant/base.sh']","Kibosh does not properly handle an empty JSON object sent by Trogdor to clear all injected faults, expects {""faults"":[]} instead."
5a95c2e1cd555d5f3ec148cc7c765d1bb7d716f9,1558042169,"Add '?expand' query param for additional info on '/connectors'. (#6658)

Per KIP-465, kept existing behavior of `/connectors` resource in the Connect's REST API, but added the ability to specify `?expand` query parameter to get list of connectors with status details on each connector. Added unit tests, and verified passing existing system tests (which use the older list form).

See https://cwiki.apache.org/confluence/display/KAFKA/KIP-465%3A+Add+Consolidated+Connector+Endpoint+to+Connect+REST+API.

Author: Dan Norwood <norwood@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestServerTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResourceTest.java']",The '/connectors' resource in Connect's REST API lacks the ability to provide status details on each connector when generating list of connectors.
c552c06aed50b4d4d9a85f73ccc89bc06fa7e094,1572958285,"KAFKA-9110: Improve efficiency of disk reads when TLS is enabled (#7604)

1. Avoid a buffer allocation and a buffer copy per file read.
2. Ensure we flush `netWriteBuffer` successfully before reading from
disk to avoid wasted disk reads.
3. 32k reads instead of 8k reads to reduce the number of disk reads
(improves efficiency for magnetic drives and reduces the number of
system calls).
4. Update SslTransportLayer.write(ByteBuffer) to loop until the socket
buffer is full or the src buffer has no remaining bytes.
5. Renamed `MappedByteBuffers` to `ByteBufferUnmapper` since it's also
applicable for direct byte buffers.
6. Skip empty `RecordsSend`
7. Some minor clean-ups for readability.

I ran a simple consumer perf benchmark on a 6 partition topic (large
enough not to fit into page cache) starting from the beginning of the
log with TLS enabled on my 6 core MacBook Pro as a sanity check.
This laptop has fast SSDs so it benefits less from the larger reads
than the case where magnetic disks are used. Consumer throughput
was ~260 MB/s before the changes and ~300 MB/s after
(~15% improvement).

Credit to @junrao  for pointing out that this code could be more efficient.

Reviewers: Jun Rao <junrao@confluent.io>, Colin P. McCabe <cmccabe@apache.org>","['clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java', 'clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java', 'clients/src/main/java/org/apache/kafka/common/utils/ByteBufferUnmapper.java', 'clients/src/main/java/org/apache/kafka/common/utils/Utils.java', 'clients/src/test/java/org/apache/kafka/common/utils/ByteBufferUnmapperTest.java', 'core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala', 'core/src/main/scala/kafka/log/AbstractIndex.scala']","TLS-enabled disk reads in Kafka are inefficient, leading to performance degradation, excessively small reads, wasteful disk reads, and unnecessary buffer allocations and copies."
379b6978a04c171bcc64331a095f1c97eb4e1830,1680109960,"KAFKA-14829: Consolidate reassignment logic into PartitionReassignmentReplicas (#13440)

Currently, we have various bits of reassignment logic spread across different classes. For example, ReplicationControlManager contains logic for when a reassignment is in progress, which is duplication in PartitionChangeBuilder. Another example is PartitionReassignmentRevert which contains logic for how to undo/revert a reassignment. The idea here is to move the logic to PartitionReassignmentReplicas so it's more testable and easier to reason about.

Reviewers: José Armando García Sancio <jsancio@apache.org>","['metadata/src/main/java/org/apache/kafka/controller/PartitionChangeBuilder.java', 'metadata/src/main/java/org/apache/kafka/controller/PartitionReassignmentReplicas.java', 'metadata/src/main/java/org/apache/kafka/controller/PartitionReassignmentRevert.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java', 'metadata/src/test/java/org/apache/kafka/controller/PartitionReassignmentReplicasTest.java', 'metadata/src/test/java/org/apache/kafka/metadata/PartitionRegistrationTest.java']",Reassignment logic is spread across different classes such as ReplicationControlManager and PartitionChangeBuilder causing duplication of code and difficulty in testing and reasoning about reassignment processes.
b980ca8709dbef31890a84de02ccf277af5743c7,1627940823,"KAFKA-12158; Better return type of RaftClient.scheduleAppend (#10909)

This patch improves the return type for `scheduleAppend` and `scheduleAtomicAppend`. Previously we were using a `Long` value and using both `null` and `Long.MaxValue` to distinguish between different error cases. In this PR, we change the return type to `long` and only return a value if the append was accepted. For the error cases, we instead throw an exception. For this purpose, the patch introduces a couple new exception types: `BufferAllocationException` and `NotLeaderException`.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/raft/RaftManager.scala', 'core/src/main/scala/kafka/tools/TestRaftServer.scala', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/test/java/org/apache/kafka/metalog/LocalLogManager.java', 'raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/RaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/ReplicatedCounter.java', 'raft/src/main/java/org/apache/kafka/raft/errors/BufferAllocationException.java', 'raft/src/main/java/org/apache/kafka/raft/errors/NotLeaderException.java', 'raft/src/main/java/org/apache/kafka/raft/errors/RaftException.java', 'raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java', 'raft/src/main/java/org/apache/kafka/snapshot/SnapshotWriter.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java']","The `scheduleAppend` and `scheduleAtomicAppend` methods in RaftClient give ambiguous error indications by returning `null` and `Long.MaxValue`, making it hard to distinguish between different error cases."
a92b986c855592d630fbabf49d1e9a160ad5b230,1614694847,"KAFKA-12268: Implement task idling semantics via currentLag API (#10137)

Implements KIP-695

Reverts a previous behavior change to Consumer.poll and replaces
it with a new Consumer.currentLag API, which returns the client's
currently cached lag.

Uses this new API to implement the desired task idling semantics
improvement from KIP-695.

Reverts fdcf8fbf72bee9e672d0790cdbe5539846f7dc8e / KAFKA-10866: Add metadata to ConsumerRecords (#9836)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <guozhang@apache.org>","['clients/src/main/java/org/apache/kafka/clients/consumer/Consumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecords.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetchedRecords.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'core/src/test/scala/integration/kafka/api/PlaintextConsumerTest.scala', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreatorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java', 'streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java']",The change in Consumer.poll’s behavior impacts task idling semantics and there is no way to fetch client's cached lag.
4548c272ae39a6630174ab428bc82fce7c98f7fd,1671472132,"KAFKA-14264; New logic to discover group coordinator (#12862)

[KAFKA-14264](https://issues.apache.org/jira/browse/KAFKA-14264)
In this patch, we refactored the existing FindCoordinator mechanism. In particular, we first centralize all of the network operation (send, poll) in `NetworkClientDelegate`, then we introduced a RequestManager interface that is responsible to handle the timing of different kind of requests, based on the implementation.  In this path, we implemented a `CoordinatorRequestManager` which determines when to create an `UnsentRequest` upon polling the request manager.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/CoordinatorRequestManager.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/DefaultBackgroundThread.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/DefaultEventHandler.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ErrorEventHandler.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/NetworkClientDelegate.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/PrototypeAsyncConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestManager.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestState.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEvent.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEventProcessor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/BackgroundEvent.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/ErrorBackgroundEvent.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/NoopApplicationEvent.java', 'clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorResponse.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/CoordinatorRequestManagerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/DefaultBackgroundThreadTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/DefaultEventHandlerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/NetworkClientDelegateTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/RequestStateTest.java']","FindCoordinator logic in Kafka has inconsistencies and lacks centralization which leads to difficulty in managing network operations (send, poll) and handling the timing of different kinds of requests."
6abb913c6449113141582921340994c0d5e50839,1591837172,"KAFKA-9841: Revoke duplicate connectors and tasks when zombie workers return with an outdated assignment (#8453)

With Incremental Cooperative Rebalancing, if a worker returns after it's been out of the group for sometime (essentially as a zombie worker) and hasn't voluntarily revoked its own connectors and tasks in the meantime, there's the possibility that these assignments have been distributed to other workers and redundant connectors and tasks might be running now in the Connect cluster. 

This PR complements previous fixes such as KAFKA-9184, KAFKA-9849 and KAFKA-9851 providing a last line of defense against zombie tasks: if at any rebalance round the leader worker detects that there are duplicate assignments in the group, it revokes them completely and resolves duplication with a correct assignment in the rebalancing round that will follow task revocation. 

Author: Wang <ywang50@ebay.com>

Reviewer: Konstantine Karantasis <konstantine@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignor.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignorTest.java']","Potential risk of duplicate connectors and tasks running in the Connect cluster when zombie workers return with an outdated assignment, due to current Incremental Cooperative Rebalancing mechanism."
f0a2b62b0e7660ca9e0f0159da636463356206c5,1649178213,"KAFKA-13794; Fix comparator of `inflightBatchesBySequence` in `TransactionManager` (#11991)

Fixes a bug in the comparator used to sort producer inflight batches for a topic partition. This can cause batches in the map `inflightBatchesBySequence` to be removed incorrectly: i.e. one batch may be removed by another batch with the same sequence number. This leads to an `IllegalStateException` when the inflight request finally returns. This patch fixes the comparator to check equality of the `ProducerBatch` instances if the base sequences match.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java']","Incorrect removal of batches due to equality bug in comparator for 'inflightBatchesBySequence', leading to 'IllegalStateException' when inflight request returns."
f34bb28ab6c69945d944abc1283cacecdaf4ef8e,1627073011,"KAFKA-13116: Fix message_format_change_test and compatibility_test_new_broker_test failures (#11108)

These failures were caused by a46b82bea9abbd08e5. Details for each test:

* message_format_change_test: use IBP 2.8 so that we can write in older message
formats.
* compatibility_test_new_broker_test_failures: fix down-conversion path to handle
empty record batches correctly. The record scan in the old code ensured that
empty record batches were never down-converted, which hid this bug.
* upgrade_test: set the IBP 2.8 when message format is < 0.11 to ensure we are
actually writing with the old message format even though the test was passing
without the change.

Verified with ducker that some variants of these tests failed without these changes
and passed with them. Also added a unit test for the down-conversion bug fix.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecords.java', 'clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'tests/kafkatest/tests/client/message_format_change_test.py', 'tests/kafkatest/tests/core/upgrade_test.py']","Tests fail due to incorrect handling of older message formats and empty record batches in down-conversion path, additionally, test cases relating to the old message format may not be accurate."
4153e754f1a4ebbd9a3d10be8bf75a7057c82f1d,1620947795,"MINOR: prevent cleanup() from being called while Streams is still shutting down (#10666)

Currently KafkaStreams#cleanUp only throw an IllegalStateException if the state is RUNNING or REBALANCING, however the application could be in the process of shutting down in which case StreamThreads may still be running. We should also throw if the state is PENDING_ERROR or PENDING_SHUTDOWN

Reviewers: Walker Carlson <wcarlson@confluent.io>, Guozhang Wang <guozhang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java']","KafkaStreams#cleanUp throws IllegalStateException if application is RUNNING or REBALANCING, yet fails to address cases where application is shutting down causing lingering StreamThreads."
3f4816dd3eafaf1a0636d3ee689069f897c99e28,1692322817,"KAFKA-15345; KRaft leader notifies leadership when listener reaches epoch start (#14213)

In a non-empty log the KRaft leader only notifies the listener of leadership when it has read to the leader's epoch start offset. This guarantees that the leader epoch has been committed and that the listener has read all committed offsets/records.

Unfortunately, the KRaft leader doesn't do this when the log is empty. When the log is empty the listener is notified immediately when it has become leader. This makes the API inconsistent and harder to program against.

This change fixes that by having the KRaft leader wait for the listener's nextOffset to be greater than the leader's epochStartOffset before calling handleLeaderChange.

The RecordsBatchReader implementation is also changed to include control records. This makes it possible for the state machine learn about committed control records. This additional information can be used to compute the committed offset or for counting those bytes when determining when to snapshot the partition.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java', 'raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/internals/RecordsBatchReader.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java', 'raft/src/test/java/org/apache/kafka/raft/internals/RecordsBatchReaderTest.java', 'raft/src/test/java/org/apache/kafka/raft/internals/RecordsIteratorTest.java']","In KRaft mode, leadership notification inconsistencies occur between empty and non-empty logs, making the API harder to use. Control records are excluded in RecordsBatchReader, preventing state machine from learning committed control records."
d20865ae792baa287cf9707ab6c05ea6da6be517,1625858314,"KAFKA-13053; Bump kraft frame version for incompatible changes from 2.8 (#11010)

This patch bumps the default frame version for kraft records from 0 to 1. At the same time, we reset all
records versions back to 0 and we enable flexible version support for UnregisterBrokerRecord, which was
missed previously. Note that the frame version bump also affects the KIP-405 records since they are
sharing AbstractApiMessageSerde. Since these records were not part of any previous releases, this should
not cause an issue.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['metadata/src/test/java/org/apache/kafka/controller/ClientQuotaControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ConfigurationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/metadata/MetadataRecordSerdeTest.java', 'server-common/src/main/java/org/apache/kafka/server/common/serialization/AbstractApiMessageSerde.java']","The default frame version for Kraft records is inconsistent with UnregisterBrokerRecord potentially causing serialization compatibility issues, impacting KIP-405 records as well."
4218fc61fedb02b78d35c88e56ab253baaf09f39,1649177783,"KAFKA-13778: Fetch from follower should never run the preferred read replica selection (#11965)

The current preferred read replica selection logic relies on `partition.leaderReplicaIdOpt` to determine if the selection must be run. The issue is that `partition.leaderReplicaIdOpt` is defined for both the leader and the followers thus the logic is ran all the time. The impact is not too bad as the leader is selected most of the time when the logic is ran by the follower and the leader is filtered out. However there are cases where the selection on a follower could redirect the consumer to another follower under certain rare conditions. For instance with the `RackAwareReplicaSelector `, the follower must have stale replica states from a previous leadership and must have other followers in the same rack for instance. Other implementation of the selection logic could be more impacted.

This patch ensures that the preferred read replica selection is only ran by the leader.

Reviewers: David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala']","Preferred read replica selection logic incorrectly runs on both the leader and followers, occasionally causing follower to redirect consumer to another follower under rare conditions."
d798dbf497d91509517c60d67ceeea02e2b9b383,1557762473,"KAFKA-8335; Clean empty batches when sequence numbers are reused (#6715)

The log cleaner attempts to preserve the last entry for each producerId in order to ensure that sequence/epoch state is not lost. The current validation checks only the last sequence number for each producerId in order to decide whether a batch should be retained. There are two problems with this:

1. Sequence numbers are not unique alone. It is the tuple of sequence number and epoch which is uniquely defined.
2. The group coordinator always writes batches beginning with sequence number 0, which means there could be many batches which have the same sequence number.

The complete fix for the second issue would probably add proper sequence number bookkeeping in the coordinator. For now, we have left the coordinator implementation unchanged and changed the cleaner logic to use the last offset written by a producer instead of the last sequence number. 

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/main/scala/kafka/log/ProducerStateManager.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerTest.scala']","The log cleaner preserves the last entry for each producer ID, potentially retaining unnecessary batches due to only checking the last sequence number for deciding on batch retention. The group coordinator is found to always write batches starting with sequence number 0, causing possible duplication of the same sequence number."
e8d32563f323a245b3127394d97d6356e84f0294,1665164455,"MINOR; Fix error message when validating KRaft config (#12717)

The error message reported when advertised.listeners is used in
controller only is confusing. When the KRaft server is configured to
controller only the following must be true:

1. `advertised.listeners` is not set
2. `listeners` contains a listener for every name in `controller.listener.names`
3. `controller.listener.names` contains a name for every listener in `listeners`

Reviewers: Jason Gustafson <jason@confluent.io>, Igor Soarez <i@soarez.me>","['core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']",The error message displayed when `advertised.listeners` is used in a KRaft configuration with controller-only setting is unclear and confusing.
0d9a7022a4486e08dc4bd71e2fd7632ecdd3b76f,1673569437,"KAFKA-14612: Make sure to write a new topics ConfigRecords to metadata log iff the topic is created (#13104)

### JIRA
https://issues.apache.org/jira/browse/KAFKA-14612

### Details
Makes sure we emit `ConfigRecord`s for a topic iff it actually gets created. Currently, we might emit `ConfigRecord`s even if the topic creation fails later in the `createTopics` method.

I created a new method `incrementalAlterConfig` in `ConfigurationControlManager` that is similar to `incrementalAlterConfig` but it just handles one config at a time. This is used in `ReplicationControlManager` for each topic. By handling one topic's config at a time, it's easier to isolate each topic's config records. This enables us to make sure we only write config records for topics that get created.

I refactored `incrementalAlterConfigResource` to return an `ApiError`. This made it easier to implement the new method `incrementalAlterConfig` in `ConfigurationControlManager` because it then doesnt have to search in the `Map` for the result.

### Testing
Enhanced pre-existing test `ReplicationControlManagerTest.testCreateTopicsWithConfigs`. I ran the tests without the changes to `ReplicationControlManager` and made sure each assertion ends up failing. Also ran `./gradlew metadata:test --tests org.apache.kafka.controller.ReplicationControlManagerTest`.

Reviewers: Jason Gustafson <jason@confluent.io>","['metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/ConfigurationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java']",New topics ConfigRecords are being written to metadata log even in cases when the topic creation fails during the 'createTopics' method execution.
f1e7a64bf60c4af4d8f5b3bf09402a52a1a5d0a0,1681474971,"MINOR: Refine `PartitionAssignor` server-side interface (#13524)

This patch updates the `PartitionAssignor` server-side interface used in the new group coordinator for the new consumer group protocol as follow:
- It switches subscription from topic names to topic ids in order to be closer to the server side implementation.
- It switches assignment from Set to Map<Integer, Set> to be closer to the server side implementation.
- It adds getters for all attributes.
- It makes all attributes final private.

Reviewers: Jeff Kim <jeff.kim@confluent.io>, Alexandre Dupriez <alexandre.dupriez@gmail.com>, David Jacot <djacot@confluent.io>","['group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentMemberSpec.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentSpec.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentTopicMetadata.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/GroupAssignment.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/MemberAssignment.java']","The `PartitionAssignor` server-side interface for new consumer group protocol needs refinement, it might not be closely aligned with the server-side implementation."
b6278ee79da722f85aaefe7755a2be920887175c,1617229600,"KAFKA-12575: Eliminate Log.isLogDirOffline boolean attribute (#10430)

This PR is a precursor to the recovery logic refactor work (KAFKA-12553).

I have made a change to eliminate Log.isLogDirOffline attribute. This boolean also comes in the way of refactoring the recovery logic. This attribute was added in #9676. But it is redundant and can be eliminated in favor of looking up LogDirFailureChannel to check if the logDir is offline. The performance/latency implication of such a ConcurrentHashMap lookup inside LogDirFailureChannel should be very low given that ConcurrentHashMap reads are usually lock free.

Tests:
Relying on existing unit/integration tests.

Reviewers: Dhruvil Shah <dhruvil@confluent.io>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/server/LogDirFailureChannel.scala']","The attribute Log.isLogDirOffline in the recovery logic creates redundancy and potentially hinders the refactoring process, as its purpose overlaps with the function of LogDirFailureChannel checks."
cad514bff9140b75a3da18c56efaf26df68310c7,1617663297,"KAFKA-12294; forward auto topic request within envelope on behalf of clients (#10142)

When auto-creating topics in KIP-500, the broker will send a `CreateTopics` request to the controller. It is useful in this case to preserve the original principal from the corresponding `Metadata` request by wrapping the `CreateTopics` request in an envelope so that the controller may repeat the authorization and to improve auditability. This follows a similar pattern to how standard `CreateTopics` requests are forwarded to the controller.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'core/src/main/scala/kafka/server/AutoTopicCreationManager.scala', 'core/src/main/scala/kafka/server/ForwardingManager.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/unit/kafka/server/AutoTopicCreationManagerTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala']","Auto-creating topics in KIP-500 does not preserve the original principal from the corresponding 'Metadata' request, causing potential issues with authorization and compromising auditability."
3e7eddecd6a63ea6a9793d3270bef6d0be5c9021,1666048586,"MINOR: Address flakiness in `KRaftClusterTest::testDescribeQuorumRequestToBrokers` (#12738)

We have seen some errors such as the following:
```
org.opentest4j.AssertionFailedError: expected: not equal but was: <OptionalLong.empty>
Stacktrace
org.opentest4j.AssertionFailedError: expected: not equal but was: <OptionalLong.empty>
	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)
	at org.junit.jupiter.api.AssertNotEquals.failEqual(AssertNotEquals.java:276)
	at org.junit.jupiter.api.AssertNotEquals.assertNotEquals(AssertNotEquals.java:265)
	at org.junit.jupiter.api.AssertNotEquals.assertNotEquals(AssertNotEquals.java:260)
	at org.junit.jupiter.api.Assertions.assertNotEquals(Assertions.java:2815)
	at kafka.server.KRaftClusterTest.$anonfun$testDescribeQuorumRequestToBrokers$5(KRaftClusterTest.scala:818)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at kafka.server.KRaftClusterTest.testDescribeQuorumRequestToBrokers(KRaftClusterTest.scala:814)
```
The patch changes some of the assertions to wait longer for the condition to be satisfied.

Reviewers: Jason Gustafson <jason@confluent.io>",['core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala'],"`KRaftClusterTest::testDescribeQuorumRequestToBrokers` is encountering occasional assertion errors, possibly due to insufficient wait time for conditions to be met."
8e12c3eda6ba0f837979eefdd8d3f9d4d4d907c4,1575626015,"KAFKA-9267: ZkSecurityMigrator should not create /controller node

[KAFKA-9267](https://issues.apache.org/jira/browse/KAFKA-9267)

ZkSecurityMigrator might create a PERSISTENT /controller node with null data, it will lead to controller can't elect.

*More detailed description of your change,
if necessary. The PR title and PR message become
the squashed commit message, so use a separate
comment to ping reviewers.*

*Summary of testing strategy (including rationale)
for the feature or bug fix. Unit and/or integration
tests are expected for any behaviour change and
system tests should be considered for larger changes.*

Author: NanerLee <nanerlee@qq.com>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #7778 from NanerLee/fix-ZkSecurityMigrator
",['core/src/main/scala/kafka/admin/ZkSecurityMigrator.scala'],"The ZkSecurityMigrator may inadvertently create a PERSISTENT /controller node with null data, resulting in the controller failing to elect."
1233c963f81ebf09762754c7d4cde97c85ae364f,1561099294,"MINOR: Remove legacy kafka.admin.AdminClient (#6947)

It has been deprecated since 0.11.0, it was never meant as a publicly
supported API and people should use
`org.apache.kafka.clients.admin.AdminClient` instead. Its presence
causes confusion and people still use them accidentally at times.

`BrokerApiVersionsCommand` uses one method that is not available
in `org.apache.kafka.clients.admin.AdminClient`, we inline it for now.

Reviewers: David Arthur <mumrah@gmail.com>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/admin/AdminClient.scala', 'core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala', 'core/src/test/scala/integration/kafka/api/LegacyAdminClientTest.scala']","Legacy kafka.admin.AdminClient, a deprecated and unsupported API, is still accessible and causing confusion. Additionally, a method used in 'BrokerApiVersionsCommand' is missing from its proposed replacement, 'org.apache.kafka.clients.admin.AdminClient'."
deac5d93cef000f408332a7648a3062e4e25e388,1567699852,"KAFKA-8724; Improve range checking when computing cleanable partitions (#7264)

This patch contains a few improvements on the offset range handling when computing the cleanable range of offsets.

1. It adds bounds checking to ensure the dirty offset cannot be larger than the log end offset. If it is, we reset to the log start offset.
2. It adds a method to get the non-active segments in the log while holding the lock. This ensures that a truncation cannot lead to an invalid segment range.
3. It improves exception messages in the case that an inconsistent segment range is provided so that we have more information to find the root cause.

The patch also fixes a few problems in `LogCleanerManagerTest` due to unintended reuse of the underlying log directory.

Reviewers: Vikas Singh <soondenana@users.noreply.github.com>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/main/scala/kafka/log/LogCleanerManager.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerTest.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']","Cleanable partition computation in Kafka might suffer from offset range handling issues. This includes dirty offsets being larger than log end offsets, unintended log directory reuse in tests, and inconsistent segment ranges due to possible truncation."
7c280c1d5f6267acbbebd10d3e58ea4b8fe7a4ef,1646314841,"KAFKA-13673: disable idempotence when config conflicts (#11788)

Disable idempotence when conflicting config values for acks, retries
and max.in.flight.requests.per.connection are set by the user. For the
former two configs, we log at info level when we disable idempotence
due to conflicting configs. For the latter, we log at warn level since
it's due to an implementation detail that is likely to be surprising.

This mitigates compatibility impact of enabling idempotence by default.

Added unit tests to verify the change in behavior.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>, Mickael Maison <mickael.maison@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java', 'clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java']","Conflicting config values for acks, retries, and max.in.flight.requests.per.connection are causing issues when idempotence is enabled."
a848e0c4208318e5db305876d14af4be0c3ce5fc,1614715200,"KAFKA-10357: Extract setup of changelog from Streams partition assignor (#10163)

To implement the explicit user initialization of Kafka Streams as
described in KIP-698, we first need to extract the code for the
setup of the changelog topics from the Streams partition assignor
so that it can also be called outside of a rebalance.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <guozhang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogTopics.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ChangelogTopicsTest.java']","Changelog topic setup tightly coupled with Streams partition assignor, restricting user initialization of Kafka Streams outside of a rebalance."
340cd07bab7ea0f9caeb0cbf50987bbe38cf126f,1601322996,"KAFKA-9584: Fix Headers ConcurrentModificationException in Streams (#8181)

Avoid forwarding a shared reference to the record context in punctuate calls.
Note, this fix isn't airtight, since all processors triggered by a single punctuate
call will still see the same reference to the record context. It's also not a terribly
principled approach, since the context is still technically not defined, but this
is about the best we can do without significant refactoring. We will probably
follow up with a more comprehensive solution, but this should avoid the issue
for most programs.

Reviewers: Matthias J. Sax <mjsax@apache.org>, John Roesler <vvcephei@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java']","Shared reference to record context in punctuate calls risks concurrent modification of Kafka stream headers, leading to ConcurrentModificationException."
f699bd98c14e31f07c5a3f6ba9ce2c4b441e7fdb,1594753174,"MINOR: Remove call to Exit.exit() to prevent infinite recursion in Connect integration tests (#9015)

If we call org.apache.kafka.common.utils.Exit#exit(int code) with code=0, the current implementation will go into an infinite recursion and kill the VM with a stack overflow error. This happens only in integration tests because of the overrides of shutdown procedures and this commit addresses this issue by removing the redundant call to Exit#exit. 

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",['connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectCluster.java'],Redundant call to org.apache.kafka.common.utils.Exit#exit(int code) with code=0 in integration tests leads to infinite recursion and subsequent VM stack overflow error.
b892acae5e026e1affd51ef9756772807674b964,1693885250,"KAFKA-15424: Make the transaction verification a dynamic configuration (#14324)

This will allow enabling and disabling transaction verification (KIP-890 part 1) without having to roll the cluster.

Tested that restarting the cluster persists the configuration.

If a verification is disabled/enabled while we have an inflight request, depending on the step of the process, the change may or may not be seen in the inflight request (enabling will typically fail unverified requests, but we may still verify and reject when we first disable) Subsequent requests/retries will behave as expected for verification.

Sequence checks will continue to take place after disabling until the first message is written to the partition (thus clearing the verification entry with the tentative sequence) or the broker restarts/partition is reassigned which will clear the memory. On enabling, we will only track sequences that for requests received after the verification is enabled.

Reviewers: Jason Gustafson <jason@confluent.io>, Satish Duggana <satishd@apache.org>","['core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala', 'core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala', 'core/src/test/scala/unit/kafka/log/UnifiedLogTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'storage/src/main/java/org/apache/kafka/storage/internals/log/ProducerStateManagerConfig.java']","The inability to enable and disable transaction verification dynamically without restarting the entire cluster often causes issues when dealing with inflight requests, sequence checks, and memory management."
6b5e9e989b7a1f8c387a79dea0117e52401853e1,1673054879,"MINOR: add error reason when controller failed to handle events (#13050)

In KRaft, when controller failed to handle events, we'll log error and return back to brokers. But in some cases, we only log error class name, and return error class name back to brokers, which is un-useful for troubleshooting. Ex: When broker registration failed with unsupported version error, it showed:

2022-12-28T17:46:42.876+0800 [DEBUG] [TestEventLogger]     [2022-12-28 17:46:42,877] INFO [Controller 3000] registerBroker: failed with UnsupportedVersionException in 2888 us (org.apache.kafka.controller.QuorumController:447)

2022-12-28T17:46:42.877+0800 [DEBUG] [TestEventLogger]     [2022-12-28 17:46:42,878] INFO [BrokerLifecycleManager id=0] Unable to register broker 0 because the controller returned error UNSUPPORTED_VERSION (kafka.server.BrokerLifecycleManager:66)

Checking the logs, we still don't know which version it supports.
After this PR, it will show:

2022-12-28T17:54:59.671+0800 [DEBUG] [TestEventLogger]     [2022-12-28 17:54:59,671] INFO [Controller 3000] registerBroker: failed with UnsupportedVersionException in 291 us. Reason: Unable to register because the broker does not support version 8 of metadata.version. It wants a version between 4 and 4, inclusive. (org.apache.kafka.controller.QuorumController:447)

2022-12-28T17:54:59.671+0800 [DEBUG] [TestEventLogger]     [2022-12-28 17:54:59,672] INFO [BrokerLifecycleManager id=0] Unable to register broker 0 because the controller returned error UNSUPPORTED_VERSION (kafka.server.BrokerLifecycleManager:66)

Reviewers: dengziming <dengziming1993@gmail.com>, Federico Valeri <fvaleri@redhat.com >",['metadata/src/main/java/org/apache/kafka/controller/QuorumController.java'],"When KRaft controller fails to handle events, it only logs and returns the error class name to brokers, which is not useful for troubleshooting such as identifying unsupported version errors during broker registration."
1dbcb7da9e3625ec2078a82f84542a3127730fef,1687454379,"KAFKA-14694: RPCProducerIdManager should not wait on new block (#13267)

RPCProducerIdManager initiates an async request to the controller to grab a block of producer IDs and then blocks waiting for a response from the controller.

This is done in the request handler threads while holding a global lock. This means that if many producers are requesting producer IDs and the controller is slow to respond, many threads can get stuck waiting for the lock.

This patch aims to:
* resolve the deadlock scenario mentioned above by not waiting for a new block and returning an error immediately
* remove synchronization usages in RpcProducerIdManager.generateProducerId()
* handle errors returned from generateProducerId() so that KafkaApis does not log unexpected errors
* confirm producers backoff before retrying
* introduce backoff if manager fails to process AllocateProducerIdsResponse

Reviewers: Artem Livshits <alivshits@confluent.io>, Jason Gustafson <jason@confluent.io>","['clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'core/src/main/scala/kafka/coordinator/transaction/ProducerIdManager.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/test/scala/integration/kafka/coordinator/transaction/ProducerIdsIntegrationTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/ProducerIdManagerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/server/AddPartitionsToTxnRequestServerTest.scala', 'server-common/src/main/java/org/apache/kafka/server/common/ProducerIdsBlock.java', 'server-common/src/test/java/org/apache/kafka/server/common/ProducerIdsBlockTest.java']",RPCProducerIdManager while holding a global lock can cause threads to get stuck waiting for the lock if many producers are requesting IDs and the controller is slow to respond.
2b26db0d38f7245505812e4cb3fa622fc07ba6c8,1680245530,"Switch to SplittableRandom in ProducerPerformance utility (#13482)

Why:
Using java.util.Random to generate every byte sent from the ProducerPerformance
appears to be a limiting factor. Throughput of the ProducerPerformance script is
higher with a file of records as compared to randomly generated records.

On my machine a single thread can generate ~100MB/second of uppercase letters using
java.util.Random and ~300MB/sec using java.util.SplittableRandom. This is a limit on
throughput.

Note: you can optimise further by expanding it from 26 letters to 32 letter generated
as it is more efficient to generate a nicely distributed int when the bound is a
power of two.

Reviewers: Luke Chen <showuon@gmail.com>","['tools/src/main/java/org/apache/kafka/tools/ProducerPerformance.java', 'tools/src/test/java/org/apache/kafka/tools/ProducerPerformanceTest.java']","The use of java.util.Random in the ProducerPerformance utility is reducing throughput especially when generating random records, as compared to fixed record files."
9a36d9f913e3474a3c4c83e91699759c5fa848a2,1587053642,"KAFKA-9796; Ensure broker shutdown is not stuck when Acceptor is waiting on connection queue (#8448)

This commit reworks the SocketServer to always start the acceptor threads after the processor threads and to always stop the acceptor threads before the processor threads. It ensures that the acceptor shutdown is not blocked waiting on the processors to be fully shutdown by decoupling the shutdown signal and the awaiting. It also ensure that the processor threads drain its newConnection queue to unblock acceptors that may be waiting. However, the acceptors still bind during the startup, only the processing of new connections and requests is further delayed.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>","['core/src/main/scala/kafka/network/SocketServer.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/test/scala/unit/kafka/network/SocketServerTest.scala']",Broker shutdown process gets stuck when acceptor is waiting on the connection queue in SocketServer.
c5954175a487639faf5e82d8e96c1364e5861c69,1663081480,"KAFKA-14222; KRaft's memory pool should always allocate a buffer (#12625)

Because the snapshot writer sets a linger ms of Integer.MAX_VALUE it is
possible for the memory pool to run out of memory if the snapshot is
greater than 5 * 8MB.

This change allows the BatchMemoryPool to always allocate a buffer when
requested. The memory pool frees the extra allocated buffer when released if
the number of pooled buffers is greater than the configured maximum
batches.

Reviewers: Jason Gustafson <jason@confluent.io>","['raft/src/main/java/org/apache/kafka/raft/internals/BatchMemoryPool.java', 'raft/src/test/java/org/apache/kafka/raft/internals/BatchMemoryPoolTest.java']",KRaft's memory pool runs out of memory when snapshot size exceeds 40MB due to lingering snapshots and inability to allocate additional buffers.
6380652a5a3c66bef4dc8231c3eb4842b3556148,1652978808,"KAFKA-13863; Prevent null config value when create topic in KRaft mode (#12109)

This patch ensures consistent handling of null-valued topic configs between the zk and kraft controller. Prior to this patch, we returned INVALID_REQUEST in zk mode and it was not an error in kraft. After this patch, we return INVALID_CONFIG consistently for this case.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/ControllerConfigurationValidator.scala', 'core/src/main/scala/kafka/server/ZkAdminManager.scala', 'core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/ControllerConfigurationValidatorTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java']","Inconsistent handling of null-valued topic configs between zk and kraft controller, which aren't returning INVALID_CONFIG in the latter scenario."
b7f20be809c47202ace4b54e69be5428012df1a9,1662674740,"KAFKA-14201; Consumer should not send group instance ID if committing with empty member ID (#12599)

The consumer group instance ID is used to support a notion of ""static"" consumer groups. The idea is to be able to identify the same group instance across restarts so that a rebalance is not needed. However, if the user sets `group.instance.id` in the consumer configuration, but uses ""simple"" assignment with `assign()`, then the instance ID nevertheless is sent in the OffsetCommit request to the coordinator. This may result in a surprising UNKNOWN_MEMBER_ID error.

This PR fixes the issue on the client side by not setting the group instance id if the member id is empty (no generation).

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java']","Consumer using simple assignment with `assign()` sends group instance ID in the OffsetCommit request resulting in unexpected UNKNOWN_MEMBER_ID error, whenever `group.instance.id` is set in the consumer configuration."
cda5da9b65f78b51cdfe5371f712a0d392dbdb4d,1663863637,"KAFKA-14209: Change Topology optimization to accept list of rules 1/3 (#12641)

This PR is part of a series implementing the self-join rewriting. As part of it, we decided to clean up the TOPOLOGY_OPTIMIZATION_CONFIG and make it a list of optimization rules. Acceptable values are: NO_OPTIMIZATION, OPTIMIZE which applies all optimization rules or a comma separated list of specific optimizations.

Reviewers: Guozhang Wang <guozhang@apache.org>, John Roesler <vvcephei@apache.org>","['streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java', 'streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java']","The TOPOLOGY_OPTIMIZATION_CONFIG currently doesn't accept a list of specific optimization rules, limiting the flexibility of topology optimization configuration."
df6efda1f2618c922821b14784857ab69c76c8d3,1562774254,"MINOR: Only invoke hw update logic for follower fetches (#7064)

We noticed a lot of messages like the following in the broker logs recently:
```
[2019-07-10 02:01:23,946] WARN [ReplicaManager broker=0] While updating the HW for follower -1 for partition connect-storage-topic-connect-cluster-0, the replica could not be found. (kafka.server.ReplicaManager:70)
```
In the KIP-392 PR, we added logic to track the high watermark of followers, but it is invoked even for consumer fetches. This doesn't cause any harm other than all the log noise. 

This patch just adds the missing follower check.

Reviewers: David Arthur <mumrah@gmail.com>",['core/src/main/scala/kafka/server/ReplicaManager.scala'],"The high watermark tracking for followers in the KIP-392 update is invoked even for consumer fetches, generating unnecessary log noise."
c49775bf072f7265b744fd4f3fbfd123d7a20c49,1570568796,"KAFKA-7190; Retain producer state until transactionalIdExpiration time passes (#7388)

As described in KIP-360, this patch changes producer state retention so that producer state remains cached even after it is removed from the log. Producer state will only be removed now when the transactional id expiration time has passed. This is intended to reduce the incidence of UNKNOWN_PRODUCER_ID errors for producers when records are deleted or when a topic has a short retention time. Tested with unit tests.

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/ProducerStateManager.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala', 'core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala']",Removal of producer state from cache upon removal from log is causing UNKNOWN_PRODUCER_ID errors when records are deleted or a topic has a short retention time.
f86cb1d1da4f73bf92b0fcfd5ef75b3ce77658cf,1624928419,"KAFKA-12631; Implement `resign` API in `KafkaRaftClient` (#10913)

This patch adds an implementation of the `resign()` API which allows the controller to proactively resign leadership in case it encounters an unrecoverable situation. There was not a lot to do here because we already supported a `Resigned` state to facilitate graceful shutdown.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, David Arthur <mumrah@gmail.com>","['raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/LeaderState.java', 'raft/src/main/java/org/apache/kafka/raft/QuorumState.java', 'raft/src/main/java/org/apache/kafka/raft/RaftClient.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java']","The KafkaRaftClient lacks a `resign()` function, preventing controllers from proactively resigning leadership in unrecoverable situations."
f19cd6ca48ccd2971948c9589279bb8ee20e5d88,1598293139,"KAFKA-10312; Fix error code returned in Metadata response when leader is not available (#9112)

MetadataCache#getPartitionMetadata returns an error when the topic's leader Id
is present at MetadataCache but listener endpoint is not present for this leader.
For older versions, LEADER_NOT_AVAILABLE is returned while LISTENER_NOT_FOUND is
returned for new metadata versions.

The problem is that getPartitionMetadata was looking up MetadataCache's host brokerId rather
than the topic's leader id while determining what error to return. This
could result in the call returning LISTENER_NOT_FOUND when it should
have returned LEADER_NOT_AVAILABLE. This commit corrects this behavior.

Unit tests were already present to test out the error codes returned
under different situations but they were giving out a false positive.
The test was using same broker id for both the MetadataCache's host as
well as for the topic's leader. Error manifests when the MetadataCache's
host id is changed. Improved the test.

This commit also consolidated couple of related tests to reduce code
duplication.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/MetadataCache.scala', 'core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala']",Inconsistent error code returned in Metadata response due to incorrect lookup between MetadataCache's host brokerId and the topic's leader id.
fea0eb4ca3c0b18ab876ba3820ee05b0f27a5f8a,1669637804,"KAFKA-14299: Handle double rebalances better (#12904)

The original implementation of the state updater could not
handle double rebalances within one poll phase correctly,
because it could create tasks more than once if they hadn't
finished initialization yet.

In a55071a, we
moved initialization to the state updater to fix this. However,
with more testing, I found out that this implementation has
it's problems as well: There are problems with locking the
state directory (state updater acquired the lock to the state
directory, so the main thread wouldn't be able to clear the
state directory when closing the task), and benchmarks also
show that this can lead to useless work (tasks are being
initialized, although they will be taken from the thread soon
after in a follow-up rebalance).

In this PR, I propose to revert the original change, and fix
the original problem in a much simpler way: When we
receive an assignment, we simply clear out the
list of tasks pending initialization. This way, no double
tasks instantiations can happen.

The change was tested in benchmarks, system tests,
and the existing unit & integration tests. We also add
the state updater to the smoke integration test, which
triggered the double task instantiations before.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Tasks.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TasksRegistry.java', 'streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']",State updater unable to handle double rebalances within a single poll phase could result in multiple creation of tasks and issues with task initialization leading to potential locking issues and unnecessary task loads.
b0935e548b9d698f3ee7e9cb2ea5efc9647d6ba3,1562130922,"MINOR: Embedded connect cluster should mask exit procedures by default (#7028)

`EmbeddedConnectCluster` has the ability to mask system exits to avoid killing the jvm. It appears that the default was intended to be `true`, but is actually `false`. The `maskExitProcedures` method on `EmbeddedConnectCluster.Builder` documents the parameter as:

```
* @param mask if false, exit and halt procedures remain unchanged; true is the default.
```
Because this is not enabled by default as intended, we are seeing some build failures which exit abruptly:
```
17:29:11 Execution failed for task ':connect:runtime:integrationTest'.
17:29:11 > Process 'Gradle Test Executor 25' finished with non-zero exit value 1
```
The culprit often appears to be `ExampleConnectIntegrationTest`, which indeed does not override the default value of `maskExitProcedures`.

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>",['connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectCluster.java'],"The default value for 'maskExitProcedures' in `EmbeddedConnectCluster` is incorrectly set as `false`, causing abrupt build failures in 'ExampleConnectIntegrationTest' due to non-masked system exit procedures."
528a777df66ab1e66544ad2a8fb3d020ff229bff,1676336804,"KAFKA-14491: [6/N] Support restoring RocksDB versioned store from changelog (#13189)

This PR builds on the new RocksDB-based versioned store implementation (see KIP-889) by adding code for restoring from changelog. The changelog topic format is the same as for regular timestamped key-value stores: record keys, values, and timestamps are stored in the Kafka message key, value, and timestamp, respectively. The code for actually writing to this changelog will come in a follow-up PR.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBVersionedStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBVersionedStoreRestoreWriteBuffer.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBVersionedStoreTest.java']","The new RocksDB-based versioned store implementation lacks the functionality to restore from the changelog, limiting its usability.
"
2bb74bfc3bc7e94c00280a7de884b95f160f8797,1551881612,"KAFKA-7979 - Clean up threads and increase timeout in PartitionTest (#6378)

Stack trace generated from the test failure shows that test failed even though threads were runnable and making progress, indicating that the timeout may be too small when test machine is slow. Increasing timeout from 10 to 15 seconds, consistent with the default wait in other tests. Thread dump also showed a lot of left over threads from other tests, so added clean up of those as well.

Reviewers: Ismael Juma <ismael@juma.me.uk>",['core/src/test/scala/unit/kafka/cluster/PartitionTest.scala'],"PartitionTest failure due to timeout while threads are still runnable and active, suggesting short timeout duration especially on slower machines. Presence of leftover threads from previous tests detected."
9e71d818d6d90dc427a1f30979192991c570e041,1661970958,"KAFKA-13990: KRaft controller should return right features in ApiVersionResponse (#12294)

Previously, the KRaft controller was incorrectly reporting an empty feature set in
ApiVersionResponse. This was preventing any multi-node clusters from being upgraded via
kafka-features.sh, since they would incorrectly believe that metadata.version was not a supported
feature. This PR adds a regression test for this bug, KRaftClusterTest.testUpdateMetadataVersion.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Colin P. McCabe <cmccabe@apache.org>","['clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java', 'core/src/main/scala/kafka/server/ApiVersionManager.scala', 'core/src/main/scala/kafka/server/BrokerFeatures.scala', 'core/src/test/scala/integration/kafka/network/DynamicNumNetworkThreadsTest.scala', 'core/src/test/scala/integration/kafka/server/FetchRequestBetweenDifferentIbpTest.scala', 'core/src/test/scala/integration/kafka/server/FetchRequestTestDowngrade.scala', 'core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala', 'core/src/test/scala/integration/kafka/server/MetadataVersionIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala']","KRaft controller mistakenly reports an empty feature set in ApiVersionResponse, preventing multi-node clusters from being upgradeable via kafka-features.sh due to a misunderstanding that metadata.version is unsupported."
c15cd5cfeb52ce9cd0d40b7d44e442bcd4ab61c1,1588804213,"MINOR: Only add 'Data' suffix for generated request/response/header types (#8625)

Currently we add ""Data"" to all generated classnames in order to avoid naming collisions with existing Request/Response objects. Generated classes for other persistent schema definitions (such as those used in `GroupCoordinator` and `TransactionCoordinator`) will not necessarily have the same problem, so it would be nice if the generated types could use the name defined in the schema directly.

Reviewers: Boyang Chen <boyang@confluent.io>, Colin P. McCabe <cmccabe@apache.org>","['generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java', 'generator/src/main/java/org/apache/kafka/message/MessageSpec.java']","Unnecessary 'Data' suffix is being added to all generated classnames in persistent schema definitions, causing potential confusion despite no existing naming collisions."
ddd3dfbfae77e789eedbb9a0a32a0f382e194b7f,1584269169,"MINOR: comment apikey types in generated switch (#8201)

As a developer, it would be convenient if the generated
{request,response}HeaderVersion case statements in ApiMessageType.java
included a comment to remind me which type each of them is so I don't
need to manually cross-reference the newer/rarer ones.

Also include commented lines for the two special cases around
ApiVersionsResponse and ControllerShutdownRequest which are hardcoded in
the ApiMessageTypeGenerator.java and not covered by the message format
json files.

Before:
```java
    public short requestHeaderVersion(short _version) {
        switch (apiKey) {
            case 0:
                return (short) 1;
            case 1:
                return (short) 1;
            case 2:
                return (short) 1;
            case 3:
                if (_version >= 9) {
                    return (short) 2;
                } else {
                    return (short) 1;
                }
            // ...etc
```

After:
```java
    public short requestHeaderVersion(short _version) {
        switch (apiKey) {
            case 0: // Produce
                return (short) 1;
            case 1: // Fetch
                return (short) 1;
            case 2: // ListOffset
                return (short) 1;
            case 3: // Metadata
                if (_version >= 9) {
                    return (short) 2;
                } else {
                    return (short) 1;
                }
            // ...etc
```

Signed-off-by: Dominic Evans <dominic.evans@uk.ibm.com>

Reviewers: Mickael Maison <mickael.maison@gmail.com>",['generator/src/main/java/org/apache/kafka/message/ApiMessageTypeGenerator.java'],Absence of comments in generated switch statements in ApiMessageType.java makes it hard to identify API key types and special cases without manual cross-referencing.
9a0b694a6686d0dc165d7dab54be0f77535582fa,1591558920,"KAFKA-9216: Enforce internal config topic settings for Connect workers during startup (#8270)

Currently, Kafka Connect creates its config backing topic with a fire and forget approach.
This is fine unless someone has manually created that topic already with the wrong partition count.

In such a case Kafka Connect may run for some time. Especially if it's in standalone mode and once switched to distributed mode it will almost certainly fail.

This commits adds a check when the KafkaConfigBackingStore is starting.
This check will throw a ConfigException if there is more than one partition in the backing store.

This exception is then caught upstream and logged by either:
- DistributedHerder#run
- ConnectStandalone#main

A unit tests was added in KafkaConfigBackingStoreTest to verify the behaviour.

Author: Evelyn Bayes <evelyn@confluent.io>
Co-authored-by: Randall Hauch <rhauch@gmail.com>

Reviewer: Konstantine Karantasis <konstantine@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java']","Kafka Connect fails when switched to distributed mode after running in standalone mode, if the internal config topic has been manually created with the wrong partition count."
29825ee24f91e17a6193de0755bec668821e94a3,1690456690,"KAFKA-14499: [3/N] Implement OffsetCommit API (#14067)

This patch introduces the `OffsetMetadataManager` and implements the `OffsetCommit` API for both the old rebalance protocol and the new rebalance protocol. It introduces version 9 of the API but keeps it as unstable for now. The patch adds unit tests to test the API. Integration tests will be done separately.

Reviewers: Jeff Kim <jeff.kim@confluent.io>, Justine Olshan <jolshan@confluent.io>","['clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/Group.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorConfig.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/OffsetAndMetadata.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/OffsetMetadataManager.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/ReplicatedGroupCoordinator.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/Utils.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/generic/GenericGroup.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorConfigTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetAndMetadataTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/ReplicatedGroupCoordinatorTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroupTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/generic/GenericGroupTest.java']","OffsetCommit API not implemented for both old and new rebalance protocol, no means to manage offset metadata."
790a39e353d957632b8296346890dbc8d4694255,1579906042,"KAFKA-9254; Overridden topic configs are reset after dynamic default change (#7870)

Currently, when a dynamic change is made to the broker-level default log configuration, existing log configs will be recreated with an empty overridden configs. In such case, when updating dynamic broker configs a second round, the topic-level configs are lost. This can cause unexpected data loss, for example, if the cleanup policy changes from ""compact"" to ""delete.""

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Jason Gustafson <jason@confluent.io>
","['core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala']","When a dynamic change is made to the broker-level default log configuration, existing log configs with overridden topic configurations are lost in subsequent dynamic broker config updates, potentially leading to unexpected data loss."
7a4618a793aacd240745c8de0ad7e502121f5dc2,1557452108,"MINOR: Remove header and key/value converter config value logging (#6660)

The debug log lines in the `Plugins` class that log header and key/value converter configurations should be altered as the configurations for these converters may contain secrets that should not be logged in plaintext. Instead, only the keys for these configs are safe to expose.

Author: Chris Egerton <cegerton@oberlin.edu>
Reviewer: Randall Hauch <rhauch@gmail.com>",['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java'],"The configuration for header and key/value converters, containing potential secrets, are being logged in plaintext, posing a security risk."
df2236d73bc630a588adc049e7e63a4a5095da32,1642627241,"KAFKA-13412; Ensure initTransactions() safe for retry after timeout (#11452)

If the user's `initTransactions` call times out, the user is expected to retry. However, the producer will continue retrying the `InitProducerId` request in the background. If it happens to return before the user retry of `initTransactions`, then the producer will raise an exception about an invalid state transition. 

The patch fixes the issue by tracking the pending state transition until the user has acknowledged the operation's result. In the case of `initTransactions`, even if the `InitProducerId` returns in the background and the state changes, we can still retry the `initTransactions` call to obtain the result.

Reviewers: David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionalRequestResult.java', 'clients/src/main/java/org/apache/kafka/common/utils/ProducerIdAndEpoch.java', 'clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java', 'core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/TransactionsTest.scala']","Timeout during `initTransactions` call leads to exception about an invalid state transition when a user retries, due to producer's unexpected state changes in handling `InitProducerId` in the background."
3072b3d23e43f3bc3478eca1087312edb620dc6b,1655828673,"MINOR: Fix AlterPartitionManager topic id handling in response handler (#12317)

https://github.com/apache/kafka/commit/f83d95d9a28267f7ef7a7b1e584dcdb4aa842210 introduced topic ids in the AlterPartitionRequest/Response and we just found a bug in the request handling logic. The issue is the following.

When the `AlterPartitionManager` receives the response, it builds the `partitionResponses` mapping `TopicIdPartition` to its result. `TopicIdPartition` is built from the response. Therefore if version < 2 is used, `TopicIdPartition` will have the `ZERO` topic id. Then the `AlterPartitionManager` iterates over the item sent to find their response. If an item has a topic id in its `TopicIdPartition` and version < 2 was used, it cannot find it because one has it and the other one has not.

This patch fixes the issue by using `TopicPartition` as a key in the `partitionResponses` map. This ensures that the result can be found regardless of the topic id being set or not.

Note that the case where version 2 is used is handled correctly because we already have logic to get back the topic name from the topic id in order to construct the `TopicPartition`.

`testPartialTopicIds` test was supposed to catch this but it didn't due to the ignorable topic id field being present. This patch fixes the test as well.

Reviewers: Kvicii <42023367+Kvicii@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/AlterPartitionManager.scala', 'core/src/test/scala/unit/kafka/server/AlterPartitionManagerTest.scala']","AlterPartitionManager fails to build correct partitionResponses mapping due to inconsistent use of TopicIdPartition, resulting in inability to locate responses for items when version < 2 is used."
dfef2a3ec2f69a5159cc6d1def3130197dba9cf9,1582754596,"KAFKA-9614: Not initialize topology twice in StreamTask (#8173)

We only initialize topology when transiting from restoring -> running.

Also tighten some unit tests for this fix:
a. restoring -> suspended should just write checkpoint file without committing.
b. suspended -> restoring should not need any inner updates.
c. restoring -> running should always try to fetch committed offsets, and forward timeout exceptions.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java']","Topology initialization is incorrectly triggered twice in StreamTask, specifically when transitioning from the restoring to running state, causing discrepancies in task transitions."
dcff0878c48803e2d68f7e43c1e73735b643ace0,1693565124,"KAFKA-14499: [5/N] Refactor GroupCoordinator.fetchOffsets and GroupCoordinator.fetchAllOffsets (#14310)

This patch refactors the GroupCoordinator.fetchOffsets and GroupCoordinator.fetchAllOffsets methods to take an OffsetFetchRequestGroup and to return an OffsetFetchResponseGroup. It prepares the ground for adding the member id and the member epoch to the OffsetFetchRequest. This change also makes those two methods more aligned with the others in the interface.

Reviewers: Calvin Liu <caliu@confluent.io>, Justine Olshan <jolshan@confluent.io>","['core/src/main/scala/kafka/coordinator/group/GroupCoordinatorAdapter.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorAdapterTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorShard.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/OffsetMetadataManager.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java']",GroupCoordinator.fetchOffsets and GroupCoordinator.fetchAllOffsets methods lack alignment with other methods in the interface and do not support addition of member id and member epoch to the OffsetFetchRequest.
09136235db2fcae5282a218de75b6356f9420031,1551394047,"KAFKA-7912: Support concurrent access in InMemoryKeyValueStore (#6336)

Previously the InMemoryKeyValue store would throw a ConcurrentModificationException if the store was modified beneath an open iterator. The TreeMap implementation was swapped with a ConcurrentSkipListMap for similar performance while supporting concurrent access.

Added one test to AbstractKeyValueStoreTest, no existing tests caught this.

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractKeyValueStoreTest.java']","InMemoryKeyValueStore throws a ConcurrentModificationException when the store is modified beneath an open iterator, inhibiting concurrent access."
a46da90b8f6cca04dd0f89fa774267898c039e47,1695718717,"KAFKA-10199: Add missing catch for lock exception (#14403)

The state directory throws a lock exception during initialization if a task state directory is still locked by the stream thread that previously owned the task. When this happens, Streams catches the lock exception, ignores the exception, and tries to initialize the task in the next exception.

In the state updater code path, we missed catching the lock exception when Streams recycles a task. That leads to the lock exception thrown to the exception handler, which is unexpected and leads to test failures.

Reviewer: Lucas Brutschy <lbrutschy@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","The state directory initialization throws an unexpected lock exception when recycling a task in the state updater code path, which results in test failures."
2d04370bca7ab5995371ce5501b2248c279e1d6f,1695729955,"KAFKA-10199: Fix restoration behavior for paused tasks (#14437)

State updater can get into a busy loop when all tasks are paused, because changelogReader will never return that all changelogs have been read completely. Fix this, by awaiting if updatingTasks is empty.

Related and included: if we are restoring and all tasks are paused, we should return immediately from StoreChangelogReader.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java']","State updater gets into a busy loop when all tasks are paused as changelogReader never confirms complete reading of all changelogs, causing inefficient task restoration."
6fe74f78dc725466319bd0465764913532949ba3,1686489286,"KAFKA-10199: Re-add revived tasks to the state updater after handling (#13829)

Fixes a bug regarding the state updater where tasks that experience corruption
during restoration are passed from the state updater to the stream thread
for closing and reviving but then the revived tasks are not re-added to
the state updater.

Reviewers: Lucas Brutschy <lbrutschy@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","Revived tasks after corruption during restoration are not being re-added to the state updater, causing inconsistency in task handling."
eebc6f279e61bcd9a331a3a0b305b57ffdc1ab66,1614886100,"MINOR: Enable topic deletion in the KIP-500 controller (#10184)

This patch enables delete topic support for the new KIP-500 controller. Also fixes the following:
- Fix a bug where feature level records were not correctly replayed.
- Fix a bug in TimelineHashMap#remove where the wrong type was being returned.

Reviewers: Jason Gustafson <jason@confluent.io>, Justine Olshan <jolshan@confluent.io>, Ron Dagostino <rdagostino@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Jun Rao <junrao@gmail.com>

Co-authored-by: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/ControllerApis.scala', 'core/src/test/java/kafka/test/MockController.java', 'core/src/test/scala/unit/kafka/server/ControllerApisTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/BrokersToIsrs.java', 'metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/Controller.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ResultOrError.java', 'metadata/src/main/java/org/apache/kafka/timeline/SnapshottableHashTable.java', 'metadata/src/main/java/org/apache/kafka/timeline/TimelineHashMap.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/timeline/TimelineHashMapTest.java']","KIP-500 controller lacks support for topic deletion. Additionally, feature level records aren't accurately replayed and TimelineHashMap#remove returns the incorrect type."
e5c4ebdd7433d746a46d6962ee04ff2d782d892b,1561328527,"KAFKA-8179: Part 2, ConsumerCoordinator Algorithm (#6778)

1. In ConsumerCoordinator, select the protocol as the common protocol from all configured assignor instances' supported protocols with the highest number.
1.b. In onJoinPrepare: only call onPartitionRevoked with EAGER.
1.a. In onJoinComplete: call onPartitionAssigned with EAGER; call onPartitionRevoked following onPartitionAssigned with COOPERATIVE, and then request re-join if the error indicates so.
1.c. In performAssignment: update the user's assignor returned assignments by excluding all partitions that are still owned by some other members.

2. I've refactored the Subscription / Assignment such that: assigned partitions, error codes, and group instance id are not-final anymore, instead they can be updated. For the last one, it is directly related to the logic of this PR but I felt it is more convienent to go with other fields.

3. Testing: primarily in ConsumerCoordinatorTest, make it parameterized with protocol, and add necessary scenarios for COOPERATIVE protocol.

I intentionally omitted the documentation change since there are some behavioral updates that needs to be finalized in later PRs, and hence I will also only add the docs in later PRs.

Reviewers: Bill Bejeck <bbejeck@gmail.com>, Boyang Chen <boyang@confluent.io>, Sophie Blee-Goldman <sophie@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocol.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignor.java', 'clients/src/test/java/org/apache/kafka/clients/MockClient.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocolTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/MockPartitionAssignor.java', 'clients/src/test/java/org/apache/kafka/test/TestUtils.java']","ConsumerCoordinator's protocol selection can't handle COOPERATIVE scenarios properly, causing issues when trying to update non-final fields such as assigned partitions, error codes, and group instance ID."
7cd09b6a14f51aeef9e701fe24adf9f4f07170b0,1623355197," KAFKA-10585: Kafka Streams should clean up the state store directory from cleanup (#9414)

1. Update StateDirectory#clean
  - Delete application's statestore directory in cleanup process if it is empty.
2. Add Tests
  - StateDirectoryTest#shouldDeleteAppDirWhenCleanUpIfEmpty: asserting the empty application directory is deleted with StateDirectory#clean.
  - StateDirectoryTest#shouldNotDeleteAppDirWhenCleanUpIfNotEmpty: asserting the non-empty application directory is not deleted with StateDirectory#clean and appropriate log message is generated.
  - Add Integration test: StateDirectoryIntegrationTest
3. Improve EOSUncleanShutdownIntegrationTest: test all available cases regarding cleanup process on unclean shutdown.

Reviewers: John Roesler <vvcephei@apache.org>, Guozhang Wang <guozhang@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StateDirectoryIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java']",Kafka Streams state store does not clean up empty application directory during cleanup process.
fd9a20e4167c51c6645c55ed98800b768518c863,1559231445,"KAFKA-8429; Handle offset change when OffsetForLeaderEpoch inflight (#6811)

It is possible for the offset of a partition to be changed while we are in the middle of validation. If the OffsetForLeaderEpoch request is in-flight and the offset changes, we need to redo the validation after it returns. We had a check for this situation previously, but it was only checking if the current leader epoch had changed. This patch fixes this and moves the validation in `SubscriptionState` where it can be protected with a lock.

Additionally, this patch adds test cases for the SubscriptionState validation API. We fix a small bug handling broker downgrades. Basically we should skip validation if the latest metadata does not include leader epoch information.

Reviewers: David Arthur <mumrah@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java']",In-flight OffsetForLeaderEpoch requests may not handle sudden offset changes correctly during validation process and may exhibit issues with latest metadata not including leader epoch information.
6d2c7802da3e3ca29c1b225bd9d41ad29a6e5417,1595010453,"MINOR: Fix flaky system test assertion after static member fencing (#9033)

The test case `OffsetValidationTest.test_fencing_static_consumer` fails periodically due to this error:
```
Traceback (most recent call last):
  File ""/home/jenkins/workspace/system-test-kafka_2.6/kafka/venv/lib/python2.7/site-packages/ducktape-0.7.8-py2.7.egg/ducktape/tests/runner_client.py"", line 134, in run
    data = self.run_test()
  File ""/home/jenkins/workspace/system-test-kafka_2.6/kafka/venv/lib/python2.7/site-packages/ducktape-0.7.8-py2.7.egg/ducktape/tests/runner_client.py"", line 192, in run_test
    return self.test_context.function(self.test)
  File ""/home/jenkins/workspace/system-test-kafka_2.6/kafka/venv/lib/python2.7/site-packages/ducktape-0.7.8-py2.7.egg/ducktape/mark/_mark.py"", line 429, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/home/jenkins/workspace/system-test-kafka_2.6/kafka/tests/kafkatest/tests/client/consumer_test.py"", line 257, in test_fencing_static_consumer
    assert len(consumer.dead_nodes()) == num_conflict_consumers
AssertionError
```
When a consumer stops, there is some latency between when the shutdown is observed by the service and when the node is added to the dead nodes. This patch fixes the problem by giving some time for the assertion to be satisfied.

Reviewers: Boyang Chen <boyang@confluent.io>",['tests/kafkatest/tests/client/consumer_test.py'],"The test case `OffsetValidationTest.test_fencing_static_consumer` fails sporadically due to assertion error related to the latency in observing shutdown and adding thee node to dead nodes, impacting system test execution."
0e6a3fa97893d3c78fc871de6307c0e0f0faa776,1661790362,"KAFKA-10199: Handle restored tasks output by state updater (#12554)

Once the state updater restored an active task it puts it
into an output queue. The stream thread reads the restored
active task from the output queue and after it verified
that the task is still owned by the stream thread it transits
it to RUNNING.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Tasks.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TasksRegistry.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']",Active tasks restored by the state updater and placed in the output queue may not transit to RUNNING state if they are no longer owned by the stream thread.
a1f429d4f636b8cfe99e052a20b85ed1cc7a0cef,1592284021,"MINOR: Documentation for KIP-585 (#8839)

* Add documentation for using transformation predicates.
* Add `PredicateDoc` for generating predicate config docs, following the style of `TransformationDoc`.
* Fix the header depth mismatch.
* Avoid generating HTML ids based purely on the config name since there
are very likely to conflict (e.g. #name). Instead allow passing a function
which can be used to generate an id from a config key.

The docs have been generated and tested locally. 

Reviewer: Konstantine Karantasis <konstantine@confluent.io>","['clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/tools/PredicateDoc.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/tools/TransformationDoc.java', 'connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKey.java', 'connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/RecordIsTombstone.java', 'connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatches.java']","The current documentation lacks information on transformation predicates and has mismatched header depths. Additionally, HTML ids generated purely based on config names may cause conflicts."
14314e3d687b4c7b77750d27a085b803c934e3e9,1556981058,"[HOT FIX] in-memory store behavior should match rocksDB (#6657)

While working on consolidating the various store unit tests I uncovered some minor ""bugs"" in the in-memory stores (inconsistencies with the behavior as established by the RocksDB stores).

open iterators should be properly closed in the case the store is closed
fetch/findSessions should always throw NPE if key is null
window end time should be truncated at Long.MAX_VALUE rather than throw exception
(Verified in-memory stores pass all applicable rocksDB tests now, unified unit tests coming in another PR)

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java']","Inconsistent behavior between in-memory stores and RocksDB: iterators not being closed when store is closed, fetch/findSessions not throwing NullPointerException for null keys, and window end time not being truncated at Long.MAX_VALUE."
f538caf13819c79b37fcdd300d7dacd724180197,1585004294,"KAFKA-9742: Fix broken StandbyTaskEOSIntegrationTest (#8330)

Relax the requirement that tasks' reported offsetSum is less than the endOffsetSum for those
tasks. This was surfaced by a test for corrupted tasks, but it can happen with real corrupted
tasks. Rather than throw an exception on the leader, we now de-prioritize the corrupted task.
Ideally, that instance will not get assigned the task and the stateDirCleaner will make
the problem ""go away"". If it does get assigned the task, then it will detect the corruption and
delete the task directory before recovering the entire changelog. Thus, the estimate we provide
accurately reflects the amount of lag such a corrupted task would have to recover (the whole log).

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Bruno Cadonna <bruno@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTest.java']","Corrupted tasks caused by offsetSum exceeding endOffsetSum are not handled properly in StandbyTaskEOSIntegrationTest, potentially leading to issues in task assignment and directory recovery."
56051e763965d439f11f20f876475732eed7b307,1584675874,"KAFKA-8820: kafka-reassign-partitions.sh should support the KIP-455 API (#8244)

Rewrite ReassignPartitionsCommand to use the KIP-455 API when possible, rather
than direct communication with ZooKeeper.  Direct ZK access is still supported,
but deprecated, as described in KIP-455.

As specified in KIP-455, the tool has several new flags.  --cancel stops
an assignment which is in progress.  --preserve-throttle causes the
--verify and --cancel commands to leave the throttles alone.
--additional allows users to execute another partition assignment even
if there is already one in progress.  Finally, --show displays all of
the current partition reassignments.

Reorganize the reassignment code and tests somewhat to rely more on unit
testing using the MockAdminClient and less on integration testing.  Each
integration test where we bring up a cluster seems to take about 5 seconds, so
it's good when we can get similar coverage from unit tests.  To enable this,
MockAdminClient now supports incrementalAlterConfigs, alterReplicaLogDirs,
describeReplicaLogDirs, and some other APIs.  MockAdminClient is also now
thread-safe, to match the real AdminClient implementation.

In DeleteTopicTest, use the KIP-455 API rather than invoking the reassignment
command.","['clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/ConnectUtilsTest.java', 'core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala', 'core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala', 'core/src/test/scala/other/kafka/ReplicationQuotasTestRig.scala', 'core/src/test/scala/unit/kafka/admin/DeleteTopicTest.scala', 'core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala', 'core/src/test/scala/unit/kafka/admin/ReassignPartitionsCommandArgsTest.scala', 'core/src/test/scala/unit/kafka/admin/ReassignPartitionsCommandTest.scala', 'core/src/test/scala/unit/kafka/admin/ReassignPartitionsUnitTest.scala', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetricsTest.java']","'ReassignPartitionsCommand' currently uses direct ZooKeeper communication rather than the more efficient KIP-455 API, leading to issues like an inability to stop ongoing assignments, lack of reassignment tracking, and inefficient testing methods."
30216ea1c58761e62f51af40033f24e3ae1c5c2a,1655736422,"KAFKA-13998: JoinGroupRequestData 'reason' can be too large (#12298)

The `reason` field cannot contain more than 32767 chars. We did not expect to ever reach this but it turns out that it is possible if the the message provided in the `Throwable` somehow contains the entire stack trace. This patch ensure that the reason crafted based on exceptions remain small.

Co-authored-by: David Jacot <djacot@confluent.io>

Reviewers:  Bruno Cadonna <cadonna@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java']",The 'reason' field in JoinGroupRequestData can exceed the character limit (32767 chars) when it contains the full stack trace from 'Throwable'.
3799708ff09b174883eddc1f6a857ddc518fa35f,1674548060,"KAFKA-14533: re-enable 'false' and disable the 'true' parameter of SmokeTestDriverIntegrationTest (#13156)

I immediately saw a failure with stateUpdaterEnabled = true after disabling the false parameter, which suggests the problem actually does lie in the state updater itself and not the act of parametrization of the test. To verify this theory, and help stabilize the 3.4 release branch, let's try one more test by swapping out the true build in favor of the false one. If the listOffsets requests stop failing and causing this integration test to hit the global timeout as is currently happening at such a high rate, then we have pretty good evidence pointing at the state updater and should be able to debug things more easily from there.

After getting in a few builds to see whether the flakiness subsides, we should merge this PR to re-enable both parameters going forward: https://github.com/apache/kafka/pull/13155

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",['streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java'],The 'true' parameter of SmokeTestDriverIntegrationTest causes frequent failures due to listOffsets requests hitting the global timeout.
a450fb70c12ba66257e8f61cc4903290f1e435ea,1658780680,"KAFKA-14078; Do leader/epoch validation in Fetch before checking for valid replica (#12411)

After the fix for https://github.com/apache/kafka/pull/12150, if a follower receives a request from another replica, it will return UNKNOWN_LEADER_EPOCH even if the leader epoch matches. We need to do epoch leader/epoch validation first before we check whether we have a valid replica.

Reviewers: David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala']","Follower behaves incorrectly upon receiving a request from another replica, returning UNKNOWN_LEADER_EPOCH even when the leader epoch matches."
4efd9bf0cfde34b2438fdb013581f57cfc51cae1,1627856764,"KAFKA-13114; Revert state and reregister raft listener (#11116)

RaftClient's scheduleAppend may split the list of records into multiple
batches. This means that it is possible for the active controller to
see a committed offset for which it doesn't have an in-memory snapshot.

If the active controller needs to renounce and it is missing an
in-memory snapshot, then revert the state and reregister the Raft
listener. This will cause the controller to replay the entire metadata
partition.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/test/java/kafka/test/MockController.java', 'metadata/src/main/java/org/apache/kafka/controller/Controller.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTestEnv.java', 'metadata/src/test/java/org/apache/kafka/metalog/LocalLogManager.java', 'metadata/src/test/java/org/apache/kafka/metalog/LocalLogManagerTestEnv.java']","Active controller might lack in-memory snapshot for a committed offset when RaftClient's scheduleAppend splits records into batches, causing problematic renouncement and requiring replay of entire metadata partition."
fe2742e1b23158ca697cdf00cd027031403f979d,1589224372,"KAFKA-9972: Only commit tasks with valid states (#8632)

We spotted a case in the soak test where a standby task could be in CREATED state during commit, which causes an illegal state exception. To prevent this from happening, the solution is to always enforce a state check.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']",Standby tasks in CREATED state during commit cause an Illegal State Exception in the soak test.
959f9ca4c02817cce85f0375d431ed72f22699e2,1689280035,"MINOR: Standardize controller log4j output for replaying records (#13703)

Standardize controller log4j output for replaying important records. The log message should include
word ""replayed"" to make it clear that this is a record replay. Log the replay of records for ACLs,
client quotas, and producer IDs, which were previously not logged. Also fix a case where we weren't
logging changes to broker registrations.

AclControlManager, ClientQuotaControlManager, and ProducerIdControlManager didn't previously have a
log4j logger object, so this PR adds one. It also converts them to using Builder objects. This
makes junit tests more readable because we don't need to specify paramaters where the test can use
the default (like LogContexts).

Throw an exception in replay if we get another TopicRecord for a topic which already exists.

Example log messages:
  INFO [QuorumController id=3000] Replayed a FeatureLevelRecord setting metadata version to 3.6-IV0
  DEBUG [QuorumController id=3000] Replayed a ZkMigrationStateRecord which did not alter the state from NONE.
  INFO [QuorumController id=3000] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 0: BrokerRegistrationChangeRecord(brokerId=0, brokerEpoch=3, fenced=-1, inControlledShutdown=0)
  INFO [QuorumController id=3000] Replayed ClientQuotaRecord for ClientQuotaEntity(entries={user=testkit}) setting request_percentage to 0.99.

Reviewers: Divij Vaidya <diviv@amazon.com>, Ron Dagostino <rndgstn@gmail.com>, David Arthur <mumrah@gmail.com>","['metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ScramControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ClientQuotaControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/ProducerIdControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java']","Controller log4j output lack standardization for replaying important records, resulting in unclear log messages. Additionally, failure to log replay of records for ACLs, client quotas, and producer IDs, along with changes to broker registrations. Incorrect handling of a TopicRecord for an already existing topic in replay."
c85fd07bd161c9a4fd531fcb633e62c25db1f660,1586995270,"KAFKA-9703; Free up compression buffer after splitting a large batch

Method split takes up too many resources and might
cause outOfMemory error when the bigBatch is huge.
Call closeForRecordAppends() to free up resources
like compression buffers.

Change-Id: Iac6519fcc2e432330b8af2d9f68a8d4d4a07646b
Signed-off-by: Jiamei Xie <jiamei.xiearm.com>

*More detailed description of your change,
if necessary. The PR title and PR message become
the squashed commit message, so use a separate
comment to ping reviewers.*

*Summary of testing strategy (including rationale)
for the feature or bug fix. Unit and/or integration
tests are expected for any behaviour change and
system tests should be considered for larger changes.*

Author: Jiamei Xie <jiamei.xie@arm.com>

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jiangjie (Becket) Qin <becket.qin@gmail.com>

Closes #8286 from jiameixie/outOfMemory
",['clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java'],The 'split' method may cause an outOfMemory error when splitting a large batch due to excessive resource usage like retaining compression buffers.
a3930aad6c386f80d791a25abf1926e3bdbeb11b,1616191799,"MINOR: Assume unclean shutdown for metadata log (#10363)

Currently the metadata log assumes successful shutdown and skips recovery. For now, we prefer to err on the safe side and assume instead that the replica was shutdown uncleanly so that we can force full recovery. This is justified in the short term because:

1. Snapshots are not fully implemented for the metadata log
2. The replicas (controllers and brokers) need to read the entire metadata log to load it into memory.

In other words, we need to read through the metadata log once on startup anyway. A long-term fix will be provided in https://issues.apache.org/jira/browse/KAFKA-12504.

Reviewers: Jason Gustafson <jason@confluent.io>",['core/src/main/scala/kafka/raft/KafkaMetadataLog.scala'],"Metadata log skips recovery on reboot, assuming a successful shutdown which may lead to potential data inconsistency issues."
6d36487b684fd41522cccd4da4fd88f0b89ff0b7,1650316953,"MINOR: Fix TestDowngrade.test_upgrade_and_downgrade (#12027)

The second validation does not verify the second bounce because the verified producer and the verified consumer are stopped in `self.run_validation`. This means that the second `run_validation` just spit out the same information as the first one. Instead, we should just run the validation at the end.

Reviewers: Jason Gustafson <jason@confluent.io>","['tests/kafkatest/tests/core/downgrade_test.py', 'tests/kafkatest/tests/end_to_end.py']","The `run_validation` in TestDowngrade.test_upgrade_and_downgrade unexpectedly stops the verified producer and consumer, leading to repeated and incorrect validation results."
153db488660575e8c033c3c4424211a6f567bb57,1597508856,"KAFKA-9273: Extract testShouldAutoShutdownOnIncompleteMetadata from S… (#9108)

The main goal is to remove usage of embedded broker (EmbeddedKafkaCluster) in AbstractJoinIntegrationTest and its subclasses.
This is because the tests under this class are no longer using the embedded broker, except for two.
testShouldAutoShutdownOnIncompleteMetadata is one of such tests.
Furthermore, this test does not actually perfom stream-table join; it is testing an edge case of joining with a non-existent topic, so it should be in a separate test.

Testing strategy: run existing unit and integration test

Reviewers: Boyang Chen <boyang@confluent.io>, Bill Bejeck <bbejeck@apache.org>","['streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/JoinStoreIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/JoinWithIncompleteMetadataIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java']","AbstractJoinIntegrationTest is still using an embedded broker for testShouldAutoShutdownOnIncompleteMetadata when it shouldn't, and the test doesn't perform a stream-table join as intended."
4cd6dfc825b934e8e360bdd586c41931b0b3b8e6,1594698912,"KAFKA-10240: Suppress WakeupExceptions during sink task shutdown (#9003)

A benign `WakeupException` can be thrown by a sink task's consumer if the task is scheduled for shutdown by the worker. This is caught and handled gracefully if the exception is thrown when calling `poll` on the consumer, but not if calling `commitSync`, which is invoked by a task during shutdown and also when its partition assignment is updated.

If thrown during a partition assignment update, the `WakeupException` is caught and handled gracefully as part of the task's `iteration` loop. If thrown during shutdown, however, it is not caught and instead leads to the misleading log message ""Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted."".

These changes catch the `WakeupException` during shutdown and handle it gracefully with a `TRACE`-level log message.

A unit test is added to verify this behavior by simulating a thrown `WakeupException` during `Consumer::commitSync`, running through the `WorkerSinkTask::execute` method, and confirming that it does not throw a `WakeupException` itself.

Reviewers: Greg Harris <gregh@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Konstantine Karantasis <k.karantasis@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java']","Sink task shutdown throws a misleading ""Task threw an uncaught and unrecoverable exception"" error message upon encountering a WakeupException, potentially causing manual task restarts."
76de61475bf61fdd746ba035cc8410b38462982d,1613760320,"MINOR: Fix Raft broker restart issue when offset partitions are deferred #10155

A Raft-based broker is unable to restart if the broker defers partition
metadata changes for a __consumer_offsets topic-partition. The issue is
that GroupMetadataManager is asked to removeGroupsForPartition() upon
the broker becoming a follower, but in order for that code to function
it requires that the manager's scheduler be started. There are multiple
possible solutions here since removeGroupsForPartition() is a no-op at
this point in the broker startup cycle (nothing has been loaded, so
there is nothing to unload). We could just not invoke the callback. But
it seems more reasonable to not special-case this and instead start
ReplicaManager and the coordinators just before applying the deferred
partitions states.

We also mark deferred partitions for which we are a follower as being
online a bit earlier to avoid NotLeaderOrFollowerException that was
being thrown upon restart. Fixing this issue exposed the above issue
regarding the scheduler not being started.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>
","['core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/RaftReplicaChangeDelegate.scala', 'core/src/main/scala/kafka/server/RaftReplicaManager.scala']",Raft-based broker experiences issues during restart when offset partitions are deferred due to GroupMetadataManager's inability to execute removeGroupsForPartition() as its scheduler isn't started yet causing a NotLeaderOrFollowerException.
26ee78f392386862e0a125cdb9c040fda13f5604,1629742059,"KAFKA-13091; Ensure high watermark incremented after AlterIsr returns (#11245)

After we have shrunk the ISR, we have an opportunity to advance the high watermark. We do this currently in `maybeShrinkIsr` after the synchronous update through ZK. For the `AlterIsr` path, however, we cannot rely on this call since the request is sent asynchronously. Instead we should attempt to advance the high watermark in the callback when the `AlterIsr` response returns successfully.

Reviewers: David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","'High watermark not incrementing correctly due to asynchronous `AlterIsr` requests, affecting the system when ISR shrinks.'"
123e0e9ca9e9e23709cf59b83b01c8ae885c5f43,1674253151,"MINOR: fix warnings in Streams javadocs (#13132)

While working on the 3.4 release I noticed we've built up an embarrassingly long list of warnings within the Streams javadocs. It's unavoidable for some links to break as the source code changes, but let's reset back to a good state before the list gets even longer

Reviewers: Matthias J. Sax <mjsax@apache.org>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/TopologyConfig.java', 'streams/src/main/java/org/apache/kafka/streams/TopologyDescription.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/KStream.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/Produced.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/Repartitioned.java', 'streams/src/main/java/org/apache/kafka/streams/processor/StreamPartitioner.java', 'streams/src/main/java/org/apache/kafka/streams/query/Position.java', 'streams/src/main/java/org/apache/kafka/streams/query/QueryResult.java']","The 3.4 release of Streams contains a large number of warnings within the javadocs, indicating missed updates as source code has changed."
b485f92647faecc3594bdf4164999d52c859b1bb,1652122034,"KAFKA-13790; ReplicaManager should be robust to all partition updates from kraft metadata log (#12085)

This patch refactors the `Partition.makeLeader` and `Partition.makeFollower` to be robust to all partition updates from the KRaft metadata log. Particularly, it ensures the following invariants:

- A partition update is accepted if the partition epoch is equal or newer. The partition epoch is updated by the AlterPartition path as well so we accept an update from the metadata log with the same partition epoch in order to fully update the partition state.
- The leader epoch state offset is only updated when the leader epoch is bumped.
- The follower states are only updated when the leader epoch is bumped.
- Fetchers are only restarted when the leader epoch is bumped. This was already the case but this patch adds unit tests to prove/maintain it.

In the mean time, the patch unifies the state change logs to be similar in both ZK and KRaft world.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala']","Partition updates from the KRaft metadata log are causing inconsistencies in the state of the partition, leader epoch state offset, and follower states, due to the lack of robust handling in the `Partition.makeLeader` and `Partition.makeFollower` methods."
9cf7a5c4bc98e980007c7bd19d693930a56f4281,1629754384,"KAFKA-12933: Flaky test ReassignPartitionsIntegrationTest.testReassignmentWithAlterIsrDisabled (#11244)

Removes assertion added in #10471. It's unsafe to assert that
there are partition movements ongoing for some of the tests in
the suite because partitions in some of the tests have 0 data,
which may complete reassignment before `verify` can run.

Tests pass locally.

Reviewers: Luke Chen <showuon@gmail.com>, Ismael Juma <ismael@juma.me.uk>",['core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala'],"Tests that include partitions with no data in the ReassignPartitionsIntegrationTest suite might complete reassignment before verification can run, making assertions on partition movements unreliable."
1d873a9de9ad98d9f3120fa2ff59c3372650775b,1562970550,"MINOR: Use dynamic port in `RestServerTest` (#7079)

We have seen some failures recently in `RestServerTest`. It's the usual problem with reliance on static ports. 
```
Caused by: java.io.IOException: Failed to bind to 0.0.0.0/0.0.0.0:8083
	at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:346)
	at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:308)
	at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.eclipse.jetty.server.Server.doStart(Server.java:396)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.kafka.connect.runtime.rest.RestServer.initializeServer(RestServer.java:178)
	... 56 more
Caused by: java.net.BindException: Address already in use
```
This patch makes the chosen port dynamic.

Reviewers: Ismael Juma <ismael@juma.me.uk>",['connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestServerTest.java'],`RestServerTest` fails intermittently due to binding issues as static port 8083 is already in use.
4d5a28973f96492215531520b34737d529d8fd8b,1646949125,"Revert ""KAFKA-13542: add rebalance reason in Kafka Streams (#11804)"" (#11873)

This reverts commit 2ccc834faa3fffcd5d15d2463aeef3ee6f5cea13.

This reverts commit 2ccc834. We were seeing serious regressions in our state heavy benchmarks. We saw that our state heavy benchmarks were experiencing a really bad regression. The State heavy benchmarks runs with rolling bounces with 10 nodes.

We regularly saw this exception:  java.lang.OutOfMemoryError: Java heap space                                                                                                                                                                                              

I ran through a git bisect and found this commit. We verified that the commit right before did not have the same issues as this one did. I then reverted the problematic commit and ran the benchmarks again on this commit and did not see any more issues. We are still looking into the root cause, but for now since this isn't a critical improvement so we can remove it temporarily.

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>, David Jacot <djacot@confluent.io>, Ismael Juma <ismael@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']",State-heavy benchmarks experience serious regressions and frequent OutOfMemoryError occurrences due to a recent commit. The issue disappears upon reverting it.
f6f8da70713be6486b9ca085f4130043a3b9e9aa,1553059054,"KAFKA-8098: Fix Flaky Test testConsumerGroups

- The flaky failure is caused by the fact that the main thread sometimes issues DescribeConsumerGroup request before the consumer assignment takes effect. Added a latch to make sure such situation is not going to happen.

Author: huxihx <huxi_2b@hotmail.com>
Author: huxi <huxi_2b@hotmail.com>
Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #6441 from huxihx/KAFKA-8098
",['core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala'],"Test failure due to DescribeConsumerGroup request being issued before consumer assignment activates, leading to variability in test results."
cf1ebcbbbde0430663afd20d4f9753583ed35d81,1593723458," KAFKA-10006: Don't create internal topics when LeaderNotAvailableException (#8712)

1. return the topicsNotReady to makeReady including tempUnknownTopics, and not create topic to wait for next retry
2. tempUnknownTopics will be created each retry since we count the tempUnknownTopics as part of topicsNotReady
3. add 2 more tests to total test 3 cases:
  3.1 shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound
  3.2 shouldCompleteValidateWhenTopicLeaderNotAvailableAndThenDescribeSuccess
  3.3 shouldThrowExceptionWhenKeepsTopicLeaderNotAvailable

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java', 'streams/src/test/java/org/apache/kafka/test/MockInternalTopicManager.java']","Internal topics are being created when LeaderNotAvailableException occurs, leading to unnecessary retries and potential topic creation issues."
d4961038649b4f498b0fdd667ef4ad2a27f09605,1623336691,"MINOR: Small optimizations and removal of unused code in Streams (#10856)

Remove unused methods in internal classes
Mark fields that can be final as final
Remove unneeded generic type annotation
Convert single use fields to local final variables
Use method reference in lambdas when it's more readable

Reviewers: Matthias J. Sax <mjsax@apache.org>, Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/StoreQueryParameters.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/ConsumedInternal.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKTableJoinProcessor.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/TransformerSupplierAdapter.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKey.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/BaseJoinProcessorNode.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideDeserializer.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/MemoryNavigableLRUCache.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java', 'streams/src/test/java/org/apache/kafka/streams/integration/AbstractResetIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinMultiIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java']","Existence of unused methods in internal classes, non-final fields that could be final, unneeded generic type annotations, and inadequately used single-use fields and lambdas in Streams."
dd22b3f01b6bab41dce4d608f405ec6c78f7759e,1582681512,"KAFKA-9498; Topic validation during the topic creation triggers unnecessary TopicChange events (#8062)

This PR avoids generating unnecessary TopicChange events during the topic validation. It does so by adding a registerWatch field in the GetChildrenRequest request. This allows to not register the watch when topics are queried from the topic validation logic.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>, Jun Rao <junrao@gmail.com>
","['core/src/main/scala/kafka/admin/TopicCommand.scala', 'core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/main/scala/kafka/log/LogManager.scala', 'core/src/main/scala/kafka/zk/AdminZkClient.scala', 'core/src/main/scala/kafka/zk/KafkaZkClient.scala', 'core/src/main/scala/kafka/zookeeper/ZooKeeperClient.scala', 'core/src/test/scala/integration/kafka/api/ConsumerTopicCreationTest.scala', 'core/src/test/scala/unit/kafka/admin/TopicCommandTest.scala', 'core/src/test/scala/unit/kafka/security/auth/SimpleAclAuthorizerTest.scala', 'core/src/test/scala/unit/kafka/security/authorizer/AclAuthorizerTest.scala', 'core/src/test/scala/unit/kafka/zk/AdminZkClientTest.scala', 'core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala', 'core/src/test/scala/unit/kafka/zookeeper/ZooKeeperClientTest.scala', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java']",The current topic validation during topic creation is leading to the unwanted generation of TopicChange events. This is observed when topics are queried from the topic validation logic.
d3612ebc775ea401e6170b3248d146b228d85d1f,1613783221,"KAFKA-9672: Leader with ISR as a superset of replicas (#9631)

It is possible for the the controller to send LeaderAndIsr requests with
an ISR that contains ids not in the replica set. This is used during
reassignment so that the partition leader doesn't add replicas back to
the ISR. This is needed because the controller updates ZK and the
replicas through two rounds:

1. The first round of ZK updates and LeaderAndIsr requests shrinks the ISR.

2. The second round of ZK updates and LeaderAndIsr requests shrinks the replica
set.

This could be avoided by doing 1. and 2. in one round. Unfortunately the
current controller implementation makes that non-trivial.

This commit changes the leader to allow the state where the ISR contains
ids that are not in the replica set and to remove such ids from the ISR
if required.

Reviewers: Jun Rao <junrao@gmail.com>",['core/src/main/scala/kafka/cluster/Partition.scala'],"During reassignment, the controller sends LeaderAndIsr requests with an ISR that contains IDs not in the replica set, causing a state where the ISR contains IDs not included in the replica set."
05f9803d72ba2eeedd10901ebf3a39ed2240f092,1602004316,"KAFKA-10527; Voters should not reinitialize as leader in same epoch (#9348)

One of the invariants that the raft replication protocol relies on is that each record is uniquely identified by leader epoch and offset. This can be violated if a leader remains elected with the same epoch between restarts since unflushed data could be lost.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['raft/src/main/java/org/apache/kafka/raft/QuorumState.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/QuorumStateTest.java', 'raft/src/test/java/org/apache/kafka/raft/RaftEventSimulationTest.java']","In the Raft replication protocol, a leader remaining elected with the same epoch after restarts may violate the unique identification of each record by leader epoch and offset due to potential loss of unflushed data."
24a2ed26a642d13701dfb501e14a05280ba01415,1611548452,"MINOR: Update zstd-jni to 1.4.8-2 (#9957)

* The latest version zstd-jni doesn't use `RecyclingBufferPool` by default, so we
pass it via the relevant constructors to maintain the behavior before this
change.
* zstd-jni fixes an issue when using Alpine, see https://github.com/luben/zstd-jni/issues/157.
* zstd 1.4.7 includes several months of improvements across many axis,
from performance to various fixes. Details: https://github.com/facebook/zstd/releases/tag/v1.4.7
* zstd 1.4.8 is a hotfix release, details: https://github.com/facebook/zstd/releases/tag/v1.4.8

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",['clients/src/main/java/org/apache/kafka/common/record/CompressionType.java'],"The current version of zstd-jni doesn't use `RecyclingBufferPool` by default and has issues when used with Alpine. In addition, updates and hotfixes are available in versions 1.4.7 and 1.4.8 respectively."
9ef52dd2dba51f01f856560132056ebe4bacd51a,1616701346,"KAFKA-12508: Disable KIP-557 (#10397)

A major issue has been raised that this implementation of
emit-on-change is vulnerable to a number of data-loss bugs
in the presence of recovery with dirty state under at-least-once
semantics. This should be fixed in the future when we implement
a way to avoid or clean up the dirty state under at-least-once,
at which point it will be safe to re-introduce KIP-557 and
complete it.

Reviewers: A. Sophie Blee-Goldman <ableegoldman@apache.org>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableSourceTest.java']",Implementation of emit-on-change under at-least-once semantics in KIP-557 is causing data-loss during recovery with dirty state.
c40985049fd02601913c95a71fcaa2481d57d403,1602781311,"KAFKA-10613: Only set leader epoch when list-offset version >= 4 (#9438)

The leader epoch field is added in version 4, and the auto-generated protocol code would throw unsupported version exception if the field is set to any non-default values for version < 4. This would cause older versioned clients to never receive list-offset results.

Reviewers: Boyang Chen <boyang@confluent.io>","['core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala']",List-offset results are not received by older version clients due to unsupported version exception thrown when the leader epoch field is set to non-default values for version less than 4.
81428226332005c27870aacfccc813950c84386c,1658182040,"KAFKA-14079 - Ack failed records in WorkerSourceTask when error tolerance is ALL (#12415)

Make sure to ack all records where produce failed, when a connector's `errors.tolerance` config property is set to `all`. Acking is essential so that the task will continue to commit future record offsets properly and remove the records from internal tracking, preventing a memory leak.

(cherry picked and slightly modified from commit 63e06aafd0cf37f8488c3830946051b3a30db2a0)

Reviewers: Chris Egerton <fearthecellos@gmail.com>, Randall Hauch <rhauch@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTask.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java']","When `errors.tolerance` is set to `all`, records where produce failed are not being acknowledged, leading to improper future record offest commits and potential memory leaks due to records not being removed from internal tracking."
d32a2d12757e24311cae8f495a3b6f2150f3e041,1566969799,"KAFKA-8837: KafkaMetricReporterClusterIdTest may not shutdown ZooKeeperTestHarness (#7255)

- Call `assertNoNonDaemonThreads` in test method instead of tear down method
to avoid situation where parent's class tear down is not invoked.
- Pass the thread prefix in tests that call `assertNoNonDaemonThreads` so that it
works correctly.
- Rename `verifyNonDaemonThreadsStatus` to `assertNoNonDaemonThreads` to
make it clear that it may throw.

Reviewers: Anna Povzner <anna@confluent.io>, Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/server/KafkaServerStartable.scala', 'core/src/test/scala/unit/kafka/server/KafkaMetricReporterClusterIdTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/ServerGenerateBrokerIdTest.scala', 'core/src/test/scala/unit/kafka/server/ServerGenerateClusterIdTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","ZooKeeperTestHarness might not shut down properly in KafkaMetricReporterClusterIdTest, leading to potential conflicts with non-daemon threads."
400185421f008662ee6f92298154151493486c1e,1573862775,"KAFKA-9183; Remove redundant admin client integration testing (#7690)

This patch creates a `BaseAdminIntegrationTest` to be the root for all integration test extensions. Most of the existing tests will only be tested in `PlaintextAdminIntegrationTest`, which extends from `BaseAdminIntegrationTest`. This should cut off about 30 minutes from the overall build time.

Reviewers: David Arthur <mumrah@gmail.com>, Ismael Juma <ismael@juma.me.uk>","['core/src/test/scala/integration/kafka/api/AdminClientWithPoliciesIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/BaseAdminIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/SaslSslAdminIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/SslAdminIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/FetchRequestMaxBytesTest.scala', 'core/src/test/scala/unit/kafka/server/ProduceRequestTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala']",Redundant administration client integration tests are causing increased overall build time.
2047fc3715002899996ba3e41d16551d2067edad,1654538741,"HOTFIX: only try to clear discover-coordinator future upon commit (#12244)

This is another way of fixing KAFKA-13563 other than #11631.

Instead of letting the consumer to always try to discover coordinator in pool with either mode (subscribe / assign), we defer the clearance of discover future upon committing async only. More specifically, under manual assign mode, there are only three places where we need the coordinator:

* commitAsync (both by the consumer itself or triggered by caller), this is where we want to fix.
* commitSync, which we already try to re-discovery coordinator.
* committed (both by the consumer itself based on reset policy, or triggered by caller), which we already try to re-discovery coordinator.

The benefits are that for manual assign mode that does not try to trigger any of the above three, then we never would be discovering coordinator. The original fix in #11631 would let the consumer to discover coordinator even if none of the above operations are required.

Reviewers: Luke Chen <showuon@gmail.com>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java']","In manual assign mode, attempting to clear discover-coordinator future upon commit for operations that do not require coordinator discovery, unnecessarily triggers coordinator discovery."
41b89a6ecf2aef6ee01b8bbf9a38c366d242b0ca,1568167722,"MINOR: Add api version to uncaught exception message (#7311)

When we have an unhandled exception in the request handler, we print some details about the request such as the api key and payload. It is also useful to see the version of the request which is not always apparent from the request payload.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['core/src/main/scala/kafka/server/KafkaApis.scala'],"Uncaught exceptions in the request handler aren't providing the version detail of the request, making troubleshooting more complex."
d479d129e0b24f2c2173f2bfd1fb261ec2be757b,1677709215,"KAFKA-13999: Add ProducerCount metrics (KIP-847) (#13078)

This is the PR for the implementation of KIP-847: https://cwiki.apache.org/confluence/display/KAFKA/KIP-847%3A+Add+ProducerIdCount+metrics
Add ProducerIdCount metric at the broker level:

kafka.server:type=ReplicaManager,name=ProducerIdCount
Added unit tests below to ensure the metric reported the count correctly.

---------

Co-authored-by: Artem Livshits <84364232+artemlivshits@users.noreply.github.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Divij Vaidya <diviv@amazon.com>, Christo Lolov <christo_lolov@yahoo.com>, Alexandre Dupriez <alexandre.dupriez@gmail.com>, Justine Olshan <jolshan@confluent.io>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/log/UnifiedLog.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/server/BrokerMetricNamesTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'storage/src/main/java/org/apache/kafka/storage/internals/log/ProducerStateManager.java']","The Kafka broker lacks a metric to track the count of ProducerIds, limiting visibility into system performance."
b76bcaf3a8f28a5ad553a9edd477c4d6d887f08a,1632473468,"KAFKA-13102: Topic IDs not propagated to metadata cache quickly enough for Fetch path (#11170)

Before we used the metadata cache to determine whether or not to use topic IDs. Unfortunately, metadata cache updates with ZK controllers are in a separate request and may be too slow for the fetcher thread. This results in switching between topic names and topic IDs for topics that could just use IDs.

This patch adds topic IDs to FetcherState created in LeaderAndIsr requests. It also supports updating this state for follower threads as soon as a LeaderAndIsr request provides a topic ID.

We've opted to only update replica fetcher threads. AlterLogDir threads will use either topic name or topic ID depending on what was present when they were created.

Reviewers: David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/common/internals/PartitionStates.java', 'core/src/main/scala/kafka/server/AbstractFetcherManager.scala', 'core/src/main/scala/kafka/server/AbstractFetcherThread.scala', 'core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala', 'core/src/main/scala/kafka/server/ReplicaFetcherThread.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherManagerTest.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherThreadWithIbp26Test.scala', 'core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java']","Slow metadata cache updates with ZK controllers result in incorrect switching between topic names and topic IDs in the fetcher thread, impacting FetcherState in LeaderAndIsr requests."
78c55c8d66f5570d975caa53a9751b126ca10538,1559309382,"KAFKA-6958: Overload KStream methods to allow to name operation name using the new Named class (#6411)

Sub-task required to allow to define custom processor names with KStreams DSL(KIP-307) :
 - overload methods for stateless operations to accept a Named parameter (filter, filterNot, map, mapValues, foreach, peek, branch, transform, transformValue, flatTransform)
 - overload process method to accept a Named parameter
 - overload join/leftJoin/outerJoin methods

Reviewers: John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>,
Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/kstream/KStream.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/NamedInternal.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/RepartitionTopicNamingTest.java']",Existing KStream methods do not support naming operations using the new Named class leading to inability to define custom processor names with KStreams DSL (KIP-307).
59c1d4ece309311dc670c280eea37953049ed19d,1600515313,"MINOR: Generator config-specific HTML ids (#8878)

Currently the docs have HTML ids for each config key. That doesn't work
correctly for config keys like bootstrap.servers which occur across
producer, consumer, admin configs: We generate duplicate ids. So arrange
for each config to prefix the ids it generates with the HTML id of its
section heading.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java', 'clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java', 'clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java', 'core/src/main/scala/kafka/log/LogConfig.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/unit/kafka/log/LogConfigTest.scala', 'streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java']","Duplicate HTML ids are being generated for config keys like 'bootstrap.servers' that occur in multiple config sections - producer, consumer, admin."
07a18478be51ae32e9a9a1954f5ecb9eea792443,1694419100,"KAFKA-15326: [7/N] Processing thread non-busy waiting (#14180)

Avoid busy waiting for processable tasks. We need to be a bit careful here to not have the task executors to sleep when work is available. We have to make sure to signal on the condition variable any time a task becomes ""processable"". Here are some situations where a task becomes processable:

- Task is unassigned from another TaskExecutor.
- Task state is changed (should only happen inside when a task is locked inside the polling phase).
- When tasks are unlocked.
- When tasks are added.
- New records available.
- A task is resumed.

So in summary, we

- We should probably lock tasks when they are paused and unlock them when they are resumed. We should also wake the task executors after every polling phase. This belongs to the StreamThread integration work (separate PR). We add DefaultTaskManager.signalProcessableTasks for this.
- We need to awake the task executors in DefaultTaskManager.unassignTask, DefaultTaskManager.unlockTasks and DefaultTaskManager.add.


Reviewers: Walker Carlson <wcarlson@confluent.io>, Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskExecutor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/tasks/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskExecutorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/tasks/DefaultTaskManagerTest.java']","Processing thread undergoes busy waiting for tasks that are processable, leading to potential inefficiencies and risks of TaskExecutors sleeping when work is available."
195a8b0ed153a711d88e51b9dff66d27cc451776,1624001103,"KAFKA-12835: Topic IDs can mismatch on brokers (after interbroker protocol version update) (#10754)

Upon upgrading to IBP 2.8, topic ID can end up getting reassigned which can cause errors in LeaderAndIsr handling when the partition metadata files from the previous ID are still on the broker. 

Topic IDs are stored in the TopicZNode. The behavior of the code before this fix is as follows:
Consider we had a controller with IBP 2.8+. Each topic will be assigned topic IDs and LeaderAndIsr requests will write partition.metadata files to the brokers. If we re-elect the controller and end up with a controller with an older IBP version and we reassign partitions, the TopicZNode is overwritten and we lose the topic ID. Upon electing a 2.8+ IBP controller, we will see the TopicZNode is missing a topic ID and will generate a new one. If the broker still has the old partition metadata file, we will see an ID mismatch that causes the error.

This patch changes controller logic so that we maintain the topic ID in the controller and the ZNode even when IBP < 2.8. This means that in the scenario above, reassigning partitions will not result in losing the topic ID and reassignment.

Topic IDs may be lost when downgrading the code below version 2.8, but upon re-upgrading to code version 2.8+, before bumping the IBP, all partition metadata files will be deleted to prevent any errors.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/raft/KafkaMetadataLog.scala', 'core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala', 'core/src/test/scala/unit/kafka/log/LogTestUtils.scala']","After an interbroker protocol version update to IBP 2.8, Topic IDs can mismatch on brokers due to them getting reassigned, leading to errors in LeaderAndIsr handling when old partition metadata files are still present."
96b1db510a32b99b57c27a9b3001c9e15af722d4,1670376162,"KAFKA-14415: Faster ThreadCache (#12903)

Optimization of `ThreadCache`. The original implementation showed significant slow-down when many caches were registered.

`sizeBytes` was called at least once, and potentially many times
in every `put` and was linear in the number of caches (= number of
state stores, so typically proportional to number of tasks). That
means, with every additional task, every put gets a little slower.
This was confirmed experimentally.

In this change, we modify the implementation of `ThreadCache` to
keep track of the total size in bytes. To be independent of the
concrete implementation of the underlying cache, we update the size
by subtracting the old and adding the new size of the cache before
and after every modifying operation. For this we acquire the object
monitor of the cache, but since all modifying operations on the caches
are synchronized already, this should not cause extra overhead.

This change also fixes a `ConcurrentModificationException` that could
be thrown in a race between `sizeBytes` and `getOrCreate`.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",['streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java'],ThreadCache slows down significantly when multiple caches are registered due to sizeBytes being called multiple times during every “put” operation. ConcurrentModificationException can occur due to a race between sizeBytes and getOrCreate.
c8675d4723a45d99aee6f64c6bf4703eeaea81aa,1671525359,"KAFKA-14343: Upgrade tests for state updater  (#12801)

A test that verifies the upgrade from a version of Streams with
state updater disabled to a version with state updater enabled
and vice versa, so that we can offer a save upgrade path.

 - upgrade test from a version of Streams with state updater
disabled to a version with state updater enabled
 - downgrade test from a version of Streams with state updater
 enabled to a version with state updater disabled

Reviewer: Bruno Cadonna <cadonna@apache.org>",['tests/kafkatest/tests/streams/streams_upgrade_test.py'],"There are no tests verifying the upgrade from a version of Streams with state updater disabled to a version with it enabled, and vice versa, to ensure a safe upgrade path."
9e5b77fb9687c319dd2b188d9669ca2bd01e9bb8,1621295387,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>","['metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicaPlacer.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/SimpleReplicaPlacementPolicy.java', 'metadata/src/main/java/org/apache/kafka/controller/StripedReplicaPlacer.java', 'metadata/src/main/java/org/apache/kafka/metadata/OptionalStringComparator.java', 'metadata/src/main/java/org/apache/kafka/metadata/UsableBroker.java', 'metadata/src/test/java/org/apache/kafka/controller/BrokerHeartbeatManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/StripedReplicaPlacerTest.java', 'metadata/src/test/java/org/apache/kafka/metadata/OptionalStringComparatorTest.java', 'tests/kafkatest/tests/core/round_trip_fault_test.py']","KRaft replica placement algorithm is non-rack-aware and randomly places replicas, even neglecting placement on fenced brokers as per KIP-631."
b424553101c56547beafab2ae39f16671fc05c9e,1641446193,"KAFKA-13553: Add PAPI Window and Session store tests for IQv2 (#11650)

During some recent reviews, @mjsax pointed out that StateStore layers
are constructed differently the stores are added via the PAPI vs. the DSL.

This PR adds PAPI construction for Window and Session stores to the
IQv2StoreIntegrationTest so that we can ensure IQv2 works on every
possible state store.

Reviewer: Guozhang Wang <guozhang@apache.org>",['streams/src/test/java/org/apache/kafka/streams/integration/IQv2StoreIntegrationTest.java'],Inconsistent state store construction between PAPI and DSL could potentially lead to IQv2 not functioning correctly for every possible state store.
976409ee081b5a4e5edfe73b5d597998feb2f402,1627588879,"KAFKA-13137; KRaft Controller Metric MBean names incorrectly quoted (#11131)

Controller metric names that are in common between the ZooKeeper-based and KRaft-based controller must remain the same, but they were not in the AK 2.8 early access release of KRaft. For example, the non-KRaft MBean name `kafka.controller:type=KafkaController,name=OfflinePartitionsCount` incorrectly became `""kafka.controller"":type=""KafkaController"",name=""OfflinePartitionCount""` (note the added quotes and the lack of plural).  This patch fixes the issues, closes the test gap that allowed the divergence to occur, and adds de-registration logic to remove the metrics when the controller is closed (this logic was missing).

Reviewers: Luke Chen <showuon@gmail.com>, Jason Gustafson <jason@confluent.io>","['metadata/src/main/java/org/apache/kafka/controller/ControllerMetrics.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumControllerMetrics.java', 'metadata/src/test/java/org/apache/kafka/controller/MockControllerMetrics.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerMetricsTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java']","Mismatch between ZooKeeper-based and KRaft-based controller metric names with additional undesired quotes and incorrect format, combined with the lack of de-registration logic for metrics when the controller closes."
cdbc9a8d88c1ddc9dd088a33d047783a5b13c282,1691721734,"KAFKA-15083: add config with ""remote.log.metadata"" prefix (#14151)

When configuring RLMM, the configs passed into configure method is the RemoteLogManagerConfig. But in RemoteLogManagerConfig, there's no configs related to remote.log.metadata.*, ex: remote.log.metadata.topic.replication.factor. So, even if users have set the config in broker, it'll never be applied.

This PR fixed the issue to allow users setting RLMM prefix: remote.log.metadata.manager.impl.prefix (default is rlmm.config.), and then, appending the desired remote.log.metadata.* configs, it'll pass into RLMM, including remote.log.metadata.common.client./remote.log.metadata.producer./ remote.log.metadata.consumer. prefixes.

Ex:

# default value
# remote.log.storage.manager.impl.prefix=rsm.config.
# remote.log.metadata.manager.impl.prefix=rlmm.config.

rlmm.config.remote.log.metadata.topic.num.partitions=50
rlmm.config.remote.log.metadata.topic.replication.factor=4

rsm.config.test=value

Reviewers: Christo Lolov <christololov@gmail.com>, Kamal Chandraprakash <kchandraprakash@uber.com>, Divij Vaidya <diviv@amazon.com>","['core/src/main/java/kafka/log/remote/RemoteLogManager.java', 'core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogManagerConfig.java']","The RemoteLogManagerConfig is not recognising any configs related to ""remote.log.metadata.*"", leading to configurations set in broker not being applied for Remote Log Metadata Manager."
6fa9f3c97d7b6dd47eea236fc4d60b2791b6d040,1601924815,"KAFKA-10531: Check for negative values to Thread.sleep call (#9347)

System.currentTimeMillis() is not monotonic, so using that to calculate time to sleep can result in negative values. That will throw IllegalArgumentException.

This change checks for that and sleeps for a second (to avoid tight loop) if the value returned is negative.

Author: Shaik Zakir Hussain <zhussain@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",['connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java'],"Calculations using System.currentTimeMillis() for sleep duration potentially leads to negative value inputs to Thread.sleep call, resulting in IllegalArgumentException."
bc90c29fafc69747daeecada8bb0c347e138edc8,1660785102,"KAFKA-14167; Completion exceptions should not be translated directly to error codes (#12518)

There are a few cases in `ControllerApis` where we may see an `ApiException` wrapped as a `CompletionException`. This can happen in `QuorumController.allocateProducerIds` where the returned future is the result of calling `thenApply` on the future passed to the controller. The danger when this happens is that the `CompletionException` gets passed to `Errors.forException`, which translates it to an `UNKNOWN_SERVER_ERROR`. At a minimum, I found that the `AllocateProducerIds` and `UpdateFeatures` APIs were affected by this bug, but it is difficult to root out all cases. 

Interestingly, `DeleteTopics` is not affected by this bug as I originally suspected. This is because we have logic in `ApiError.fromThrowable` to check for both `CompletionException` and `ExecutionException` and to pull out the underlying cause. This patch duplicates this logic from `ApiError.fromThrowable` into `Errors.forException` to be sure that we handle all cases where exceptions are converted to error codes.

Reviewers: David Arthur <mumrah@gmail.com>","['clients/src/main/java/org/apache/kafka/common/protocol/Errors.java', 'clients/src/main/java/org/apache/kafka/common/requests/AllocateProducerIdsResponse.java', 'clients/src/main/java/org/apache/kafka/common/requests/ApiError.java', 'clients/src/test/java/org/apache/kafka/common/requests/ApiErrorTest.java', 'core/src/main/scala/kafka/server/ControllerApis.scala', 'core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java', 'core/src/test/scala/unit/kafka/server/AllocateProducerIdsRequestTest.scala', 'core/src/test/scala/unit/kafka/server/ControllerApisTest.scala']","`ApiException` wrapped as a `CompletionException` within `ControllerApis` is incorrectly translated to `UNKNOWN_SERVER_ERROR`, impacting `AllocateProducerIds` and `UpdateFeatures` APIs."
3624daef2addf8ab6339e068d4d0f77f08ab6c6e,1593462128,"KAFKA-9509: Increase timeout when consuming records to fix flaky test in MM2 (#8894)

A simple increase in the timeout of the consumer that verifies that records have been replicated seems to fix the integration tests in `MirrorConnectorsIntegrationTest` that have been failing more often recently. 

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Sanjana Kaundinya <skaundinya@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",['connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java'],'MirrorConnectorsIntegrationTest' is failing frequently due to short timeout while verifying record replication in the consumer.
de90175fc24357e20306c5a4de4f0f8ec8675ad2,1579729716,"KAFKA-9418; Add new sendOffsetsToTransaction API to KafkaProducer (#7952)

This patch adds a new API to the producer to implement transactional offset commit fencing through the group coordinator as proposed in KIP-447. This PR mainly changes on the Producer end for compatible paths to old `sendOffsetsToTxn(offsets, groupId)` vs new `sendOffsetsToTxn(offsets, groupMetadata)`.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerGroupMetadata.java', 'clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/Producer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerGroupMetadataTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/PartitionAssignorAdapterTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java', 'clients/src/test/java/org/apache/kafka/common/message/MessageTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/OffsetCommitRequestTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/TxnOffsetCommitRequestTest.java', 'core/src/test/scala/integration/kafka/api/TransactionsBounceTest.scala', 'core/src/test/scala/integration/kafka/api/TransactionsTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala']","KafkaProducer lacks an API for transactional offset commit fencing through the group coordinator, causing potential inconsistencies in transactions."
38a3ddb5627f7860aea40cb602186bde79808d3d,1634710776,"MINOR: Add a replication system test which simulates a slow replica (#11395)

This patch adds a new system test which exercises the shrining/expansion process of the partition leader. It does so by introducing a network partition which isolates a broker from the other brokers in the cluster but not from KRaft Controller/ZK.

Reviewers: Jason Gustafson <jason@confluent.io>","['tests/kafkatest/services/kafka/kafka.py', 'tests/kafkatest/tests/core/replication_replica_failure_test.py', 'tests/kafkatest/tests/end_to_end.py']",The system lacks a test that simulates a slow replica during partition leader shrink/expand process involving network partition situations with the broker.
66a6fc7204b3903ae8015bc8bceb200b6838ec10,1551199112,"MINOR: Refactor replica log dir fetching for improved logging (#6313)

In order to debug problems with log directory reassignments, it is helpful to know when the fetcher thread begins moving a particular partition. This patch refactors the fetch logic so that we stick to a selected partition as long as it is available and log a message when a different partition is selected.

Reviewers: Viktor Somogyi-Vass <viktorsomogyi@gmail.com>, Dong Lin <lindong28@gmail.com>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala', 'core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala']","Fetcher thread doesn't log when it begins moving a particular partition, making it difficult to debug issues with log directory reassignments."
84351efd51adb6d4dbd50431a464a9d7ad475b9f,1678151613,"KAFKA-14491: [13/N] Add versioned store builder and materializer (#13274)

This PR introduces VersionedKeyValueStoreBuilder for building the new versioned stores introduced in KIP-889, analogous to the existing TimestampedKeyValueStoreBuilder for building timestamped stores. This PR also updates the existing KTable store materializer class to materialize versioned stores in addition to timestamped stores. As part of this change, the materializer is renamed from TimestampedKeyValueStoreMaterializer to simply KeyValueStoreMaterializer.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedTableImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KeyValueStoreMaterializer.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamToTableNode.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableSourceNode.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractReadOnlyDecorator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractReadWriteDecorator.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/VersionedKeyValueStoreBuilder.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/KeyValueStoreMaterializerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TimestampedKeyValueStoreMaterializerTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedKeyValueStoreBuilderTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/VersionedKeyValueStoreBuilderTest.java']","Current KTable store materializer only supports materializing timestamped stores, and lacks support for versioned stores introduced in KIP-889. Also, there is no store builder available for versioned stores."
cf32a1a6a06df910dd11f26abe7b62e560392e25,1566942093,"KAFKA-8179: Part 4, add CooperativeStickyAssignor (#7130)

Splits the existing StickyAssignor logic into an AbstractStickyAssignor class, which is extended by the existing (eager) StickyAssignor and by the new CooperativeStickyAssignor which supports incremental cooperative rebalancing.

There is no actual change to the logic -- most methods from StickyAssignor were moved to AbstractStickyAssignor to be shared with CooperativeStickyAssignor, and the abstract MemberData memberData(Subscription) method converts the Subscription to the embedded list of owned partitions for each assignor.

The ""generation"" logic is left in, however this is always Optional.empty() for the CooperativeStickyAssignor as onPartitionsLost should always be called when a generation is missed.

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignorAdapter.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java']","The current StickyAssignor logic doesn't support incremental cooperative rebalancing, leading to inefficiencies in rebalancing partitions among members in a consumer group."
ffef0871c2d64bcbc171b129c2057b572c2f41b2,1567716655,"KAFKA-7149 : Reducing streams assignment data size  (#7185)

* Leader instance uses dictionary encoding on the wire to send topic partitions
* Topic names (most expensive component) are mapped to an integer using the dictionary
* Follower instances receive the dictionary, decode topic names back
* Purely an on-the-wire optimization, no in-memory structures changed
* Test case added for version 5 AssignmentInfo

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java', 'streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java', 'tests/kafkatest/tests/streams/streams_upgrade_test.py']","Increased data size during streams assignment due to topic names being sent as is, leading to potential inefficiencies on the wire."
3c43adff1d4562c6b33732f399691c9e2f887903,1592582754,"KAFKA-9891: add integration tests for EOS and StandbyTask (#8890)

Ports the test from #8886 to trunk -- this should be merged to 2.6 branch.

One open question. In 2.6 and trunk we rely on the active tasks to wipe out the store if it crashes. However, assume there is a hard JVM crash and we don't call closeDirty() the store would not be wiped out. Thus, I am wondering, if we would need to fix this (for both active and standby tasks) and do a check on startup if a local store must be wiped out?

The current test passes, as we do a proper cleanup after the exception is thrown.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/StoreQueryParameters.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java']","Potential JVM crash doesn't trigger closeDirty(), leaving local stores uncleared. Uncertain behavior upon startup and whether a check is needed for local store cleaning."
446196d6e9d66f45c64b483e3d375aaeaca28e3b,1592450625,"KAFKA-10123; Fix incorrect value for AWAIT_RESET#hasPosition (#8841)

## Background

When a partition subscription is initialized it has a `null` position and is in the INITIALIZING state. Depending on the consumer, it will then transition to one of the other states. Typically a consumer will either reset the offset to earliest/latest, or it will provide an offset (with or without offset metadata). For the reset case, we still have no position to act on so fetches should not occur.

Recently we made changes for KAFKA-9724 (#8376) to prevent clients from entering the AWAIT_VALIDATION state when targeting older brokers. New logic to bypass offset validation as part of this change exposed this new issue.

## Bug and Fix

In the partition subscriptions, the AWAIT_RESET state was incorrectly reporting that it had a position. In some cases a position might actually exist (e.g., if we were resetting offsets during a fetch after a truncation), but in the initialization case no position had been set. We saw this issue in system tests where there is a race between the offset reset completing and the first fetch request being issued.

Since AWAIT_RESET#hasPosition was incorrectly returning `true`, the new logic to bypass offset validation was transitioning the subscription to FETCHING (even though no position existed).

The fix was simply to have AWAIT_RESET#hasPosition to return `false` which should have been the case from the start. Additionally, this fix includes some guards against NPE when reading the position from the subscription.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java']","AWAIT_RESET state incorrectly reports that it has a position during partition subscription initialization, causing potential race condition between offset reset completion and first fetch request."
6f197301646135e0bb39a461ca0a07c09c3185fb,1684372057,"KAFKA-9579 Fetch implementation for records in the remote storage through a specific purgatory. (#13535)

This change includes
- Recognize the fetch requests with out of range local log offsets
- Add fetch implementation for the data lying in the range of [logStartOffset, localLogStartOffset]
- Add a new purgatory for async remote read requests which are served through a specific thread pool

We have an extended version of remote fetch that can fetch from multiple remote partitions in parallel, which we will raise as a followup PR.

A few tests for the newly introduced changes are added in this PR. There are some tests available for these scenarios in 2.8.x, refactoring with the trunk changes, will add them in followup PRs.

Other contributors:
Kamal Chandraprakash <kchandraprakash@uber.com> - Further improvements and adding a few tests
Luke Chen <showuon@gmail.com> - Added a few test cases for these changes.

PS: This functionality is pulled out from internal branches with other functionalities related to the feature in 2.8.x. The reason for not pulling all the changes as it makes the PR huge, and burdensome to review and it also needs other metrics, minor enhancements(including perf), and minor changes done for tests. So, we will try to have followup PRs to cover all those.

Reviewers: Jun Rao <junrao@gmail.com>, Alexandre Dupriez <alexandre.dupriez@gmail.com>, Divij Vaidya <diviv@amazon.com>,  Jorge Esteban Quilcate Otoya <quilcate.jorge@gmail.com> ","['core/src/main/java/kafka/log/remote/RemoteLogManager.java', 'core/src/main/java/kafka/log/remote/RemoteLogReader.java', 'core/src/main/java/kafka/server/builders/ReplicaManagerBuilder.java', 'core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/DelayedFetch.scala', 'core/src/main/scala/kafka/server/DelayedRemoteFetch.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java', 'core/src/test/java/kafka/log/remote/RemoteLogReaderTest.java', 'core/src/test/scala/integration/kafka/server/DelayedFetchTest.scala', 'core/src/test/scala/integration/kafka/server/DelayedRemoteFetchTest.scala', 'core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala', 'storage/src/main/java/org/apache/kafka/storage/internals/log/FetchDataInfo.java', 'storage/src/main/java/org/apache/kafka/storage/internals/log/RemoteLogReadResult.java', 'storage/src/main/java/org/apache/kafka/storage/internals/log/RemoteStorageFetchInfo.java', 'storage/src/main/java/org/apache/kafka/storage/internals/log/RemoteStorageThreadPool.java']","Out of range local log offsets in fetch requests are not well-managed. For data in the range of [logStartOffset, localLogStartOffset], fetch implementation is absent. Lack of a dedicated purgatory for async remote read requests being served through a specific thread pool."
aea059a07bf82697635caccb208313163ff89f10,1617297961,"KAFKA-12474: Handle failure to write new session keys gracefully (#10396)

If a distributed worker fails to write (or read back) a new session key to/from the config topic, it dies. This fix softens the blow a bit by instead restarting the herder tick loop anew and forcing a read to the end of the config topic until the worker is able to successfully read to the end.

At this point, if the worker was able to successfully write a new session key in its first attempt, it will have read that key back from the config topic and will not write a new key during the next tick iteration. If it was not able to write that key at all, it will try again to write a new key (if it is still the leader).

Verified with new unit tests for both cases (failure to write, failure to read back after write).

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java']",Distributed worker fails and shuts down upon unsuccessful attempts of writing or reading back new session keys to/from the config topic.
67d00e25e941f73be8b959c6732ac4db1d1083bf,1652891966,"MINOR: Enable some AdminClient integration tests (#12110)

Enable KRaft in `AdminClientWithPoliciesIntegrationTes`t and `PlaintextAdminIntegrationTest`. There are some tests not enabled or not as expected yet:

- testNullConfigs, see KAFKA-13863
- testDescribeCluster and testMetadataRefresh, currently we don't get the real controller in KRaft mode so the test may not run as expected

This patch also changes the exception type raised from invalid `IncrementalAlterConfig` requests with the `SUBTRACT` and `APPEND` operations. When the configuration value type is not a list, we now raise `INVALID_CONFIG` instead of `INVALID_REQUEST`.

Reviewers: Luke Chen <showuon@gmail.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/ConfigAdminManager.scala', 'core/src/test/scala/integration/kafka/api/AdminClientWithPoliciesIntegrationTest.scala', 'core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala']","AdminClient integration tests in KRaft mode aren't fully enabled or exhibit unexpected behavior, including 'testNullConfigs' and 'testDescribeCluster'. Incorrect exception type raised for invalid IncrementalAlterConfig requests with SUBTRACT and APPEND operations."
2b8d41b468cc67b90f1ba0b54577ebb59e634a35,1626122409,"KAFKA-13003: In kraft mode also advertise configured advertised port instead of socket port (#10935)

In Kraft mode, Apache Kafka 2.8.0 advertises the socket port instead of the configured advertised port.
A broker with the following configuration:

listeners=PUBLIC://0.0.0.0:19092,REPLICATION://0.0.0.0:9091
advertised.listeners=PUBLIC://envoy-kafka-broker:9091,REPLICATION://kafka-broker1:9091

advertises on the PUBLIC listener envoy-kafka-broker:19092, however I would expect that
envoy-kafka-broker:9091 is advertised. In ZooKeeper mode it works as expected. This PR 
changes the BrokerServer class so that in Kraft mode the configured advertised port is
registered as expected.

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/test/java/kafka/testkit/BrokerNode.java', 'core/src/test/java/kafka/testkit/TestKitNodes.java', 'core/src/test/scala/integration/kafka/server/RaftClusterTest.scala']","In Kraft mode, Apache Kafka 2.8.0 incorrectly advertises the socket port instead of the defined advertised port, leading to inconsistencies with ZooKeeper mode."
4a61b48d3dca81e28b57f10af6052f36c50a05e3,1688602849,"KAFKA-14966; [2/N] Extract OffsetFetcher reusable logic (#13898)

This is a follow up on the initial OffsetFetcher refactoring to extract reusable logic, needed for the new consumer implementation (initial refactoring merged with PR-13815).

Similar to the initial refactoring, this PR brings no changes to the existing logic, just extracting functions or pieces of logic.

There were no individual tests for the extracted functions, so no tests were migrated.

Reviewers: Jun Rao <junrao@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcherUtils.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java']","OffsetFetcher's current structure doesn't allow for reusable logic, impeding the development and implementation of a new consumer."
63fee01366e6ce98b9dfafd279a45d40b80e282d,1680582439,"KAFKA-14491: [19/N] Combine versioned store RocksDB instances into one (#13431)

The RocksDB-based versioned store implementation introduced in KIP-889 currently uses two physical RocksDB instances per store instance: one for the ""latest value store"" and another for the ""segments store."" This PR combines those two RocksDB instances into one by representing the latest value store as a special ""reserved"" segment within the segments store. This reserved segment has segment ID -1, is never expired, and is not included in the regular Segments methods for getting or creating segments, but is represented in the physical RocksDB instance the same way as any other segment.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/state/internals/LogicalKeyValueSegment.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/LogicalKeyValueSegments.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBVersionedStore.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/LogicalKeyValueSegmentTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/LogicalKeyValueSegmentsTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java']",The RocksDB-based versioned store currently utilizes two physical RocksDB instances per store instance leading to storages issues and increased complexity in handling.
7a0e371e0ed0988f5f66f207d5ec665ecfaaac88,1614627125,"MINOR: Remove stack trace of the lock exception in a debug log4j (#10231)

Although the lock exception log is at the DEBUG level only, many people were confused with stack traces that something serious happened; plus, in the source code there is only one call path that can lead to the capture of LockException at task manager/stream thread, so even for debugging purposes there’s no extra information we can get from anyways.

Reviewers: Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Ismael Juma <ismael@juma.me.uk>",['streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java'],"Lock exception stack traces in debug log4j create confusion due to their severity appearance, despite only one call path leading to LockException and providing no additional information for debugging."
c1f23b6c9a11781eecf03405a5fab437acbbba53,1664677366,"MINOR: Fix delegation token system test (#12693)

KIP-373 added a ""token requester"" field to the output of kafka-delegation-tokens.sh. The system test was failing since it was not expecting this new field. This patch adds support for this field and improves the error output if we can't parse.

Reviewers: José Armando García Sancio <jsancio@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>",['tests/kafkatest/services/delegation_tokens.py'],"The system test for kafka-delegation-tokens.sh fails due to unanticipated ""token requester"" field added by KIP-373, and inadequate error outputs if parsing fails."
fe16912dfc59aa9f2379b904df89c9531cc9a2d5,1621258472,"KAFKA-12736: KafkaProducer.flush holds onto completed ProducerBatch(s) until flush completes (#10620)

When flush is called a copy of incomplete batches is made. This
means that the full ProducerBatch(s) are held in memory until the flush
has completed. Note that the `Sender` removes producer batches
from the original incomplete collection when they're no longer
needed.

For batches where the existing memory pool is used this
is not as wasteful as the memory will be returned to the pool,
but for non pool memory it can only be GC'd after the flush has
completed. Rather than use copyAll we can make a new array with only the
produceFuture(s) and await on those.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/IncompleteBatches.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java']","Flush in KafkaProducer retains completed ProducerBatch(s) in memory until flush is complete, leading to potential memory wastage especially for non-pool memory."
a48b5d900c6b5c9c52a97124a1b51aff3636c32c,1564501813,"KAFKA-8717; Reuse cached offset metadata when reading from log (#7081)

Although we currently cache offset metadata for the high watermark and last stable offset, we don't use it when reading from the log. Instead we always look it up from the index. This patch pushes fetch isolation into `Log.read` so that we are able to reuse the cached offset metadata.

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala', 'core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala', 'core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogSegment.scala', 'core/src/test/scala/other/kafka/StressTestLog.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala', 'core/src/test/scala/unit/kafka/log/BrokerCompressionTest.scala', 'core/src/test/scala/unit/kafka/log/LogManagerTest.scala', 'core/src/test/scala/unit/kafka/log/LogSegmentTest.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala', 'core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala', 'core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala', 'core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala']",Cached offset metadata for high watermark and last stable offset is not being reused when reading from the log leading to unnecessary index lookups.
6a30b4e38502f3821d3e500bb8fa6f00ec39aa92,1591646268,"MINOR: equals() should check _unknownTaggedFields (#8640)

_unknownTaggedFields contains tagged fields which we don't understand
with the current schema.  However, we still want to keep the data around
for various purposes. For example, if we are printing out a JSON form of
the message we received, we want to include a section containing the
tagged fields that couldn't be parsed. To leave these out would give an
incorrect impression of what was sent over the wire.  Since the unknown
tagged fields represent real data, they should be included in the fields
checked by equals().

Reviewers: Ismael Juma <ismael@juma.me.uk>, Boyang Chen <boyang@confluent.io>","['clients/src/main/java/org/apache/kafka/common/protocol/MessageUtil.java', 'clients/src/test/java/org/apache/kafka/common/message/MessageTest.java', 'clients/src/test/java/org/apache/kafka/common/protocol/MessageUtilTest.java', 'generator/src/main/java/org/apache/kafka/message/FieldSpec.java', 'generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java']","The equals() method doesn't check _unknownTaggedFields, potentially leading to inaccuracies when reproducing the exact data sent over the wire, especially in JSON form."
92305c2cf26764de89fabcbe33401382f43610ff,1649260123,"KAFKA-13687: Limiting the amount of bytes to be read in a segment logs (#11842)

This PR allows to limit the output batches while they are inspected via the kafka-dump-log.sh script.

The idea is to take samples from the logsegments without affecting a production cluster as the current script will read the whole files, this could create issues related to performance.

Please see the KIP-824

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/tools/DumpLogSegments.scala', 'core/src/test/scala/unit/kafka/tools/DumpLogSegmentsTest.scala']","kafka-dump-log.sh script reads entire files possibly degrading performance on a production cluster, need to limit its output batches."
08aa33127a4254497456aa7a0c1646c7c38adf81,1695657930,"MINOR: Push logic to resolve the transaction coordinator into the AddPartitionsToTxnManager (#14402)

This patch refactors the ReplicaManager.appendRecords method and the AddPartitionsToTxnManager class in order to move the logic to identify the transaction coordinator based on the transaction id from the former to the latter. While working on KAFKA-14505, I found pretty annoying that we require to pass the transaction state partition to appendRecords because we have to do the same from the group coordinator. It seems preferable to delegate that job to the AddPartitionsToTxnManager.

Reviewers: Justine Olshan <jolshan@confluent.io>","['core/src/main/scala/kafka/server/AddPartitionsToTxnManager.scala', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/main/scala/kafka/server/KafkaRequestHandler.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/kafka/server/KafkaRequestHandlerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/CoordinatorPartitionWriterTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala', 'core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala', 'core/src/test/scala/unit/kafka/server/AddPartitionsToTxnManagerTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala']",Identifying the transaction coordinator based on the transaction id in the ReplicaManager.appendRecords method requires passing the transaction state partition from the group coordinator – a cumbersome operation.
96c69da8c164966c1281df7001d5d239053442fd,1581630291,"KAFKA-8507; Unify connection name flag for command line tool [KIP-499] (#8023)

This change updates ConsoleProducer, ConsumerPerformance, VerifiableProducer, and VerifiableConsumer classes to add and prefer the --bootstrap-server flag for defining the connection point of the Kafka cluster. This change is part of KIP-499: https://cwiki.apache.org/confluence/display/KAFKA/KIP-499+-+Unify+connection+name+flag+for+command+line+tool.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>,  Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/tools/ConsoleProducer.scala', 'core/src/main/scala/kafka/tools/ConsumerPerformance.scala', 'core/src/test/scala/unit/kafka/tools/ConsoleProducerTest.scala', 'core/src/test/scala/unit/kafka/tools/ConsumerPerformanceTest.scala', 'tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java', 'tools/src/main/java/org/apache/kafka/tools/VerifiableProducer.java']",Different command-line tools use inconsistent flag terms for defining connection point to the Kafka cluster.
37a86598269330fb3180b8742dc19ee072391dd9,1622895288,"KAFKA-12892: Use dedicated root in ZK ACL test (#10821)

Having the `testChrootExistsAndRootIsLocked` test in a separate `ZookeeperTestHarness` isn't enough to prevent the ACL changes to the ZK root from affecting other integration tests. So instead, let's use a dedicated znode for this test. It still works because `makeSurePersistentPathExists` uses `createRecursive`, which will recurse and act the same for the root or a given znode.

 Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>","['core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala', 'core/src/test/scala/unit/kafka/zk/ZkClientAclTest.scala']","ACL changes to the ZK root in the `testChrootExistsAndRootIsLocked` test are impacting other integration tests, despite residing in a separate `ZookeeperTestHarness`."
97d1a3248a4184da1e8ef47c1b94986665b941bf,1600378671,"MINOR: Fix common struct `JsonConverter` and `Schema` generation (#9279)

This patch fixes a couple problems with the use of the `StructRegistry`. First, it fixes registration so that it is consistently based on the typename of the struct. Previously structs were registered under the field name which meant that fields which referred to common structs resulted in multiple entries. Second, the patch fixes `SchemaGenerator` so that common structs are considered first.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['clients/src/test/java/org/apache/kafka/common/message/SimpleExampleMessageTest.java', 'generator/src/main/java/org/apache/kafka/message/FieldType.java', 'generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java', 'generator/src/main/java/org/apache/kafka/message/SchemaGenerator.java', 'generator/src/main/java/org/apache/kafka/message/StructRegistry.java']","Structs registered under field names result in multiple entries, and `SchemaGenerator` does not prioritize common structs, leading to generation issues."
11fc953c05eb2c8db61c61bc765aa8be0598d638,1588111459,"KAFKA-9176: Retry on getting local stores from KafkaStreams (#8568)

This PR fixes and improves two major issues:

1. When calling KafkaStreams#store we can always get an InvalidStateStoreException, and even waiting for Streams state to become RUNNING is not sufficient (this is also how OptimizedKTableIntegrationTest failed). So I wrapped all the function with a Util wrapper that captures and retries on that exception.

2. While trouble-shooting this issue, I also realized a potential bug in test-util's produceKeyValuesSynchronously, which creates a new producer for each of the record to send in that batch --- i.e. if you are sending N records with a single call, within that call it will create N producers used to send one record each, which is very slow and costly.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <john@confluent.io>","['streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StoreQueryIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StoreUpgradeIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java']","Calling `KafkaStreams#store` can result in an `InvalidStateStoreException` even when state has become RUNNING, causing test failures. Additionally, test-util's `produceKeyValuesSynchronously` is inefficiently creating a new producer for each record sent in a batch."
b8c3a67012218188330110ce94636b70116131f9,1636551866,"MINOR: Remove topic null check from `TopicIdPartition` and adjust constructor order (#11403)

`TopicPartition` allows a null `topic` and there are cases where we have
a topic id, but no topic name. Even for `TopicIdPartition`, the non null
topic name check was only enforced in one constructor.

Also adjust the constructor order to move the nullable parameter to the
end, update tests and javadoc.

Reviewers: David Jacot <djacot@confluent.io>, Luke Chen <showuon@gmail.com>","['clients/src/main/java/org/apache/kafka/common/TopicIdPartition.java', 'clients/src/test/java/org/apache/kafka/common/TopicIdPartitionTest.java']","`TopicIdPartition` incorrectly enforces non-null topic name in one of its constructors, causing inconsistencies with `TopicPartition` that allows null `topic`. Additionally, the order of constructors places nullable parameter not at the end."
fdb98df839146a6ac742c4d603c6f73ae00ba484,1644959214,"KAFKA-12648: avoid modifying state until NamedTopology has passed validation (#11750)

Previously we were only verifying the new query could be added after we had already inserted it into the TopologyMetadata, so we need to move the validation upfront.

Also adds a test case for this and improves handling of NPE in case of future or undiscovered bugs.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTaskCreator.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TopologyMetadata.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/AddNamedTopologyResult.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/KafkaStreamsNamedTopologyWrapper.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/NamedTopologyTest.java']","The new query is being added to TopologyMetadata prior to verification, potentially causing harmful modifications if the query fails validation."
efe6029f9c2b70cebc4359913ff809b7efa31daa,1625895302,"KAFKA-10675: Add schema name to ConnectSchema.validateValue() error message (#9541)

The following error message
`org.apache.kafka.connect.errors.DataException: Invalid Java object for schema type INT64: class java.lang.Long for field: ""moderate_time""`
can be confusing because java.lang.Long is acceptable type for schema INT64.

In fact, in this case `org.apache.kafka.connect.data.Timestamp` is used but this info is not logged.

Reviewers: Randall Hauch <rhauch@gmail.com>, Chris Egerton <chrise@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>","['connect/api/src/main/java/org/apache/kafka/connect/data/ConnectSchema.java', 'connect/api/src/test/java/org/apache/kafka/connect/data/StructTest.java']","The error message in ConnectSchema.validateValue() is ambiguous and does not provide enough information about the cause, specifically when a type mismatch is involved due to usage of Timestamp."
648497a5e5e9373e17f632d0c2d101cc369ff20e,1575917862,"KAFKA-9241: Some SASL Clients not forced to re-authenticate (#7784)

Brokers are supposed to force SASL clients to re-authenticate (and kill such connections in the absence of a timely and successful re-authentication) when KIP-368 SASL Re-Authentication is enabled via a positive connections.max.reauth.ms configuration value. There was a flaw in the logic that caused connections to not be killed in the absence of a timely and successful re-authentication if the client did not leverage the SaslAuthenticateRequest API (which was defined in KIP-152).

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticator.java', 'core/src/test/scala/unit/kafka/network/SocketServerTest.scala']","SASL Re-Authentication is not forcing some clients to re-authenticate or killing the connections in absence of timely re-authentication, when leveraging the SaslAuthenticateRequest API."
b04937dc65e33e6e2867d259a8d3d4cc10ca6360,1655304300,"MINOR: Fix force kill of KRaft colocated controllers in system tests (#11238)

I noticed that a system test using a KRaft cluster with 3 brokers but only 1 co-located controller did not force-kill the second and third broker after shutting down the first broker (the one with the controller).  The issue was a floating point rounding error.  This patch adjusts for the rounding error and also makes the logic work for an even number of controllers.  A local run of `tests/kafkatest/sanity_checks/test_bounce.py` succeeded (and I manually increased the cluster size for the 1 co-located controller case and observed the correct kill behavior: the second and third brokers were force-killed as expected).

Reviewers: Luke Chen <showuon@gmail.com>, José Armando García Sancio <jsancio@users.noreply.github.com>, David Jacot <djacot@confluent.io>",['tests/kafkatest/services/kafka/kafka.py'],"In system tests using a KRaft cluster with multiple brokers but a single co-located controller, not all brokers are force-killed after shutting down the first one due to a floating point rounding error."
34f749db307c2886af82b31aec4c9a21588e097d,1594079571,"MINOR: prune the metadata upgrade test matrix (#8971)

Most of the values in the metadata upgrade test matrix are just testing
the upgrade/downgrade path between two previous releases. This is
unnecessary. We run the tests for all supported branches, so what we
should test is the up-/down-gradability of released versions with respect
to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['tests/kafkatest/tests/streams/streams_upgrade_test.py'],"Redundancy issue observed in the metadata upgrade test matrix, as it continuously tests up-/down-grade paths between two pre-released versions, instead of current branch compatibility."
9077d83672a4d08273ce4a6012f1787f5313f948,1558131011,"MINOR: Add select changes from 3rd KIP-307 PR for incrementing name index counter (#6754)

When users provide a name for operation via the Streams DSL, we need to increment the counter used for auto-generated names to make sure any operators downstream of a named operator still produce a compatible name.

This PR is a subset of #6411 by @fhussonnois. We need to merge this PR now because it covers cases when users name repartition topics or state stores.

Updated tests to reflect the counter produces expected number even when the user provides a name.

Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/Named.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/NamedInternal.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/NamedInternalTest.java']","The auto-generated names counter lacks incrementation when users provide their own names for operations via the Streams DSL, affecting the compatibility of names produced by downstream operators."
fe6a827e20d30af5328d7376a831f9666e0c8110,1685948689,"KAFKA-14633: Reduce data copy & buffer allocation during decompression (#13135)

After this change,

    For broker side decompression: JMH benchmark RecordBatchIterationBenchmark demonstrates 20-70% improvement in throughput (see results for RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize).
    For consumer side decompression: JMH benchmark RecordBatchIterationBenchmark a mix bag of single digit regression for some compression type to 10-50% improvement for Zstd (see results for RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize).

Reviewers: Luke Chen <showuon@gmail.com>, Manyanda Chitimbo <manyanda.chitimbo@gmail.com>, Ismael Juma <mail@ismaeljuma.com>","['clients/src/main/java/org/apache/kafka/common/compress/ZstdFactory.java', 'clients/src/main/java/org/apache/kafka/common/record/CompressionType.java', 'clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java', 'clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java', 'clients/src/main/java/org/apache/kafka/common/utils/ByteBufferInputStream.java', 'clients/src/main/java/org/apache/kafka/common/utils/ByteUtils.java', 'clients/src/main/java/org/apache/kafka/common/utils/ChunkedBytesStream.java', 'clients/src/main/java/org/apache/kafka/common/utils/Utils.java', 'clients/src/test/java/org/apache/kafka/common/record/CompressionTypeTest.java', 'clients/src/test/java/org/apache/kafka/common/record/DefaultRecordBatchTest.java', 'clients/src/test/java/org/apache/kafka/common/record/DefaultRecordTest.java', 'clients/src/test/java/org/apache/kafka/common/utils/ByteBufferInputStreamTest.java', 'clients/src/test/java/org/apache/kafka/common/utils/ByteUtilsTest.java', 'clients/src/test/java/org/apache/kafka/common/utils/ChunkedBytesStreamTest.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/BaseRecordBatchBenchmark.java', 'raft/src/main/java/org/apache/kafka/raft/internals/RecordsIterator.java']","Decompression on both broker and consumer sides in Kafka involves excessive data copies and buffer allocations, leading to inefficiencies in throughput."
6fb8bd0987d2da1ded55383a02692a34f120f3b4,1570903632,"KAFKA-9029: Flaky Test CooperativeStickyAssignorTest.testReassignmentWithRand: bump to 4 (#7503)

One of the sticky assignor tests involves a random change in subscriptions that the current assignor algorithm struggles to react to and in cooperative mode ends up requiring more than one followup rebalance.

Apparently, in rare cases it can also require more than 2. Bumping the ""allowed subsequent rebalances"" to 4 (increase of 2) to allow some breathing room and reduce flakiness (technically any number is ""correct"", but if it turns out to ever require more than 4 we should revisit and improve the algorithm because that would be excessive (see KAFKA-8767)

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['clients/src/test/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignorTest.java'],"The CooperativeStickyAssignorTest.testReassignmentWithRand test occasionally fails due to the current assignor algorithm inadequately handling a random change in subscriptions in cooperative mode, necessitating an excessive number of followup rebalances."
32ff347b2c0ca21b9c567ab4cfe54869d7148e28,1689349266,"KAFKA-14462; [23/23] Wire GroupCoordinatorService in BrokerServer (#13991)

This patch wires the new group coordinator in BrokerServer (KRaft only). With this, it is now possible to run a cluster with the new group coordinator and to use the ConsumerGroupHeartbeat API by specifying the following two properties:
- group.coordinator.new.enable = true (to enable the new group coordinator)
- unstable.api.versions.enable = true (to enable unreleased APIs)

Note that the new group coordinator does not support all the existing APIs yet.

Reviewers: Jeff Kim <jeff.kim@confluent.io>, Justine Olshan <jolshan@confluent.io>","['core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/unit/kafka/server/ConsumerGroupHeartbeatRequestTest.scala', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/util/SystemTimerReaper.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/util/SystemTimerReaperTest.java']","New group coordinator in BrokerServer (KRaft only) not wired correctly, leading to inability to utilize ConsumerGroupHeartbeat API and unsupported existing APIs."
f70ece26d1cce23556f8f69ca2ccb2bb9e4f4de1,1564901946,"MINOR: Fix potential bug in LogConfig.getConfigValue and improve test coverage (#7159)

LogConfig.getConfigValue would throw a NoSuchElementException if any log
config was defined without a server default mapping.

Added a unit test for `getConfigValue` and a sanity test for
`toHtml`/`toRst`/`toEnrichedRst`, which were previously not exercised during
the test suite.

Reviewers: Jason Gustafson <jason@confluent.io>, José Armando García Sancio <jsancio@users.noreply.github.com>","['clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java', 'core/src/main/scala/kafka/log/LogConfig.scala', 'core/src/test/scala/unit/kafka/log/LogConfigTest.scala']","LogConfig.getConfigValue throws a NoSuchElementException when a log config is defined without a server default mapping, lacking sufficient test coverage for this functionality and other methods like toHtml/toRst/toEnrichedRst."
f97646488cff1984455ffb1fe9a147a522e6ac76,1649871195,"KAFKA-13651; Add audit logging to `StandardAuthorizer` (#12031)

This patch adds audit support through the kafka.authorizer.logger logger to StandardAuthorizer. It
follows the same conventions as AclAuthorizer with a similarly formatted log message. When
logIfAllowed is set in the Action, then the log message is at DEBUG level; otherwise, we log at
trace. When logIfDenied is set, then the log message is at INFO level; otherwise, we again log at
TRACE.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['clients/src/main/java/org/apache/kafka/common/resource/ResourcePattern.java', 'metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAcl.java', 'metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerData.java', 'metadata/src/test/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerTest.java']","`StandardAuthorizer` lacks audit logging capabilities, resulting in loss of traceability and accountability for actions undertaken."
150fd5b0b18c4761d8f7d7ba9a480aa9f622024f,1661009846,"KAFKA-13914: Add command line tool kafka-metadata-quorum.sh (#12469)

 Add `MetadataQuorumCommand` to describe quorum status, I'm trying to use arg4j style command format, currently, we only support one sub-command which is ""describe"" and we can specify 2 arguments which are --status and --replication.

```
# describe quorum status
kafka-metadata-quorum.sh --bootstrap-server localhost:9092 describe --replication

ReplicaId	LogEndOffset	Lag	LastFetchTimeMs	LastCaughtUpTimeMs	Status  	
0        	10          	        0  	-1             	        -1                	                 Leader  	
1        	10          	        0  	-1             	        -1                	                 Follower	
2        	10          	        0  	-1             	        -1                	                 Follower	

kafka-metadata-quorum.sh --bootstrap-server localhost:9092 describe --status
ClusterId:                             fMCL8kv1SWm87L_Md-I2hg
LeaderId:                             3002
LeaderEpoch:                      2
HighWatermark:                  10
MaxFollowerLag:                 0
MaxFollowerLagTimeMs:   -1
CurrentVoters:                    [3000,3001,3002]
CurrentObservers:              [0,1,2]

# specify AdminClient properties
kafka-metadata-quorum.sh --bootstrap-server localhost:9092 --command-config config.properties describe --status
```

Reviewers: Jason Gustafson <jason@confluent.io>","['bin/kafka-metadata-quorum.sh', 'bin/windows/kafka-metatada-quorum.bat', 'clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/main/java/org/apache/kafka/clients/admin/QuorumInfo.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java', 'core/src/main/scala/kafka/admin/MetadataQuorumCommand.scala', 'core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java', 'core/src/test/scala/unit/kafka/admin/MetadataQuorumCommandTest.scala', 'core/src/test/scala/unit/kafka/server/DescribeQuorumRequestTest.scala', 'server-common/src/main/java/org/apache/kafka/server/util/ToolsUtils.java', 'tools/src/main/java/org/apache/kafka/tools/ProducerPerformance.java', 'tools/src/main/java/org/apache/kafka/tools/TransactionsCommand.java']",Lack of a command line tool to view the status and replication of the Kafka metadata quorum.
4be4420b3da781265032d651a9dd20087d67a83f,1593478662,"KAFKA-10212: Describing a topic with the TopicCommand fails if unauthorised to use ListPartitionReassignments API

Since https://issues.apache.org/jira/browse/KAFKA-8834, describing topics with the TopicCommand requires privileges to use ListPartitionReassignments or fails to describe the topics with the following error:

> Error while executing topic command : Cluster authorization failed.

This is a quite hard restriction has most of the secure clusters do not authorize non admin members to access ListPartitionReassignments.

This patch catches the `ClusterAuthorizationException` exception and gracefully fails back. We already do this when the API is not available so it remains consistent.

Author: David Jacot <djacot@confluent.io>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #8947 from dajac/KAFKA-10212
","['clients/src/test/java/org/apache/kafka/clients/admin/AdminClientTestUtils.java', 'core/src/main/scala/kafka/admin/TopicCommand.scala', 'core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala']","Describing topics with TopicCommand is failing for non-admin members due to a requirement for privileges to use ListPartitionReassignments, which is not commonly granted in secure clusters."
b97e8203eb1fb016cd7cccd3dbf5fecc716969be,1692136888,"MINOR: CommitRequestManager should only poll when the coordinator node is known (#14179)

As title, we discovered a flaky bug during testing that the commit request manager would seldomly throw a NOT_COORDINATOR exception, which means the request was routed to a non-coordinator node. We discovered that if we don't check the coordinator node in the commitRequestManager, the request manager will pass on an empty node to the NetworkClientDelegate, which implies the request can be sent to any node in the cluster. This behavior is incorrect as the commit requests need to be routed to a coordinator node.

Because the timing coordinator's discovery during integration testing isn't entirely deterministic; therefore, the test became extremely flaky. After fixing this: The coordinator node is mandatory before attempt to enqueue these commit request to the NetworkClient.

Reviewers: Jun Rao <junrao@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/CommitRequestManagerTest.java']","CommitRequestManager produces a NOT_COORDINATOR exception on sending requests to non-coordinator nodes, leading to test instability due to the non-deterministic discovery of coordinators in integration testing."
74dfe80bb8b63b3916560143ce2c85f8b41f0f2e,1614310701,"KAFKA-12365; Disable APIs not supported by KIP-500 broker/controller (#10194)

This patch updates request `listeners` tags to be in line with what the KIP-500 broker/controller support today. We will re-enable these APIs as needed once we have added the support.

I have also updated `ControllerApis` to use `ApiVersionManager` and simplified the envelope handling logic.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/server/ControllerApis.scala', 'core/src/main/scala/kafka/server/ControllerServer.scala', 'core/src/test/scala/unit/kafka/server/ControllerApisTest.scala', 'tests/kafkatest/tests/core/group_mode_transactions_test.py', 'tests/kafkatest/tests/core/transactions_test.py']","APIs that are not currently supported by the KIP-500 broker/controller are still enabled, causing inconsistencies and potential operational issues."
d9f0be45231fd19036095ccc544b3df304811724,1571330654,"KAFKA-9004; Prevent older clients from fetching from a follower (#7531)

With KIP-392, we allow consumers to fetch from followers. This capability is enabled when a replica selector has been provided in the configuration. When not in use, the intent is to preserve current behavior of fetching only from leader. The leader epoch is the mechanism that keeps us honest. When there is a leader change, the epoch gets bumped, consumer fetches fail due to the fenced epoch, and we find the new leader.

However, for old consumers, there is no similar protection. The leader epoch was not available to clients until recently. If there is a preferred leader election (for example), the old consumer will happily continue fetching from the demoted leader until a periodic metadata fetch causes us to discover the new leader. This does not create any problems from a correctness perspective–fetches are still bound by the high watermark–but it is unexpected and may cause unexpected performance characteristics.

This patch fixes this problem by enforcing leader-only fetching for older versions of the fetch request.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala']",Older Kafka clients without leader epoch support continue fetching from demoted leaders after preferred leader elections. This doesn't compromise correctness but may affect system performance unexpectedly.
1c439da590771c604ba25f4776ba9deb1670a891,1595554402,"KAFKA-10268: dynamic config like ""--delete-config log.retention.ms"" doesn't work (#9051)

* KAFKA-10268: dynamic config like ""--delete-config log.retention.ms"" doesn't work

https://issues.apache.org/jira/browse/KAFKA-10268

Currently, ConfigCommand --delete-config API does not restore the config to default value, no matter at broker-level or broker-default level. Besides, Admin.incrementalAlterConfigs API also runs into this problem. This patch fixes it by removing the corresponding config from the newConfig properties when reconfiguring dynamic broker config.
","['core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala']","Dynamic configuration command `--delete-config log.retention.ms` doesn't restore the configuration to its default value at both broker-level or broker-default level, causing configuration inconsistencies."
4ea9394e7ebf71e2dcf96e9b96191dac253f4930,1689927962,"MINOR Fix the build failure (#14065)

Fixing the build failure caused by the earlier commit https://github.com/apache/kafka/commit/27ea025e33aab525e96bef24840414f7a4e132f1 


```
[Error] /Users/satishd/repos/apache-kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala:3526:77: the result type of an implicit conversion must be more specific than Object
[Error] /Users/satishd/repos/apache-kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala:3530:70: the result type of an implicit conversion must be more specific than Object
[Warn] /Users/satishd/repos/apache-kafka/core/src/test/scala/unit/kafka/server/ServerGenerateBrokerIdTest.scala:23:21: imported `QuorumTestHarness` is permanently hidden by definition of object QuorumTestHarness in package server
[Warn] /Users/satishd/repos/apache-kafka/core/src/test/scala/unit/kafka/server/ServerGenerateClusterIdTest.scala:29:21: imported `QuorumTestHarness` is permanently hidden by definition of object QuorumTestHarness in package server
[Error] /Users/satishd/repos/apache-kafka/core/src/test/scala/unit/kafka/utils/TestUtils.scala:1438:15: ambiguous reference to overloaded definition,
both method doReturn in class Mockito of type (x$1: Any, x$2: Object*)org.mockito.stubbing.Stubber
and  method doReturn in class Mockito of type (x$1: Any)org.mockito.stubbing.Stubber
match argument types (kafka.log.UnifiedLog)
```

Reviewers: Luke Chen <showuon@gmail.com>","['core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","A recent commit has led to build failure due to several warnings and errors, including issues related to implicit conversion result types and ambiguous reference to overloaded definition, within different Scala test files in the Apache Kafka project."
beac86f049385932309158c1cb49c8657e53f45f,1657802854,"KAFKA-13043: Implement Admin APIs for offsetFetch batching (#10964)

This implements the AdminAPI portion of KIP-709: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=173084258. The request/response protocol changes were implemented in 3.0.0. A new batched API has been introduced to list consumer offsets for different groups. For brokers older than 3.0.0, separate requests are sent for each group.

Co-authored-by: Rajini Sivaram <rajinisivaram@googlemail.com>
Co-authored-by: David Jacot <djacot@confluent.io>

Reviewers: David Jacot <djacot@confluent.io>,  Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/clients/admin/Admin.java', 'clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupOffsetsOptions.java', 'clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupOffsetsResult.java', 'clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupOffsetsSpec.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiDriver.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/CoordinatorStrategy.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/ListConsumerGroupOffsetsHandler.java', 'clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java', 'clients/src/test/java/org/apache/kafka/clients/admin/AdminClientTestUtils.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java', 'clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java', 'clients/src/test/java/org/apache/kafka/clients/admin/internals/ListConsumerGroupOffsetsHandlerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java', 'core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala', 'core/src/test/scala/unit/kafka/admin/ConsumerGroupServiceTest.scala', 'core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java']","Admin API doesn't support batched listing of consumer offsets for different groups, resulting in separate requests needing to be sent for each group in brokers older than 3.0.0."
d91a94e7bff7a2ffe94b562ef1108307afce1bc7,1569027012,"KAFKA-8609: Add consumer rebalance metrics (#7347)

Adding the following metrics in:

1. AbstractCoordinator (for both consumer and connect)
* rebalance-latency-avg
* rebalance-latency-max
* rebalance-total
* rebalance-rate-per-hour
* failed-rebalance-total
* failed-rebalance-rate-per-hour
* last-rebalance-seconds-ago

2. ConsumerCoordinator
* partition-revoked-latency-avg
* partition-revoked-latency-max
* partition-assigned-latency-avg
* partition-assigned-latency-max
* partition-lost-latency-avg
* partition-lost-latency-max

Reviewers: Bruno Cadonna <bruno@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Heartbeat.java', 'clients/src/main/java/org/apache/kafka/common/metrics/JmxReporter.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java']","Consumer and connect coordinator lack metrics to measure rebalance latency, total rebalances, failed rebalances, and time since last rebalance. Also, consumer coordinator doesn't provide metrics for partition revoked, assigned and lost latencies."
247c271353752a588162983a1a6f7eb96cf9870f,1639712395,"MINOR: retry when deleting offsets for named topologies (#11604)

When this was made I didn't expect deleteOffsetsResult to be set if an exception was thrown. But it is and to retry we need to reset it to null. Changing the KafkaStreamsNamedTopologyWrapper for remove topology when resetting offsets to retry upon GroupSubscribedToTopicException and swallow/complete upon GroupIdNotFoundException

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@ache.>",['streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/KafkaStreamsNamedTopologyWrapper.java'],GroupSubscribedToTopicException occurs while deleting offsets for named topologies due to unexpected set of deleteOffsetsResult even after an exception is thrown.
6f6248509c0c38dcd867b28e6481de5370c6e082,1636645148,"MINOR: Introduce `ApiKeyVersionsSource` annotation for `ParameterizedTest` (#11468)

It is common in our code base to have unit tests which must be run for all the versions of a given request. Most of the time, we do so by iterating over all the versions in the test itself which is error prone.

With JUnit5 and ParameterizedTest, we can now use a custom arguments source for this case, which is way more convenient. It looks likes this:

```
@ParameterizedTest
@ApiKeyVersionsSource(apiKey = ApiKeys.ADD_PARTITIONS_TO_TXN)
public void mytest(short version) {
  // do smth based on version...
}
```

This patch introduces the new annotation and updates `AddPartitionsToTxnRequestTest` test as a first example. I will migrate all the other cases in subsequent PRs.

Reviewers: Luke Chen <showuon@gmail.com>, Jason Gustafson <jason@confluent.io>","['clients/src/test/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequestTest.java', 'clients/src/test/java/org/apache/kafka/common/utils/annotation/ApiKeyVersionsProvider.java', 'clients/src/test/java/org/apache/kafka/common/utils/annotation/ApiKeyVersionsSource.java']",Unit tests for all versions of a request are prone to errors due to iteration within the test itself. JUnit5 and ParameterizedTest could increase convenience with a custom arguments source.
169fa0efcc9089bdcf37e6f8b1b9a6f3377f38f8,1585456086,"MINOR: Fix error message in exception when records have schemas in Connect's Flatten transformation (#3982)

In case of an error while flattening a record with schema, the Flatten transformation was reporting an error about a record without schema, as follows: 

```
org.apache.kafka.connect.errors.DataException: Flatten transformation does not support ARRAY for record without schemas (for field ...)
```

The expected behaviour would be an error message specifying ""with schemas"". 

This looks like a simple copy/paste typo from the schemaless equivalent methods, in the same file 

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Konstantine Karantasis <konstantine@confluent.io>",['connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java'],"Flatten transformation in Connect throws inaccurate error message when encountering an issue flattening a record with a schema, indicating a record without a schema instead."
f610f9ff1f59f90434c3614f00c95001f50100e4,1578001524,"MINOR: Remove spammy, unhelpful log message in the controller (#7879)

This patch removes a spammy log message in the controller which is printed every time the leader imbalance ratio is checked. It is unhelpful because preferred leaders are generally deterministic and is spammy because it includes _every_ partition in the cluster.

Reviewers: Jonathan Santilli <jonathansantilli@users.noreply.github.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",['core/src/main/scala/kafka/controller/KafkaController.scala'],"Unnecessary log message printed every time the leader imbalance ratio is checked, causing excessive logging and lack of utility as preferred leaders are normally deterministic."
ebf78c04c9aa841f7bd702afdbc782cc07b35ad8,1565837078,"KAFKA-8788: Optimize client metadata handling with a large number of partitions (#7192)

Credit to @lbradstreet for profiling the producer with a large number of partitions.

Cache `topicMetadata`, `brokers` and `controller` in the `MetadataResponse`
the first time it's needed avoid unnecessary recomputation. We were previously
computing`brokersMap` 4 times per partition in one code path that was invoked from
multiple places. This is a regression introduced via a42f16f980 and first released
in 2.3.0.

The `Cluster` constructor became significantly more allocation heavy due to
2c44e77e2f20, first released in 2.2.0. Replaced `merge` calls with more verbose,
but more efficient code. Added a test to verify that the returned collections are
unmodifiable.

Add `topicAuthorizedOperations` and `clusterAuthorizedOperations` to
`MetadataResponse` and remove `data()` method.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Lucas Bradstreet <lucasbradstreet@gmail.com>, Colin P. McCabe <cmccabe@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Justine Olshan <jolshan@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/main/java/org/apache/kafka/common/Cluster.java', 'clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/StickyPartitionCacheTest.java', 'clients/src/test/java/org/apache/kafka/common/ClusterTest.java']","Client metadata handling is inefficient with a high number of partitions. This causes unnecessary recomputation and allocation - affecting `MetadataResponse` and `Cluster`. Also, `MetadataResponse` lacks `topicAuthorizedOperations` and `clusterAuthorizedOperations` and needs to eliminate `data()` method."
da58d75c4389aa988ac6028bb8bd073646f07300,1634084343,"MINOR: Fix highest offset when loading KRaft metadata snapshots (#11386)

When loading a snapshot the broker BrokerMetadataListener was using the batch's append time, offset
and epoch. These are not the same as the append time, offset and epoch from the log. This PR fixes
it to instead use the lastContainedLogTimeStamp, lastContainedLogOffset and lastContainedLogEpoch
from the SnapshotReader.

This PR refactors the MetadataImage and MetadataDelta to include an offset and epoch. It also swaps
the order of the arguments for ReplicaManager.applyDelta, in order to be more consistent with
MetadataPublisher.publish.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/main/scala/kafka/server/metadata/BrokerMetadataListener.scala', 'core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala', 'core/src/main/scala/kafka/server/metadata/BrokerMetadataSnapshotter.scala', 'core/src/main/scala/kafka/server/metadata/MetadataPublisher.scala', 'core/src/main/scala/kafka/server/metadata/MetadataSnapshotter.scala', 'core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerConcurrencyTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataListenerTest.scala', 'core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataSnapshotterTest.scala', 'metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java', 'metadata/src/main/java/org/apache/kafka/image/MetadataImage.java', 'metadata/src/test/java/org/apache/kafka/image/MetadataImageTest.java', 'metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java']","BrokerMetadataListener is incorrectly using batch's append time, offset, and epoch when loading a snapshot, leading to discrepancies with the log's append time, offset, and epoch."
adf5cc5371db6253e070b11dc78dedc99c8065f9,1650047323,"KAFKA-13769: Explicitly route FK join results to correct partitions (#11945)

Prior to this commit FK response sink routed FK results to
SubscriptionResolverJoinProcessorSupplier using the primary key.

There are cases, where this behavior is incorrect. For example,
if KTable key serde differs from the data source serde which might
happen without a key changing operation.

Instead of determining the resolver partition by serializing the PK
this patch includes target partition in SubscriptionWrapper payloads.
Default FK response-sink partitioner extracts the correct partition
from the value and routes the message accordingly.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignJoinSubscriptionProcessorSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignJoinSubscriptionSendProcessorSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionJoinForeignProcessorSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapper.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerde.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapper.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerde.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplierTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerdeTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerdeTest.java']","FK response sink incorrectly routes results using the primary key, especially when KTable key serde differs from the data source serde without a key changing operation."
22e8e71156762b40ac93e2cbd42eacba00dbfb0c,1609970462,"KAFKA-9274: Fix commit-TimeoutException handling for EOS (#9800)

If EOS is enabled and the TX commit fails with a timeout,
we should not process more messages (what is ok for non-EOS)
because we don't really know the status of the TX.
If the commit was indeed successful, we won't have an open TX
can calling send() would fail with an fatal error.

Instead, we should retry the (idempotent) commit of the TX,
and start a new TX afterwards.

Reviewers: Boyang Chen <boyang@confluent.io>, John Roesler <john@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java']","If a transaction commit fails with a timeout when EOS is enabled, the transaction status becomes uncertain, resulting in potential fatal error when attempting to process more messages or start a new transaction."
4c7e0a9fa66fd1becc590d6060228b0305dd400b,1692440010,"MINOR: Decouple purging committed records from committing (#14227)

Currently, Kafka Streams only tries to purge records whose
offset are committed from a repartition topic when at
least one offset was committed in the current commit.
The coupling between committing some offsets and purging
records is not needed and might delay purging of records.
For example, if a in-flight call for purging records has not
completed yet when a commit happens, a new call
is not issued.
If then the earlier in-flight call for purging records
finally completes but the next commit does not commit any
offsets, Streams does not issue the call for purging records
whose offset were committed in the previous commit
because the purging call was still in-flight.

This change issues calls for purging records during any commit
if the purge interval passed, even if no offsets were committed
in the current commit.

Reviewers: Lucas Brutschy <lbrutschy@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']","Purging records from a repartition topic in Kafka Streams is currently coupled with committing offsets, potentially causing delays and incomplete purges during commits with no committed offsets."
ca5d6f9229c170beb23809159113037f05a1120f,1644188879,"KAFKA-13563: clear FindCoordinatorFuture for non consumer group mode (#11631)

After KAFKA-10793, we clear the findCoordinatorFuture in 2 places:

1. heartbeat thread
2. AbstractCoordinator#ensureCoordinatorReady

But in non consumer group mode with group id provided (for offset commitment. So that there will be consumerCoordinator created), there will be no (1)heartbeat thread , and it only call (2)AbstractCoordinator#ensureCoordinatorReady when 1st time consumer wants to fetch committed offset position. That is, after 2nd lookupCoordinator call, we have no chance to clear the findCoordinatorFuture , and causes the offset commit never succeeded.

To avoid the race condition as KAFKA-10793 mentioned, it's not safe to clear the findCoordinatorFuture in the future listener. So, I think we can fix this issue by calling AbstractCoordinator#ensureCoordinatorReady when coordinator unknown in non consumer group case, under each ConsumerCoordinator#poll.


Reviewers: Guozhang Wang <wangguoz@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java', 'core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala']","In non-consumer group mode with group id provided, there's no chance to clear the findCoordinatorFuture after the 2nd lookupCoordinator call, causing the offset commit to never succeed."
44be5d2221c9faff2ecb17878d6193504b7289aa,1552064644,"KAFKA-8069; Fix early expiration of offsets due to invalid loading of expire timestamp (#6401)

After the 2.1 release, if the broker hasn't been upgrade to the latest inter-broker protocol version, the committed offsets stored in the __consumer_offset topic will get cleaned up way earlier than it should be when the offsets are loaded back from the __consumer_offset topic in GroupCoordinator, which will happen during leadership transition or after broker bounce. This patch fixes the bug by setting expireTimestamp to None if it is the default value after loading v1 offset records from __consumer_offsets.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala']",Committed offsets in the __consumer_offset topic are getting cleaned up prematurely after a broker bounce or leadership transition due to improper loading of expiration timestamps.
f875576f218c90bc4a682a641ff149d66f75e0b8,1638290150,"KAFKA-13469: Block for in-flight record delivery before end-of-life source task offset commit (#11524)

Although committing source task offsets without blocking on the delivery of all in-flight records is beneficial most of the time, it can lead to duplicate record delivery if there are in-flight records at the time of the task's end-of-life offset commit.

A best-effort attempt is made here to wait for any such in-flight records to be delivered before proceeding with the end-of-life offset commit for source tasks. Connect will block for up to offset.flush.timeout.ms milliseconds before calculating the latest committable offsets for the task and flushing those to the persistent offset store.

Author: Chris Egerton <chrise@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SubmittedRecords.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/SubmittedRecordsTest.java']",Committing source task offsets without waiting for in-flight record delivery can potentially result in the delivery of duplicate records at task's end-of-life offset commit.
0189298d8667f770a69fb453d99971475f56af4b,1617148962,"KAFKA-12288: remove task-level filesystem locks (#10342)

The filesystem locks don't protect access between StreamThreads, only across different instances of the same Streams application. Running multiple processes in the same physical state directory is not supported, and as of PR #9978 it's explicitly guarded against), so there's no reason to continue locking the task directories with anything heavier than an in-memory map.

Reviewers: Rohan Desai <rodesai@confluent.io>, Walker Carlson <wcarlson@confluent.io>, Guozhang Wang <guozhang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreatorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerUtilTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']","Task-level filesystem locks are unnecessary, as they don't provide protection between StreamThreads and cause redundancy when running under same Streams application instance."
1785e1223eb2ce6b6c00af8b6ab406f60795f593,1641844466,"KAFKA-13582: TestVerifiableProducer.test_multiple_kraft_security_protocols fails (#11664)

KRaft brokers always use the first controller listener, so if there is not also a colocated KRaft controller on the node be sure to only publish one controller listener in `controller.listener.names` even when the inter-controller listener name differs.  System tests were failing due to unnecessarily publishing a second entry in `controller.listener.names` for a broker-only config and not also publishing a mapping for it in `listener.security.protocol.map`.  Removing the unnecessary entry in `controller.listener.names` solves the problem.

Reviewers: David Jacot <djacot@confluent.io>",['tests/kafkatest/services/kafka/kafka.py'],KRaft brokers' system tests fail due to a second unnecessary entry in `controller.listener.names` in broker-only config without a corresponding mapping in `listener.security.protocol.map`.
560ab2cc319899d7ce1181408507061e90752d3e,1685602051,"KAFKA-14133: Migrate ActiveTaskCreator mock in TaskManagerTest to Mockito (#13681)

This pull requests migrates the ActiveTaskCreator mock in TaskManagerTest from EasyMock to Mockito
The change is restricted to a single mock to minimize the scope and make it easier for review.
Please see two examples that follow the same pattern below:
#13529
#13621

Reviewers: Divij Vaidya <diviv@amazon.com>, Manyanda Chitimbo <manyanda.chitimbo@gmail.com>, Christo Lolov <lolovc@amazon.com>, Bruno Cadonna <cadonna@apache.org>",['streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java'],"'ActiveTaskCreator mock in TaskManagerTest is currently using EasyMock, which needs to be migrated for consistency with other tests.'"
31f9a54cba38fbdc015590bd82c1f1d62839f09f,1680696692,"KAFKA-14850: introduce InMemoryLeaderEpochCheckpoint (#13456)

The motivation for introducing InMemoryLeaderEpochCheckpoint is to allow remote log manager to create the RemoteLogSegmentMetadata(RLSM) with the correct leader epoch info for a specific segment. To do that, we need to rely on the LeaderEpochCheckpointCache to truncate from start and end, to get the epoch info. However, we don't really want to truncate the epochs in cache (and write to checkpoint file in the end). So, we introduce this InMemoryLeaderEpochCheckpoint to feed into LeaderEpochCheckpointCache, and when we truncate the epoch for RLSM, we can do them in memory without affecting the checkpoint file, and without interacting with file system.

Reviewers: Divij Vaidya <diviv@amazon.com>, Satish Duggana <satishd@apache.org>","['core/src/test/scala/unit/kafka/server/checkpoints/InMemoryLeaderEpochCheckpointTest.scala', 'server-common/src/main/java/org/apache/kafka/server/common/CheckpointFile.java', 'storage/src/main/java/org/apache/kafka/storage/internals/checkpoint/InMemoryLeaderEpochCheckpoint.java', 'storage/src/main/java/org/apache/kafka/storage/internals/epoch/LeaderEpochFileCache.java']",Inability to create RemoteLogSegmentMetadata with correct leader epoch info for a specific segment without interacting with the file system or affecting the checkpoint file.
353aa6206d43b4923d21510797709bbbb210f6b6,1585697968,"KAFKA-9753: Add active tasks process ratio (#8370)

Measure the percentage ratio the stream thread spent on processing each task among all assigned active tasks (KIP-444). Also add unit tests to cover the added metrics in this PR and the previous #8358. Also trying to fix the flaky test reported in KAFKA-5842

Co-authored-by: John Roesler <vvcephei@apache.org>

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java', 'streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/metrics/TaskMetricsTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetricsTest.java']","The stream thread's processing time ratio amongst all assigned active tasks is not being measured, leading to a lack of visibility in performance metrics. Additionally, test coverage for these metrics is missing and there is a reported flaky test (KAFKA-5842)."
b74204fa0a41e5779d513116cb32c0f24e7662c2,1685132177,"KAFKA-14996: Handle overly large user operations on the kcontroller (#13742)

Previously, if a user tried to perform an overly large batch operation on the KRaft controller
(such as creating a million topics), we would create a very large number of records in memory. Our
attempt to write these records to the Raft layer would fail, because there were too many to fit in
an atomic batch. This failure, in turn, would trigger a controller failover.

(Note: I am assuming here that no topic creation policy was in place that would prevent the
creation of a million topics. I am also assuming that the user operation must be done atomically,
which is true for all current user operations, since we have not implemented KIP-868 yet.)

With this PR, we fail immediately when the number of records we have generated exceeds the
threshold that we can apply. This failure does not generate a controller failover. We also now
fail with a PolicyViolationException rather than an UnknownServerException.

In order to implement this in a simple way, this PR adds the BoundedList class, which wraps any
list and adds a maximum length. Attempts to grow the list beyond this length cause an exception to
be thrown.

Reviewers: David Arthur <mumrah@gmail.com>, Ismael Juma <ijuma@apache.org>, Divij Vaidya <diviv@amazon.com>","['core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/ClientQuotaControlManagerTest.java', 'server-common/src/main/java/org/apache/kafka/server/mutable/BoundedList.java', 'server-common/src/main/java/org/apache/kafka/server/mutable/BoundedListTooLongException.java', 'server-common/src/test/java/org/apache/kafka/server/mutable/BoundedListTest.java']","When attempting large batch operations on the KRaft controller (like creating too many topics), an excessive number of records are generated in memory, leading to a failure while writing to the Raft layer due to atomic batch size constraints, and triggering a controller failover."
3467036e017adc3ac0919bbc0c067b1bb1b621f3,1654272726,"KAFKA-13803: Refactor Leader API Access (#12005)

This PR refactors the leader API access in the follower fetch path.

Added a LeaderEndPoint interface which serves all access to the leader.

Added a LocalLeaderEndPoint and a RemoteLeaderEndPoint which implements the LeaderEndPoint interface to handle fetches from leader in local & remote storage respectively.

Reviewers: David Jacot <djacot@confluent.io>, Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/server/AbstractFetcherManager.scala', 'core/src/main/scala/kafka/server/AbstractFetcherThread.scala', 'core/src/main/scala/kafka/server/BrokerBlockingSender.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/main/scala/kafka/server/LeaderEndPoint.scala', 'core/src/main/scala/kafka/server/LocalLeaderEndPoint.scala', 'core/src/main/scala/kafka/server/RemoteLeaderEndPoint.scala', 'core/src/main/scala/kafka/server/ReplicaAlterLogDirsManager.scala', 'core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala', 'core/src/main/scala/kafka/server/ReplicaFetcherManager.scala', 'core/src/main/scala/kafka/server/ReplicaFetcherThread.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherManagerTest.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/epoch/util/MockBlockingSender.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java']",The follower fetch path has issues with differentiating between accessing the leader API in local and remote storage.
96fa468106f538ca83973ce040e6cfe900f389ed,1643138626,"MINOR: fix NPE in iqv2 (#11702)

There is a brief window between when the store is registered and when
it is initialized when it might handle a query, but there is no context.
We treat this condition just like a store that hasn't caught up to the
desired position yet.

Reviewers: Guozhang Wang <guozhang@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org>, Patrick Stuedi <pstuedi@apache.org>","['streams/src/main/java/org/apache/kafka/streams/query/QueryResult.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/MemoryNavigableLRUCache.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSessionStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/StoreQueryUtils.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/StoreQueryUtilsTest.java']","Queries may be handled without a context in a small window between store registration and initialization, leading to a Null Pointer Exception (NPE)."
b029902b129333f0bafa2d0776cc827004e2532b,1580844153,"KAFKA-9491; Increment high watermark after full log truncation (#8037)

When a follower's fetch offset is behind the leader's log start offset, the
follower will do a full log truncation. When it does so, it must update both
its log start offset and high watermark. The previous code did the former,
but not the latter. Failure to update the high watermark in this case can lead
to out of range errors if the follower becomes leader before getting the latest
high watermark from the previous leader. The out of range errors occur when
we attempt to resolve the log position of the high watermark in DelayedFetch
in order to determine if a fetch is satisfied.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/log/Log.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala']","Full log truncation by a follower in Kafka fails to update the high watermark, potentially leading to out-of-range errors if the follower transitions to leader before obtaining the latest high watermark from the previous leader."
56ab2f80343d7d842c1b0e3069833106d4fe3259,1668839938,"KAFKA-14382: wait for current rebalance to complete before triggering followup (#12869)

Fix for the subtle bug described in KAFKA-14382 that was causing rebalancing loops. If we trigger a new rebalance while the current one is still ongoing, it may cause some members to fail the first rebalance if they weren't able to send the SyncGroup request in time (for example due to processing records during the rebalance). This means those consumers never receive their assignment from the original rebalance, and won't revoke any partitions they might have needed to. This can send the group into a loop as each rebalance schedules a new followup cooperative rebalance due to partitions that need to be revoked, and each followup rebalance causes some consumer(s) to miss the SyncGroup and never revoke those partitions.

Reviewers: John Roesler <vvcephei@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']",Triggering a new rebalance while the current rebalance is still ongoing is causing a rebalancing loop due to certain members failing the first rebalance. This causes some consumers never to receive their assignment from the original rebalance or revoke any partitions.
59d30a06fc1371d01a853139053bd6bd9bb770a0,1686149703,"KAFKA-10337: await async commits in commitSync even if no offsets given (#13678)

The contract for Consumer#commitSync() guarantees that the callbacks for all prior async commits will be invoked before it returns. Prior to this patch the contract could be violated if an empty offsets map were passed in to Consumer#commitSync().

Reviewers: Philip Nee <philipnee@gmail.com>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java']","Consumer#commitSync() is not invoking callbacks for all prior async commits before returning when an empty offsets map is passed, violating its contract."
600cc48fa56ca675421eaf383929e89a04b529f1,1565356592,"KAFKA-8748: Fix flaky testDescribeLogDirsRequest (#7182)

The introduction of KIP-480: Sticky Producer Partitioner had the
side effect that generateAndProduceMessages can often write
messages to a lower number of partitions to improve batching.

testDescribeLogDirsRequest (and potentially other tests) relies
on the messages being written somewhat uniformly to the topic
partitions. We fix the issue by including a monotonically
increasing key in the produced messages.

I also included a couple of minor clean-ups I noticed while
debugging the issue.

The test failed very frequently when executed locally before the
change and it passed 100 times consecutively after the change.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>","['core/src/test/scala/unit/kafka/server/DescribeLogDirsRequestTest.scala', 'core/src/test/scala/unit/kafka/utils/JaasTestUtils.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","The implementation of KIP-480: Sticky Producer Partitioner has caused non-uniform message distribution across topic partitions, which is affecting the testDescribeLogDirsRequest and potentially other tests."
3b05dc685b4bf7bafb8057b15c837fd5333789c5,1570229891,"MINOR: just remove leader on trunk like we did on 2.3 (#7447)

Small follow-up to trunk PR #7423

While debugging the 2.3 VP PR we realized we should remove the leader-tracking from the VP system test altogether. We'd already merged the corresponding trunk PR so I made a quick new PR for trunk (also fixes a missed version bump in one of the log messages)

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['tests/kafkatest/tests/streams/streams_upgrade_test.py'],"The leader-tracking in the VP system test is redundant, causing potential errors. Mismatched version bump in one of the log messages might also cause confusion."
d99562a1452ac09c8b5d6bc96b1e6b5a5a481f6d,1626974827,"HOTFIX: Set session interval back to 10s for StreamsCooperativeRebalanceUpgradeTest (#11103)

This test is hitting pretty frequent timeouts after bouncing a node and waiting for it to come back and fully rejoin the group. It seems to now take 45s for the initial JoinGroup to succeed, which I suspect is due to the new default session.interval.ms (which was recently changed to 45s). Let's try fixing this config to the old value of 10s and see if that helps it rejoin in time.

Reviewer: Bruno Cadonna <cadonna@apache.org>",['streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java'],"StreamsCooperativeRebalanceUpgradeTest experiences frequent timeouts after bouncing a node while it attempts to rejoin the group, possibly due to the recent changes in default session.interval.ms value."
e820eb42b2c7cb812c9db25c4788347029fd7119,1616427956,"KAFKA-12383: Get RaftClusterTest.java and other KIP-500 junit tests working (#10220)

Introduce ""testkit"" package which includes KafkaClusterTestKit class for enabling integration tests of self-managed clusters. Also make use of this new integration test harness in the ClusterTestExtentions JUnit extension. 

Adds RaftClusterTest for basic self-managed integration test. 

Reviewers: Jason Gustafson <jason@confluent.io>, Colin P. McCabe <cmccabe@apache.org>

Co-authored-by: Colin P. McCabe <cmccabe@apache.org>

","['core/src/main/scala/kafka/raft/RaftManager.scala', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/ControllerServer.scala', 'core/src/main/scala/kafka/server/KafkaRaftServer.scala', 'core/src/main/scala/kafka/tools/TestRaftServer.scala', 'core/src/test/java/kafka/test/ClusterInstance.java', 'core/src/test/java/kafka/test/ClusterTestExtensionsTest.java', 'core/src/test/java/kafka/test/annotation/Type.java', 'core/src/test/java/kafka/test/junit/ClusterTestExtensions.java', 'core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java', 'core/src/test/java/kafka/testkit/BrokerNode.java', 'core/src/test/java/kafka/testkit/ControllerNode.java', 'core/src/test/java/kafka/testkit/KafkaClusterTestKit.java', 'core/src/test/java/kafka/testkit/TestKitNode.java', 'core/src/test/java/kafka/testkit/TestKitNodes.java', 'core/src/test/scala/integration/kafka/server/RaftClusterTest.scala', 'raft/src/main/java/org/apache/kafka/raft/RaftConfig.java']","Self-managed clusters integration tests, including RaftClusterTest, aren't functioning properly as they lack an appropriate test harness."
e7de280b0f1c7a924293dba79be77f56a08d0e15,1606776508,"KAFKA-10702; Skip bookkeeping of empty transactions (#9632)

Compacted topics can accumulate a large number of empty transaction markers as the data from the transactions gets cleaned. For each transaction, there is some bookkeeping that leaders and followers must do to keep the transaction index up to date. The cost of this overhead can degrade performance when a replica needs to catch up if the log has mostly empty or small transactions. This patch improves the cost by skipping over empty transactions since these will have no effect on the last stable offset and do not need to be reflected in the transaction index.

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/ProducerStateManager.scala', 'core/src/test/scala/unit/kafka/log/LogTest.scala', 'core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala']",Large numbers of empty transaction markers in compacted topics lead to performance degradation due to unnecessary bookkeeping overhead which impacts transaction index updates and replica catch up efforts.
5aed178048dd5a79112274ee5e8a72611d9f0ec2,1648591178,"KAFKA-13418: Support key updates with TLS 1.3 (#11966)

Key updates with TLS 1.3 trigger code paths similar to renegotiation with TLS 1.2.
Update the read/write paths not to throw an exception in this case (kept the exception
in the `handshake` method).

With the default configuration, key updates happen after 2^37 bytes are encrypted.
There is a security property to adjust this configuration, but the change has to be
done before it is used for the first time and it cannot be changed after that. As such,
it is best done via a system test (filed KAFKA-13779).

To validate the change, I wrote a unit test that forces key updates and manually ran
a producer workload that produced more than 2^37 bytes. Both cases failed without
these changes and pass with them.

Note that Shylaja Kokoori attached a patch with the SslTransportLayer fix and hence
included them as a co-author of this change.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Co-authored-by: Shylaja Kokoori","['clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java', 'clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java', 'clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java', 'clients/src/test/java/org/apache/kafka/common/network/Tls12SelectorTest.java', 'clients/src/test/java/org/apache/kafka/common/network/Tls13SelectorTest.java']","TLS 1.3 key updates after the encryption of 2^37 bytes trigger similar code paths to TLS 1.2 renegotiation, which results in exceptions being thrown in read/write paths."
161c5638b8b1b12fdac04fa56bbaeb270fa36f8d,1617637324,"KAFKA-12614: Use Jenkinsfile for trunk and release branch builds (#10473)

* Run all JDK/Scala version combinations for trunk/release branch builds.
* Only retry failures in PR builds for now (we can remove this distinction if/when
we report flaky failures as described in KAFKA-12216).
* Disable concurrent builds
* Send email to dev list on build failure
* Use triple double quotes in `doValidation` since we use string interpolation
for `SCALA_VERSION`.
* Update release.py to output new `Unit/integration tests` Jenkins link

Reviewers: Gwen Shapira <cshapi@gmail.com>, David Arthur <mumrah@gmail.com>",['release.py'],"Jenkinsfile is not being used for trunk and release branch builds and there is no mechanism to retry failures in PR builds, report flaky failures or disable concurrent builds. Notification of build failure is not being sent to the dev list."
bbcf40ad0dc7739d803244bbbfbaa4598850344b,1695970039,"MINOR: improve info log for memberIDRequired exception (#14192)

Currently, when consumer startup, there will be a log message said:

[2023-08-11 15:47:17,713] INFO [Consumer clientId=console-consumer, groupId=console-consumer-28605] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)

It confused users and make them think there's something wrong in the consumer application. Since we already log need to re-join with the given member-id logs in the joinGroupResponseHandler and already requestRejoined. So, we can skip this confusion logs.

Reviewers: Lucas Brutschy <lbrutschy@confluent.io>, Paolo Patierno <ppatiern@redhat.com>, vamossagar12 <sagarmeansocean@gmail.com>, David Jacot <djacot@confluent.io>",['clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java'],Consumer startup logs misleading error message indicating 'The group member needs to have a valid member id before actually entering a consumer group.' even though members have valid ids.
a63bf93dcea092642314765fbcf09b3cf2c6798e,1694521460,"KAFKA-14993: Improve TransactionIndex instance handling while copying to and fetching from RSM (#14363)

- Updated the contract for RSM's fetchIndex to throw a ResourceNotFoundException instead of returning an empty InputStream when it does not have a TransactionIndex.
- Updated the LocalTieredStorage implementation to adhere to the new contract.
- Added Unit Tests for the change.

Reviewers: Satish Duggana <satishd@apache.org>, Luke Chen <showuon@gmail.com>, Divij Vaidya <diviv@amazon.com>, Christo Lolov <lolovc@amazon.com>, Kamal Chandraprakash<kamal.chandraprakash@gmail.com>","['core/src/test/scala/unit/kafka/log/remote/RemoteIndexCacheTest.scala', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageManager.java', 'storage/src/main/java/org/apache/kafka/storage/internals/log/RemoteIndexCache.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/LocalTieredStorage.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/LocalTieredStorageTest.java']","Fetching transaction index from RSM's fetchIndex if the index does not exist results in an empty InputStream, rather than throwing a ResourceNotFoundException."
e2c8612d01f5e6e97476b1656fe728b2f316efef,1565709414,"KAFKA-7941: Catch TimeoutException in KafkaBasedLog worker thread (#6283)

When calling readLogToEnd(), the KafkaBasedLog worker thread should catch TimeoutException and log a warning, which can occur if brokers are unavailable, otherwise the worker thread terminates.

Includes an enhancement to MockConsumer that allows simulating exceptions not just when polling but also when querying for offsets, which is necessary for testing the fix.

Author: Paul Whalen <pgwhalen@gmail.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Arjun Satish <arjun@confluent.io>, Ryanne Dolan <ryannedolan@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/KafkaBasedLogTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java']","The KafkaBasedLog worker thread terminates when a TimeoutException is not caught during the execution of readLogToEnd(), likely caused by unavailable brokers."
6c6b8e2f96397f75c3300969f21edadbf43427b7,1663707258,"KAFKA-14214: Introduce read-write lock to StandardAuthorizer for consistent ACL reads. (#12628)

Fixes an issue with StandardAuthorizer#authorize that allowed inconsistent results. The underlying 
concurrent data structure (ConcurrentSkipListMap) had weak consistency guarantees. This meant
that a concurrent update to the authorizer data could result in the authorize function processing 
ACL updates out of order.

This patch replaces the concurrent data structures with regular non-thread safe equivalents and uses
a read/write lock for thread safety and strong consistency.

Reviewers: David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>, Colin P. McCabe <cmccabe@apache.org>, Luke Chen <showuon@gmail.com>
","['metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizer.java', 'metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerData.java']",Concurrent updates to StandardAuthorizer data can process ACL updates out of order due to weak consistency guarantees of the underlying ConcurrentSkipListMap.
6bcc497c36a1aef19204b1bfe3b17a8c1c84c059,1683288320,"KAFKA-14766: Improve performance of VarInt encoding and decoding (#13312)

Motivation

Reading/writing the protocol buffer varInt32 and varInt64 (also called varLong in our code base) is in the hot path of data plane code in Apache Kafka. We read multiple varInt in a record and in long. Hence, even a minor change in performance could extrapolate to larger performance benefit.

In this PR, we only update varInt32 encoding/decoding.
Changes

This change uses loop unrolling and reduces the amount of repetition of calculations. Based on the empirical results from the benchmark, the code has been modified to pick up the best implementation.
Results

Performance has been evaluated using JMH benchmarks on JDK 17.0.6. Various implementations have been added in the benchmark and benchmarking has been done for different sizes of varints and varlongs. The benchmark for various implementations have been added at ByteUtilsBenchmark.java

Reviewers: Ismael Juma <mlists@juma.me.uk>, Luke Chen <showuon@gmail.com>, Alexandre Dupriez <alexandre.dupriez@gmail.com>","['clients/src/main/java/org/apache/kafka/common/utils/ByteUtils.java', 'clients/src/test/java/org/apache/kafka/common/utils/ByteUtilsTest.java', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/util/ByteUtilsBenchmark.java']","There is an issue with the performance of VarInt encoding and decoding in the data plane code of Apache Kafka, which is affecting overall application performance."
672dd3ab6aea413eaa8170236f351a0f2a35a89c,1680884754,"KAFKA-13020; Implement reading Snapshot log append timestamp (#13345)

The SnapshotReader exposes the ""last contained log time"". This is mainly used during snapshot cleanup. The previous implementation used the append time of the snapshot record. This is not accurate as this is the time when the snapshot was created and not the log append time of the last record included in the snapshot.

The log append time of the last record included in the snapshot is store in the header control record of the snapshot. The header control record is the first record of the snapshot.

To be able to read this record, this change extends the RecordsIterator to decode and expose the control records in the Records type.

Reviewers: Colin Patrick McCabe <cmccabe@apache.org>","['clients/src/main/java/org/apache/kafka/common/record/ControlRecordUtils.java', 'core/src/main/scala/kafka/raft/KafkaMetadataLog.scala', 'core/src/main/scala/kafka/raft/RaftManager.scala', 'core/src/main/scala/kafka/tools/DumpLogSegments.scala', 'core/src/test/scala/kafka/raft/KafkaMetadataLogTest.scala', 'core/src/test/scala/unit/kafka/tools/DumpLogSegmentsTest.scala', 'metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java', 'raft/src/main/java/org/apache/kafka/raft/Batch.java', 'raft/src/main/java/org/apache/kafka/raft/ControlRecord.java', 'raft/src/main/java/org/apache/kafka/raft/internals/RecordsIterator.java', 'raft/src/main/java/org/apache/kafka/snapshot/RecordsSnapshotReader.java', 'raft/src/main/java/org/apache/kafka/snapshot/RecordsSnapshotWriter.java', 'raft/src/test/java/org/apache/kafka/ControlRecordTest.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java', 'raft/src/test/java/org/apache/kafka/raft/internals/RecordsIteratorTest.java', 'raft/src/test/java/org/apache/kafka/snapshot/SnapshotWriterReaderTest.java']","SnapshotReader's ""last contained log time"" inaccurately uses the snapshot creation time instead of the actual log append time of the last record included in the snapshot."
6d11261d5deaca300e273bebe309f9e4f814f815,1675708670,"MINOR: IBP_3_4_IV1 should be IBP_3_5_IV0 because it is not in 3.4 (#13198)

The KIP-405 MetadataVersion changes will be released as part of AK 3.5, but were added as BP_3_4_IV1.
This change fixes them to be IBP_3_5_IV0. There is no incompatibility  because this feature has not yet
been released. Also set didMetadataChange to false because KRaft metadata log records did not change.

Reviewers: Satish Duggana <satishd@apache.org>, Christo Lolov <christo_lolov@yahoo.com>, Colin P. McCabe <cmccabe@apache.org>","['core/src/test/java/kafka/test/ClusterTestExtensionsTest.java', 'core/src/test/java/kafka/test/annotation/ClusterTest.java', 'core/src/test/scala/integration/kafka/server/MetadataVersionIntegrationTest.scala', 'core/src/test/scala/unit/kafka/admin/FeatureCommandTest.scala', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'server-common/src/main/java/org/apache/kafka/server/common/MetadataVersion.java', 'server-common/src/test/java/org/apache/kafka/server/common/MetadataVersionTest.java']","The KIP-405 MetadataVersion changes are incorrectly marked as part of AK 3.4 (as BP_3_4_IV1), while they are intended to be part of AK 3.5, leading to potential versioning confusion."
1a3fe9aa52555eb24ce692963e4461d6f05b771d,1559613960,"KAFKA-8404: Add HttpHeader to RestClient HTTP Request and Connector REST API (#6791)

When Connect forwards a REST request from one worker to another, the Authorization header was not forwarded. This commit changes the Connect framework to add include the authorization header when forwarding requests to other workers.

Author: Hai-Dang Dam <damquanghaidang@gmail.com>
Reviewers: Robert Yokota <rayokota@gmail.com>, Randall Hauch <rhauch@gmail.com>
","['connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/JaasBasicAuthFilter.java', 'connect/basic-auth-extension/src/test/java/org/apache/kafka/connect/rest/basic/auth/extension/JaasBasicAuthFilterTest.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResourceTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResourceTest.java']","The Authorization header is missing when Connect forwards a REST request from one worker to another, causing authentication issues in the Connect framework."
fd7991ae23d2c169fe3f353c22f9fb888260be2d,1576599625,"MINOR: Fix throttle usage in reassignment test case (#7822)

The replication throttle in `testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress` was not setting the quota on the partition correctly, so the test case was not working as expected. This patch fixes the problem and also fixes a bug in `waitForTopicCreated` which caused the function to always wait for the full timeout.

Reviewers: Ismael Juma <ismael@juma.me.uk>","['core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala', 'core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","The replication throttle in `testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress` is not setting the quota correctly, resulting in improper function of the test case. Moreover, `waitForTopicCreated` function always waits for full timeout."
7ef8701cca92d8d4ecf07b2b12832aaafb8a5331,1641438277,"KAFKA-13553: add PAPI KV store tests for IQv2 (#11624)

During some recent reviews, @mjsax pointed out that StateStore layers
are constructed differently the stores are added via the PAPI vs. the DSL.

This PR adds KeyValueStore PAPI construction to the
IQv2StoreIntegrationTest so that we can ensure IQv2 works on every
possible state store.

Reviewers: Patrick Stuedi <pstuedi@apache.org>, Guozhang Wang <guozhang@apache.org>",['streams/src/test/java/org/apache/kafka/streams/integration/IQv2StoreIntegrationTest.java'],Differences in StateStore layer constructions when stores are added via PAPI vs the DSL may impact IQv2's functionality with every possible state store.
4ac2ad3a2bea9db1e4835a9dffc4b029dc1d9fcd,1586967870,"MINOR: Eliminate unnecessary partition lookups (#8484)

There are two cases in the fetch pass where a partition is unnecessarily looked up
from the partition Pool, when one is already accessible. This will be a fairly minor
improvement on high partition count clusters, but could be worth 1% from some
profiles I have seen.

More importantly, the code is cleaner this way.

Reviewers: Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/server/DelayedFetch.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala']","Fetch pass is executing unnecessary partition lookups from the partition Pool, even when one is available, potentially affecting performance on high partition count clusters."
d817b1b5900e16d76ceae570d6e93d4d57783b73,1696515107,"KAFKA-15415: On producer-batch retry, skip-backoff on a new leader (#14384)

When producer-batch is being retried, new-leader is known for the partition Vs the leader used in last attempt, then it is worthwhile to retry immediately to this new leader. A partition-leader is considered to be newer, if the epoch has advanced.

Reviewers: Walker Carlson <wcarlson@apache.org>, Kirk True <kirk@kirktrue.pro>, Andrew Schofield <andrew_schofield@uk.ibm.com","['clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerBatchTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java']","Issue with producer-batch retry where it doesn't immediately retry with a new identified leader for the partition, causing unnecessary delays."
e0824e26ec194dddbc3397b8c4725b84f72c482c,1570642325,"MINOR: Just one put and flush to generation rocksDB File in RocksDBStoreTest (#7469)

After merged #7412 we realized it does not necessarily need that long time: instead of putting 2 million records, we can just have a single put followed by a flush, to make sure that rocksDB file exists locally (verified that after flush the sst file always exist).

Now the RocksDBStoreTest takes about 2.5 seconds, and removing the integration annotation from it.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bill Bejeck <bill@confluent.io>",['streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java'],The RocksDBStoreTest takes an unnecessarily long time due to putting 2 million records when it could be confirmed with just a single put and a flush.
8d6968e8322d74ebb0fde513113d42bef69fb72b,1647639052,"KAFKA-13682; KRaft Controller auto preferred leader election (#11893)

Implement auto leader rebalance for KRaft by keeping track of the set of topic partitions which have a leader that is not the preferred replica. If this set is non-empty then schedule a leader balance event for the replica control manager.

When applying PartitionRecords and PartitionChangeRecords to the ReplicationControlManager, if the elected leader is not the preferred replica then remember this topic partition in the set of imbalancedPartitions.

Anytime the quorum controller processes a ControllerWriteEvent it schedules a rebalance operation if the there are no pending rebalance operations, the feature is enabled and there are imbalance partitions.

This KRaft implementation only supports the configurations properties auto.leader.rebalance.enable and leader.imbalance.check.interval.seconds. The configuration property leader.imbalance.per.broker.percentage is not supported and ignored.

Reviewers: Jun Rao <junrao@gmail.com>, David Arthur <mumrah@gmail.com>","['core/src/main/scala/kafka/server/ControllerServer.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'metadata/src/main/java/org/apache/kafka/controller/BrokersToIsrs.java', 'metadata/src/main/java/org/apache/kafka/controller/PartitionChangeBuilder.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/TopicIdPartition.java', 'metadata/src/test/java/org/apache/kafka/controller/BrokersToIsrsTest.java', 'metadata/src/test/java/org/apache/kafka/controller/PartitionChangeBuilderTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTestEnv.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java']","KRaft Controller does not automatically balance leadership among replicas, leading to situations where the elected leader is not the preferred replica for a given partition."
5afce2de68517498e23c6c224a686115252d2611,1686464545,"KAFKA-15077: Code to trim token in FileTokenRetriever (#13835)

The FileTokenRetriever class is used to read the access_token from a file on the clients system and then it is passed along with the jaas config to the OAuthBearerSaslServer. In case the token was sent using FileTokenRetriever on the client side, some EOL character is getting appended to the token, causing authentication to fail with the message:


Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>","['clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/FileTokenRetriever.java', 'clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginCallbackHandlerTest.java']",End of line (EOL) characters appended to tokens read by FileTokenRetriever causing authentication failures with OAuthBearerSaslServer.
c1f2b0ffb820c4af030f24ca50693d8169261f95,1566591761,"KAFKA-8753; Expose controller topic deletion metrics (KIP-503) (#7156)

This is the implementation for [KIP-503](https://cwiki.apache.org/confluence/display/KAFKA/KIP-503%3A+Add+metric+for+number+of+topics+marked+for+deletion)

When deleting a large number of topics, the Controller can get quite bogged down. One problem with this is the lack of visibility into the progress of the Controller. We can look into the ZK path for topics marked for deletion, but in a production environment this is inconvenient. This PR adds a JMX metric `kafka.controller:type=KafkaController,name=TopicsToDeleteCount` to make it easier to see how many topics are being deleted.

Reviewers: Stanislav Kozlovski <stanislav@confluent.io>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/test/scala/unit/kafka/metrics/MetricsTest.scala']","The Controller bogs down with no way to visualise the progress when deleting a large number of topics, and inspecting ZK path for topics marked for deletion is impractical in a production environment."
8db3b1a09af0bad274e07161336994610d616b35,1607355274,"KAFKA-10811: Correct the MirrorConnectorsIntegrationTest to correctly mask the exit procedures (#9698)

Normally the `EmbeddedConnectCluster` class masks the `Exit` procedures using within the Connect worker. This normally works great when a single instance of the embedded cluster is used. However, the `MirrorConnectorsIntegrationTest` uses two `EmbeddedConnectCluster` instances, and when the first one is stopped it would reset the (static) exit procedures, and any problems during shutdown of the second embedded Connect cluster would cause the worker to shut down the JVM running the tests.

Instead, the `MirrorConnectorsIntegrationTest` class should mask the `Exit` procedures and instruct the `EmbeddedConnectClusters` instances (via the existing builder method) to not mask the procedures.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Mickael Maison <mickael.maison@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>","['connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectCluster.java']","Shutting down the first instance of `EmbeddedConnectCluster` in `MirrorConnectorsIntegrationTest` resets the exit procedures, causing the JVM running the tests to shut down if there are issues during shutdown of the second instance."
af71375d6d720a264cbe9c9247da374f1a9309df,1654415229,"KAFKA-13933: Fix stuck SSL unit tests in case of authentication failure (#12159)

When there is an authentication error after the initial TCP connection, the selector never becomes READY, and these tests wait forever waiting for this state.

This will happen while using an JDK like OpenJDK build that does not support the required cipher suites.

Reviewers: Luke Chen <showuon@gmail.com>,  Tom Bentley <tbentley@redhat.com>, Divij Vaidya <diviv@amazon.com>","['clients/src/test/java/org/apache/kafka/common/network/NetworkTestUtils.java', 'clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java', 'clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java', 'clients/src/test/java/org/apache/kafka/common/network/Tls12SelectorTest.java', 'clients/src/test/java/org/apache/kafka/common/network/Tls13SelectorTest.java']","SSL unit tests hang indefinitely due to selector not becoming READY in the event of an authentication failure after initial TCP connection, particularly when using a JDK that doesn't support required cipher suites."
dafe51b65885c476d3e06d5cad7cb553392cce42,1691590328,"KAFKA-15100; KRaft data race with the expiration service (#14141)

The KRaft client uses an expiration service to complete FETCH requests that have timed out. This expiration service uses a different thread from the KRaft polling thread. This means that it is unsafe for the expiration service thread to call tryCompleteFetchRequest. tryCompleteFetchRequest reads and updates a lot of states that is assumed to be only be read and updated from the polling thread.

The KRaft client now does not call tryCompleteFetchRequest when the FETCH request has expired. It instead will send the FETCH response that was computed when the FETCH request was first handled.

This change also fixes a bug where the KRaft client was not sending the FETCH response immediately, if the response contained a diverging epoch or snapshot id.

Reviewers: Jason Gustafson <jason@confluent.io>","['raft/src/main/java/org/apache/kafka/raft/ElectionState.java', 'raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/LeaderState.java', 'raft/src/main/java/org/apache/kafka/raft/QuorumState.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/MockLog.java', 'raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java', 'raft/src/test/java/org/apache/kafka/raft/RaftEventSimulationTest.java']","KRaft client's expiration service is calling tryCompleteFetchRequest unsafely, causing race conditions due to simultaneous state updates from different threads. Additionally, instant FETCH responses are not sent when they contain a diverging epoch or snapshot id."
9ea3d0d1c8d975f5d53e3bd74d78e1a9913169c4,1669652234,"KAFKA-12679: Handle lock exceptions in state updater  (#12875)

In this change, we enable backing off when the state directory
is still locked during initialization of a task. When the state
directory is locked, the task is reinserted into the
initialization queue. We will reattempt to acquire the lock
after the next round of polling.

Tested through a new unit test.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","During initialization of a task, if the state directory is locked, there isn't an attempt to acquire the lock again after a round of polling."
0fc029c6a47a7a930a2b078569de1173cdb547a4,1677003429,"KAFKA-14299: Fix pause and resume with state updater (#13025)

* Fixes required to make the PauseResumeIntegrationTest pass. It was not enabled and it does not pass for the state updater code path.

* Make sure no progress is made on paused topologies. The state updater restored one round of polls from the restore
consumer before realizing that a newly added task was already in paused state when being added.

* Wake up state updater when tasks are being resumed. If a task is resumed, it may be necessary to wake up the state updater from waiting on the tasksAndActions condition.

* Make sure that allTasks methods also return the tasks that are currently being restored.

* Enable PauseResumeIntegrationTest and upgrade it to JUnit5.

Reviewers: Bruno Cadonna <cadonna@apache.org>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ReadOnlyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/KafkaStreamsNamedTopologyWrapper.java', 'streams/src/test/java/org/apache/kafka/streams/integration/PauseResumeIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/ReadOnlyTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","The PauseResumeIntegrationTest currently fails for the state updater path. Tasks in paused state still progress due to premature polling from the restore consumer. Also, the state updater might not be woken up appropriately when tasks are resumed.
"
680e68413b4648f8539dc175f7e8dabab59ec5e3,1596820756,"KAFKA-10193: Add preemption for controller events that have callbacks

JIRA: https://issues.apache.org/jira/browse/KAFKA-10193
* add `preempt(): Unit` method for all `ControllerEvent` so that all events (and future events) must implement it
* for events that have callbacks, move the preemption from individual methods to `preempt()`
* add preemption for `ApiPartitionReassignment` and `ListPartitionReassignments`
* add integration tests:
1. test whether `preempt()` is called when controller shuts down
2. test whether the events with callbacks have the correct error response (`NOT_CONTROLLER`)
* explicit typing for `ControllerEvent` methods

Author: jeff kim <jeff.kim@confluent.io>

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>,Stanislav Kozlovski <stanislav@confluent.io>, David Arthur <mumrah@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #9050 from jeffkbkim/KAFKA-10193-controller-events-add-preemption
","['core/src/main/scala/kafka/controller/ControllerEventManager.scala', 'core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/test/scala/unit/kafka/controller/ControllerFailoverTest.scala', 'core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala']",Controller events with callbacks lack preemption causing potential issues during controller shutdown and incorrect error responses.
46a8a2877b18600abb608da5afcb3bef13e882f0,1690336283,"KAFKA-15218: Avoid NPE thrown while deleting topic and fetch from follower concurrently (#14051)

When deleting topics, we'll first clear all the remoteReplicaMap when stopPartitions here. But this time, there might be fetch request coming from follower, and try to check if the replica is eligible to be added into ISR here. At this moment, NPE will be thrown. Although it's fine since this topic is already deleted, it'd be better to avoid it happen.

Reviewers: Luke Chen <showuon@gmail.com>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala']",Deletion of topics concurrently with fetch requests from followers results in NullPointerException being thrown due to attempts to access cleared remoteReplicaMap.
6975f1dfa9864728605a7e430cf929b3280aae2b,1571859571,"KAFKA-8700: Flaky Test QueryableStateIntegrationTest#queryOnRebalance (#7548)

This is not guaranteed to actually fix queryOnRebalance, since the
failure could never be reproduced locally. I did not bump timeouts
because it looks like that has been done in the past for this test
without success. Instead this change makes the following improvements:

It waits for the application to be in a RUNNING state before
proceeding with the test.

It waits for the remaining instance to return to RUNNING state
within a timeout after rebalance. I observed once that we were able to
do the KV queries but the instance was still in REBALANCING, so this
should reduce some opportunity for flakiness.

The meat of this change: we now iterate over all keys in one shot
(vs. one at a time with a timeout) and collect various failures, all of
which are reported at the end. This should help us to narrow down the
cause of flakiness if it shows up again.

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['clients/src/test/java/org/apache/kafka/test/NoRetryException.java', 'clients/src/test/java/org/apache/kafka/test/TestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java']","The test 'queryOnRebalance' in QueryableStateIntegrationTest is flaky. Failure has not been reproduced locally, issues occur post rebalance with KV queries and application states."
ea72edebf2d484e42a4251c53cd6e383743b5d1a,1581579722,"MINOR: Do not override retries for idempotent producers (#8097)

The KafkaProducer code would set infinite retries (MAX_INT) if the producer was configured with idempotence and no retries were configured by the user. This is superfluous because KIP-91 changed the retry functionality to both be time-based and the default retries config to be MAX_INT.

Reviewers: Jason Gustafson <jason@confluent.io>",['clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java'],"The KafkaProducer code unnecessarily sets infinite retries for idempotent producers, even when no user-defined retries are configured."
19a62697803a91fc24c1a704881e531f848a759c,1648527562,"MINOR: Fix log4j entry in RepartitionTopics (#11958)

I noticed two issues in the log4j entry:

1. It's formatted as ""{}...{}"" + param1, param2; effectively it is one param only, and the printed line is effectively mis-aligned: we always print Subtopology [sourceTopics set] was missing source topics {}
2. Even fix 1) is not enough, since topologyName may be null. On the other hand I think the original goal is not to print the topology name but the sub-topology id since it's within the per-sub-topology loop.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['streams/src/main/java/org/apache/kafka/streams/processor/internals/RepartitionTopics.java'],"Log4j entry in RepartitionTopics has formatting issues leading to misalignment and possible null reference for topologyName, resulting in incorrect log messages."
7a22c0c98f33ce0514ef93318bab322e84ba1e41,1674143759,"KAFKA-14637 Fix upgrade compatibility issue from older versions to 3.4 (#13130)

3.4 introduced a change that requires cluster.id to be present in meta.properties if the file is available. This information is not persisted by the brokers in old versions (< 0.10). So on upgrade, the requirement check fails and halts the broker start up. This patch fixes the requirement to ensure that cluster.id is not required in zk mode on upgrade.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>","['core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala', 'core/src/main/scala/kafka/server/KafkaRaftServer.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/test/scala/kafka/server/BrokerMetadataCheckpointTest.scala']","Upgrading from older Kafka versions to 3.4 halts broker start up due to new requirement for 'cluster.id' in 'meta.properties', which isn't persisted by brokers in the old versions."
a6f30945facc7b77cd90ba4c4562932c8cd53f2d,1622180512,"MINOR: make sure all fiedls of o.p.k.s.a.Action are NOT null (#10764)

I'm migrating Ranger's kafka plugin from deprecated Authorizer (this is already removed by 976e78e) to new API (see https://issues.apache.org/jira/browse/RANGER-3231). The kafka plugin needs to take something from field resourcePattern but it does not know whether the field is nullable (or users need to add null check). I check all usages and I don't observe any null case.

Reviewers: Ismael Juma <ismael@juma.me.uk>",['clients/src/main/java/org/apache/kafka/server/authorizer/Action.java'],"Uncertainty in field resourcePattern of the new API in Ranger's Kafka plugin leads to potential null cases, causing functionality issues."
86450bf9aca481113fadbb0ecf0eb4b180762a30,1695812450,"KAFKA-15498: bump snappy-java version to 1.1.10.4 (#14434)

bump snappy-java version to 1.1.10.4, and add more tests to verify the compressed data can be correctly decompressed and read.

For LogCleanerParameterizedIntegrationTest, we increased the message size for snappy decompression since in the new version of snappy, the decompressed size is increasing compared with the previous version. But since the compression algorithm is not kafka's scope, all we need to do is to make sure the compressed data can be successfully decompressed and parsed/read.

Reviewers: Divij Vaidya <diviv@amazon.com>, Ismael Juma <ismael@juma.me.uk>, Josep Prat <josep.prat@aiven.io>, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>","['core/src/test/scala/integration/kafka/api/ProducerCompressionTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerParameterizedIntegrationTest.scala']","The current snappy-java version could cause issues as the decompressed size is increasing compared to previous version, potentially impacting success in decompressing and parsing/reading compressed data."
4b03d67e106c6d3b9dd465a308a62b400ead70a4,1583599598,"MINOR: Break up StreamsPartitionAssignor's gargantuan #assign (#8245)

Just a minor refactoring of StreamsPartitionAssignor's endless assign method into logical chunks to hopefully improve readability. No logical changes, literally just moving code around and adding docs.

The hope is to make it easier to write and review KIP-441 PRs that dig into the assignment logic.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java'],"StreamsPartitionAssignor's assign method is excessively long, hindering readability and ease of modification for work on KIP-441 PRs."
464b6ed0349f28779a48bdbb50e29d1319848416,1573855847,"MINOR. Replace Utils::readFileAsString method to read file as stream (#7208)

The current Utils::readFileAsString method creates a FileChannel and
memory maps file and copies its content to a String and returns it. But
that means that we need to know the size of the file in advance. This
precludes us from reading files whose size is not known in advance, i.e.
any file opened with flag S_IFIFO.

This change updates the method to use stream to read the content of the file.
It has couple of practical advantages:

Allows bash process substitution to pass in strings as file. So we can
say `./bin/kafka-reassign-partitions.sh --reassignment-json-file <(echo
""reassignment json"")

When adding systest for commands that take file, we don't have to
create real physical file. Instead we can just dump the content of the
file on the command line.

Reviewers: Ismael Juma <ismael@juma.me.uk>","['clients/src/main/java/org/apache/kafka/common/utils/Utils.java', 'clients/src/test/java/org/apache/kafka/common/utils/UtilsTest.java']","The current Utils::readFileAsString method requires knowing the file size in advance, limiting the ability to read files whose size is not known in advance, including those opened with S_IFIFO flag."
e8754ba7a01f33ba503ef79de66a5f644cc2ced8,1617166936,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>","['clients/src/main/java/org/apache/kafka/common/acl/AclBinding.java', 'clients/src/main/java/org/apache/kafka/common/acl/AclBindingFilter.java', 'clients/src/main/java/org/apache/kafka/common/config/SaslConfigs.java', 'clients/src/main/java/org/apache/kafka/common/config/SslConfigs.java', 'clients/src/main/java/org/apache/kafka/common/config/internals/BrokerSecurityConfigs.java', 'clients/src/main/java/org/apache/kafka/common/network/ChannelBuilders.java', 'clients/src/main/java/org/apache/kafka/common/network/PlaintextChannelBuilder.java', 'clients/src/main/java/org/apache/kafka/common/network/SslChannelBuilder.java', 'clients/src/main/java/org/apache/kafka/common/resource/Resource.java', 'clients/src/main/java/org/apache/kafka/common/resource/ResourceFilter.java', 'clients/src/main/java/org/apache/kafka/common/security/auth/DefaultPrincipalBuilder.java', 'clients/src/main/java/org/apache/kafka/common/security/auth/PrincipalBuilder.java', 'clients/src/main/java/org/apache/kafka/common/security/authenticator/DefaultKafkaPrincipalBuilder.java', 'clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticator.java', 'clients/src/test/java/org/apache/kafka/common/network/ChannelBuildersTest.java', 'clients/src/test/java/org/apache/kafka/common/resource/ResourceFilterTest.java', 'clients/src/test/java/org/apache/kafka/common/security/auth/DefaultKafkaPrincipalBuilderTest.java']","Deprecated security classes/methods in SslConfigs, SaslConfigs, AclBinding, and PrincipalBuilder persist in the system, leading to unnecessary complexity and potential security threats."
92d56ed93325c3849a36fc6dceb9ccab1c42edce,1591844798,"KAFKA-9845: Warn users about using config providers with plugin.path property (#8455)

* KAFKA-9845: Fix plugin.path when config provider is used

* Revert ""KAFKA-9845: Fix plugin.path when config provider is used""

This reverts commit 96caaa9a4934bcef78d7b145d18aa1718cb10009.

* KAFKA-9845: Emit ERROR-level log message when config provider is used for plugin.path property

* KAFKA-9845: Demote log message level from ERROR to WARN

Co-Authored-By: Nigel Liang <nigel@nigelliang.com>

* KAFKA-94845: Fix failing unit tests

* KAFKA-9845: Add warning message to docstring for plugin.path config

* KAFKA-9845: Apply suggestions from code review

Co-authored-by: Randall Hauch <rhauch@gmail.com>

Co-authored-by: Nigel Liang <nigel@nigelliang.com>
Co-authored-by: Randall Hauch <rhauch@gmail.com>",['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java'],Using config providers with the plugin.path property leads to misconfigurations and could cause potential issues.
e00c0f3719ad0803620752159ef8315d668735d6,1625655300,"KAFKA-12234: Implement request/response for offsetFetch batching (KIP-709) (#10962)

This implements the request and response portion of KIP-709. It updates the OffsetFetch request and response to support fetching offsets for multiple consumer groups at a time. If the broker does not support the new OffsetFetch version, clients can revert to the previous behaviour and use a request for each coordinator.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Konstantine Karantasis <konstantine@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/admin/internals/ListConsumerGroupOffsetsHandler.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java', 'clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java', 'clients/src/test/java/org/apache/kafka/clients/admin/internals/ListConsumerGroupOffsetsHandlerTest.java', 'clients/src/test/java/org/apache/kafka/common/message/MessageTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/OffsetFetchRequestTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/OffsetFetchResponseTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/OffsetFetchRequestTest.scala']","The OffsetFetch request and response lacks support for fetching offsets for multiple consumer groups simultaneously, necessitating individual requests for each coordinator."
f3a9ce4a69d17db7b8ba21134eb8118070176e48,1593537503,"MINOR: Do not swallow exception when collecting PIDs (#8914)

During Streams' system tests the PIDs of the Streams
clients are collected. The method the collects the PIDs
swallows any exception that might be thrown by the
ssh_capture() function. Swallowing any exceptions
might make the investigation of failures harder,
because no information about what happened are recorded.

Reviewers: John Roesler <vvcephei@apache.org>",['tests/kafkatest/services/streams.py'],"Exception swallowed during PID collection in Streams' system tests can obscure potential issues, hindering failure investigations due to lack of recorded information."
517b5d2b09ec22cf1734ba1e2d8be9ece5fb0365,1677795270,"KAFKA-14491: [12/N] Relax requirement that KTable stores must be TimestampedKVStores (#13264)

As part of introducing versioned key-value stores in KIP-889, we want to lift the existing DSL restriction that KTable stores are always TimestampedKeyValueStores to allow for KTable stores which are VersionedKeyValueStores instead. This PR lifts this restriction by replacing raw usages of TimestampedKeyValueStore with a new KeyValueStoreWrapper which supports either TimestampedKeyValueStore or VersionedKeyValueStore.

Reviewers: Matthias J. Sax <matthias@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamAggregate.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamReduce.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableAggregate.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableFilter.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableJoinMerger.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMapValues.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMaterializedValueGetterSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTablePassThrough.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableReduce.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSourceValueGetterSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableTransformValues.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/KTableKTableJoinNode.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamToTableNode.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableSourceNode.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueStoreWrapper.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueStoreWrapperTest.java']","Current DSL restriction only allows KTable stores to be TimestampedKeyValueStores, impeding the introduction of versioned key-value stores in KIP-889."
ff603c63bb3e0b47ce14121bb0e96ce0fc42d3c1,1550718590,"KAFKA-4730: Streams does not have an in-memory windowed store (#6239)

Implemented an in-memory window store allowing for range queries. A finite retention period defines how long records will be kept, ie the window of time for fetching, and the grace period defines the window within which late-arriving data may still be written to the store.

Unit tests were written to test the functionality of the window store, including its insert/update/delete and fetch operations. Single-record, all records, and range fetch were tested, for both time ranges and key ranges. The logging and metrics for late-arriving (dropped)records were tested as well as the ability to restore from a changelog.

Reviewers: John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/state/Stores.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowBytesStoreSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryWindowStoreTest.java']","The Kafka Streams API does not have an in-memory windowed store, prohibiting range queries and causing difficulties in maintaining record timelines and handling late-arriving data."
3709901c9ee0f61277b122a4863fea46f039335f,1690565404,"KAFKA-14702: Extend server side assignor to support rack aware replica placement (#14099)

This patch updates the `PartitionAssignor` interface to support rack-awareness. The change introduces the `SubscribedTopicDescriber` interface that can be used to retrieve topic metadata such as the number of partitions or the racks from within an assignor. We use an interface because it allows us to wrap internal data structures instead of having to copy them. It is more efficient.

Reviewers: David Jacot <djacot@confluent.io>
","['group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/RecordHelpers.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentSpec.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/AssignmentTopicMetadata.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/PartitionAssignor.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/RangeAssignor.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/assignor/SubscribedTopicDescriber.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/SubscribedTopicMetadata.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/TargetAssignmentBuilder.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/TopicMetadata.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/RecordHelpersTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/assignor/RangeAssignorTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroupTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/consumer/SubscribedTopicMetadataTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/consumer/TargetAssignmentBuilderTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/consumer/TopicMetadataTest.java']","The `PartitionAssignor` interface lacks rack-awareness, leading to inefficiencies and preventing topic metadata retrieval within an assignor."
5e399fe6f3aa65b42b9cdbf1c4c53f6989a570f0,1666894124,"Move to mockito (#12465)

This PR build on top of #11017. I have added the previous author's comment in this PR for attribution. I have also addressed the pending comments from @chia7712 in this PR.

Notes to help the reviewer:

Mockito has mockStatic method which is equivalent to PowerMock's method.
When we run the tests using @RunWith(MockitoJUnitRunner.StrictStubs.class) Mockito performs a verify() for all stubs that are mentioned, hence, there is no need to explicitly verify the stubs (unless you want to verify the number of times etc.). Note that this does not work for static mocks.

Reviewers: Bruno Cadonna <cadonna@apache.org>, Walker Carlson <wcarlson@confluent.io>, Bill Bejeck <bbejeck@apache.org>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java']","The repository is utilizing PowerMock for their unit tests, which could be replaced with Mockito's 'mockStatic' method, however, it's noted that explicit verification for static mocks isn't supported."
68b7031dc443b6f6b5dfac81316ab22fe250ec54,1693231376,"KAFKA-14499: [4/N] Implement OffsetFetch API (#14120)

This patch implements the OffsetFetch API in the new group coordinator.

I found out that implementing the `RequireStable` flag is hard (to not say impossible) in the current model. For the context, the flag is here to ensure that an OffsetRequest request does not return stale offsets if there are pending offsets to be committed. In the scala code, we basically check the pending offsets data structure and if they are any pending offsets, we return the `UNSTABLE_OFFSET_COMMIT` error. This tells the consumer to retry.

In our new model, we don't have the pending offsets data structure. Instead, we use a timeline data structure to handle all the pending/uncommitted changes. Because of this we don't know whether offsets are pending for a particular group. Instead of doing this, I propose to not return the `UNSTABLE_OFFSET_COMMIT` error anymore. Instead, when `RequireStable` is set, we use a write operation to ensure that we read the latest offsets. If they are uncommitted offsets, the write operation ensures that the response is only return when they are committed. This gives a similar behaviour in the end.

Reviewers: Justine Olshan <jolshan@confluent.io>","['group-coordinator/src/main/java/org/apache/kafka/coordinator/group/Group.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorShard.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/OffsetMetadataManager.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java', 'group-coordinator/src/main/java/org/apache/kafka/coordinator/group/generic/GenericGroup.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java', 'group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java']","In the new group coordinator model, the implementation of `RequireStable` flag in OffsetFetch API is challenging due to the absence of a pending offsets data structure, resulting in inability to avoid returning stale offsets when there are pending offsets to be committed."
d92d464b124f7e94cc240f062c6f96813ecb76d7,1617123109,"KAFKA-12552: Introduce LogSegments class abstracting the segments map (#10401)

This PR is a precursor to the recovery logic refactor work (KAFKA-12553).

In this PR, I've extracted the behavior surrounding segments map access within kafka.log.Log class into a new class: kafka.log.LogSegments. This class encapsulates a thread-safe navigable map of kafka.log.LogSegment instances and provides the required read and write behavior on the map. The Log class now encapsulates an instance of the LogSegments class.

Couple advantages of this PR:

Makes the Log class a bit more modular as it moves out certain private behavior thats otherwise within the Log class.
This is a precursor to refactoring the recovery logic (KAFKA-12553). In the future, the logic for recovery and loading of segments from disk (during Log) init will reside outside the Log class. Such logic would need to instantiate and access an instance of the newly added LogSegments class.
Tests:
Added a new test suite: kafka.log.LogSegmentsTest covering the APIs of the newly introduced class.

Reviewers: Satish Duggana <satishd@apache.org>, Dhruvil Shah <dhruvil@confluent.io>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogSegments.scala', 'core/src/test/scala/unit/kafka/log/LogSegmentsTest.scala']","The Log class in Kafka handles many responsibilities including map access behavior for LogSegments, hampering modularity and potentially complicating future recovery logic refactor work."
e034fedbd369461c7a673fa877b67638c7b1d952,1597637570,"KAFKA-10387: Fix inclusion of transformation configs when topic creation is enabled in Connect (#9172)

Addition of configs for custom topic creation with KIP-158 created a regression when transformation configs are also included in the configuration of a source connector. 

To experience the issue, just enabling topic creation at the worker is not sufficient. A user needs to supply a source connector configuration that contains both transformations and custom topic creation properties. 

The issue is that the enrichment of configs in `SourceConnectorConfig` happens on top of an `AbstractConfig` rather than a `ConnectorConfig`. Inheriting from the latter allows enrichment to be composable for both topic creation and transformations. 

Unit tests and integration tests are written to test these combinations. 

Reviewers: Randall Hauch <rhauch@gmail.com>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/integration/TransformationIntegrationTest.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicCreationTest.java']","Inclusion of transformation configs and custom topic creation properties together in a source connector configuration results in regression due to incorrect inheritance from `AbstractConfig`, instead of `ConnectorConfig`."
80784271043d2da8e93055a5f9b1bcfd53347461,1558015673,"KAFKA-8347: Choose next record to process by timestamp (#6719)

When choosing the next record to process, we should look at the head record's timestamp of each partition and choose the lowest rather than choosing the lowest of the partition's streamtime.

This change effectively makes RecordQueue return the timestamp of the head record rather than its streamtime. Streamtime is removed (replaced) from RecordQueue as it was only being tracked in order to choose the next partition to poll from.

Reviewers: Matthias J. Sax <mjsax@apache.org>,  Bill Bejeck <bbejeck@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java']",The current method of selecting next record based on the lowest of the partition's streamtime may not always result in the most timely record being processed.
89e12f3c6b4f37c405455d5cb0fca60f2be3ff92,1597246199,"KAFKA-10388; Fix struct conversion logic for tagged structures (#9166)

The message generator was missing conversion logic for tagged structures. This led to casting errors when either `fromStruct` or `toStruct` were invoked. This patch also adds missing null checks in the serialization of tagged byte arrays, which was found from improved test coverage.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['clients/src/test/java/org/apache/kafka/common/message/SimpleExampleMessageTest.java', 'generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java']",Missing conversion logic for tagged structures in message generator causing casting errors when invoking 'fromStruct' or 'toStruct'. Serialization of tagged byte arrays lacks necessary null checks.
3fb836f507d35e2a1ab39df57edca671ffc66073,1622130960,"MINOR: Improve Log layer segment iteration logic and few other areas (#10684)

In Log.collectAbortedTransactions() I've restored a previously used logic, such that it would handle the case where the starting segment could be null. This was the case previously, but the PR #10401 accidentally changed the behavior causing the code to assume that the starting segment won't be null.

In Log.rebuildProducerState() I've removed usage of the allSegments local variable. The logic looks a bit simpler after I removed it.

I've introduced a new LogSegments.higherSegments() API. This is now used to make the logic a bit more readable in Log. collectAbortedTransactions() and Log.deletableSegments() APIs.

I've removed the unnecessary use of java.lang.Long in LogSegments class' segments map definition.

I've converted a few LogSegments API from public to private, as they need not be public.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Cong Ding <cong@ccding.com>, Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/log/LogSegments.scala', 'core/src/test/scala/unit/kafka/log/LogSegmentsTest.scala']","Starting segment in Log.collectAbortedTransactions() can potentially be null causing issues, and unnecessary java.lang.Long usage in LogSegments class' segments map, along with public visibility of certain APIs that need not be public."
146a6976aed0d9f90c70b6f21dca8b887cc34e71,1685735475,"KAFKA-15048: Improve handling of unexpected quorum controller errors (#13799)

When the active quorum controller encounters an ""unexpected"" error, such as a NullPointerException,
it currently resigns its leadership. This PR fixes it so that in addition to doing that, it also
increments the metadata error count metric. This will allow us to better track down these errors.

This PR also fixes a minor bug where performing read operations on a standby controller would
result in an unexpected RuntimeException. The bug happened because the standby controller does not
take in-memory snapshots, and read operations were attempting to read from the epoch of the latest
committed offset. The fix is for the standby controller to simply read the latest value of each
data structure. This is always safe, because standby controllers don't contain uncommitted data.

Also, fix a bug where listPartitionReassignments was reading the latest data, rather than data from
the last committed offset.

Reviewers: dengziming <dengziming1993@gmail.com>, David Arthur <mumrah@gmail.com>","['core/src/main/scala/kafka/server/ControllerServer.scala', 'core/src/main/scala/kafka/server/SharedServer.scala', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/main/java/org/apache/kafka/controller/errors/ControllerExceptions.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTestEnv.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/errors/ControllerExceptionsTest.java']","Active quorum controller resignation upon encountering unexpected errors, like NullPointerException, and unexpected RuntimeExceptions when performing read operations on a standby controller. There's also an issue with listPartitionReassignments reading the latest data rather than data from the last committed offset."
1e459271d777e4721c3f7a36c5b4fbb2a5793f63,1639175187,"KAFKA-12648: fix IllegalStateException in ClientState after removing topologies (#11591)

Fix for one of the causes of failure in the NamedTopologyIntegrationTest: org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: Must initialize prevActiveTasks from ownedPartitions before initializing remaining tasks.

This exception could occur if a member sent in a subscription where all of its ownedPartitions were from a named topology that is no longer recognized by the group leader, eg because it was just removed from the client. We should filter each ClientState based on the current topology only so the assignor only processes the partitions/tasks it can identify. The member with the out-of-date tasks will eventually clean them up when the #removeNamedTopology API is invoked on them

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/KafkaStreamsNamedTopologyWrapper.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTest.java']","The NamedTopologyIntegrationTest fails intermittently with IllegalStateException in ClientState when a member tries to subscribe to a topology that is not recognized by the group leader, typically because it was recently removed."
33853c154ff24ab4e68efa23893646f093c6333f,1639770662,"MINOR: Correct usage of ConfigException in file and directory config providers

The two-arg variant is intended to take a property name and value, not an exception message and a cause.

As-is, this leads to confusing log messages like:

```
org.apache.kafka.common.config.ConfigException: Invalid value java.nio.file.NoSuchFileException: /my/missing/secrets.properties for configuration Could not read properties from file /my/missing/secrets.properties
```

Author: Chris Egerton <chrise@confluent.io>

Reviewers: Gwen Shapira

Closes #11555 from C0urante/patch-1
","['clients/src/main/java/org/apache/kafka/common/config/provider/DirectoryConfigProvider.java', 'clients/src/main/java/org/apache/kafka/common/config/provider/FileConfigProvider.java']","The ConfigException in file and directory config providers is incorrectly used, resulting in misleading log messages. This is occurring due to the incorrect use of the two-arg variant.
"
f8173c2df573ae9b53ab7ea6e5d6700acf8e0cf4,1584906165,"MINOR: Update Connect error message to point to the correct config validation REST endpoint (#7991)

When incorrect connector configuration is detected, the returned exception message suggests to check the connector's configuration against the `{connectorType}/config/validate` endpoint. 

Changing the error message to refer to the exact REST endpoint which is `/connector-plugins/{connectorType}/config/validate` 

This aligns the exception message with the documentation at: https://kafka.apache.org/documentation/#connect_rest 

Reviewers: Konstantine Karantasis <konstantine@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java']","The error message for incorrect connector configuration points to a non-existent config validation REST endpoint, causing confusion when troubleshooting."
d9b898b678158626bd2872bbfef883ca60a41c43,1625007560,"MINOR: Refactor the MetadataCache interface (#10887)

Remove getNonExistingTopics, which was not necessary. MetadataCache
already lets callers check for the existence of topics by calling
MetadataCache#contains.

Add MetadataCache#getAliveBrokerNode and getAliveBrokerNodes. This
simplifies the calling code, which always wants a Node.

Fix a case where we were calling getAliveBrokers and filtering by id,
rather than simply calling getAliveBroker(id) and making use of the hash
map.

Reviewers: Jason Gustafson <jason@confluent.io>, Jose Sancio <jsancio@gmail.com>","['core/src/main/scala/kafka/admin/AdminUtils.scala', 'core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala', 'core/src/main/scala/kafka/server/KafkaApis.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/main/scala/kafka/server/MetadataCache.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/main/scala/kafka/server/ZkAdminManager.scala', 'core/src/main/scala/kafka/server/metadata/RaftMetadataCache.scala', 'core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/AutoTopicCreationManagerTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaApisTest.scala', 'core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/ServerStartupTest.scala', 'core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","Unnecessary getNonExistingTopics in MetadataCache and lack of efficient methods for broker node retrieval. Additionally, incorrect call to getAliveBrokers where getAliveBroker(id) would suffice."
34a7ba56a161c066aeacaf88c1ead6109a111d8a,1584559180,"KAFKA-9047; AdminClient group operations should respect retries and backoff (#8161)

Previously, `AdminClient` group operations did not respect a `Call`'s number of configured tries and retry backoff. This could lead to tight retry loops that put a lot of pressure on the broker. This PR introduces fixes that ensures for all group operations the `AdminClient` respects the number of tries and the backoff a given `Call` has.

Reviewers: Vikas Singh <vikas@confluent.io>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java']","`AdminClient` group operations are not respecting the configured number of tries and retry backoff in a `Call`, leading to potential pressure on the broker due to tight retry loops."
ec05f90a3d18637bb9a50da09d75ffdb0485c918,1643795774,"KAFKA-13599: Upgrade RocksDB to 6.27.3 (#11690)

RocksDB v6.27.3 has been released and it is the first release to support s390x. RocksDB is currently the only dependency in gradle/dependencies.gradle without s390x support.

RocksDB v6.27.3 has added some new options that require an update to streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java but no other changes are needed to upgrade.

I have run the unit/integration tests locally on s390x and also the :streams tests on x86_64 and they pass.

Reviewers: Luke Chen <showuon@gmail.com>, Bruno Cadonna <cadonna@apache.org>",['streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java'],Current version of RocksDB does not support s390x. This is limiting compatibility as it is the only dependency without s390x support in gradle/dependencies.gradle.
71f1cb7bfb2dc801c9ea47e0794d915e7f279635,1632874925,"MINOR: optimize performAssignment to skip unnecessary check (#11218)

Found this while reading the code. We did a ""a little heavy"" check each time after performing assignment, which is to compare the ""assigned topics"" set and the ""subscribed topics"" set, to see if there's any topics not existed in another set. Also, the ""assigned topics"" set is created by traversing all the assigned partitions, which will be a little heavy if partition numbers are large.

However, as the comments described, it's a safe-guard for user-customized assignor, which might do assignment that we don't expected. In most cases, user will just use the in-product assignor, which guarantee that we only assign the topics from subscribed topics. Therefore, no need this check for in-product assignors.

In this PR, I added an ""in-product assignor names"" list, and we'll in consumerCoordinator check if the assignor is one of in-product assignors, to decide if we need to do the additional check. Also add test for it.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <guozhang@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java']","Excessive checks are being performed after each assignment operation, comparing ""assigned topics"" with ""subscribed topics"". This check may impact performance when the number of partitions is significantly large."
8fd7cd6a439a3914b49f7c6919e84e6937ee5a18,1577732550,"MINOR: upgrade system test should check for ISR rejoin on each roll (#7827)

The upgrade system test correctly rolls by upgrading the broker and 
leaving the IBP, and then rolling again with the latest IBP version.
Unfortunately, this is not sufficient to pick up many problems in our IBP
gating as we charge through the rolls and after the second roll all of
the brokers will rejoin the ISR and the test will be treated as a
success.

This test adds two new checks:
1. We wait for the ISR to stabilize for all partitions. This is best
practice during rolls, and is enough to tell us if a broker hasn't
rejoined after each roll.
2. We check the broker logs for some common protocol errors. This is a
fail safe as it's possible for the test to be successful even if some
protocols are incompatible and the ISR is rejoined.

Reviewers: Nikhil Bhatia <nikhil@confluent.io>, Jason Gustafson <jason@confluent.io>","['tests/kafkatest/services/kafka/kafka.py', 'tests/kafkatest/tests/core/upgrade_test.py']","The upgrade system test may incorrectly identify a success, despite protocol errors or not fully stable ISR, due to lack of adequate checks after each broker roll/update."
b68463c250ec99d86ffacdbd45e58059d0ca51e1,1647450144,"KAFKA-6718 / Add rack awareness configurations to StreamsConfig (#11837)

This PR is part of KIP-708 and adds rack aware standby task assignment logic.

Rack aware standby task assignment won't be functional until all parts of this KIP gets merged.

Splitting PRs into three smaller PRs to make the review process easier to follow. Overall plan is the following:

⏭️ Rack aware standby task assignment logic #10851
⏭️ Protocol change, add clientTags to SubscriptionInfoData #10802
👉 Add required configurations to StreamsConfig (public API change, at this point we should have full functionality)

This PR implements last point of the above mentioned plan.

Reviewers: Luke Chen <showuon@gmail.com>, Bruno Cadonna <cadonna@apache.org>","['clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java', 'clients/src/test/java/org/apache/kafka/common/config/ConfigDefTest.java', 'streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientTagAwareStandbyTaskAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/DefaultStandbyTaskAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ReferenceContainer.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StandbyTaskAssignmentUtils.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StandbyTaskAssignorFactory.java', 'streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RackAwarenessIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientTagAwareStandbyTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StandbyTaskAssignmentUtilsTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StandbyTaskAssignorFactoryTest.java']","The Kafka StreamsConfig lacks the necessary configurations for rack-aware standby task assignment, impeding full functionality of rack aware configuration features."
a855f6ac37149d6908499c68df46671a2754d21a,1619713521,"KAFKA-12265; Move the BatchAccumulator in KafkaRaftClient to LeaderState (#10480)

The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java', 'clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java', 'raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/LeaderState.java', 'raft/src/main/java/org/apache/kafka/raft/QuorumState.java', 'raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java', 'raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java', 'raft/src/test/java/org/apache/kafka/raft/MockLogTest.java', 'raft/src/test/java/org/apache/kafka/raft/QuorumStateTest.java', 'raft/src/test/java/org/apache/kafka/raft/internals/BatchAccumulatorTest.java', 'raft/src/test/java/org/apache/kafka/raft/internals/KafkaRaftMetricsTest.java']","BatchAccumulator in KafkaRaftClient is only in use when it is the leader, implying an inefficient design. The recent change in EpochState and QuorumState underscores a need for redesign."
2a409200dc9d1fa3198908e231161988affa1754,1606255156,"KAFKA-10754: fix flaky tests by waiting kafka streams be in running state before assert (#9629)

The flaky test is because we didn't wait for the streams become RUNNING before verifying the state becoming ERROR state. This fix explicitly wait for the streams become RUNNING state. Also, put the 2nd stream into try resource block so it will be closed after the test.

Reviewers: Walker Carlson <wcarlson@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>","['streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java', 'streams/src/test/java/org/apache/kafka/streams/integration/StreamsUncaughtExceptionHandlerIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java']",Test instability due to assertions made on Kafka streams before they reach the RUNNING state results in flaky tests.
0e95c9f3a829110f0cd8c3695f40ba47f146fef7,1559770603,"KAFKA-8400; Do not update follower replica state if the log read failed (#6814)

This patch checks for errors handling a fetch request before updating follower state. Previously we were unsafely passing the failed `LogReadResult` with most fields set to -1 into `Replica` to update follower state. Additionally, this patch attempts to improve the test coverage for ISR shrinking and expansion logic in `Partition`.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/cluster/Replica.scala', 'core/src/main/scala/kafka/log/Log.scala', 'core/src/main/scala/kafka/server/LogOffsetMetadata.scala', 'core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala', 'core/src/main/scala/kafka/server/ReplicaFetcherThread.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala', 'core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/cluster/ReplicaTest.scala', 'core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala', 'core/src/test/scala/unit/kafka/server/ISRExpirationTest.scala', 'core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala']","Errors handling a fetch request passed to `Replica` to update follower state might contain a failed `LogReadResult`, leading to inaccurate follower state updates."
99472c54f01e1f654a3ccb774a7857f10d69e2e3,1595777327,"KAFKA-10158: Fix flaky testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress (#9022)

Set `replica.fetch.max.bytes` to `1` and produce multiple record batches to allow
for throttling to take place. This helps avoid a race condition where the
reassignment would complete more quickly than expected causing an
assertion to fail some times.

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Jason Gustafson <jason@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>","['core/src/test/scala/unit/kafka/admin/TopicCommandWithAdminClientTest.scala', 'jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/CheckpointBench.java']","There's a race condition in testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress causing reassignment to sometimes complete faster than expected, leading to intermittent assertion failures."
a72f0c1eac67d531a4725d3e48dcab4dce361b95,1602604742,"KAFKA-10533; KafkaRaftClient should flush log after appends (#9352)

This patch adds missing flush logic to `KafkaRaftClient`. The initial flushing behavior is simplistic. We guarantee that the leader will not replicate above the last flushed offset and we guarantee that the follower will not fetch data above its own flush point. More sophisticated flush behavior is proposed in KAFKA-10526.

We have also extended the simulation test so that it covers flush behavior. When a node is shutdown, all unflushed data is lost. We were able to confirm that the monotonic high watermark invariant fails without the added `flush` calls.

This patch also piggybacks a fix to the `TestRaftServer` implementation. The initial check-in contained a bug which caused `RequestChannel` to fail sending responses because the disabled APIs did not have metrics registered. As a result of this, it is impossible to elect leaders.

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['core/src/main/scala/kafka/network/RequestChannel.scala', 'core/src/main/scala/kafka/network/SocketServer.scala', 'core/src/main/scala/kafka/raft/KafkaMetadataLog.scala', 'core/src/main/scala/kafka/tools/TestRaftServer.scala', 'raft/src/main/java/org/apache/kafka/raft/FollowerState.java', 'raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/LeaderState.java', 'raft/src/main/java/org/apache/kafka/raft/ReplicatedLog.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/MockLog.java', 'raft/src/test/java/org/apache/kafka/raft/MockLogTest.java', 'raft/src/test/java/org/apache/kafka/raft/RaftEventSimulationTest.java']","`KafkaRaftClient` fails to retain unflushed data upon node shutdown, consequently violating the monotonic high watermark invariant. Additionally, `TestRaftServer` implementation has a bug that prevents the election of leaders due to metrics registration failure in disabled APIs."
ca6ac9393bae9161465275cb8f836bc987a5098d,1552899110,"MINOR: Retain public constructors of classes from public API (#6455)

TopicDescription and ConsumerGroupDescription in org.apache.kafka.clients.admin. are part of the public API, so we should retain the existing public constructor. Changed the new constructor with authorized operations to be package-private to avoid maintaining more public constructors since we only expect admin client to use this.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>","['clients/src/main/java/org/apache/kafka/clients/admin/ConsumerGroupDescription.java', 'clients/src/main/java/org/apache/kafka/clients/admin/TopicDescription.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java', 'tools/src/test/java/org/apache/kafka/trogdor/common/WorkerUtilsTest.java']","The new constructors with authorized operations in TopicDescription and ConsumerGroupDescription classes are public leading to potential maintenance issues, despite it supposed to be used solely by admin client."
6c806430093c152b916d8e8f1fa542980c03a181,1631809035,"[KAFKA-8522] Streamline tombstone and transaction marker removal (#10914)

This PR aims to remove tombstones that persist indefinitely due to low throughput. Previously, deleteHorizon was calculated from the segment's last modified time.

In this PR, the deleteHorizon will now be tracked in the baseTimestamp of RecordBatches. After the first cleaning pass that finds a record batch with tombstones, the record batch is recopied with deleteHorizon flag and a new baseTimestamp that is the deleteHorizonMs. The records in the batch are rebuilt with relative timestamps based on the deleteHorizonMs that is recorded. Later cleaning passes will be able to remove tombstones more accurately on their deleteHorizon due to the individual time tracking on record batches.

KIP 534: https://cwiki.apache.org/confluence/display/KAFKA/KIP-534%3A+Retain+tombstones+and+transaction+markers+for+approximately+delete.retention.ms+milliseconds

Co-authored-by: Ted Yu <yuzhihong@gmail.com>
Co-authored-by: Richard Yu <yohan.richard.yu@gmail.com>","['clients/src/main/java/org/apache/kafka/common/record/AbstractLegacyRecordBatch.java', 'clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java', 'clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java', 'clients/src/main/java/org/apache/kafka/common/record/MemoryRecordsBuilder.java', 'clients/src/main/java/org/apache/kafka/common/record/RecordBatch.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/common/record/DefaultRecordBatchTest.java', 'clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java', 'clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java', 'core/src/main/scala/kafka/log/LogCleaner.scala', 'core/src/main/scala/kafka/log/LogCleanerManager.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerTest.scala', 'core/src/test/scala/unit/kafka/log/LogValidatorTest.scala', 'core/src/test/scala/unit/kafka/log/UnifiedLogTest.scala', 'raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java']","Tombstones persist indefinitely due to low throughput as deleteHorizon was previously calculated from segment's last modified time, causing inefficiencies in the removal process."
ef65b6e566ef69b2f9b58038c98a5993563d7a68,1662073157,"KAFKA-14195: Fix KRaft AlterConfig policy usage for Legacy/Full case (#12578)

#12374 adjusted the invocation of the alter configs policy check in KRaft to match the behavior in ZooKeeper, which is to only provide the configs that were explicitly sent in the request. While the code was correct for the incremental alter configs case, the code actually included the implicit deletions for the legacy/non-incremental alter configs case, and those implicit deletions are not included in the ZooKeeper-based invocation. This patch adds a test to check for this and adjusts ConfigurationControlManager code so that the test passes -- the adjusted test is confirmed to fail locally otherwise. We also add a log statement to emit any unexpected stack traces in the alter config code path.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Colin P. McCabe <cmccabe@apache.org>","['metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/ConfigurationControlManagerTest.java']","In KRaft, during legacy/non-incremental alter configs, implicit deletions are incorrectly included, which is inconsistent with the behavior in ZooKeeper."
ca29727d2afd85156ab8e48771785193270f28a1,1612464484,"MINOR: Introduce KafkaBroker trait for use in dynamic reconfiguration (#10019)

Dynamic broker reconfiguration needs to occur for both ZooKeeper-based
brokers and brokers that use a Raft-based metadata quorum.  DynamicBrokerConfig
currently operates on KafkaServer, but it needs to operate on BrokerServer
(the broker implementation that will use the Raft metadata log) as well.
This PR introduces a KafkaBroker trait to allow dynamic reconfiguration to
work with either implementation.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Colin Patrick McCabe <cmccabe@confluent.io>, Ismael Juma <ismael@juma.me.uk>","['core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/main/scala/kafka/server/KafkaBroker.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/test/scala/unit/kafka/metrics/MetricsTest.scala']",Dynamic broker reconfiguration is unable to operate with both ZooKeeper-based brokers and brokers using a Raft-based metadata quorum due to its current working only on KafkaServer.
6fa44a3a14aadaa140fb99962823a0fe2cb55f52,1591729772,"KAFKA-9716; Clarify meaning of compression rate metrics (#8664)

There is some confusion over the compression rate metrics, as the meaning of the value isn't clearly stated in the metric description. In this case, it was assumed that a higher compression rate value meant better compression. This PR clarifies the meaning of the value, to prevent misunderstandings.

Reviewers: Jason Gustafson <jason@confluent.io>",['clients/src/main/java/org/apache/kafka/clients/producer/internals/SenderMetricsRegistry.java'],"The meaning of compression rate metrics is not clearly defined, leading to confusion and incorrect assumptions about its interpretation."
bca29e29000d9f223c4289002c8565cf9779e807,1616055343,"KAFKA-12454: Add ERROR logging on kafka-log-dirs when given brokerIds do not  exist in current kafka cluster (#10304)

When non-existent brokerIds value are given, the kafka-log-dirs tool will have a timeout error:

Exception in thread ""main"" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs
at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
at kafka.admin.LogDirsCommand$.describe(LogDirsCommand.scala:50)
at kafka.admin.LogDirsCommand$.main(LogDirsCommand.scala:36)
at kafka.admin.LogDirsCommand.main(LogDirsCommand.scala)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs

When the brokerId entered by the user does not exist, an error message indicating that the node is not present should be printed.

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>","['core/src/main/scala/kafka/admin/LogDirsCommand.scala', 'core/src/test/scala/unit/kafka/admin/LogDirsCommandTest.scala']","The kafka-log-dirs tool is experiencing a timeout error when provided with non-existing brokerIds, instead of indicating absence of the node."
921885d31ff9604a498a9d0d872dc141715a28f1,1662686039,"MINOR; Remove redundant version system test (#12612)

This patch removes test_kafka_version.py, which contains two tests at the moment. The first test verifies we can start a 0.8.2 cluster. The second verifies we can start a cluster with one node on 0.8.2 and another on the latest. These test are covered in greater depth by upgrade_test.py and downgrade_test.py.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>",['tests/kafkatest/sanity_checks/test_kafka_version.py'],"The tests in `test_kafka_version.py` are redundant as they are already covered in `upgrade_test.py` and `downgrade_test.py`, leading to unnecessary duplication of tests."
3453e9e2eee1400901ef8e1965d657b825d5d64a,1577846671,"HOTFIX: fix system test race condition (#7836)

In some system tests a Streams app is started and then prints a message to stdout, which the system test waits for to confirm the node has successfully been brought up. It then greps for certain log messages in a retriable loop.

But waiting on the Streams app to start/print to stdout does not mean the log file has been created yet, so the grep may return an error. Although this occurs in a retriable loop it is assumed that grep will not fail, and the result is piped to wc and then blindly converted to an int in the python function, which fails since the error message is a string (throws ValueError)

We should catch the ValueError and return a 0 so it can try again rather than immediately crash

Reviewers: Bill Bejeck <bbejeck@gmail.com>, John Roesler <vvcephei@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>
","['streams/src/test/java/org/apache/kafka/streams/tests/StreamsBrokerDownResilienceTest.java', 'tests/kafkatest/tests/streams/base_streams_test.py', 'tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py']","System tests assume log files are immediately created after Streams app start, leading to grep failures and consequent crashes when attempting to convert error strings to int (ValueError occurrences)."
327809024fab07c44cd6d87985db5b9d14fd43bb,1618334043,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>","['clients/src/test/java/org/apache/kafka/test/TestUtils.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataManager.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java', 'storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageException.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogLeaderEpochState.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataCache.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataCacheTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManager.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManager.java', 'storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManagerTest.java']",Lack of in-memory implementations for RemoteStorageManager and RemoteLogMetadataManager to be used in default implementation based on topic storage and unit tests for tiered storage.
89a4735c35b80abcf5cd80f5c5ae80fc39107571,1675146705,"KAFKA-14656: Send UMR first during ZK migration (#13159)

When in migration-from-ZK mode and sending RPCs to ZK-based brokers, the KRaft controller must send
full UpdateMetadataRequests prior to sending full LeaderAndIsrRequests. If the controller sends the
requests in the other order, and the ZK-based broker does not already know about some of the nodes
referenced in the LeaderAndIsrRequest, it will reject the request.

This PR includes an integration test, and a number of other small fixes for dual-write.

Co-authored-by: Akhilesh C <akhileshchg@users.noreply.github.com>
Reviewers: Colin P. McCabe <cmccabe@apache.org>","['core/src/main/scala/kafka/common/InterBrokerSendThread.scala', 'core/src/main/scala/kafka/migration/MigrationPropagator.scala', 'core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala', 'core/src/main/scala/kafka/zk/ZkMigrationClient.scala', 'core/src/test/java/kafka/test/junit/ZkClusterInvocationContext.java', 'core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala', 'core/src/test/scala/unit/kafka/integration/KafkaServerTestHarness.scala', 'core/src/test/scala/unit/kafka/zk/ZkMigrationClientTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java', 'metadata/src/main/java/org/apache/kafka/image/ClientQuotaImage.java', 'metadata/src/main/java/org/apache/kafka/image/ConfigurationImage.java', 'metadata/src/main/java/org/apache/kafka/image/ConfigurationsImage.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java']","When migrating from ZK, sending RPCs to ZK-based brokers in the wrong order (LeaderAndIsrRequest before UpdateMetadataRequest) causes referenced nodes in LeaderAndIsrRequest to be rejected if not recognized."
f4996629239a49afb570a725642fb0311dd42e71,1685659421,"KAFKA-15003: Fix ZK sync logic for partition assignments (#13735)

Fixed the metadata change events in the Migration component to check correctly for the diff in
existing topic changes and replicate the metadata to the Zookeeper. Also, made the diff check
exhaustive enough to handle the partial writes in Zookeeper when we're try to replicate changes
using a snapshot in the event of Controller failover.

Add migration client and integration tests to verify the change.

Co-authored-by: Akhilesh Chaganti <akhileshchg@users.noreply.github.com>
","['core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala', 'core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala', 'core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala', 'metadata/src/main/java/org/apache/kafka/image/TopicDelta.java', 'metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java', 'metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java', 'metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java', 'metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java']",Incorrect synchronization logic in metadata change events can result in partial writes and failed data replication during Controller failover in the Migration component.
6279b03813af58411951a94cfa3509f6ef14bec0,1552898848,"KAFKA-8118; Ensure ZK clients are closed in tests, fix verification (#6456)

We verify that ZK clients are closed in tests since these can affect subsequent tests and that makes it hard to debug test failures. But because of changes to ZooKeeper client, we were checking the wrong thread name. The thread name used now is <creatorThreadName>-EventThread where creatorThreadName varies depending on the test. Fixed ZooKeeperTestHarness to check this format and fixed tests which were leaving ZK clients behind. Also added a test to make sure we can detect changes to the thread name when we update ZK clients in future.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>
","['core/src/test/scala/unit/kafka/zk/ZooKeeperTestHarness.scala', 'core/src/test/scala/unit/kafka/zookeeper/ZooKeeperClientTest.scala']","Changes to ZooKeeper client resulting in incorrect thread name checks in ZooKeeperTestHarness, leading to ZK clients from a test not being closed and affecting subsequent tests."
94e61c3979dbdd4884772bd41a0acca7dd5b1712,1600109327,"KAFKA-10458; Updating controller quota does not work since Token Bucket (#9272)

This PR fixes two issues that have been introduced by #9114.
- When the metric was switched from Rate to TokenBucket in the ControllerMutationQuotaManager, the metrics were mixed up. That broke the quota update path.
- When a quota is updated, the ClientQuotaManager updates the MetricConfig of the KafkaMetric. That update was not reflected into the Sensor so the Sensor was still using the MetricConfig that it has been created with.

Reviewers: Anna Povzner <anna@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java', 'clients/src/test/java/org/apache/kafka/common/metrics/SensorTest.java', 'core/src/main/scala/kafka/server/ClientQuotaManager.scala', 'core/src/main/scala/kafka/server/ClientRequestQuotaManager.scala', 'core/src/main/scala/kafka/server/ControllerMutationQuotaManager.scala', 'core/src/test/scala/unit/kafka/server/ControllerMutationQuotaTest.scala']","Updating controller quota doesn't reflect in Sensor due to mixed-up metrics, as the updated MetricConfig is not recognised by the Sensor."
2fa18792478baee45dfa3989a11e5a93765e2559,1670637102,"KAFKA-14454: Making unique StreamsConfig for tests (#12971)

Newly added test KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest#shouldThrowIllegalArgumentExceptionWhenCustomPartionerReturnsMultiplePartitions as part of KIP-837 passes when run individually but fails when is part of IT class and hence is marked as Ignored.

That seemed to have been because of the way StreamsConfig was being initialised so any new test would have used the same names. Because of which the second test never got to the desired state. With this PR, every test gets a unique app name which seems to have fixed the issue. Also, a couple of cosmetic changes

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",['streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest.java'],The test `KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest#shouldThrowIllegalArgumentExceptionWhenCustomPartionerReturnsMultiplePartitions` fails when run as part of the IT class due to shared StreamsConfig initialisation leading to same app names.
3a42baa260d4e39b82843d5458b56c1c4026abf7,1621367960,"HOTFIX: undo renaming of public part of Subtopology API (#10713)

In #10676 we renamed the internal Subtopology class that implemented the TopologyDescription.Subtopology interface. By mistake, we also renamed the interface itself, which is a public API. This wasn't really the intended point of that PR, so rather than do a retroactive KIP, let's just reverse the renaming.

Reviewers: Walker Carlson <wcarlson@confluent.io>, Guozhang Wang <guozhang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/TopologyDescription.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java']","Accidental renaming of the public Subtopology API when renaming the internal Subtopology class, resulting in unintended changes to a public interface."
3ea7b418fb3d7e9fc74c27751c1b02b04877f197,1652286646,"MINOR: Make TopicPartitionBookkeeper and TopicPartitionEntry top level (#12097)

This is the first step towards refactoring the `TransactionManager` so
that it's easier to understand and test. The high level idea is to push
down behavior to `TopicPartitionEntry` and `TopicPartitionBookkeeper`
and to encapsulate the state so that the mutations can only be done via
the appropriate methods.

Inner classes have no mechanism to limit access from the outer class,
which presents a challenge when mutability is widespread (like we do
here).

As a first step, we make `TopicPartitionBookkeeper` and
`TopicPartitionEntry` top level and rename them and a couple
of methods to make the intended usage clear and avoid
redundancy.

To make the review easier, we don't change anything else
except access changes required for the code to compile.
The next PR will contain the rest of the refactoring.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TxnPartitionEntry.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/TxnPartitionMap.java']","TransactionManager's current structure makes it hard to understand and test due to its inner classes ('TopicPartitionBookkeeper' and 'TopicPartitionEntry') not having mechanisms to limit access from the outer class, leading to challenges in managing mutability."
faaef2c2dff3208fe3ed23f4b075260475543957,1613065457,"MINOR: Support Raft-based metadata quorums in system tests (#10093)

We need to be able to run system tests with Raft-based metadata quorums -- both
co-located brokers and controllers as well as remote controllers -- in addition to the
ZooKepeer-based mode we run today. This PR adds this capability to KafkaService in a
backwards-compatible manner as follows.

If no changes are made to existing system tests then they function as they always do --
they instantiate ZooKeeper, and Kafka will use ZooKeeper. On the other hand, if we want
to use a Raft-based metadata quorum we can do so by introducing a metadata_quorum
argument to the test method and using @matrix to set it to the quorums we want to use for
the various runs of the test. We then also have to skip creating a ZooKeeperService when
the quorum is Raft-based.

This PR does not update any tests -- those will come later after all the KIP-500 code is
merged.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['tests/kafkatest/services/kafka/config.py', 'tests/kafkatest/services/kafka/config_property.py', 'tests/kafkatest/services/kafka/kafka.py', 'tests/kafkatest/services/kafka/quorum.py']",System tests lack support for Raft-based metadata quorums which makes testing both co-located brokers and controllers as well as remote controllers difficult. Current implementation is limited to only ZooKeeper-based mode.
0971f66ff546f780b46cf341b41a1513f916398e,1571845508,"KAFKA-9056; Inbound/outbound byte metrics should reflect incomplete sends/receives (#7551)

Currently we only record completed sends and receives in the selector metrics. If there is a disconnect in the middle of the respective operation, then it is not counted. The metrics will be more accurate if we take into account partial sends and receives.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com","['clients/src/main/java/org/apache/kafka/common/network/ByteBufferSend.java', 'clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java', 'clients/src/main/java/org/apache/kafka/common/network/NetworkReceive.java', 'clients/src/main/java/org/apache/kafka/common/network/Selector.java', 'clients/src/test/java/org/apache/kafka/common/network/KafkaChannelTest.java', 'clients/src/test/java/org/apache/kafka/common/network/NetworkReceiveTest.java', 'clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java', 'clients/src/test/java/org/apache/kafka/test/TestCondition.java', 'core/src/test/scala/unit/kafka/network/SocketServerTest.scala']","Incomplete sends or receives during a disconnect are not being reflected in the selector metrics, causing inaccuracies in the recorded data."
4962c8193e2faa589914be52962c701aba0980d1,1568738800,"KAFKA-8839 : Improve streams debug logging (#7258)

* log lock acquistion failures on the state store
* Document required uniqueness of state.dir path
* Move bunch of log calls around task state changes to DEBUG
* More readable log messages during partition assignment

Reviewers: Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java']","State store lock acquisition failures are not logged, state directory path uniqueness requirements are not documented, and task state changes and partition assignments lack detailed and readable debug log messages."
c034388a0a528bb7cef9aff6dd9c08a3db1e1ad3,1667926537,"KAFKA-14299: Avoid allocation & synchronization overhead in StreamThread loop (#12808)

The state updater code path introduced allocation and synchronization
overhead by performing relatively heavy operations in every iteration of
the StreamThread loop. This includes various allocations and acquiring
locks for handling `removedTasks` and `failedTasks`, even if the
corresponding queues are empty.

This change introduces `hasRemovedTasks` and
`hasExceptionsAndFailedTasks` in the `StateUpdater` interface that
can be used to skip over any allocation or synchronization. The new
methods do not require synchronization or memory allocation.

This change increases throughput by ~15% in one benchmark.

We extend existing unit tests to cover the slightly modified
behavior.

Reviewer: Bruno Cadonna <cadonna@apache.org>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StateUpdater.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdaterTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","StreamThread loop introduces an allocation and synchronization overhead in state updater with every iteration, reducing throughput efficiency due to handling of `removedTasks` and `failedTasks`, even when their queues are empty."
f36de0744b915335de6b636e6bd6b5f1276f34f6,1652827205,"MINOR: Remove redundant metric reset in KafkaController (#12158)

The following variables in `KafkaController` are used for metrics：
```
    offlinePartitionCount 
    preferredReplicaImbalanceCount
    globalTopicCount 
    globalPartitionCount
    topicsToDeleteCount 
    replicasToDeleteCount 
    ineligibleTopicsToDeleteCount 
    ineligibleReplicasToDeleteCount 
```
When the controller goes from active to non-active, these variables will be reset to 0. Currently, this is done explicitly in in `KafkaController.onControllerResignation()` and also after every loop iteration in `KafkaController.updateMetrics()` .
The first of these is redundant and can be removed. This patch fixes this and also simplifies `updateMetrics`. 

Reviewers: Jason Gustafson <jason@confluent.io>",['core/src/main/scala/kafka/controller/KafkaController.scala'],Redundant metric reset in `KafkaController.onControllerResignation()` and `KafkaController.updateMetrics()` resulting in unnecessary resetting of controller state variables when the transition occurs from active to non-active.
88725669e7c4de153493c9c1ed91466280ee3d6c,1670606578,"MINOR: Move MetadataQuorumCommand from `core` to `tools` (#12951)

`core` should only be  used for legacy cli tools and tools that require
access to `core` classes instead of communicating via the kafka protocol
(typically by using the client classes).

Summary of changes:
1. Convert the command implementation and tests to Java and move it to
    the `tools` module.
2. Introduce mechanism to capture stdout and stderr from tests.
3. Change `kafka-metadata-quorum.sh` to point to the new command class.
4. Adjusted the test classpath of the `tools` module so that it supports tests
    that rely on the `@ClusterTests` annotation.
5. Improved error handling when an exception different from `TerseFailure` is
    thrown.
6. Changed `ToolsUtils` to avoid usage of arrays in favor of `List`.

Reviewers: dengziming <dengziming1993@gmail.com>","['bin/kafka-metadata-quorum.sh', 'bin/windows/kafka-metatada-quorum.bat', 'clients/src/main/java/org/apache/kafka/common/utils/Utils.java', 'clients/src/test/java/org/apache/kafka/common/utils/UtilsTest.java', 'core/src/main/scala/kafka/admin/MetadataQuorumCommand.scala', 'core/src/test/scala/unit/kafka/admin/MetadataQuorumCommandTest.scala', 'server-common/src/main/java/org/apache/kafka/server/util/ToolsUtils.java', 'tools/src/main/java/org/apache/kafka/tools/MetadataQuorumCommand.java', 'tools/src/main/java/org/apache/kafka/tools/TerseException.java', 'tools/src/main/java/org/apache/kafka/tools/TransactionsCommand.java', 'tools/src/test/java/org/apache/kafka/tools/MetadataQuorumCommandErrorTest.java', 'tools/src/test/java/org/apache/kafka/tools/MetadataQuorumCommandTest.java', 'tools/src/test/java/org/apache/kafka/tools/ToolsTestUtils.java', 'tools/src/test/java/org/apache/kafka/tools/TransactionsCommandTest.java']","The `MetadataQuorumCommand` is misplaced in the `core` module, which should be reserved only for legacy tools or those requiring direct access to `core` classes. This results in problems with application structure and utilization."
f708dc58ed7f7e9d7aed2d43cd36aadaf32c75ee,1647377688,"MINOR: fix shouldWaitForMissingInputTopicsToBeCreated test (#11902)

This test was falling occasionally. It does appear to be a matter of the tests assuming perfecting deduplication/caching when asserting the test output records, ie a bug in the test not in the real code. Since we are not assuming that it is going to be perfect I changed the test to make sure the records we expect arrive, instead of only those arrive.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['streams/src/test/java/org/apache/kafka/streams/integration/NamedTopologyIntegrationTest.java'],Occasional test failures due to erroneous assumptions of perfect deduplication/caching during output record assertion in the shouldWaitForMissingInputTopicsToBeCreated test.
ea8ae976504e7a3f5c6f4a7efa5069d03316b093,1608071898,"KAFKA-10839; improve consumer group coordinator unavailable message (#9729)

When a consumer encounters an issue that triggers marking it to mark coordinator as unknown, the error message it prints does not give much context about the error that triggered it. This change includes the response error that triggered the transition or any other cause if not triggered by an error code in a response.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java']","Consumer encounters an error marking the coordinator as unknown, but the provided error message lacks the necessary context to understand the initial underlying issue."
b02bdd3227f08eef78080ef471c0950a4f77e5fb,1586488543,"MINOR: Only start log dir fetcher after LeaderAndIsr epoch validation (#8460)

Currently a `LeaderAndIsr` request with a stale leader epoch for some partition may still result in the starting of the log dir fetcher for that partition (if the future log exists). I am not sure if this causes any correctness problem since we don't use any state from the request to start the fetcher, but it seems unnecessary to rely on this side effect.

Reviewers: Jun Rao <junrao@gmail.com>",['core/src/main/scala/kafka/server/ReplicaManager.scala'],"`LeaderAndIsr` requests with stale leader epochs may unnecessarily start the log dir fetcher for a partition with a future log, potentially leading to reliance on undesired side effects."
c638235dbc0019b67fd520378d7efdcfd8dec0c0,1557467303,"KAFKA-8344; Fix vagrant-up.sh to work with AWS properly

For now, `vagrant/vagrant-up.sh --aws` fails because
the `vagrant hostmanager` command in that script lacks
the `--aws` option. This PR adds it.

I ran `vagrant/vagrant-up.sh --aws` with and without
`--no-parallel` option and confirmed both worked
as expected.

Author: Kengo Seki <sekikn@apache.org>

Reviewers: Gwen Shapira

Closes #6703 from sekikn/KAFKA-8344
",['vagrant/vagrant-up.sh'],Running `vagrant/vagrant-up.sh --aws` command fails due to missing `--aws` option in the `vagrant hostmanager` command.
add662907d8002a4c38196c4d8654a0db19a061a,1566924951,"MINOR: fix ProduceBenchWorker not to fail on final produce (#7254)

When sending bad records, the Trogdor task will fail if the final record produced is bad. Instead we should catch the exception to allow the task to finish since sending bad records is a valid use case.

Reviewers: Tu V. Tran <tuvtran97@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",['tools/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchWorker.java'],"Trogdor tasks fail when the final record produced is bad as exceptions aren't caught, invalidating the use case for sending bad records."
8c88cdb7186b1d594f991eb324356dcfcabdf18a,1680254862,"KAFKA-14617: Update AlterPartitionRequest and enable Kraft controller to reject stale request. (#13408)

Second part of the [KIP-903](https://cwiki.apache.org/confluence/display/KAFKA/KIP-903%3A+Replicas+with+stale+broker+epoch+should+not+be+allowed+to+join+the+ISR), it updates the AlterPartitionRequest:
- Deprecate the NewIsr field
- Create a new field BrokerState with BrokerId and BrokerEpoch
- Bump the AlterPartition version to 3

With this change, the Quorum Controller is enabled to reject stale AlterPartition request.

Reviewers: Jun Rao <junrao@gmail.com>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/common/requests/AlterPartitionRequest.java', 'clients/src/test/java/org/apache/kafka/common/requests/AlterPartitionRequestTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java', 'core/src/main/scala/kafka/controller/KafkaController.scala', 'core/src/main/scala/kafka/server/AlterPartitionManager.scala', 'core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/server/AlterPartitionManagerTest.scala', 'metadata/src/main/java/org/apache/kafka/controller/PartitionChangeBuilder.java', 'metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java', 'metadata/src/test/java/org/apache/kafka/controller/PartitionChangeBuilderTest.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java', 'metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java']","Stale AlterPartition requests not rejected by Quorum Controller due to lack of appropriate fields in the AlterPartitionRequest, causing potential inconsistencies."
3b6573c150527efb3e90b2b3d1a059ac5c2db80b,1582244085,"KAFKA-9481: Graceful handling TaskMigrated and TaskCorrupted (#8058)

1. Removed task field from TaskMigrated; the only caller that encodes a task id from StreamTask actually do not throw so we only log it. To handle it on StreamThread we just always enforce rebalance (and we would call onPartitionsLost to remove all tasks as dirty).

2. Added TaskCorruptedException with a set of task-ids. The first scenario of this is the restoreConsumer.poll which throws InvalidOffset indicating that the logs are truncated / compacted. To handle it on StreamThread we first close the corresponding tasks as dirty (if EOS is enabled we would also wipe out the state stores), and then revive them into the CREATED state.

3. Also fixed a bug while investigating KAFKA-9572: when suspending / closing a restoring task we should not commit the new offsets but only updating the checkpoint file.

4. Re-enabled the unit test.","['clients/src/main/java/org/apache/kafka/common/utils/FixedOrderMap.java', 'clients/src/test/java/org/apache/kafka/common/utils/FixedOrderMapTest.java', 'streams/src/main/java/org/apache/kafka/streams/errors/TaskCorruptedException.java', 'streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java', 'streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java']","Encountered TaskMigrated and TaskCorrupted exceptions aren't handled properly, potentially causing rebalance, state store issues, and inappropriate task state transition. A bug also exists when suspending/closing a restoring task which incorrectly commits new offsets instead of just updating the checkpoint file."
4dd27a9f2116edfd22b4dc7368a62fedef28fe3b,1676877977,"KAFKA-14673; Add high watermark listener to Partition/Log layers (#13196)

In the context of KIP-848, we implements are new group coordinator in Java. This new coordinator follows the architecture of the new quorum controller. It is basically a replicated state machine that writes to the log and commits its internal state when the writes are committed. At the moment, the only way to know when a write is committed is to use a delayed fetch. This is not ideal in our context because a delayed fetch can be completed before the write is actually committed to the log.

This patch proposes to introduce a high watermark listener to the Partition/Log layers. This will allow the new group coordinator to simply listen to changes and commit its state based on this. This mechanism is simpler and lighter as well.

Reviewers: Christo Lolov <lolovc@amazon.com>, Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/cluster/Partition.scala', 'core/src/main/scala/kafka/log/UnifiedLog.scala', 'core/src/main/scala/kafka/server/ReplicaManager.scala', 'core/src/test/scala/unit/kafka/cluster/PartitionTest.scala', 'core/src/test/scala/unit/kafka/log/LogTestUtils.scala', 'core/src/test/scala/unit/kafka/log/UnifiedLogTest.scala', 'core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala', 'storage/src/main/java/org/apache/kafka/storage/internals/log/LogOffsetsListener.java']","The new group coordinator implemented in the context of KIP-848 requires a delayed fetch to check if a write is committed, which can often falsely indicate a commit before the write is actually committed to the log."
78c59cd2b0b5f21c2028021d9dfb72d21065bb00,1693818152,"KAFKA-15052 Fix the flaky testBalancePartitionLeaders - part II (#13908)

A follow-up to https://github.com/apache/kafka/pull/13804.
This follow-up adds the alternative fix approach mentioned in
the PR above - bumping the session timeout used in the test
with 1 second.

Reproducing the flake-out locally has been much harder than
on the CI runs, as neither Gradle with Java 11 or Java 14 nor
IntelliJ with Java 14 could show it, but IntelliJ with Java 11
could occasionally reproduce the failure the first time
immediately after a rebuild. While I was unable to see the
failure with the bumped session timeout, the testing procedure
definitely didn't provide sufficient reassurance for the
fix as even without it often I'd see hundreds of consecutive
successful test runs when the first run didn't fail.

Reviewers: Luke Chen <showuon@gmail.com>, Christo Lolov <lolovc@amazon.com>",['metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java'],"The testBalancePartitionLeaders test is presenting occasional flakiness, failing reproducibly on certain environments with different Java versions and build systems, with its session timeout possibly at fault."
c708f7ba5f4f449920cec57a5b69e84e92128b54,1682648126,"MINOR: remove spurious call to fatalFaultHandler (#13651)

Remove a spurious call to fatalFaultHandler accidentally introduced by KAFKA-14805.  We should only
invoke the fatal fault handller if we are unable to generate the activation records. If we are
unable to write the activation records, a controller failover should be sufficient to remedy the
situation.

Co-authored-by: Luke Chen showuon@gmail.com

Reviewers: Luke Chen <showuon@gmail.com>, David Arthur <mumrah@gmail.com>","['metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java']","Unnecessary invocation of fatalFaultHandler even when activation records generation is successful, causing potential unwarranted controller failovers."
eb8cc09625c9913c4029763a46d4b7e5639c85f5,1550897469,"KAFKA-7961; Ignore assignment for un-subscribed partitions (#6304)

Whenever the consumer coordinator sends a response that doesn't match the client consumer subscription, we should check the subscription to see if it has changed. If it has, we can ignore the assignment and request a rebalance. Otherwise, we can throw an exception as before.

Testing strategy: create a mocked client that first sends an assignment response that doesn't match the client subscription followed by an assignment response that does match the client subscription.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java']",Consumer coordinator is sending responses which doesn't match the client consumer subscription causing exception to be thrown or unnecessary rebalance requests.
ed8659b4a09a4affff6798b8077ed2d8fb94b6da,1606122256,"KAFKA-10727; Handle Kerberos error during re-login as transient failure in clients (#9605)

We use a background thread for Kerberos to perform re-login before tickets expire. The thread performs logout() followed by login(), relying on the Java library to clear and then populate credentials in Subject. This leaves a timing window where clients fail to authenticate because credentials are not available. We cannot introduce any form of locking since authentication is performed on the network thread. So this commit treats NO_CRED as a transient failure rather than a fatal authentication exception in clients.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Manikumar Reddy <manikumar.reddy@gmail.com>","['clients/src/main/java/org/apache/kafka/common/network/SaslChannelBuilder.java', 'clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java', 'clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosError.java', 'clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosLogin.java', 'core/src/test/scala/integration/kafka/server/GssapiAuthenticationTest.scala']","During Kerberos re-login, transient timing window causes client authentication failure due to unavailability of credentials."
bb8de0b8c5f98f7a9d6b5ae7342ba7a0e1af8868,1559937111,"KAFKA-8003; Fix flaky testFencingOnTransactionExpiration

We see this failure from time to time:
```
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at kafka.api.TransactionsTest.testFencingOnTransactionExpiration(TransactionsTest.scala:512)
```
The cause is probably that we are using `consumeRecordsFor` which has no expectation on the number of records to fetch and a timeout of just 1s. This patch changes the code to use `consumeRecords` and the default 15s timeout.

Note we have also fixed a bug in the test case itself, which was using the wrong topic for the second write, which meant it could never have failed in the anticipated way anyway.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Gwen Shapira

Closes #6905 from hachikuji/fix-flaky-transaction-test
",['core/src/test/scala/integration/kafka/api/TransactionsTest.scala'],Flaky failure in testFencingOnTransactionExpiration due to unexpected number of records fetched. Incorrect topic being used for second write in test case.
706adc39f6aff378f98000b0466181d7fcb1d997,1662672959,"KAFKA-14201; Consumer should not send group instance ID if committing with empty member ID (server side) (#12598)

The consumer group instance ID is used to support a notion of ""static"" consumer groups. The idea is to be able to identify the same group instance across restarts so that a rebalance is not needed. However, if the user sets `group.instance.id` in the consumer configuration, but uses ""simple"" assignment with `assign()`, then the instance ID nevertheless is sent in the OffsetCommit request to the coordinator. This may result in a surprising UNKNOWN_MEMBER_ID error.

This PR attempts to fix this issue for existing consumers by relaxing the validation in this case. One way is to simply ignore the member id and the static id when the generation id is -1. -1 signals that the request comes from either the admin client or a consumer which does not use the group management. This does not apply to transactional offsets commit.

Reviewers: Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala', 'core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala']","Using ""simple"" assignment with `assign()` in consumer configuration with a set `group.instance.id` sends an unexpected instance id in the OffsetCommit request, causing UNKNOWN_MEMBER_ID error."
c08120f83f7318f15dcf14d525876d18caf6afd0,1683152732,"MINOR: Allow tagged fields with version subset of flexible version range (#13551)

The generated message types are missing a range check for the case when the tagged version range is a subset of
the flexible version range. This causes the tagged field count, which is computed correctly, to conflict with the
number of tags serialized.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['clients/src/test/java/org/apache/kafka/common/message/SimpleExampleMessageTest.java', 'generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java']",The generated message types have a missing range check leading to conflict between computed tagged field count and number of tags serialized whenever tagged version range is a subset of flexible version range.
efa6410611aa0862065ad804323c280a4d8a372d,1559256124,"KAFKA-8199: Implement ValueGetter for Suppress (#6781)

See also #6684

KTable processors must be supplied with a KTableProcessorSupplier, which in turn requires implementing a ValueGetter, for use with joins and groupings.

For suppression, a correct view only includes the previously emitted values (not the currently buffered ones), so this change also involves pushing the Change value type into the suppression buffer's interface, so that it can get the prior value upon first buffering (which is also the previously emitted value).

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@confluent.io>","['streams/src/main/java/org/apache/kafka/streams/kstream/internals/FullChangeSerde.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSourceValueGetterSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessor.java', 'streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/BufferKey.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/BufferValue.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/ContextualRecord.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/Maybe.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBuffer.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressScenarioTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorMetricsTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorTest.java', 'streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/SuppressSuite.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/MaybeTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java']","The current suppression mechanism in KTable processors doesn't implement a ValueGetter, causing inconsistencies in views especially with regards to previously emitted values when conducting joins and grouping operations."
5cddf9860bfb6271dae3a5286051a23a12c13dfe,1575390377,"KAFKA-9203: Revert ""MINOR: Remove workarounds for lz4-java bug affecting byte buffers (#6679)"" (#7769)

This reverts commit 90043d5f as it caused a regression in some cases:

> Caused by: java.io.IOException: Stream frame descriptor corrupted
>         at org.apache.kafka.common.record.KafkaLZ4BlockInputStream.readHeader(KafkaLZ4BlockInputStream.java:132)
>         at org.apache.kafka.common.record.KafkaLZ4BlockInputStream.<init>(KafkaLZ4BlockInputStream.java:78)
>         at org.apache.kafka.common.record.CompressionType$4.wrapForInput(CompressionType.java:110)

I will investigate why after, but I want to get the safe fix into 2.4.0.
The reporter of KAFKA-9203 has verified that reverting this change
makes the problem go away.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",['clients/src/main/java/org/apache/kafka/common/record/KafkaLZ4BlockInputStream.java'],Regression due to recent commit 90043d5f - causing 'Stream frame descriptor corrupted' error in KafkaLZ4BlockInputStream impacting stream initialization and wrapping input.
8cd04cb1a0e7e592d765a8b77dea85011a6e6f12,1625174195,"KAFKA-13007; KafkaAdminClient getListOffsetsCalls reuse cluster snapshot (#10940)

In getListOffsetsCalls, we rebuild the cluster snapshot for every topic partition. instead, we should reuse a snapshot.

For manual testing (used AK 2.8), i've passed in a map of 6K topic partitions to listOffsets

Without snapshot reuse:
duration of building futures from metadata response: **15582** milliseconds
total duration of listOffsets: **15743** milliseconds

With reuse:
duration of building futures from metadata response: **24** milliseconds
total duration of listOffsets: **235** milliseconds

Reviewers: Luke Chen <showuon@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala', 'core/src/test/scala/unit/kafka/server/MetadataRequestTest.scala']","In KafkaAdminClient's getListOffsetsCalls, the cluster snapshot is unnecessarily being rebuilt for every topic partition, leading to significant performance issues."
481fefb4f9ca0ecf83b72116977416d3a0472127,1661548279,"MINOR: Adds KRaft versions of most streams system tests (#12458)

Migrates Streams sustem tests to either use kraft brokers or to use both kraft and zk in a testing matrix.

This skips tests which use various forms of Kafka versioning since those seem to have issues with KRaft at the moment. Running these tests with KRaft will require a followup PR.

Reviewers: Guozhang Wang <guozhang@apache.org>, John Roesler <vvcephei@apache.org>","['tests/kafkatest/tests/streams/streams_broker_bounce_test.py', 'tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py', 'tests/kafkatest/tests/streams/streams_eos_test.py', 'tests/kafkatest/tests/streams/streams_named_repartition_topic_test.py', 'tests/kafkatest/tests/streams/streams_optimized_test.py', 'tests/kafkatest/tests/streams/streams_relational_smoke_test.py', 'tests/kafkatest/tests/streams/streams_shutdown_deadlock_test.py', 'tests/kafkatest/tests/streams/streams_smoke_test.py', 'tests/kafkatest/tests/streams/streams_standby_replica_test.py', 'tests/kafkatest/tests/streams/streams_static_membership_test.py']","Existing Streams system tests fail to operate with kraft brokers or combinations of kraft and zk, particularly tests that involve different versions of Kafka."
54af64c33a1796bcea8a9990aa61c804d9fc0df4,1659108085,"KAFKA-14108: Ensure both JUnit 4 and JUnit 5 tests run (#12441)

When the migration of the Streams project to JUnit 5 started with PR #12285, we discovered that the migrated tests were not run by the PR builds. This PR ensures that Streams' tests that are written in JUnit 4 and JUnit 5 are run in the PR builds.

Co-authored-by: Divij Vaidya <diviv@amazon.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Bruno Cadonna <cadonna@apache.org>","['streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/EmitOnChangeIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/FineGrainedAutoResetIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/GlobalThreadShutDownOrderTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/IQv2IntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/InternalTopicIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinCustomPartitionerIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/KTableSourceTopicRestartIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RackAwarenessIntegrationTest.java', 'streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java']","After migrating Streams project to JUnit 5, the PR builds are not running both JUnit 4 and JUnit 5 tests."
a4a3d7064e16d4964a5b64a114579512c22ae6d2,1596839400,"KAFKA-10371; Partition reassignments can result in crashed ReplicaFetcherThreads (#9140)

The patch https://github.com/apache/kafka/pull/8672 introduced a bug leading to crashing the replica fetcher threads. The issue is that https://github.com/apache/kafka/pull/8672 deletes the Partitions prior to stopping the replica fetchers. As the replica fetchers relies access the Partition in the ReplicaManager, they crash with a NotLeaderOrFollowerException that is not handled.

This PR reverts the code to the original ordering to avoid this issue.

The regression was caught and validated by our system test: `kafkatest.tests.core.reassign_partitions_test`. 

Reviewers: Vikas Singh <vikas@confluent.io>, Jason Gustafson <jason@confluent.io>",['core/src/main/scala/kafka/server/ReplicaManager.scala'],Deleting partitions before stopping the replica fetchers in Kafka leads to ReplicaFetcherThreads crashes due to unhandled NotLeaderOrFollowerException.
69a4661d7a7578789f4752928622c010b2264565,1626801181,"KAFKA-13100: Create KRaft controller snapshot during promotion (#11084)

The leader assumes that there is always an in-memory snapshot at the last
committed offset. This means that the controller needs to generate an in-memory
snapshot when getting promoted from inactive to active.  This PR adds that
code. This fixes a bug where sometimes we would try to look for that in-memory
snapshot and not find it.

The controller always starts inactive, and there is no requirement that there
exists an in-memory snapshot at the last committed offset when the controller
is inactive. Therefore we can remove the initial snapshot at offset -1.

We should also optimize when a snapshot is cancelled or completes, by deleting
all in-memory snapshots less that the last committed offset.

SnapshotRegistry's createSnapshot should allow the creating of a snapshot if
the last snapshot's offset is the given offset. This allows for simpler client
code. Finally, this PR renames createSnapshot to getOrCreateSnapshot.

Reviewers: Colin P. McCabe <cmccabe@apache.org>","['jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java', 'metadata/src/main/java/org/apache/kafka/controller/QuorumController.java', 'metadata/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java', 'metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java', 'metadata/src/test/java/org/apache/kafka/timeline/SnapshotRegistryTest.java', 'metadata/src/test/java/org/apache/kafka/timeline/SnapshottableHashTableTest.java', 'metadata/src/test/java/org/apache/kafka/timeline/TimelineHashMapTest.java', 'metadata/src/test/java/org/apache/kafka/timeline/TimelineHashSetTest.java', 'metadata/src/test/java/org/apache/kafka/timeline/TimelineIntegerTest.java', 'metadata/src/test/java/org/apache/kafka/timeline/TimelineLongTest.java', 'raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java']","Lack of an in-memory snapshot at the last committed offset upon the transition from inactive to active incorrectly triggers a search for the missing snapshot, leading to potential bugs."
3acc1938534131c6e84a0be4e8839c33bc034bfe,1612377020,"MINOR: Add mock implementation of `BrokerToControllerChannelManager` (#10026)

Tests involving `BrokerToControllerChannelManager` are simplified by being able to leverage `MockClient`. This patch introduces a `MockBrokerToControllerChannelManager` implementation which makes that possible.

The patch updates `ForwardingManagerTest` to use `MockBrokerToControllerChannelManager`. We also add a couple additional timeout cases, which exposed a minor bug. Previously we were using the wrong `TimeoutException`, which meant that expected timeout errors were in fact translated to `UNKNOWN_SERVER_ERROR`.

Reviewers: David Arthur <david.arthur@confluent.io>","['clients/src/test/java/org/apache/kafka/clients/MockClient.java', 'core/src/main/scala/kafka/server/AlterIsrManager.scala', 'core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala', 'core/src/main/scala/kafka/server/ForwardingManager.scala', 'core/src/test/scala/unit/kafka/server/ForwardingManagerTest.scala', 'core/src/test/scala/unit/kafka/server/MockBrokerToControllerChannelManager.scala', 'core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala']","Testing with `BrokerToControllerChannelManager` is problematic due to difficulty in leveraging `MockClient`, also existing `TimeoutException` is resulting in `UNKNOWN_SERVER_ERROR`, which is incorrect."
10164a6d2e1373c5cf57e37f63a3b10d07db6091,1676397218,"KAFKA-14693; Kafka node should halt instead of exit (#13227)

Extend the implementation of ProcessTerminatingFaultHandler to support calling either Exit.halt or Exit.exit. Change the fault handler used by the Controller thread and the KRaft thread to use a halting fault handler.

Those threads cannot call Exit.exit because Runtime.exit joins on the default shutdown hook thread. The shutdown hook thread joins on the controller and kraft thread terminating. This causes a deadlock.

Reviewers: Colin Patrick McCabe <cmccabe@apache.org>, Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/common/utils/Exit.java', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/main/scala/kafka/server/SharedServer.scala', 'core/src/main/scala/kafka/tools/TestRaftServer.scala', 'server-common/src/main/java/org/apache/kafka/server/fault/ProcessExitingFaultHandler.java', 'server-common/src/main/java/org/apache/kafka/server/fault/ProcessTerminatingFaultHandler.java', 'server-common/src/test/java/org/apache/kafka/server/fault/ProcessTerminatingFaultHandlerTest.java']",Controller and KRaft threads attempting to use Exit.exit causes a deadlock due to conflict with the default shutdown hook thread.
38f86d139cbd8a3b92ce555f185e52252f626cd4,1562719914,"MINOR: Use `Topic::isInternalTopic` instead of directly checking (#7047)

We don't allow changing number of partitions for internal topics. To do
so we check if the topic name belongs to the set of internal topics
directly instead of using the ""isInternalTopic"" method. This breaks the
encapsulation by making client aware of the fact that internal topics
have special names.

This is a simple change to use the method `Topic::isInternalTopic`
method instead of checking it directly in ""alterTopic"" command. We
also reduce visibility to `Topic::INTERNAL_TOPICS` to avoid 
unnecessary reliance on it in the future.

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/common/internals/Topic.java', 'clients/src/test/java/org/apache/kafka/test/TestUtils.java', 'core/src/main/scala/kafka/admin/TopicCommand.scala']","There's no method consistency for checking whether a topic is internal, causing a scope leak as non-internal client code is aware of special internal topic names."
ca90a8480183bf287d07baf6b3794bc51f76b89e,1583528932,"KAFKA-9668: Iterating over KafkaStreams.getAllMetadata() results in ConcurrentModificationException (#8233)

`KafkaStreams.getAllMetadata()` returns `StreamsMetadataState.getAllMetadata()`. All the latter methods is `synchronized` it returns a reference to internal mutable state.  Not only does this break encapsulation, but it means any thread iterating over the returned collection when the metadata gets rebuilt will encounter a `ConcurrentModificationException`.

This change:
 * switches from clearing and rebuild `allMetadata` when `onChange` is called to building a new list and swapping this in. This is thread safe and has the benefit that the returned list is not empty during a rebuild: you either get the old or the new list.
 * removes synchronisation from `getAllMetadata` and `getLocalMetadata`. These are returning member variables. Synchronisation adds nothing.
 * changes `getAllMetadata` to wrap its return value in an unmodifiable wrapper to avoid breaking encapsulation.
 * changes the getters in `StreamsMetadata` to wrap their return values in unmodifiable wrapper to avoid breaking encapsulation.

Co-authored-by: Andy Coates <big-andy-coates@users.noreply.github.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>","['streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java', 'streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java', 'streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/StreamsMetadataTest.java']",Iterating over the collection returned by KafkaStreams.getAllMetadata() triggers a ConcurrentModificationException due to thread interaction during metadata rebuild. Encapsulation is also broken due to exposed mutable state.
0e47fa75371dda275de73610b414c16a639895f3,1694606876,"KAFKA-15275 - Client state machine basic components, states and initial transitions (#14323)

Initial definition of the core components for maintaining group membership on the client following the new consumer group protocol.

This PR includes:
- Membership management for keeping member state and assignment, based on the heartbeat responses received.
- Assignor selection logic to support server side assignors.
This only includes the basic initial states and transitions, to be extended as we implement the protocol.

This is intended to be used from the heartbeat and assignment requests manager that actually build and process the heartbeat and assignment related requests.

Reviewers: Philip Nee <pnee@confluent.io>, Kirk True <ktrue@confluent.io>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/AssignorSelection.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/MemberState.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/MembershipManager.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/MembershipManagerImpl.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/AssignorSelectionTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/MembershipManagerImplTest.java']",Consumer group protocol's client membership management lacks a functional mechanism to maintain member state and assignment; client-side does not support server side assignors.
1546fc30af520f8d133aa3454cdc8a036bae0f3e,1564704619,"KAFKA-7548; KafkaConsumer should not discard fetched data for paused partitions (#6988)

This is an updated implementation of #5844 by @MayureshGharat (with Mayuresh's permission). As described in the original ticket:

> Today when we call KafkaConsumer.poll(), it will fetch data from Kafka asynchronously and is put in to a local buffer (completedFetches).
>
> If now we pause some TopicPartitions and call KafkaConsumer.poll(), we might throw away any buffered data that we might have in the local buffer for these TopicPartitions. Generally, if an application is calling pause on some TopicPartitions, it is likely to resume those TopicPartitions in near future, which would require KafkaConsumer to re-issue a fetch for the same data that it had buffered earlier for these TopicPartitions. This is a wasted effort from the application's point of view.

This patch fixes the problem by retaining the paused data in the completed fetches queue, essentially moving it to the back on each call to `fetchedRecords`. 

Reviewers: Jason Gustafson <jason@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java']","When KafkaConsumer.poll() is invoked after pausing some TopicPartitions, previously buffered data for these partitions might be discarded, leading to potential wastage and redundancy as data might need to be fetched again when resuming."
9c3f605fc78f297ecf5accdcdec18471c19cf7d6,1650399436,"KAFKA-13835: Fix two bugs related to dynamic broker configs in KRaft (#12063)

Fix two bugs related to dynamic broker configs in KRaft. The first bug is that we are calling reloadUpdatedFilesWithoutConfigChange when a topic configuration is changed, but not when a
broker configuration is changed. This is backwards. This function must be called only for broker 
configs, and never for topic configs or cluster configs.

The second bug is that there were several configurations such as max.connections which are related
to broker listeners, but which do not involve changing the registered listeners. We should support
these configurations in KRaft. This PR fixes the configuration change validation to support this case.

Reviewers: Jason Gustafson <jason@confluent.io>, Matthew de Detrich <mdedetrich@gmail.com>","['core/src/main/scala/kafka/server/BrokerServer.scala', 'core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/main/scala/kafka/server/metadata/BrokerMetadataListener.scala', 'core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala', 'core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala']","Dynamic broker configs in KRaft are not reloading when a broker configuration is changed and several broker-related configurations like max.connections aren't supported, leading to a validation failure."
02221bd907a23041c95ce6446986bff631652b3a,1555008914,"MINOR: Remove SubscriptionState.Listener and replace with assignmentId tracking (#6559)

We have not had great experience with listeners. They make the code harder to understand because they result in indirectly maintained circular dependencies. Often this leads to tricky deadlocks when we try to introduce locking. We were able to remove the Metadata listener in KAFKA-7831. Here we do the same for the listener in SubscriptionState.

Reviewers: Viktor Somogyi-Vass <viktorsomogyi@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java']",Use of SubscriptionState.Listener creates circular dependencies making the code harder to comprehend and leading to deadlocks during the introduction of locking.
6bf5bfc2982158c3a1bfff4a0f65ea901ea84e7a,1656648441,"KAFKA-14036; Set local time in `ControllerApis` when `handle` returns (#12372)

In `ControllerApis`, we are missing the logic to set the local processing end time after `handle` returns. As a consequence of this, the remote time ends up reported as the local time in the request level metrics. The patch adds the same logic we have in `KafkaApis` to set `apiLocalCompleteTimeNanos`.

Reviewers: José Armando García Sancio <jsancio@gmail.com>","['core/src/main/scala/kafka/server/ControllerApis.scala', 'core/src/test/scala/unit/kafka/server/ControllerApisTest.scala']","'Missing logic to set local processing end time in `ControllerApis` after `handle` returns, causing incorrect reporting of remote time as local time in the request level metrics.'
"
1d26391368d146fc0b7be9abfcf98d3475d7a834,1603307247,"KAFKA-10520; Ensure transactional producers poll if leastLoadedNode not available with max.in.flight=1 (#9406)

We currently stop polling in `Sender` in a transactional producer if there is only one broker in the bootstrap server list and `max.in.flight.requests.per.connection=1` and Metadata response is pending when InitProducerId request is ready to be sent. In this scenario, we attempt to send FindCoordinator to `leastLoadedNode`, but since that is blocked due to `max.in.flight=1` as a result of the pending metadata response, we never unblock unless we poll. This PR ensures we poll in this case.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>, David Jacot <djacot@confluent.io>","['clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java', 'core/src/test/scala/integration/kafka/api/TransactionsWithMaxInFlightOneTest.scala', 'core/src/test/scala/unit/kafka/utils/TestUtils.scala']","Transactional producers halt polling in `Sender` when only one broker is listed in bootstrap server, `max.in.flight.requests.per.connection=1`, and metaData response is pending, resulting in a blocking scenario where the FindCoordinator cannot be sent to `leastLoadedNode`."
ebb80f568da59cf60c76170176584bcb34d88ecb,1562825944,"KAFKA-8653; Default rebalance timeout to session timeout for JoinGroup v0 (#7072)

The rebalance timeout was added to the JoinGroup protocol in version 1. Prior to 2.3,
we handled version 0 JoinGroup requests by setting the rebalance timeout to be equal
to the session timeout. We lost this logic when we converted the API to use the
generated schema definition (#6419) which uses the default value of -1. The impact
of this is that the group rebalance timeout becomes 0, so rebalances finish immediately
after we enter the PrepareRebalance state and kick out all old members. This causes
consumer groups to enter an endless rebalance loop. This patch restores the old
behavior.

Reviewers: Ismael Juma <ismael@juma.me.uk>","['clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java', 'clients/src/test/java/org/apache/kafka/common/requests/JoinGroupRequestTest.java', 'clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java']","Conversion of JoinGroup API to use the generated schema definition led to rebalance timeout defaulting to -1 rather than session timeout, causing consumer groups to enter an endless rebalance loop."
edd64fa251b665b1ff88ed80112ac56a1ac0f9e2,1688997670,"MINOR: more KRaft Metadata Image tests (#13724)

Adds additional testing for the various KRaft *Image classes. For every image that we create we already test that we can get there by applying all the records corresponding to that image written out as a list of records. This patch adds more tests to confirm that we can get to each such final image with intermediate stops at all possible intermediate images along the way.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>
","['metadata/src/test/java/org/apache/kafka/image/AclsImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/ClientQuotasImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/ClusterImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/ConfigurationsImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/MetadataImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/ProducerIdsImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/ScramImageTest.java', 'metadata/src/test/java/org/apache/kafka/image/TopicsImageTest.java', 'metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java']","Certain KRaft *Image classes might have inadequate testing, potentially missing intermediate image states and not validating the transition between each state."
2ecdca2814e232e038909c562fbfd667935ab69e,1587667065,"KAFKA-9883: Add better error message when REST API forwards a request and leader is not known (#8536)

When the Connect worker forwards a REST API request to the leader, it might get back a `RequestTargetException` that suggests the worker should forward the request to a different worker. This can happen when the leader changes, and the worker that receives the original request forwards the request to the worker that it thinks is the current leader, but that worker is not the current leader. In this case. In most cases, the worker that received the forwarded request includes the URL of the current leader, but it is possible (albeit rare) that the worker doesn’t know the current leader and will include a null leader URL in the resulting `RequestTargetException`.

When this rare case happens, the user gets a null pointer exception in their response and the NPE is logged. Instead, the worker should catch this condition and provide a more useful error message that is similar to other existing error messages that might occur.

Added a unit test that verifies this corner case is caught and this particular NPE does not occur.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>","['connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java', 'connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResourceTest.java']","When the Connect worker forwards a REST API request based on an old leader resulting in a `RequestTargetException`, it leads to null pointer exception being logged and returned in user's response in rare cases."
a46c07ec8d8351e05cdccf55b52861b670fd1307,1600193557,"KAFKA-10292: Set min.insync.replicas to 1 of __consumer_offsets (#9286)

The test StreamsBrokerBounceTest.test_all_brokers_bounce() fails on
2.5 because in the last stage of the test there is only one broker
left and the offset commit cannot succeed because the
min.insync.replicas of __consumer_offsets is set to 2 and acks is
set to all. This causes a time out and extends the closing of the
Kafka Streams client to beyond the duration passed to the close
method of the client.

This affects especially the 2.5 branch since there Kafka Streams
commits offsets for each task, i.e., close() needs to wait for the
timeout for each task. In 2.6 and trunk the offset commit is done
per thread, so close() does only need to wait for one time out per
stream thread.

I opened this PR on trunk, since the test could also become
flaky on trunk and we want to avoid diverging system tests across
branches.

A more complete solution would be to improve the test by defining
a better success criteria.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",['tests/kafkatest/tests/streams/streams_broker_bounce_test.py'],StreamsBrokerBounceTest.test_all_brokers_bounce() fails due to timeout as offset commit can't succeed for __consumer_offsets when there's only one broker left since min.insync.replicas and acks are set to 2.
898ad8271ac80d30ae7e1446d3d811996c4b5d9e,1573748711,"MINOR: Add method `hasMetrics()` to class `Sensor` (#7692)

Sometimes to be backwards compatible regarding metrics the simplest
solution is to create an empty sensor. Recording an empty sensor on
the hot path may negatively impact performance. With hasMetrics()
recordings of empty sensors on the hot path can be avoided without
being to invasive.

Reviewers: Bill Bejeck <bbejeck@gmail.com>","['clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java', 'clients/src/test/java/org/apache/kafka/common/metrics/SensorTest.java']",Creating an empty sensor for maintaining backwards compatibility negatively impacts performance due to sensor recordings on the hot path.
a674ded0b34b79e04b4242a3db19c90ae87b6f87,1554412616,"MINOR: fix throttling and status in ConnectionStressWorker

Each separate thread should have its own throttle, so that it can sleep
for an appropriate amount of time when needed.

ConnectionStressWorker should avoid recalculating the status after
shutting down the runnables.  Otherwise, if one runnable is slow to
stop, it will skew the average down in a way that doesn't reflect
reality.  This change moves the status calculation into a separate
periodic runnable that gets shut down cleanly before the other ones.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Gwen Shapira, Stanislav Kozlovski

Closes #6533 from cmccabe/fix_connection_stress_worker
",['tools/src/main/java/org/apache/kafka/trogdor/workload/ConnectionStressWorker.java'],"ConnectionStressWorker experienced inaccurate status calculation due to skewed averages when runnables stop unevenly, and thread throttling isn't individualized leading to incorrect sleep timing."
d89b26ff443c8dd5d584ad0a979ac3944366cc06,1691051160,"KAFKA-12969: Add broker level config synonyms for topic level tiered storage configs (#14114)

KAFKA-12969: Add broker level config synonyms for topic level tiered storage configs.

Topic -> Broker Synonym:
local.retention.bytes -> log.local.retention.bytes
local.retention.ms -> log.local.retention.ms

We cannot add synonym for `remote.storage.enable` topic property as it depends on KIP-950

Reviewers: Divij Vaidya <diviv@amazon.com>, Satish Duggana <satishd@apache.org>, Luke Chen <showuon@gmail.com>","['core/src/main/java/kafka/server/ReplicaFetcherTierStateMachine.java', 'core/src/main/scala/kafka/log/UnifiedLog.scala', 'core/src/main/scala/kafka/server/DynamicBrokerConfig.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/test/scala/unit/kafka/log/LogConfigTest.scala', 'core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala', 'server-common/src/main/java/org/apache/kafka/server/config/ServerTopicConfigSynonyms.java', 'storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogManagerConfig.java', 'storage/src/main/java/org/apache/kafka/storage/internals/log/LogConfig.java']","There are no broker level config synonyms for topic level tiered storage configurations, hindering the ability to manage storage retention at a broker level."
bbf1ee74d719494ed5a5cac9dc54b7093171707c,1629216771,"KAFKA-13207: Skip truncation on fetch response with diverging epoch if partition removed from fetcher (#11221)

AbstractFetcherThread#truncateOnFetchResponse is used with IBP 2.7 and above to truncate partitions based on diverging epoch returned in fetch responses. Truncation should only be performed for partitions that are still owned by the fetcher and this check should be done while holding partitionMapLock to ensure that any partitions removed from the fetcher thread are not truncated. Truncation will be performed by any new fetcher that owns the partition when it restarts fetching.

Reviewers: David Jacot <djacot@confluent.io>, Jason Gustafson <jason@confluent.io>","['core/src/main/scala/kafka/server/AbstractFetcherThread.scala', 'core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala']",AbstractFetcherThread#truncateOnFetchResponse performs unintentional truncation on fetch responses with diverging epochs for partitions that have been removed from the fetcher.
5b6de9f2d022cf25df73067b9de77ec267b4af8c,1575493937,"KAFKA-8933; Fix NPE in DefaultMetadataUpdater after authentication failure (#7682)

This patch fixes an NPE in `DefaultMetadataUpdater` due to an inconsistency in event expectations. Whenever there is an authentication failure, we were treating it as a failed update even if was from a separate connection from an inflight metadata request. This patch fixes the problem by making the `MetadataUpdater` api clearer in terms of the events that are handled.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>","['clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java', 'clients/src/main/java/org/apache/kafka/clients/Metadata.java', 'clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java', 'clients/src/main/java/org/apache/kafka/clients/NetworkClient.java', 'clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java', 'clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java', 'clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java', 'clients/src/test/java/org/apache/kafka/clients/MetadataTest.java', 'clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java', 'clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java', 'clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java', 'clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java', 'clients/src/test/java/org/apache/kafka/test/MockSelector.java', 'connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java', 'core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala']",An inconsistency in event expectations in `DefaultMetadataUpdater` is causing a Null Pointer Exception (NPE) when there's an authentication failure.
3b08deaa761c2387a41610893dc8302ab1d97338,1651792156,"KAFKA-13785: [8/N][emit final] time-ordered session store (#12127)

Time ordered session store implementation. I introduced AbstractRocksDBTimeOrderedSegmentedBytesStore to make it generic for RocksDBTimeOrderedSessionSegmentedBytesStore and RocksDBTimeOrderedSegmentedBytesStore.

A few minor follow-up changes:

1. Avoid extra byte array allocation for fixed upper/lower range serialization.
2. Rename some class names to be more consistent.

Authored-by: Hao Li <1127478+lihaosky@users.noreply.github.com>
Reviewers: Guozhang Wang <wangguoz@gmail.com.com>, John Roesler <vvcephei@apache.org>","['streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractDualSchemaRocksDBSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBTimeOrderedSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/PrefixedSessionKeySchemas.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/PrefixedWindowKeySchemas.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedSessionSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedSessionStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowSegmentedBytesStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbIndexedTimeOrderedWindowBytesStoreSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbTimeOrderedSessionBytesStoreSupplier.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedSessionStoreIterator.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractDualSchemaRocksDBSegmentedBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedSegmentedBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowSegmentedBytesStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDbIndexedTimeOrderedWindowBytesStoreSupplierTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/SessionKeySchemaTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedWindowStoreTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/WindowKeySchemaTest.java']","Extra byte array allocation for fixed upper/lower range serialization in Time ordered session store might not be efficient, coupled with inconsistent class naming."
406635bcc9f2a4c439d198ea0549170de331323c,1584747557,"MINOR: Use Exit.exit instead of System.exit in MM2 (#8321)

Exit.exit needs to be used in code instead of System.exit.

Particularly in integration tests using System.exit is disrupting because it exits the jvm process and does not just fail the test correctly. Integration tests override procedures in Exit to protect against such cases.

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Randall Hauch <rhauch@gmail.com>",['connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMaker.java'],Usage of System.exit in MM2 is causing disruption in integration tests as it exits the JVM process rather than just failing the test appropriately.
16769d263e2e8fd91704d2e3519abf9dcba507df,1558125137,"KAFKA-8215: Upgrade Rocks to v5.18.3 (#6743)

This upgrade exposes a number of new options, including the WriteBufferManager which -- along with existing TableConfig options -- allows users to limit the total memory used by RocksDB across instances. This can alleviate some cascading OOM potential when, for example, a large number of stateful tasks are suddenly migrated to the same host.

The RocksDB docs guarantee backwards format compatibility across versions

Reviewers: Matthias J. Sax <mjsax@apache.org>, Bill Bejeck <bbejeck@gmail.com>,","['streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java', 'streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapterTest.java', 'streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java']",Upgrading to Rocks v5.18.3 could lead to cascading Out Of Memory (OOM) issues when a large number of stateful tasks get migrated to the same host due to lack of options to limit total memory used by RocksDB.
44e613c4cd1a2b7303da7dd30d47bbe87090131f,1678907201,"KAFKA-13884; Only voters flush on Fetch response (#13396)

The leader only requires that voters have flushed their log up to the fetch offset before sending a fetch request.

This change only flushes the log when handling the fetch response, if the follower is a voter. This should improve the disk performance on observers (brokers).

Reviewers: Jason Gustafson <jason@confluent.io> ","['core/src/main/scala/kafka/log/UnifiedLog.scala', 'core/src/main/scala/kafka/raft/KafkaMetadataLog.scala', 'raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java', 'raft/src/main/java/org/apache/kafka/raft/ReplicatedLog.java', 'raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java', 'raft/src/test/java/org/apache/kafka/raft/MockLog.java', 'raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java']","Non-voting followers flush their log upon handling fetch responses, possibly leading to undesired disk performance impacts."
3475347aed004141f2bf8e16ddf34ecfbbae0ed6,1630945127,"KAFKA-13270: Set JUTE_MAXBUFFER to 4 MB by default (#11295)

We restore the 3.4.x/3.5.x behavior unless the caller has set the property (note that ZKConfig
auto configures itself if certain system properties have been set).

I added a unit test that fails without the change and passes with it.

I also refactored the code to streamline the way we handle parameters passed to
KafkaZkClient and ZooKeeperClient.
 
See https://github.com/apache/zookeeper/pull/1129 for the details on why the behavior
changed in 3.6.0.

Credit to @rondagostino for finding and reporting this issue.

Reviewers: David Jacot <djacot@confluent.io>","['core/src/main/scala/kafka/admin/ConfigCommand.scala', 'core/src/main/scala/kafka/admin/ZkSecurityMigrator.scala', 'core/src/main/scala/kafka/security/authorizer/AclAuthorizer.scala', 'core/src/main/scala/kafka/server/KafkaConfig.scala', 'core/src/main/scala/kafka/server/KafkaServer.scala', 'core/src/main/scala/kafka/zk/KafkaZkClient.scala', 'core/src/main/scala/kafka/zookeeper/ZooKeeperClient.scala', 'core/src/test/scala/integration/kafka/api/SaslSetup.scala', 'core/src/test/scala/unit/kafka/security/auth/ZkAuthorizationTest.scala', 'core/src/test/scala/unit/kafka/security/authorizer/AclAuthorizerTest.scala', 'core/src/test/scala/unit/kafka/security/authorizer/AuthorizerInterfaceDefaultTest.scala', 'core/src/test/scala/unit/kafka/server/KafkaServerTest.scala', 'core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala', 'core/src/test/scala/unit/kafka/zk/ZooKeeperTestHarness.scala', 'core/src/test/scala/unit/kafka/zookeeper/ZooKeeperClientTest.scala']","Default value for JUTE_MAXBUFFER causing issues with KafkaZkClient and ZooKeeperClient functionality, and test failures."
d6da0452b6b14be81076d82a257381d42c515e08,1587349461,"MINOR: Update to Gradle 6.3 (#7677)

* Introduce `gradlewAll` script to replace `*All` tasks since the approach
used by the latter doesn't work since Gradle 6.0 and it's unclear when,
if ever, it will work again ( see https://github.com/gradle/gradle/issues/11301).
* Update release script and README given the above.
* Update zinc to 1.3.5.
* Update gradle-versions-plugin to 0.28.0.

The major improvements in Gradle 6.0 to 6.3 are:
- Improved incremental compilation for Scala
- Support for Java 14 (although some Gradle plugins
like spotBugs may need to be updated or disabled,
will do that separately)
- Improved scalac reporting, warnings are clearly
marked as such, which is very helpful.

Tested `gradlewAll` manually for the commands listed in the README
and release script. For `uploadArchive`, I tested it with a local Maven
repository.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",['release.py'],"The former '*All' tasks approach is incompatible with Gradle 6.0 and later versions, causing various task failures. Additionally, there's a need for support for newer versions of Java, improved incremental compilation for Scala, and enhanced scalac reporting."
64b8e17827251174490678dd296ce2c1a79ff5ef,1628894159,"KAFKA-13194: bound cleaning by both LSO and HWM when firstUnstableOffsetMetadata is None (#11199)

When the high watermark is contained in a non-active segment, we are not correctly bounding it by the hwm. This means that uncommitted records may overwrite committed data. I've separated out the bounding point tests to check the hwm case in addition to the existing active segment case.

Reviewers: Jun Rao <junrao@gmail.com>","['core/src/main/scala/kafka/log/LogCleanerManager.scala', 'core/src/test/scala/unit/kafka/log/AbstractLogCleanerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerIntegrationTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerLagIntegrationTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala', 'core/src/test/scala/unit/kafka/log/LogCleanerParameterizedIntegrationTest.scala']","Uncommitted records are overwriting committed data when the high watermark is in a non-active segment, due to incorrect bounding."
16d4d8cafc7394a35caf3354b449505bde56920f,1557872089,"MINOR: Fix flaky ConsumerTopicCreationTest (#6727)

`ConsumerTopicCreationTest` relied on `KafkaConsumer#poll` to send a `MetadataRequest` within 100ms to verify if a topic is auto created or not. This is brittle and does not guarantee if the request made it to the broker or was processed successfully. This PR fixes the flaky test by adding another topic; we wait until we consume a previously produced record to this topic. This ensures MetadataRequest was processed and we could then check if the topic we're interested in was created or not.

Reviewers: Boyang Chen <bchen11@outlook.com>, Jason Gustafson <jason@confluent.io>","['core/src/test/scala/integration/kafka/api/ConsumerTopicCreationTest.scala', 'core/src/test/scala/integration/kafka/api/IntegrationTestHarness.scala']","The `ConsumerTopicCreationTest` is flaky as it relies on `KafkaConsumer#poll` sending a `MetadataRequest` within 100ms to verify topic auto-creation, which doesn't ensure that the request was processed successfully."
