[{"commit_id": "c911406e5ace8742e5841a7e0df113ecb5d54685", "commit_message": "add setup.py to allow registering micrograd as package under pypi", "relative_path": "setup.py", "previous_code_file": null, "new_code_file": "import setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"micrograd\",\n    version=\"0.1.0\",\n    author=\"Andrej Karpathy\",\n    author_email=\"andrej.karpathy@gmail.com\",\n    description=\"A tiny scalar-valued autograd engine with a small PyTorch-like neural network library on top.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/karpathy/micrograd\",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n", "diff": null, "status": "added"}, {"commit_id": "315dda3d695f7f717e5d77ef9b7d4aff35f5f83f", "commit_message": "rename test file to agree better with standard naming schema", "relative_path": "test/test_engine.py", "previous_code_file": "import torch\nfrom micrograd.engine import Value\n\ndef test_sanity_check():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0]).double()\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    assert ymg.data == ypt.data.item()\n    # backward pass went well\n    assert xmg.grad == xpt.grad.item()\n\ndef test_more_ops():\n\n    a = Value(-4.0)\n    b = Value(2.0)\n    c = a + b\n    d = a * b + b**3\n    c += c + 1\n    c += 1 + c + (-a)\n    d += d * 2 + (b + a).relu()\n    d += 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g += 10.0 / f\n    g.backward()\n    amg, bmg, gmg = a, b, g\n\n    a = torch.Tensor([-4.0]).double()\n    b = torch.Tensor([2.0]).double()\n    a.requires_grad = True\n    b.requires_grad = True\n    c = a + b\n    d = a * b + b**3\n    c = c + c + 1\n    c = c + 1 + c + (-a)\n    d = d + d * 2 + (b + a).relu()\n    d = d + 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g = g + 10.0 / f\n    g.backward()\n    apt, bpt, gpt = a, b, g\n\n    tol = 1e-6\n    # forward pass went well\n    assert abs(gmg.data - gpt.data.item()) < tol\n    # backward pass went well\n    assert abs(amg.grad - apt.grad.item()) < tol\n    assert abs(bmg.grad - bpt.grad.item()) < tol\n", "new_code_file": "import torch\nfrom micrograd.engine import Value\n\ndef test_sanity_check():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0]).double()\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    assert ymg.data == ypt.data.item()\n    # backward pass went well\n    assert xmg.grad == xpt.grad.item()\n\ndef test_more_ops():\n\n    a = Value(-4.0)\n    b = Value(2.0)\n    c = a + b\n    d = a * b + b**3\n    c += c + 1\n    c += 1 + c + (-a)\n    d += d * 2 + (b + a).relu()\n    d += 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g += 10.0 / f\n    g.backward()\n    amg, bmg, gmg = a, b, g\n\n    a = torch.Tensor([-4.0]).double()\n    b = torch.Tensor([2.0]).double()\n    a.requires_grad = True\n    b.requires_grad = True\n    c = a + b\n    d = a * b + b**3\n    c = c + c + 1\n    c = c + 1 + c + (-a)\n    d = d + d * 2 + (b + a).relu()\n    d = d + 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g = g + 10.0 / f\n    g.backward()\n    apt, bpt, gpt = a, b, g\n\n    tol = 1e-6\n    # forward pass went well\n    assert abs(gmg.data - gpt.data.item()) < tol\n    # backward pass went well\n    assert abs(amg.grad - apt.grad.item()) < tol\n    assert abs(bmg.grad - bpt.grad.item()) < tol\n", "diff": null, "status": "renamed"}, {"commit_id": "5bb639209a5217b543d899dfc23ea968252fa9c1", "commit_message": "small tweaks and bug fixes to docs", "relative_path": "micrograd/engine.py", "previous_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += (other * self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other): # self + other\n        return self.__add__(other)\n\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    def __rsub__(self, other): # other - self\n        return other + (-self)\n\n    def __neg__(self): # -self\n        return self * -1\n\n    def __rmul__(self, other): # other * self\n        return self.__mul__(other)\n\n    def __truediv__(self, other): # self / other\n        return self * other**-1\n\n    def __rtruediv__(self, other): # other / self\n        return other * self**-1\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "new_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += (other * self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __neg__(self): # -self\n        return self * -1\n\n    def __radd__(self, other): # other + self\n        return self + other\n\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    def __rsub__(self, other): # other - self\n        return other + (-self)\n\n    def __rmul__(self, other): # other * self\n        return self * other\n\n    def __truediv__(self, other): # self / other\n        return self * other**-1\n\n    def __rtruediv__(self, other): # other / self\n        return other * self**-1\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "diff": "@@ -69,20 +69,20 @@ def build_topo(v):\n         for v in reversed(topo):\n             v._backward()\n \n-    def __radd__(self, other): # self + other\n-        return self.__add__(other)\n+    def __neg__(self): # -self\n+        return self * -1\n+\n+    def __radd__(self, other): # other + self\n+        return self + other\n \n     def __sub__(self, other): # self - other\n         return self + (-other)\n \n     def __rsub__(self, other): # other - self\n         return other + (-self)\n \n-    def __neg__(self): # -self\n-        return self * -1\n-\n     def __rmul__(self, other): # other * self\n-        return self.__mul__(other)\n+        return self * other\n \n     def __truediv__(self, other): # self / other\n         return self * other**-1", "status": "modified"}, {"commit_id": "315a2cb3e2cf04837c5133008e8afdedc9db5a04", "commit_message": "add a number of more ops including -,/,**, and supporting unit tests", "relative_path": "micrograd/engine.py", "previous_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "new_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += (other * self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other): # self + other\n        return self.__add__(other)\n\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    def __rsub__(self, other): # other - self\n        return other + (-self)\n\n    def __neg__(self): # -self\n        return self * -1\n\n    def __rmul__(self, other): # other * self\n        return self.__mul__(other)\n\n    def __truediv__(self, other): # self / other\n        return self * other**-1\n\n    def __rtruediv__(self, other): # other / self\n        return other * self**-1\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "diff": "@@ -32,6 +32,16 @@ def _backward():\n \n         return out\n \n+    def __pow__(self, other):\n+        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n+        out = Value(self.data**other, (self,), f'**{other}')\n+\n+        def _backward():\n+            self.grad += (other * self.data**(other-1)) * out.grad\n+        out._backward = _backward\n+\n+        return out\n+\n     def relu(self):\n         out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n \n@@ -59,11 +69,26 @@ def build_topo(v):\n         for v in reversed(topo):\n             v._backward()\n \n-    def __radd__(self, other):\n+    def __radd__(self, other): # self + other\n         return self.__add__(other)\n \n-    def __rmul__(self, other):\n+    def __sub__(self, other): # self - other\n+        return self + (-other)\n+\n+    def __rsub__(self, other): # other - self\n+        return other + (-self)\n+\n+    def __neg__(self): # -self\n+        return self * -1\n+\n+    def __rmul__(self, other): # other * self\n         return self.__mul__(other)\n \n+    def __truediv__(self, other): # self / other\n+        return self * other**-1\n+\n+    def __rtruediv__(self, other): # other / self\n+        return other * self**-1\n+\n     def __repr__(self):\n         return f\"Value(data={self.data}, grad={self.grad})\"", "status": "modified"}, {"commit_id": "315a2cb3e2cf04837c5133008e8afdedc9db5a04", "commit_message": "add a number of more ops including -,/,**, and supporting unit tests", "relative_path": "test/test_basic.py", "previous_code_file": "import torch\nfrom micrograd.engine import Value\n\ndef test_sanity_check():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0])\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    assert ymg.data == ypt.data.item()\n    # backward pass went well\n    assert xmg.grad == xpt.grad.item()\n", "new_code_file": "import torch\nfrom micrograd.engine import Value\n\ndef test_sanity_check():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0]).double()\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    assert ymg.data == ypt.data.item()\n    # backward pass went well\n    assert xmg.grad == xpt.grad.item()\n\ndef test_more_ops():\n\n    a = Value(-4.0)\n    b = Value(2.0)\n    c = a + b\n    d = a * b + b**3\n    c += c + 1\n    c += 1 + c + (-a)\n    d += d * 2 + (b + a).relu()\n    d += 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g += 10.0 / f\n    g.backward()\n    amg, bmg, gmg = a, b, g\n\n    a = torch.Tensor([-4.0]).double()\n    b = torch.Tensor([2.0]).double()\n    a.requires_grad = True\n    b.requires_grad = True\n    c = a + b\n    d = a * b + b**3\n    c = c + c + 1\n    c = c + 1 + c + (-a)\n    d = d + d * 2 + (b + a).relu()\n    d = d + 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g = g + 10.0 / f\n    g.backward()\n    apt, bpt, gpt = a, b, g\n\n    tol = 1e-6\n    # forward pass went well\n    assert abs(gmg.data - gpt.data.item()) < tol\n    # backward pass went well\n    assert abs(amg.grad - apt.grad.item()) < tol\n    assert abs(bmg.grad - bpt.grad.item()) < tol\n", "diff": "@@ -11,7 +11,7 @@ def test_sanity_check():\n     y.backward()\n     xmg, ymg = x, y\n \n-    x = torch.Tensor([-4.0])\n+    x = torch.Tensor([-4.0]).double()\n     x.requires_grad = True\n     z = 2 * x + 2 + x\n     q = z.relu() + z * x\n@@ -24,3 +24,44 @@ def test_sanity_check():\n     assert ymg.data == ypt.data.item()\n     # backward pass went well\n     assert xmg.grad == xpt.grad.item()\n+\n+def test_more_ops():\n+\n+    a = Value(-4.0)\n+    b = Value(2.0)\n+    c = a + b\n+    d = a * b + b**3\n+    c += c + 1\n+    c += 1 + c + (-a)\n+    d += d * 2 + (b + a).relu()\n+    d += 3 * d + (b - a).relu()\n+    e = c - d\n+    f = e**2\n+    g = f / 2.0\n+    g += 10.0 / f\n+    g.backward()\n+    amg, bmg, gmg = a, b, g\n+\n+    a = torch.Tensor([-4.0]).double()\n+    b = torch.Tensor([2.0]).double()\n+    a.requires_grad = True\n+    b.requires_grad = True\n+    c = a + b\n+    d = a * b + b**3\n+    c = c + c + 1\n+    c = c + 1 + c + (-a)\n+    d = d + d * 2 + (b + a).relu()\n+    d = d + 3 * d + (b - a).relu()\n+    e = c - d\n+    f = e**2\n+    g = f / 2.0\n+    g = g + 10.0 / f\n+    g.backward()\n+    apt, bpt, gpt = a, b, g\n+\n+    tol = 1e-6\n+    # forward pass went well\n+    assert abs(gmg.data - gpt.data.item()) < tol\n+    # backward pass went well\n+    assert abs(amg.grad - apt.grad.item()) < tol\n+    assert abs(bmg.grad - bpt.grad.item()) < tol", "status": "modified"}, {"commit_id": "5973e7b362565fbd5fa267285ce1c935e6cbf1e9", "commit_message": "last few silly tweaks, really off to bed now", "relative_path": "test/test_basic.py", "previous_code_file": "import torch\nfrom micrograd.engine import Value\nfrom micrograd import nn\n\ndef test_sanity_check():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0])\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    assert ymg.data == ypt.data.item()\n    # backward pass went well\n    assert xmg.grad == xpt.grad.item()\n", "new_code_file": "import torch\nfrom micrograd.engine import Value\n\ndef test_sanity_check():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0])\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    assert ymg.data == ypt.data.item()\n    # backward pass went well\n    assert xmg.grad == xpt.grad.item()\n", "diff": "@@ -1,6 +1,5 @@\n import torch\n from micrograd.engine import Value\n-from micrograd import nn\n \n def test_sanity_check():\n ", "status": "modified"}, {"commit_id": "ff25789ed7117ae26cda1b7954c7c9256778b75d", "commit_message": "add a super basic test comparing our grad backward pass to that of pytorch", "relative_path": "test/test_basic.py", "previous_code_file": null, "new_code_file": "import torch\nfrom micrograd.engine import Value\nfrom micrograd import nn\n\ndef test_sanity_check():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0])\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    assert ymg.data == ypt.data.item()\n    # backward pass went well\n    assert xmg.grad == xpt.grad.item()\n", "diff": null, "status": "added"}, {"commit_id": "9d268f5f34b09ac75ac2cfa1b0be9c1135ca4115", "commit_message": "add graphviz gracing utility which I think is cute", "relative_path": "micrograd/engine.py", "previous_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=()):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other))\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other))\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,))\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "new_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "diff": "@@ -2,16 +2,17 @@\n class Value:\n     \"\"\" stores a single scalar value and its gradient \"\"\"\n \n-    def __init__(self, data, _children=()):\n+    def __init__(self, data, _children=(), _op=''):\n         self.data = data\n         self.grad = 0\n         # internal variables used for autograd graph construction\n         self._backward = lambda: None\n         self._prev = set(_children)\n+        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n \n     def __add__(self, other):\n         other = other if isinstance(other, Value) else Value(other)\n-        out = Value(self.data + other.data, (self, other))\n+        out = Value(self.data + other.data, (self, other), '+')\n \n         def _backward():\n             self.grad += out.grad\n@@ -22,7 +23,7 @@ def _backward():\n \n     def __mul__(self, other):\n         other = other if isinstance(other, Value) else Value(other)\n-        out = Value(self.data * other.data, (self, other))\n+        out = Value(self.data * other.data, (self, other), '*')\n \n         def _backward():\n             self.grad += other.data * out.grad\n@@ -32,7 +33,7 @@ def _backward():\n         return out\n \n     def relu(self):\n-        out = Value(0 if self.data < 0 else self.data, (self,))\n+        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n \n         def _backward():\n             self.grad += (out.data > 0) * out.grad", "status": "modified"}, {"commit_id": "575bc173ceefce5e1cf8bfac541fc112db02a1cf", "commit_message": "ok much cleaner to use a set here to not have too much state and side effects", "relative_path": "micrograd/engine.py", "previous_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=()):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._visited = False\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other))\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other))\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,))\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        def build_topo(v):\n            if not v._visited:\n                v._visited = True\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "new_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=()):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other))\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other))\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,))\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "diff": "@@ -8,7 +8,6 @@ def __init__(self, data, _children=()):\n         # internal variables used for autograd graph construction\n         self._backward = lambda: None\n         self._prev = set(_children)\n-        self._visited = False\n \n     def __add__(self, other):\n         other = other if isinstance(other, Value) else Value(other)\n@@ -45,9 +44,10 @@ def backward(self):\n \n         # topological order all of the children in the graph\n         topo = []\n+        visited = set()\n         def build_topo(v):\n-            if not v._visited:\n-                v._visited = True\n+            if v not in visited:\n+                visited.add(v)\n                 for child in v._prev:\n                     build_topo(child)\n                 topo.append(v)", "status": "modified"}, {"commit_id": "9fd9cc02fe5a0e57d790b03572f0d7bcd990662d", "commit_message": "fix a bug, end up in a much better spot overall. thank you @sinjax and @evcu for help and contributions on which i drew for this commit", "relative_path": "micrograd/engine.py", "previous_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data):\n        self.data = data\n        self.grad = 0\n        self.backward = lambda: None\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data)\n\n        def backward():\n            self.grad += out.grad\n            other.grad += out.grad\n            self.backward()\n            other.backward()\n        out.backward = backward\n\n        return out\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data)\n\n        def backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n            self.backward()\n            other.backward()\n        out.backward = backward\n\n        return out\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data)\n        def backward():\n            self.grad += (out.data > 0) * out.grad\n            self.backward()\n        out.backward = backward\n        return out\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "new_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=()):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._visited = False\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other))\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other))\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,))\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        def build_topo(v):\n            if not v._visited:\n                v._visited = True\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "diff": "@@ -2,50 +2,67 @@\n class Value:\n     \"\"\" stores a single scalar value and its gradient \"\"\"\n \n-    def __init__(self, data):\n+    def __init__(self, data, _children=()):\n         self.data = data\n         self.grad = 0\n-        self.backward = lambda: None\n+        # internal variables used for autograd graph construction\n+        self._backward = lambda: None\n+        self._prev = set(_children)\n+        self._visited = False\n \n     def __add__(self, other):\n         other = other if isinstance(other, Value) else Value(other)\n-        out = Value(self.data + other.data)\n+        out = Value(self.data + other.data, (self, other))\n \n-        def backward():\n+        def _backward():\n             self.grad += out.grad\n             other.grad += out.grad\n-            self.backward()\n-            other.backward()\n-        out.backward = backward\n+        out._backward = _backward\n \n         return out\n \n-    def __radd__(self, other):\n-        return self.__add__(other)\n-\n     def __mul__(self, other):\n         other = other if isinstance(other, Value) else Value(other)\n-        out = Value(self.data * other.data)\n+        out = Value(self.data * other.data, (self, other))\n \n-        def backward():\n+        def _backward():\n             self.grad += other.data * out.grad\n             other.grad += self.data * out.grad\n-            self.backward()\n-            other.backward()\n-        out.backward = backward\n+        out._backward = _backward\n \n         return out\n \n-    def __rmul__(self, other):\n-        return self.__mul__(other)\n-\n     def relu(self):\n-        out = Value(0 if self.data < 0 else self.data)\n-        def backward():\n+        out = Value(0 if self.data < 0 else self.data, (self,))\n+\n+        def _backward():\n             self.grad += (out.data > 0) * out.grad\n-            self.backward()\n-        out.backward = backward\n+        out._backward = _backward\n+\n         return out\n \n+    def backward(self):\n+\n+        # topological order all of the children in the graph\n+        topo = []\n+        def build_topo(v):\n+            if not v._visited:\n+                v._visited = True\n+                for child in v._prev:\n+                    build_topo(child)\n+                topo.append(v)\n+        build_topo(self)\n+\n+        # go one variable at a time and apply the chain rule to get its gradient\n+        self.grad = 1\n+        for v in reversed(topo):\n+            v._backward()\n+\n+    def __radd__(self, other):\n+        return self.__add__(other)\n+\n+    def __rmul__(self, other):\n+        return self.__mul__(other)\n+\n     def __repr__(self):\n         return f\"Value(data={self.data}, grad={self.grad})\"", "status": "modified"}, {"commit_id": "7e8c3cd01464132d026fac910e9f39ad4ef97e60", "commit_message": "fix typo in neuron call from back when i was debugging, use generator there is no need to construct a list here", "relative_path": "micrograd/nn.py", "previous_code_file": "import random\nfrom micrograd.engine import Value\n\nclass Module:\n    def zero_grad(self):\n        for p in self.parameters():\n            p.grad = 0\n\nclass Neuron(Module):\n\n    def __init__(self, nin, nonlin=True):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(0)\n        self.nonlin = nonlin\n\n    def __call__(self, x):\n        act = sum([wi*xi for wi,xi in zip(self.w, x)], self.b)\n        return act.relu() if self.nonlin else act\n\n    def parameters(self):\n        return self.w + [self.b]\n\n    def __repr__(self):\n        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n\nclass Layer(Module):\n\n    def __init__(self, nin, nout, **kwargs):\n        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n    def __repr__(self):\n        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n\nclass MLP(Module):\n\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n    def __repr__(self):\n        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n", "new_code_file": "import random\nfrom micrograd.engine import Value\n\nclass Module:\n\n    def zero_grad(self):\n        for p in self.parameters():\n            p.grad = 0\n\n    def parameters(self):\n        return []\n\nclass Neuron(Module):\n\n    def __init__(self, nin, nonlin=True):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(0)\n        self.nonlin = nonlin\n\n    def __call__(self, x):\n        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n        return act.relu() if self.nonlin else act\n\n    def parameters(self):\n        return self.w + [self.b]\n\n    def __repr__(self):\n        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n\nclass Layer(Module):\n\n    def __init__(self, nin, nout, **kwargs):\n        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n    def __repr__(self):\n        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n\nclass MLP(Module):\n\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n    def __repr__(self):\n        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n", "diff": "@@ -2,10 +2,14 @@\n from micrograd.engine import Value\n \n class Module:\n+\n     def zero_grad(self):\n         for p in self.parameters():\n             p.grad = 0\n \n+    def parameters(self):\n+        return []\n+\n class Neuron(Module):\n \n     def __init__(self, nin, nonlin=True):\n@@ -14,7 +18,7 @@ def __init__(self, nin, nonlin=True):\n         self.nonlin = nonlin\n \n     def __call__(self, x):\n-        act = sum([wi*xi for wi,xi in zip(self.w, x)], self.b)\n+        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n         return act.relu() if self.nonlin else act\n \n     def parameters(self):", "status": "modified"}, {"commit_id": "0401485fe6922d879706f38a1b36f4a916716673", "commit_message": "haha", "relative_path": "micrograd/__init__.py", "previous_code_file": null, "new_code_file": "", "diff": null, "status": "added"}, {"commit_id": "0401485fe6922d879706f38a1b36f4a916716673", "commit_message": "haha", "relative_path": "micrograd/engine.py", "previous_code_file": null, "new_code_file": "\nclass Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data):\n        self.data = data\n        self.grad = 0\n        self.backward = lambda: None\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data)\n\n        def backward():\n            self.grad += out.grad\n            other.grad += out.grad\n            self.backward()\n            other.backward()\n        out.backward = backward\n\n        return out\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data)\n\n        def backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n            self.backward()\n            other.backward()\n        out.backward = backward\n\n        return out\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data)\n        def backward():\n            self.grad += (out.data > 0) * out.grad\n            self.backward()\n        out.backward = backward\n        return out\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "diff": null, "status": "added"}, {"commit_id": "0401485fe6922d879706f38a1b36f4a916716673", "commit_message": "haha", "relative_path": "micrograd/nn.py", "previous_code_file": null, "new_code_file": "import random\nfrom micrograd.engine import Value\n\nclass Module:\n    def zero_grad(self):\n        for p in self.parameters():\n            p.grad = 0\n\nclass Neuron(Module):\n\n    def __init__(self, nin, nonlin=True):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(0)\n        self.nonlin = nonlin\n\n    def __call__(self, x):\n        act = sum([wi*xi for wi,xi in zip(self.w, x)], self.b)\n        return act.relu() if self.nonlin else act\n\n    def parameters(self):\n        return self.w + [self.b]\n\n    def __repr__(self):\n        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n\nclass Layer(Module):\n\n    def __init__(self, nin, nout, **kwargs):\n        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n    def __repr__(self):\n        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n\nclass MLP(Module):\n\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n    def __repr__(self):\n        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n", "diff": null, "status": "added"}]