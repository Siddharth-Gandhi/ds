{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9ERgBpbcMmB"
   },
   "source": [
    "# HW1: Frame-Level Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLkH6GMGcWcE"
   },
   "source": [
    "In this homework, you will be working with MFCC data consisting of 27 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4vZbDmJvMp1"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qI4qfx7tiBZt",
    "outputId": "ef79a3fc-5689-4e5a-d896-329b8a9d6a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchsummaryX import summary\n",
    "import sklearn\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import wandb\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# print gpu name\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8yBgXjKV1O0Z"
   },
   "outputs": [],
   "source": [
    "### If you are using colab, you can import google drive to save model checkpoints in a folder\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "N-9qE20hmCgQ"
   },
   "outputs": [],
   "source": [
    "### PHONEME LIST\n",
    "PHONEMES = [\n",
    "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
    "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
    "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
    "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
    "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
    "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIi0Big7vPa9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBCbeRhixGM7"
   },
   "source": [
    "This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "if2Somqfbje1"
   },
   "outputs": [],
   "source": [
    "# # commands to download data from kaggle\n",
    "# !kaggle competitions download -c 11785-hw1p2-s24\n",
    "\n",
    "# !unzip -qo /content/11785-hw1p2-s24.zip -d '/content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vuzce0_TdcaR"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_7QgMbBdgPp"
   },
   "source": [
    "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
    "\n",
    "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aside():\n",
    "    ph_dict = defaultdict(int)\n",
    "    def count(obj):\n",
    "        # count = 0\n",
    "        for ph in obj:\n",
    "            ph_dict[ph] += 1\n",
    "        #     if ph == '[SIL]':\n",
    "        #         count += 1\n",
    "        # return count\n",
    "\n",
    "    path = './content/11-785-s24-hw1p2/dev-clean/transcript'\n",
    "    # total_sil = 0\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        tmp = os.path.join(path, file)\n",
    "        obj = np.load(tmp)\n",
    "        count(obj)\n",
    "        # total_sil += count_sil(obj)\n",
    "\n",
    "    # first 10 phonemes\n",
    "    print(ph_dict)\n",
    "    sorted(ph_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# aside()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YpLCvi3AJC5z"
   },
   "outputs": [],
   "source": [
    "# Dataset class to load train and validation data\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n",
    "\n",
    "        self.context    = context\n",
    "        self.phonemes   = phonemes\n",
    "\n",
    "        # MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
    "        self.mfcc_dir       = os.path.join(root, partition, 'mfcc')\n",
    "        # Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n",
    "        self.transcript_dir = os.path.join(root, partition, 'transcript')\n",
    "\n",
    "        # List files in sefl.mfcc_dir using os.listdir in sorted order\n",
    "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
    "        # List files in self.transcript_dir using os.listdir in sorted order\n",
    "        transcript_names    = sorted(os.listdir(self.transcript_dir))\n",
    "\n",
    "        # Making sure that we have the same no. of mfcc and transcripts\n",
    "        assert len(mfcc_names) == len(transcript_names)\n",
    "\n",
    "        self.mfccs, self.transcripts = [], []\n",
    "\n",
    "        # Iterate through mfccs and transcripts\n",
    "        for i in range(len(mfcc_names)):\n",
    "        #   Load a single mfcc\n",
    "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n",
    "        #   TODO: Do Cepstral Normalization of mfcc (explained in writeup)\n",
    "            mfcc_mean = np.mean(mfcc, axis=0, keepdims=True)\n",
    "            mfcc_std = np.std(mfcc, axis=0, keepdims=True)\n",
    "            mfcc = (mfcc - mfcc_mean) / (mfcc_std + 1e-10)\n",
    "\n",
    "        #   Load the corresponding transcript\n",
    "            transcript  = np.load(os.path.join(self.transcript_dir, transcript_names[i]))\n",
    "            # Remove [SOS] and [EOS] from the transcript\n",
    "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
    "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
    "            transcript = transcript[1:-1]\n",
    "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
    "            self.mfccs.append(mfcc)\n",
    "            self.transcripts.append(transcript)\n",
    "\n",
    "        # NOTE:\n",
    "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
    "        # Each transcript is of shape (T1+2), (T2+2) before removing [SOS] and [EOS]\n",
    "\n",
    "        # the final shape is T x 27 (Where T = T1 + T2 + ...)\n",
    "        self.mfccs          = np.concatenate(self.mfccs, axis=0)\n",
    "\n",
    "        # the final shape is (T,) meaning, each time step has one phoneme output\n",
    "        self.transcripts    = np.concatenate(self.transcripts, axis=0)\n",
    "\n",
    "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
    "        self.length = len(self.mfccs)\n",
    "\n",
    "        # Take some time to think about what we have done.\n",
    "        # self.mfcc is an array of the format (Frames x Features).\n",
    "        # Our goal is to recognize phonemes of each frame\n",
    "        # From hw0, you will be knowing what context is.\n",
    "        # We can introduce context by padding zeros on top and bottom of self.mfcc\n",
    "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=0)\n",
    "\n",
    "        # The available phonemes in the transcript are of string data type\n",
    "        # But the neural network cannot predict strings as such.\n",
    "        # Hence, we map these phonemes to integers\n",
    "\n",
    "        self.transcripts = np.array([self.phonemes.index(ph) for ph in self.transcripts])\n",
    "        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "\n",
    "        frames = self.mfccs[ind:ind+2*self.context+1]\n",
    "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
    "        frames = np.ndarray.flatten(frames)\n",
    "\n",
    "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
    "        phonemes    = torch.tensor(self.transcripts[ind])\n",
    "\n",
    "        return frames, phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "e8KfVP39S6o7"
   },
   "outputs": [],
   "source": [
    "class AudioTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"test-clean\"): # Feel free to add more arguments\n",
    "\n",
    "        self.context    = context\n",
    "        self.phonemes   = phonemes\n",
    "\n",
    "        self.mfcc_dir       = os.path.join(root, partition, 'mfcc')\n",
    "\n",
    "        # List files in sefl.mfcc_dir using os.listdir in sorted order\n",
    "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
    "\n",
    "\n",
    "        self.mfccs = []\n",
    "\n",
    "        # Iterate through mfccs and transcripts\n",
    "        for i in range(len(mfcc_names)):\n",
    "        #   Load a single mfcc\n",
    "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n",
    "        #   TODO: Do Cepstral Normalization of mfcc (explained in writeup)\n",
    "            mfcc_mean = np.mean(mfcc, axis=0, keepdims=True)\n",
    "            mfcc_std = np.std(mfcc, axis=0, keepdims=True)\n",
    "            mfcc = (mfcc - mfcc_mean) / (mfcc_std + 1e-10)\n",
    "            self.mfccs.append(mfcc)\n",
    "\n",
    "        # NOTE:\n",
    "        self.mfccs          = np.concatenate(self.mfccs, axis=0)\n",
    "\n",
    "        self.length = len(self.mfccs)\n",
    "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        frames = self.mfccs[ind:ind+2*self.context+1]\n",
    "        frames = np.ndarray.flatten(frames)\n",
    "\n",
    "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
    "\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNacQ8bpt9nw"
   },
   "source": [
    "# Parameters Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE7tsinAuLNy"
   },
   "source": [
    "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "PmKwlFqgt_Zq"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epochs'        : 10,\n",
    "    'batch_size'    : 2048,\n",
    "    'context'       : 50,\n",
    "    'init_lr'       : 2e-3,\n",
    "    'dropout_rate'  : 0.3,\n",
    "    'architecture'  : 'high-cutoff'\n",
    "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mlwaKlDt_2c"
   },
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "7xi7V8x8W9z4"
   },
   "outputs": [],
   "source": [
    "root = './content/11-785-s24-hw1p2'\n",
    "\n",
    "train_data = AudioDataset(root=root, partition='train-clean-100', phonemes=PHONEMES, context=config['context'])\n",
    "\n",
    "val_data = AudioDataset(root=root, partition='dev-clean', phonemes=PHONEMES, context=config['context'])\n",
    "\n",
    "test_data = AudioTestDataset(root=root, partition='test-clean', phonemes=PHONEMES, context=config['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "4mzoYfTKu14s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size     :  2048\n",
      "Context        :  50\n",
      "Input size     :  2727\n",
      "Output symbols :  42\n",
      "Train dataset samples = 36091157, batches = 17623\n",
      "Validation dataset samples = 1928204, batches = 942\n",
      "Test dataset samples = 1934138, batches = 945\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders for train, val and test datasets\n",
    "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
    "# We shuffle train dataloader but not val & test dataloader. Why?\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    num_workers = 8,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    num_workers = 4,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = test_data,\n",
    "    num_workers = 4,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Batch size     : \", config['batch_size'])\n",
    "print(\"Context        : \", config['context'])\n",
    "print(\"Input size     : \", (2*config['context']+1)*27)\n",
    "print(\"Output symbols : \", len(PHONEMES))\n",
    "\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "n-GV3UvgLSoF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 2727]) torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# Testing code to check if your data loaders are working\n",
    "for i, data in enumerate(train_loader):\n",
    "    frames, phoneme = data\n",
    "    print(frames.shape, phoneme.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxjwve20JRJ2"
   },
   "source": [
    "# Network Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NJzT-mRw6iy"
   },
   "source": [
    "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "OvcpontXQq9j"
   },
   "outputs": [],
   "source": [
    "# This architecture will make you cross the very low cutoff\n",
    "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(config['dropout_rate']),\n",
    "            torch.nn.Linear(512, 1024),\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(config['dropout_rate']),\n",
    "            torch.nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HejoSXe3vMVU"
   },
   "source": [
    "# Define Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAhGBH7-xxth"
   },
   "source": [
    "Here we define the model, loss function, optimizer and optionally a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "_qtrEM1ZvLje"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Linear                 [2727, 512]          [2048, 512]             1,396.74                 1.40\n",
      "1_BatchNorm1d                  [512]          [2048, 512]                 1.02                 0.00\n",
      "2_LeakyReLU                        -          [2048, 512]                    -                    -\n",
      "3_Dropout                          -          [2048, 512]                    -                    -\n",
      "4_Linear                 [512, 1024]         [2048, 1024]               525.31                 0.52\n",
      "5_BatchNorm1d                 [1024]         [2048, 1024]                 2.05                 0.00\n",
      "6_LeakyReLU                        -         [2048, 1024]                    -                    -\n",
      "7_Linear                 [1024, 512]          [2048, 512]               524.80                 0.52\n",
      "8_BatchNorm1d                  [512]          [2048, 512]                 1.02                 0.00\n",
      "9_LeakyReLU                        -          [2048, 512]                    -                    -\n",
      "10_Dropout                         -          [2048, 512]                    -                    -\n",
      "11_Linear                  [512, 42]           [2048, 42]                21.55                 0.02\n",
      "====================================================================================================\n",
      "# Params:    2,472.49K\n",
      "# Mult-Adds: 2.47M\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE  = (2*config['context'] + 1) * 27 # Why is this the case?\n",
    "model       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\n",
    "summary(model, frames.to(device))\n",
    "# Check number of parameters of your network\n",
    "# Remember, you are limited to 24 million parameters for HW1 (including ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "UROGEVJevKD-"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n",
    "# We use CE because the task is multi-class classification\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= config['init_lr']) #Defining Optimizer\n",
    "# Recommended : Define Scheduler for Learning Rate,\n",
    "# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n",
    "# You can refer to Pytorch documentation for more information on how to use them.\n",
    "\n",
    "# Is your training time very high?\n",
    "# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n",
    "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwunYpyugFg"
   },
   "source": [
    "# Training and Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JgeNhx4x2-P"
   },
   "source": [
    "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "XblOHEVtKab2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6177"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "8wjPz7DHqKcL"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "\n",
    "    model.train()\n",
    "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
    "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (frames, phonemes) in enumerate(dataloader):\n",
    "\n",
    "        ### Initialize Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ### Move Data to Device (Ideally GPU)\n",
    "        frames      = frames.to(device)\n",
    "        phonemes    = phonemes.to(device)\n",
    "\n",
    "        ### Forward Propagation\n",
    "        logits  = model(frames)\n",
    "\n",
    "        ### Loss Calculation\n",
    "        loss    = criterion(logits, phonemes)\n",
    "\n",
    "        ### Backward Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        ### Gradient Descent\n",
    "        optimizer.step()\n",
    "\n",
    "        tloss   += loss.item()\n",
    "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        ### Release memory\n",
    "        del frames, phonemes, logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    tloss   /= len(train_loader)\n",
    "    tacc    /= len(train_loader)\n",
    "\n",
    "    return tloss, tacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "Q5npQNFH315V"
   },
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
    "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    for i, (frames, phonemes) in enumerate(dataloader):\n",
    "\n",
    "        ### Move data to device (ideally GPU)\n",
    "        frames      = frames.to(device)\n",
    "        phonemes    = phonemes.to(device)\n",
    "\n",
    "        # makes sure that there are no gradients computed as we are not training the model now\n",
    "        with torch.inference_mode():\n",
    "            ### Forward Propagation\n",
    "            logits  = model(frames)\n",
    "            ### Loss Calculation\n",
    "            loss    = criterion(logits, phonemes)\n",
    "\n",
    "        vloss   += loss.item()\n",
    "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
    "\n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        ### Release memory\n",
    "        del frames, phonemes, logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss   /= len(val_loader)\n",
    "    vacc    /= len(val_loader)\n",
    "\n",
    "    return vloss, vacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMd_XxPku5qp"
   },
   "source": [
    "# Weights and Biases Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjIbhR1wwbgI"
   },
   "source": [
    "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n",
    "\n",
    "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCDYx5VEu6qI"
   },
   "outputs": [],
   "source": [
    "wandb.login(key=\"5508720f47b02cabd61bb6acd61dc553d313b062\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "xvUnYd3Bw2up"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ifh9mmcm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">next</strong> at: <a href='https://wandb.ai/team-ssgandhi1/hw1p2/runs/ifh9mmcm' target=\"_blank\">https://wandb.ai/team-ssgandhi1/hw1p2/runs/ifh9mmcm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240124_003919-ifh9mmcm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ifh9mmcm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c170bbd56904f0ca21a998cde38a59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111292602080438, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/bos/tmp2/ssg2/idl/hw1/wandb/run-20240124_011634-o0do3ou4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/team-ssgandhi1/hw1p2/runs/o0do3ou4' target=\"_blank\">high-pyramid</a></strong> to <a href='https://wandb.ai/team-ssgandhi1/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/team-ssgandhi1/hw1p2' target=\"_blank\">https://wandb.ai/team-ssgandhi1/hw1p2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/team-ssgandhi1/hw1p2/runs/o0do3ou4' target=\"_blank\">https://wandb.ai/team-ssgandhi1/hw1p2/runs/o0do3ou4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name    = \"high-pyramid\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
    "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw1p2\", ### Project should be created in your wandb account\n",
    "    config  = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "wft15E_IxYFi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bos/tmp2/ssg2/idl/hw1/wandb/run-20240124_011634-o0do3ou4/files/model_arch.txt']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save your model architecture as a string with str(model)\n",
    "model_arch  = str(model)\n",
    "\n",
    "### Save it in a txt file\n",
    "arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "file_write  = arch_file.write(model_arch)\n",
    "arch_file.close()\n",
    "\n",
    "### log it in your wandb run with wandb.save()\n",
    "wandb.save('model_arch.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nclx_04fu7Dd"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdLMWfEpyGOB"
   },
   "source": [
    "Now, it is time to finally run your ablations! Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "MG4F77Nm0Am9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/17623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 76.2268%\tTrain Loss 0.7317\t Learning Rate 0.0020000\n",
      "\tVal Acc 78.6004%\tVal Loss 0.6494\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/17623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 76.4508%\tTrain Loss 0.7242\t Learning Rate 0.0020000\n",
      "\tVal Acc 78.8051%\tVal Loss 0.6434\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/17623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 76.6413%\tTrain Loss 0.7182\t Learning Rate 0.0020000\n",
      "\tVal Acc 78.9916%\tVal Loss 0.6381\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/17623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 76.7881%\tTrain Loss 0.7131\t Learning Rate 0.0020000\n",
      "\tVal Acc 79.0957%\tVal Loss 0.6337\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c002bcee2034adaa31dcb3c637cc698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/17623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-64:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/ssg2/miniconda3/envs/idl/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     11\u001b[0m curr_lr                 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m train_loss, train_acc   \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m val_loss, val_acc       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, val_loader)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain Acc \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain Loss \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Learning Rate \u001b[39m\u001b[38;5;132;01m{:.07f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, train_loss, curr_lr)) \n",
      "Cell \u001b[0;32mIn[86], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_bar   \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (frames, phonemes) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m### Initialize Gradients\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m### Move Data to Device (Ideally GPU)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     frames      \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.8/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.8/site-packages/torch/optim/optimizer.py:803\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.8/site-packages/torch/autograd/profiler.py:631\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.8/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over number of epochs to train and evaluate your model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc       = eval(model, val_loader)\n",
    "\n",
    "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr)) \n",
    "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
    "\n",
    "    ### Log metrics at each epoch in your run\n",
    "    # Optionally, you can log at each batch inside train/eval functions\n",
    "    # (explore wandb documentation/wandb recitation)\n",
    "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
    "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
    "\n",
    "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
    "    if val_acc > best_acc:\n",
    "        torch.save(model.state_dict(), 'model_best_val.pth')\n",
    "        best_acc = val_acc\n",
    "        # wandb.save('model.pth')\n",
    "\n",
    "### Finish your wandb run\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_recent.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kXwf5YUo_4A"
   },
   "source": [
    "# Testing and submission to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI1hSFYLpJvH"
   },
   "source": [
    "Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "R-SU9fZ3xHtk"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval() # This is used for evaluation\n",
    "\n",
    "    # List to store predicted phonemes of test data\n",
    "    test_predictions = []\n",
    "\n",
    "    # No gradient calculation needed during inference\n",
    "    with torch.no_grad(): # Use torch.no_grad()\n",
    "\n",
    "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
    "\n",
    "            mfccs = mfccs.to(device)\n",
    "\n",
    "            logits = model(mfccs)\n",
    "\n",
    "            # Get most likely predicted phoneme with argmax\n",
    "            predicted_phonemes = logits.argmax(dim=1)\n",
    "\n",
    "            # Store predicted_phonemes in test_predictions\n",
    "            test_predictions.extend(predicted_phonemes.cpu().numpy())\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "wG9v6Xmxu7wp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3b3b6c3b2468ca471146b69239ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1934138"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "ZE1hRnvf0bFz"
   },
   "outputs": [],
   "source": [
    "### Create CSV file with predictions\n",
    "with open(\"./submission.csv\", \"w+\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(\"{},{}\\n\".format(i, PHONEMES[predictions[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjcammuCxMKN"
   },
   "outputs": [],
   "source": [
    "### Submit to kaggle competition using kaggle API (Uncomment below to use)\n",
    "# !kaggle competitions submit -c 11785-hw1p2-s24 -f ./submission.csv -m \"Test Submission\"\n",
    "\n",
    "### However, its always safer to download the csv file and then upload to kaggle"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
