{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:01.723338335Z",
     "start_time": "2023-11-27T04:01:01.497195817Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:04.805496544Z",
     "start_time": "2023-11-27T04:01:04.581595287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "from utils import (\n",
    "    AggregatedSearchResult,\n",
    "    get_combined_df,\n",
    "    prepare_triplet_data_from_df,\n",
    "    sanity_check_triplets,\n",
    "    set_seed,\n",
    "    tokenize\n",
    ")\n",
    "from BERTReranker_v4 import BERTReranker\n",
    "# set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:21.300426891Z",
     "start_time": "2023-11-27T04:01:21.229196368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  1\n",
      "Current cuda device:  0\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "# print torch devices available\n",
    "print('Available devices: ', torch.cuda.device_count())\n",
    "print('Current cuda device: ', torch.cuda.current_device())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='smalldata/ftr/index_commit_tokenized', repo_path='smalldata/ftr', k=1000, n=100, no_bm25=True, model_path='microsoft/graphcodebert-base', overwrite_cache=False, batch_size=32, num_epochs=10, learning_rate=5e-05, num_positives=10, num_negatives=10, train_depth=1000, num_workers=8, train_commits=1500, psg_cnt=5, aggregation_strategy='sump', use_gpu=True, rerank_depth=250, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4', overwrite_eval=False, sanity_check_triplets=False, debug=False, eval_before_training=False, do_combined_train=False, repo_paths=None, best_model_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']\n",
    "repo_path = args.repo_path\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at smalldata/ftr/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 output path: smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt\n",
      "Output file smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt already exists, set overwrite_eval flag to False, skipping...\n",
      "Model Name: BM25Searcher\n",
      "Sample Size: 100\n",
      "Evaluation Metrics:\n",
      "MAP: 0.1542\n",
      "P@10: 0.087\n",
      "P@100: 0.0267\n",
      "P@1000: 0.0041\n",
      "MRR: 0.2133\n",
      "Recall@100: 0.5077\n",
      "Recall@1000: 0.6845\n",
      "\n",
      "BM25 Baseline Evaluation\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "bm25_output_path = os.path.join(eval_path, f'bm25_baseline_N{n}_K{K}_metrics.txt')\n",
    "print(f'BM25 output path: {bm25_output_path}')\n",
    "\n",
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=bm25_output_path, aggregation_strategy=BM25_AGGR_STRAT)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': args.psg_cnt,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': args.rerank_depth,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': BM25_AGGR_STRAT,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Number of commits after midpoint date: 5804\n",
      "Number of commits after filtering by commit message length: 1543\n",
      "Number of commits after sampling: 1500\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "print('Preparing training data...')\n",
    "# Step 1: Filter out only the columns we need\n",
    "filtered_df = combined_df[['commit_date', 'commit_message', 'commit_id', 'file_path', 'diff']]\n",
    "\n",
    "# Step 2: Group by commit_id\n",
    "grouped_df = filtered_df.groupby(['commit_id', 'commit_date', 'commit_message'])['file_path'].apply(list).reset_index()\n",
    "grouped_df.rename(columns={'file_path': 'actual_files_modified'}, inplace=True)\n",
    "\n",
    "# Step 3: Determine midpoint and filter dataframe\n",
    "midpoint_date = np.median(grouped_df['commit_date'])\n",
    "recent_df = grouped_df[grouped_df['commit_date'] > midpoint_date]\n",
    "print(f'Number of commits after midpoint date: {len(recent_df)}')\n",
    "\n",
    "# Step 4: Filter out commits with less than average length commit messages\n",
    "average_commit_len = recent_df['commit_message'].str.split().str.len().mean()\n",
    "# filter out commits with less than average length\n",
    "recent_df = recent_df[recent_df['commit_message'].str.split().str.len() > average_commit_len] # type: ignore\n",
    "print(f'Number of commits after filtering by commit message length: {len(recent_df)}')\n",
    "\n",
    "# Step 5: randomly sample 1500 rows from recent_df\n",
    "recent_df = recent_df.sample(params['train_commits'])\n",
    "print(f'Number of commits after sampling: {len(recent_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data first\n",
    "if not os.path.exists(os.path.join(repo_path, 'cache')):\n",
    "    os.makedirs(os.path.join(repo_path, 'cache'))\n",
    "triplet_cache = os.path.join(repo_path, 'cache', 'triplet_data_cache.pkl')\n",
    "diff_cache = os.path.join(repo_path, 'cache', 'diff_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73765 entries, 0 to 73764\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   owner                  73765 non-null  string\n",
      " 1   repo_name              73765 non-null  string\n",
      " 2   commit_date            73765 non-null  int64 \n",
      " 3   commit_id              73765 non-null  string\n",
      " 4   commit_message         73765 non-null  string\n",
      " 5   file_path              73765 non-null  string\n",
      " 6   previous_commit_id     73765 non-null  string\n",
      " 7   previous_file_content  73765 non-null  string\n",
      " 8   cur_file_content       73765 non-null  string\n",
      " 9   diff                   58037 non-null  string\n",
      " 10  status                 73765 non-null  object\n",
      " 11  is_merge_request       73765 non-null  bool  \n",
      " 12  file_extension         73765 non-null  object\n",
      "dtypes: bool(1), int64(1), object(2), string(9)\n",
      "memory usage: 6.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in commit message (whitespace): 60.66661695926252\n",
      "Average number of words in commit message (AutoTokenizer): 105.2017759099844\n",
      "Approx number of tokens passed to bert: 315.6053277299532\n",
      "Approx number of tokens remaining for code: 196.39467227004678\n",
      "Average number of code tokens in diff column: 775.7933559625756\n"
     ]
    }
   ],
   "source": [
    "def aside():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "    def tokenize(x):\n",
    "        # tokenize with no max length\n",
    "        return tokenizer.encode(x, add_special_tokens=True, truncation=False, max_length=None)\n",
    "    combined_df.info()\n",
    "    # print the average number of words in commit_message column\n",
    "\n",
    "    # sample 100 rows from combined_df\n",
    "    # sample_df = combined_df.sample(100, random_state=52)\n",
    "    sample_df = combined_df\n",
    "\n",
    "    avg_words = sample_df['commit_message'].str.split().str.len().mean()\n",
    "    print(f'Average number of words in commit message (whitespace): {avg_words}')\n",
    "    avg_words = sample_df['commit_message'].apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of words in commit message (AutoTokenizer): {avg_words}')\n",
    "\n",
    "    # print approx number of tokens in passed to bert which is 2 * avg_words * 1.5\n",
    "    approx_tokens = 2 * avg_words * 1.5\n",
    "    print(f'Approx number of tokens passed to bert: {approx_tokens}')\n",
    "\n",
    "    # print remaining number of tokens in bert (max is 512)\n",
    "    print(f'Approx number of tokens remaining for code: {512 - approx_tokens}')\n",
    "\n",
    "    # print average number of code tokens in diff column by using tokenize function but only on the non-null diff values\n",
    "    avg_code_tokens = sample_df['diff'].dropna().apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of code tokens in diff column: {avg_code_tokens}')\n",
    "\n",
    "aside()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
    "Average number of words in commit message (whitespace): 60.66661695926252\n",
    "Average number of words in commit message (AutoTokenizer): 105.2017759099844\n",
    "Approx number of tokens passed to bert: 315.6053277299532\n",
    "Approx number of tokens remaining for code: 196.39467227004678\n",
    "Average number of code tokens in diff column: 775.7933559625756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data from dataframe of size: 2 with search_depth: 1000\n",
      "Percentage of positives: 0.2, Percentage of negatives: 0.8\n"
     ]
    }
   ],
   "source": [
    "# def temp_prep(df, searcher, search_depth, num_positives, num_negatives):\n",
    "\n",
    "#     data = []\n",
    "#     print(f'Preparing data from dataframe of size: {len(df)} with search_depth: {search_depth}')\n",
    "#     total_positives, total_negatives = 0, 0\n",
    "#     for _, row in df.iterrows():\n",
    "#     # for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "#         cur_positives = 0\n",
    "#         cur_negatives = 0\n",
    "#         pos_commit_ids = set()\n",
    "#         neg_commit_ids = set()\n",
    "#         commit_message = row['commit_message']\n",
    "#         actual_files_modified = row['actual_files_modified']\n",
    "\n",
    "#         agg_search_results = searcher.pipeline(commit_message, row['commit_date'], search_depth, 'sump', aggregate_on='commit')\n",
    "\n",
    "#         # for each agg_result, find out how many files it has edited are in actual_files_modified and sort by score\n",
    "\n",
    "#         for agg_result in agg_search_results:\n",
    "#             agg_result_files = set([result.file_path for result in agg_result.contributing_results])\n",
    "#             intersection = agg_result_files.intersection(actual_files_modified)\n",
    "#             # TODO maybe try this for training\n",
    "#             agg_result.score = len(intersection) / len(agg_result_files) # how focused the commit is\n",
    "#             # agg_result.score = len(intersection)\n",
    "\n",
    "#         agg_search_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "#         # go from top to bottom, first num_positives non-0 scores are positive samples and the next num_negatives are negative samples\n",
    "#         for agg_result in agg_search_results:\n",
    "#             cur_commit_msg = agg_result.contributing_results[0].commit_message\n",
    "#             if cur_positives < num_positives and agg_result.score > 0:\n",
    "#                 # meaning there is at least one file in the agg_result that is in actual_files_modified\n",
    "#                 # pos_commits.append(agg_result)\n",
    "#                 data.append((commit_message, cur_commit_msg, 1))\n",
    "#                 cur_positives += 1\n",
    "#                 pos_commit_ids.add(agg_result.commit_id)\n",
    "#             elif cur_negatives < num_negatives:\n",
    "#                 # neg_commits.append(agg_result)\n",
    "#                 data.append((commit_message, cur_commit_msg, 0))\n",
    "#                 cur_negatives += 1\n",
    "#                 neg_commit_ids.add(agg_result.commit_id)\n",
    "#             if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "#                 break\n",
    "\n",
    "#         assert len(pos_commit_ids.intersection(neg_commit_ids)) == 0, 'Positive and negative commit ids should not intersect'\n",
    "#         # print(f\"Total positives: {cur_positives}, Total negatives: {cur_negatives}\")\n",
    "#         total_positives += cur_positives\n",
    "#         total_negatives += cur_negatives\n",
    "\n",
    "#     # # Write data to cache file\n",
    "#     # with open(cache_file, 'wb') as file:\n",
    "#     #     pickle.dump(data, file)\n",
    "#     #     print(f\"Saved data to cache file: {cache_file}\")\n",
    "\n",
    "\n",
    "#     # print percentage of positives and negatives\n",
    "#     denom = total_positives + total_negatives\n",
    "#     print(f\"Percentage of positives: {total_positives / denom}, Percentage of negatives: {total_negatives / denom}\")\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = recent_df.sample(100, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prep(df, searcher, search_depth, num_positives, num_negatives, use_diff=False):\n",
    "    data = []\n",
    "    diff_data = []\n",
    "    print(f'Preparing data from dataframe of size: {len(df)} with search_depth: {search_depth}')\n",
    "    # for _, row in df.iterrows():\n",
    "    total_positives, total_negatives = 0, 0\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        cur_positives = 0\n",
    "        cur_negatives = 0\n",
    "\n",
    "        cur_diff_positives = 0\n",
    "        cur_diff_negatives = 0\n",
    "        pos_commit_ids = set()\n",
    "        neg_commit_ids = set()\n",
    "        commit_message = row['commit_message']\n",
    "        actual_files_modified = row['actual_files_modified']\n",
    "\n",
    "        agg_search_results = searcher.pipeline(commit_message, row['commit_date'], search_depth, 'sump', aggregate_on='commit')\n",
    "\n",
    "        # for each agg_result, find out how many files it has edited are in actual_files_modified and sort by score\n",
    "\n",
    "        for agg_result in agg_search_results:\n",
    "            agg_result_files = set([result.file_path for result in agg_result.contributing_results])\n",
    "            intersection = agg_result_files.intersection(actual_files_modified)\n",
    "            # TODO maybe try this for training\n",
    "            # agg_result.score = len(intersection) / len(agg_result_files) # how focused the commit is\n",
    "            agg_result.score = len(intersection) / len(agg_result_files) # how focused the commit is\n",
    "            # agg_result.score = math.log(cur_score+1)\n",
    "            # agg_result.score = len(intersection)\n",
    "\n",
    "        agg_search_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        if use_diff:\n",
    "            for agg_result in agg_search_results:\n",
    "                # now we want to get diffs for each file in agg_result which has multiple contributing results (files)\n",
    "                # agg_result.contributing_results is a list of SearchResult objects\n",
    "                # each SearchResult object has a file_path attribute\n",
    "                # just get the first contributing result for now\n",
    "                # TODO: use diff_cnt instead of just the first contributing result\n",
    "                for contributing_result in agg_result.contributing_results:\n",
    "                    # contributing_result = agg_result.contributing_results[0]\n",
    "                    # get the just the file path and commit id\n",
    "                    file_path = contributing_result.file_path\n",
    "                    commit_id = contributing_result.commit_id\n",
    "                    if file_path in actual_files_modified and cur_diff_positives < num_positives:\n",
    "                        # this is a positive sample\n",
    "                        diff_data.append((commit_message, file_path, commit_id, 1))\n",
    "                        cur_diff_positives += 1\n",
    "                    elif file_path not in actual_files_modified and cur_diff_negatives < num_negatives:\n",
    "                        # this is a negative sample\n",
    "                        diff_data.append((commit_message, file_path, commit_id, 0))\n",
    "                        cur_diff_negatives += 1\n",
    "\n",
    "                if cur_diff_positives == num_positives and cur_diff_negatives == num_negatives:\n",
    "                    break\n",
    "\n",
    "\n",
    "        # go from top to bottom, first num_positives non-0 scores are positive samples and the next num_negatives are negative samples\n",
    "        for agg_result in agg_search_results:\n",
    "            cur_commit_msg = agg_result.contributing_results[0].commit_message\n",
    "            if cur_positives < num_positives and agg_result.score > 0:\n",
    "                # meaning there is at least one file in the agg_result that is in actual_files_modified\n",
    "                # pos_commits.append(agg_result)\n",
    "                data.append((commit_message, cur_commit_msg, 1))\n",
    "                cur_positives += 1\n",
    "                pos_commit_ids.add(agg_result.commit_id)\n",
    "            elif cur_negatives < num_negatives:\n",
    "                # neg_commits.append(agg_result)\n",
    "                data.append((commit_message, cur_commit_msg, 0))\n",
    "                cur_negatives += 1\n",
    "                neg_commit_ids.add(agg_result.commit_id)\n",
    "            if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "                break\n",
    "\n",
    "        assert len(pos_commit_ids.intersection(neg_commit_ids)) == 0, 'Positive and negative commit ids should not intersect'\n",
    "        # print(f\"Total positives: {cur_positives}, Total negatives: {cur_negatives}\")\n",
    "        total_positives += cur_positives\n",
    "        total_negatives += cur_negatives\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    data = pd.DataFrame(data, columns=['query', 'passage', 'label'])\n",
    "    diff_data = pd.DataFrame(diff_data, columns=['query', 'file_path', 'commit_id', 'label'])\n",
    "    # print distribution of labels\n",
    "    print(f\"Total positives: {total_positives}, Total negatives: {total_negatives}\")\n",
    "    # print percentage of positives and negatives\n",
    "    denom = total_positives + total_negatives\n",
    "    print(f\"Percentage of positives: {total_positives / denom}, Percentage of negatives: {total_negatives / denom}\")\n",
    "    if use_diff:\n",
    "        return data, diff_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data from dataframe of size: 1500 with search_depth: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 381/1500 [01:39<03:48,  4.91it/s]"
     ]
    }
   ],
   "source": [
    "test_data, diff_data = test_prep(recent_df, bm25_searcher, params['train_depth'], params['num_positives'], params['num_negatives'], use_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23975 entries, 0 to 23974\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      23975 non-null  object\n",
      " 1   file_path  23975 non-null  object\n",
      " 2   commit_id  23975 non-null  object\n",
      " 3   label      23975 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 749.3+ KB\n"
     ]
    }
   ],
   "source": [
    "diff_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54 entries, 0 to 53\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      54 non-null     object\n",
      " 1   file_path  54 non-null     object\n",
      " 2   commit_id  54 non-null     object\n",
      " 3   label      54 non-null     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.8+ KB\n"
     ]
    }
   ],
   "source": [
    "diff_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23975/23975 [04:17<00:00, 93.11it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_diff_data(diff_data, df):\n",
    "    # given diff_data, we want to use commit_id and file_path to get the diff from the df\n",
    "\n",
    "    # first we need to get the diff from the df\n",
    "    # we can use the commit_id and file_path to get the diff\n",
    "    res_df = []\n",
    "    # for _, row in diff_data.iterrows():\n",
    "    for _, row in tqdm(diff_data.iterrows(), total=len(diff_data)):\n",
    "        commit_id = row['commit_id']\n",
    "        file_path = row['file_path']\n",
    "        # get the diff from the df\n",
    "        diff = df[(df['commit_id'] == commit_id) & (df['file_path'] == file_path)]['diff']\n",
    "        # check if diff is NA/NaN\n",
    "        if diff.isnull().values.any():\n",
    "            # if it is, then we can just skip this row\n",
    "            continue\n",
    "        diff = diff.values[0]\n",
    "\n",
    "        res_df.append((commit_id, file_path, row['query'], diff, row['label']))\n",
    "\n",
    "    res_df = pd.DataFrame(res_df, columns=['commit_id', 'file_path', 'query', 'passage', 'label'])\n",
    "    # make query and passage into strings and label into int\n",
    "    res_df['query'] = res_df['query'].astype(str)\n",
    "    res_df['passage'] = res_df['passage'].astype(str)\n",
    "    res_df['label'] = res_df['label'].astype(int)\n",
    "    return res_df\n",
    "\n",
    "processed_diff_data = process_diff_data(diff_data, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20096 entries, 0 to 20095\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   commit_id  20096 non-null  object\n",
      " 1   file_path  20096 non-null  object\n",
      " 2   query      20096 non-null  object\n",
      " 3   passage    20096 non-null  object\n",
      " 4   label      20096 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 785.1+ KB\n"
     ]
    }
   ],
   "source": [
    "processed_diff_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@@ -8,7 +8,10 @@\\n  */\\n \\n // TODO: direct imports like some-package/src/* are bad. Fix me.\\n-import ReactDebugCurrentFiber from 'react-reconciler/src/ReactDebugCurrentFiber';\\n+import {\\n+  getCurrentFiberOwnerNameInDevOrNull,\\n+  getCurrentFiberStackInDevOrNull,\\n+} from 'react-reconciler/src/ReactCurrentFiber';\\n import {registrationNameModules} from 'events/EventPluginRegistry';\\n import warning from 'shared/warning';\\n \\n@@ -45,10 +48,6 @@ import {validateProperties as validateARIAProperties} from '../shared/ReactDOMIn\\n import {validateProperties as validateInputProperties} from '../shared/ReactDOMNullInputValuePropHook';\\n import {validateProperties as validateUnknownProperties} from '../shared/ReactDOMUnknownPropertyHook';\\n \\n-const {\\n-  getCurrentFiberOwnerName,\\n-  getCurrentFiberStackAddendum,\\n-} = ReactDebugCurrentFiber;\\n let didWarnInvalidHydration = false;\\n let didWarnShadyDOM = false;\\n \\n@@ -62,7 +61,7 @@ const HTML = '__html';\\n \\n const {html: HTML_NAMESPACE} = Namespaces;\\n \\n-let getStack = () => '';\\n+let getStackInDevOrNull = () => '';\\n \\n let warnedUnknownTags;\\n let suppressHydrationWarning;\\n@@ -77,7 +76,7 @@ let normalizeMarkupForTextOrAttribute;\\n let normalizeHTML;\\n \\n if (__DEV__) {\\n-  getStack = getCurrentFiberStackAddendum;\\n+  getStackInDevOrNull = getCurrentFiberStackInDevOrNull;\\n \\n   warnedUnknownTags = {\\n     // Chrome is the only major browser not shipping <time>. But as of July\\n@@ -181,7 +180,7 @@ if (__DEV__) {\\n         registrationName,\\n         registrationName,\\n         registrationName,\\n-        getCurrentFiberStackAddendum(),\\n+        getCurrentFiberStackInDevOrNull(),\\n       );\\n     } else {\\n       warning(\\n@@ -189,7 +188,7 @@ if (__DEV__) {\\n         'Expected `%s` listener to be a function, instead got a value of `%s` type.%s',\\n         registrationName,\\n         typeof listener,\\n-        getCurrentFiberStackAddendum(),\\n+        getCurrentFiberStackInDevOrNull(),\\n       );\\n     }\\n   };\\n@@ -267,7 +266,11 @@ function setInitialDOMProperties(\\n         }\\n       }\\n       // Relies on `updateStylesByID` not mutating `styleUpdates`.\\n-      CSSPropertyOperations.setValueForStyles(domElement, nextProp, getStack);\\n+      CSSPropertyOperations.setValueForStyles(\\n+        domElement,\\n+        nextProp,\\n+        getStackInDevOrNull,\\n+      );\\n     } else if (propKey === DANGEROUSLY_SET_INNER_HTML) {\\n       const nextHtml = nextProp ? nextProp[HTML] : undefined;\\n       if (nextHtml != null) {\\n@@ -323,7 +326,11 @@ function updateDOMProperties(\\n     const propKey = updatePayload[i];\\n     const propValue = updatePayload[i + 1];\\n     if (propKey === STYLE) {\\n-      CSSPropertyOperations.setValueForStyles(domElement, propValue, getStack);\\n+      CSSPropertyOperations.setValueForStyles(\\n+        domElement,\\n+        propValue,\\n+        getStackInDevOrNull,\\n+      );\\n     } else if (propKey === DANGEROUSLY_SET_INNER_HTML) {\\n       setInnerHTML(domElement, propValue);\\n     } else if (propKey === CHILDREN) {\\n@@ -442,7 +449,7 @@ export function setInitialProperties(\\n         false,\\n         '%s is using shady DOM. Using shady DOM with React can ' +\\n           'cause things to break subtly.',\\n-        getCurrentFiberOwnerName() || 'A component',\\n+        getCurrentFiberOwnerNameInDevOrNull() || 'A component',\\n       );\\n       didWarnShadyDOM = true;\\n     }\\n@@ -516,7 +523,7 @@ export function setInitialProperties(\\n       props = rawProps;\\n   }\\n \\n-  assertValidProps(tag, props, getStack);\\n+  assertValidProps(tag, props, getStackInDevOrNull);\\n \\n   setInitialDOMProperties(\\n     tag,\\n@@ -604,7 +611,7 @@ export function diffProperties(\\n       break;\\n   }\\n \\n-  assertValidProps(tag, nextProps, getStack);\\n+  assertValidProps(tag, nextProps, getStackInDevOrNull);\\n \\n   let propKey;\\n   let styleName;\\n@@ -834,7 +841,7 @@ export function diffHydratedProperties(\\n         false,\\n         '%s is using shady DOM. Using shady DOM with React can ' +\\n           'cause things to break subtly.',\\n-        getCurrentFiberOwnerName() || 'A component',\\n+        getCurrentFiberOwnerNameInDevOrNull() || 'A component',\\n       );\\n       didWarnShadyDOM = true;\\n     }\\n@@ -895,7 +902,7 @@ export function diffHydratedProperties(\\n       break;\\n   }\\n \\n-  assertValidProps(tag, rawProps, getStack);\\n+  assertValidProps(tag, rawProps, getStackInDevOrNull);\\n \\n   if (__DEV__) {\\n     extraAttributeNames = new Set();\\n\""
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_diff_data.iloc[0]['passage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>query</th>\n",
       "      <th>passage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>566259567773a0af56e6c19da615e6802d73f834</td>\n",
       "      <td>packages/react-dom/src/client/ReactDOMFiberCom...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>@@ -8,7 +8,10 @@\\n  */\\n \\n // TODO: direct im...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>566259567773a0af56e6c19da615e6802d73f834</td>\n",
       "      <td>packages/react-dom/src/client/ReactDOMFiberInp...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>@@ -8,7 +8,10 @@\\n  */\\n \\n // TODO: direct im...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>566259567773a0af56e6c19da615e6802d73f834</td>\n",
       "      <td>packages/react-dom/src/client/ReactDOMFiberSel...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>@@ -8,16 +8,14 @@\\n  */\\n \\n // TODO: direct i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>566259567773a0af56e6c19da615e6802d73f834</td>\n",
       "      <td>packages/react-reconciler/src/ReactCapturedVal...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>@@ -9,7 +9,7 @@\\n \\n import type {Fiber} from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>013b7ad117834cbb99b4fc0a3d08fdb8622597c9</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberWorkLo...</td>\n",
       "      <td>Unify `use` and `renderDidSuspendDelayIfPossib...</td>\n",
       "      <td>@@ -200,13 +200,14 @@ const LegacyUnbatchedCon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  commit_id  \\\n",
       "0  566259567773a0af56e6c19da615e6802d73f834   \n",
       "1  566259567773a0af56e6c19da615e6802d73f834   \n",
       "2  566259567773a0af56e6c19da615e6802d73f834   \n",
       "3  566259567773a0af56e6c19da615e6802d73f834   \n",
       "4  013b7ad117834cbb99b4fc0a3d08fdb8622597c9   \n",
       "\n",
       "                                           file_path  \\\n",
       "0  packages/react-dom/src/client/ReactDOMFiberCom...   \n",
       "1  packages/react-dom/src/client/ReactDOMFiberInp...   \n",
       "2  packages/react-dom/src/client/ReactDOMFiberSel...   \n",
       "3  packages/react-reconciler/src/ReactCapturedVal...   \n",
       "4  packages/react-reconciler/src/ReactFiberWorkLo...   \n",
       "\n",
       "                                               query  \\\n",
       "0  Change warning() to automatically inject the s...   \n",
       "1  Change warning() to automatically inject the s...   \n",
       "2  Change warning() to automatically inject the s...   \n",
       "3  Change warning() to automatically inject the s...   \n",
       "4  Unify `use` and `renderDidSuspendDelayIfPossib...   \n",
       "\n",
       "                                             passage  label  \n",
       "0  @@ -8,7 +8,10 @@\\n  */\\n \\n // TODO: direct im...      1  \n",
       "1  @@ -8,7 +8,10 @@\\n  */\\n \\n // TODO: direct im...      1  \n",
       "2  @@ -8,16 +8,14 @@\\n  */\\n \\n // TODO: direct i...      1  \n",
       "3  @@ -9,7 +9,7 @@\\n \\n import type {Fiber} from ...      0  \n",
       "4  @@ -200,13 +200,14 @@ const LegacyUnbatchedCon...      1  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_diff_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.missing.NAType"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find number of rows where passage has <NA>\\nName in it\n",
    "combined_df[combined_df['commit_id'] == '1e3383a41154cb32d8d6b78b2451ee4dabfcb973' & ('packages/react-devtools-shared/src/__tests__' in combined_df['file_path'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_diff_data.head(1000)\n",
    "processed_diff_data.to_parquet(os.path.join(repo_path, 'cache', 'diff_data.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet_data = prepare_triplet_data_from_df(recent_df, bm25_searcher, search_depth=params['train_depth'], num_positives=params['num_positives'], num_negatives=params['num_negatives'], cache_file=triplet_cache, overwrite=args.overwrite_cache)\n",
    "\n",
    "# # triplet_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20096 entries, 0 to 20095\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   query    20096 non-null  object\n",
      " 1   passage  20096 non-null  object\n",
      " 2   label    20096 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 471.1+ KB\n"
     ]
    }
   ],
   "source": [
    "diff_data = pd.read_parquet(os.path.join(repo_path, 'cache', 'diff_data.parquet'))\n",
    "# drop columns that we don't need aka commit_id and file_path\n",
    "diff_data.drop(columns=['commit_id', 'file_path'], inplace=True)\n",
    "diff_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    11877\n",
       "1     8219\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see distribution of labels\n",
    "diff_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@@ -8,7 +8,10 @@\\n  */\\n \\n // TODO: direct imports like some-package/src/* are bad. Fix me.\\n-import ReactDebugCurrentFiber from 'react-reconciler/src/ReactDebugCurrentFiber';\\n+import {\\n+  getCurrentFiberOwnerNameInDevOrNull,\\n+  getCurrentFiberStackInDevOrNull,\\n+} from 'react-reconciler/src/ReactCurrentFiber';\\n import {registrationNameModules} from 'events/EventPluginRegistry';\\n import warning from 'shared/warning';\\n \\n@@ -45,10 +48,6 @@ import {validateProperties as validateARIAProperties} from '../shared/ReactDOMIn\\n import {validateProperties as validateInputProperties} from '../shared/ReactDOMNullInputValuePropHook';\\n import {validateProperties as validateUnknownProperties} from '../shared/ReactDOMUnknownPropertyHook';\\n \\n-const {\\n-  getCurrentFiberOwnerName,\\n-  getCurrentFiberStackAddendum,\\n-} = ReactDebugCurrentFiber;\\n let didWarnInvalidHydration = false;\\n let didWarnShadyDOM = false;\\n \\n@@ -62,7 +61,7 @@ const HTML = '__html';\\n \\n const {html: HTML_NAMESPACE} = Namespaces;\\n \\n-let getStack = () => '';\\n+let getStackInDevOrNull = () => '';\\n \\n let warnedUnknownTags;\\n let suppressHydrationWarning;\\n@@ -77,7 +76,7 @@ let normalizeMarkupForTextOrAttribute;\\n let normalizeHTML;\\n \\n if (__DEV__) {\\n-  getStack = getCurrentFiberStackAddendum;\\n+  getStackInDevOrNull = getCurrentFiberStackInDevOrNull;\\n \\n   warnedUnknownTags = {\\n     // Chrome is the only major browser not shipping <time>. But as of July\\n@@ -181,7 +180,7 @@ if (__DEV__) {\\n         registrationName,\\n         registrationName,\\n         registrationName,\\n-        getCurrentFiberStackAddendum(),\\n+        getCurrentFiberStackInDevOrNull(),\\n       );\\n     } else {\\n       warning(\\n@@ -189,7 +188,7 @@ if (__DEV__) {\\n         'Expected `%s` listener to be a function, instead got a value of `%s` type.%s',\\n         registrationName,\\n         typeof listener,\\n-        getCurrentFiberStackAddendum(),\\n+        getCurrentFiberStackInDevOrNull(),\\n       );\\n     }\\n   };\\n@@ -267,7 +266,11 @@ function setInitialDOMProperties(\\n         }\\n       }\\n       // Relies on `updateStylesByID` not mutating `styleUpdates`.\\n-      CSSPropertyOperations.setValueForStyles(domElement, nextProp, getStack);\\n+      CSSPropertyOperations.setValueForStyles(\\n+        domElement,\\n+        nextProp,\\n+        getStackInDevOrNull,\\n+      );\\n     } else if (propKey === DANGEROUSLY_SET_INNER_HTML) {\\n       const nextHtml = nextProp ? nextProp[HTML] : undefined;\\n       if (nextHtml != null) {\\n@@ -323,7 +326,11 @@ function updateDOMProperties(\\n     const propKey = updatePayload[i];\\n     const propValue = updatePayload[i + 1];\\n     if (propKey === STYLE) {\\n-      CSSPropertyOperations.setValueForStyles(domElement, propValue, getStack);\\n+      CSSPropertyOperations.setValueForStyles(\\n+        domElement,\\n+        propValue,\\n+        getStackInDevOrNull,\\n+      );\\n     } else if (propKey === DANGEROUSLY_SET_INNER_HTML) {\\n       setInnerHTML(domElement, propValue);\\n     } else if (propKey === CHILDREN) {\\n@@ -442,7 +449,7 @@ export function setInitialProperties(\\n         false,\\n         '%s is using shady DOM. Using shady DOM with React can ' +\\n           'cause things to break subtly.',\\n-        getCurrentFiberOwnerName() || 'A component',\\n+        getCurrentFiberOwnerNameInDevOrNull() || 'A component',\\n       );\\n       didWarnShadyDOM = true;\\n     }\\n@@ -516,7 +523,7 @@ export function setInitialProperties(\\n       props = rawProps;\\n   }\\n \\n-  assertValidProps(tag, props, getStack);\\n+  assertValidProps(tag, props, getStackInDevOrNull);\\n \\n   setInitialDOMProperties(\\n     tag,\\n@@ -604,7 +611,7 @@ export function diffProperties(\\n       break;\\n   }\\n \\n-  assertValidProps(tag, nextProps, getStack);\\n+  assertValidProps(tag, nextProps, getStackInDevOrNull);\\n \\n   let propKey;\\n   let styleName;\\n@@ -834,7 +841,7 @@ export function diffHydratedProperties(\\n         false,\\n         '%s is using shady DOM. Using shady DOM with React can ' +\\n           'cause things to break subtly.',\\n-        getCurrentFiberOwnerName() || 'A component',\\n+        getCurrentFiberOwnerNameInDevOrNull() || 'A component',\\n       );\\n       didWarnShadyDOM = true;\\n     }\\n@@ -895,7 +902,7 @@ export function diffHydratedProperties(\\n       break;\\n   }\\n \\n-  assertValidProps(tag, rawProps, getStack);\\n+  assertValidProps(tag, rawProps, getStackInDevOrNull);\\n \\n   if (__DEV__) {\\n     extraAttributeNames = new Set();\\n\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_data.iloc[0]['passage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 1442.70 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/graphcodebert-base', 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1500, 'bm25_aggr_strategy': 'sump'}\n"
     ]
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(params)\n",
    "save_model_name = params['model_name'].replace('/', '_')\n",
    "repo_name = 'facebook_react'\n",
    "bert_best_model_path = os.path.join('2_7', repo_name, f\"{save_model_name}_model_output\", 'best_model')\n",
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained(bert_best_model_path)\n",
    "bert_reranker.model.to(bert_reranker.device)\n",
    "rerankers = [bert_reranker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCodeReranker:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        # self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        # self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        print(f\"Initialized BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        # TODO change this to be for diffs instead of commit_messages\n",
    "        query_passage_pairs = []\n",
    "        for agg_result in aggregated_results:\n",
    "            # get the top psg_cnt contributing_results\n",
    "            contributing_results = agg_result.contributing_results[: self.psg_cnt]\n",
    "            # for each contributing_result, get the file_path and commit_id\n",
    "            # TODO maybe limit this with a diff_cnt\n",
    "            for contributing_result in contributing_results:\n",
    "                file_path = contributing_result.file_path\n",
    "                commit_id = contributing_result.commit_id\n",
    "                # get the diff from the df\n",
    "                diff = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['diff']\n",
    "                assert diff.shape[0] == 1, f\"diff should only have one row, but has {diff.shape[0]} rows\"\n",
    "                diff = str(diff.iloc[0])\n",
    "                query_passage_pairs.append((query, diff))\n",
    "\n",
    "\n",
    "        # for agg_result in aggregated_results:\n",
    "        #     query_passage_pairs.extend(\n",
    "        #         (query, result.commit_message)\n",
    "        #         for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "        #     )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for agg_result in aggregated_results:\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + len(agg_result.contributing_results[: self.psg_cnt]) # only use psg_cnt contributing_results\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 1443.45 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/graphcodebert-base', 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1500, 'bm25_aggr_strategy': 'sump'}\n"
     ]
    }
   ],
   "source": [
    "code_reranker = BERTCodeReranker(params)\n",
    "code_reranker.rerank_depth = 100\n",
    "rerankers = [bert_reranker, code_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on just 1 sample\n",
    "\n",
    "tmp_results = model_evaluator.evaluate_sampling(n=1, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1859, 'P@10': 0.08, 'P@100': 0.028, 'P@1000': 0.0043, 'MRR': 0.2331, 'Recall@100': 0.4443, 'Recall@1000': 0.5752}\n",
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1676, 'P@10': 0.08, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.2541, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n",
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:36<00:00,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1038, 'P@10': 0.07, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.1732, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=None, aggregation_strategy=params['aggregation_strategy']))\n",
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=[bert_reranker], aggregation_strategy=params['aggregation_strategy']))\n",
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:49<00:00, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.2113, 'P@10': 0.12, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.3317, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:34<00:00, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1807, 'P@10': 0.117, 'P@100': 0.0295, 'P@1000': 0.0041, 'MRR': 0.2545, 'Recall@100': 0.527, 'Recall@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=100, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(triplet_data, reranker, hf_output_dir, args):\n",
    "    def tokenize_hf(example):\n",
    "        return reranker.tokenizer(example['query'], example['passage'], truncation=True, padding='max_length', max_length=reranker.max_seq_length, return_tensors='pt', add_special_tokens=True)\n",
    "    print('Training the model...')\n",
    "    print('Label distribution:')\n",
    "    print(triplet_data['label'].value_counts())\n",
    "\n",
    "    if args.sanity_check:\n",
    "        print('Running sanity check on training data...')\n",
    "        triplet_data = sanity_check(triplet_data)\n",
    "\n",
    "    # Step 7: convert triplet_data to HuggingFace Dataset\n",
    "    # convert triplet_data to HuggingFace Dataset\n",
    "    triplet_data['label'] = triplet_data['label'].astype(float)\n",
    "    train_df, val_df = train_test_split(triplet_data, test_size=0.2, random_state=42, stratify=triplet_data['label'])\n",
    "    train_hf_dataset = HFDataset.from_pandas(train_df, split='train') # type: ignore\n",
    "    val_hf_dataset = HFDataset.from_pandas(val_df, split='validation') # type: ignore\n",
    "\n",
    "    # Step 8: tokenize the data\n",
    "    tokenized_train_dataset = train_hf_dataset.map(tokenize_hf, batched=True)\n",
    "    tokenized_val_dataset = val_hf_dataset.map(tokenize_hf, batched=True)\n",
    "\n",
    "    # Step 9: set format for pytorch\n",
    "    tokenized_train_dataset = tokenized_train_dataset.remove_columns(['query', 'passage'])\n",
    "    tokenized_val_dataset = tokenized_val_dataset.remove_columns(['query', 'passage'])\n",
    "\n",
    "    # rename label column to labels\n",
    "    tokenized_train_dataset = tokenized_train_dataset.rename_column('label', 'labels')\n",
    "    tokenized_val_dataset = tokenized_val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "    # set format to pytorch\n",
    "    tokenized_train_dataset = tokenized_train_dataset.with_format('torch')\n",
    "    tokenized_val_dataset = tokenized_val_dataset.with_format('torch')\n",
    "    print('Training dataset features:')\n",
    "    print(tokenized_train_dataset.features)\n",
    "\n",
    "    # Step 10: set up training arguments\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=hf_output_dir,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        logging_steps=100,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=args.num_workers,\n",
    "        )\n",
    "\n",
    "    small_train_dataset = tokenized_train_dataset.shuffle(seed=42).select(range(100))\n",
    "    small_val_dataset = tokenized_val_dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "    if args.debug:\n",
    "        print('Running in debug mode, using small datasets')\n",
    "        tokenized_train_dataset = small_train_dataset\n",
    "        tokenized_val_dataset = small_val_dataset\n",
    "\n",
    "    # Step 11: set up trainer\n",
    "    trainer = Trainer(\n",
    "        model = reranker.model,\n",
    "        args = train_args,\n",
    "        train_dataset = tokenized_train_dataset, # type: ignore\n",
    "        eval_dataset = tokenized_val_dataset, # type: ignore\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Step 12: train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Step 13: save the model\n",
    "    best_model_path = os.path.join(hf_output_dir, 'best_model')\n",
    "    trainer.save_model(best_model_path)\n",
    "    print(f'Saved model to {best_model_path}')\n",
    "    print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smalldata/ftr'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.sanity_check = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_triplets(data):\n",
    "    \"\"\"\n",
    "    Perform a sanity check on the triplets data.\n",
    "\n",
    "    Args:\n",
    "        data: The input data containing triplets.\n",
    "\n",
    "    Returns:\n",
    "        The sanitized data after removing problematic rows.\n",
    "\n",
    "    Examples:\n",
    "        >>> data = pd.DataFrame({'query': ['apple', 'banana', 'apple'], 'passage': ['red fruit', 'yellow fruit', 'red fruit'], 'label': [0, 1, 0]})\n",
    "        >>> sanity_check_triplets(data)\n",
    "        Assertion failed at index 0: query      apple\n",
    "        passage    red fruit\n",
    "        label             0\n",
    "        Name: 0, dtype: object\n",
    "        Dropped row at index 0\n",
    "        Total number of problems in sanity check of training data: 1\n",
    "        # Output: DataFrame without the problematic row\n",
    "    \"\"\"\n",
    "    problems = 0\n",
    "    for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        try:\n",
    "            if row['label'] == 0:\n",
    "                assert data[(data['query'] == row['query']) & (data['passage'] == row['passage'])]['label'].values[0] == 0\n",
    "            else:\n",
    "                assert data[(data['query'] == row['query']) & (data['passage'] == row['passage'])]['label'].values[0] == 1\n",
    "        except AssertionError:\n",
    "            print(f\"Assertion failed at index {i}: {row}\")\n",
    "            # break  # Optional: break after the first failure, remove if you want to see all failures\n",
    "            # remove the row with label 0\n",
    "\n",
    "            if row['label'] == 0:\n",
    "                problems += 1\n",
    "                data.drop(i, inplace=True)\n",
    "                print(f\"Dropped row at index {i}\")\n",
    "\n",
    "    print(f\"Total number of problems in sanity check of training data: {problems}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 775/20096 [00:02<00:52, 368.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 712: query      Land forked reconciler changes (#24817)\\n\\nThi...\n",
      "passage    @@ -44,6 +44,7 @@ import {\\n   SyncLane,\\n   N...\n",
      "label                                                      1\n",
      "Name: 712, dtype: object\n",
      "Assertion failed at index 714: query      Land forked reconciler changes (#24817)\\n\\nThi...\n",
      "passage    @@ -453,27 +453,26 @@ export function includes...\n",
      "label                                                      1\n",
      "Name: 714, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 923/20096 [00:02<00:52, 365.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 863: query      Move update scheduling to microtask (#26512)\\n...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 863, dtype: object\n",
      "Dropped row at index 863\n",
      "Assertion failed at index 864: query      Move update scheduling to microtask (#26512)\\n...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 864, dtype: object\n",
      "Dropped row at index 864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2664/20096 [00:07<00:47, 368.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 2607: query      Diff properties in the commit phase instead of...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 2607, dtype: object\n",
      "Dropped row at index 2607\n",
      "Assertion failed at index 2608: query      Diff properties in the commit phase instead of...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 2608, dtype: object\n",
      "Dropped row at index 2608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3382/20096 [00:09<00:45, 368.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 3334: query      Add a feature flag to disable legacy context (...\n",
      "passage    @@ -23,6 +23,7 @@ export const replayFailedUni...\n",
      "label                                                      1\n",
      "Name: 3334, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3456/20096 [00:09<00:45, 367.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 3415: query      offscreen double invoke effects (#19523)\\n\\nTh...\n",
      "passage    @@ -45,6 +45,7 @@ export const disableTextarea...\n",
      "label                                                      0\n",
      "Name: 3415, dtype: object\n",
      "Dropped row at index 3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 4893/20096 [00:13<00:41, 369.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 4835: query      Bugfix: Effect clean up when deleting suspende...\n",
      "passage    @@ -2082,9 +2082,18 @@ function updateSuspense...\n",
      "label                                                      0\n",
      "Name: 4835, dtype: object\n",
      "Dropped row at index 4835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6674/20096 [00:18<00:36, 368.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 6600: query      Apply #20778 to new fork, too (#20782)\\n\\n* Ap...\n",
      "passage    @@ -34,6 +34,7 @@ import {\\n   disableSchedule...\n",
      "label                                                      0\n",
      "Name: 6600, dtype: object\n",
      "Dropped row at index 6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 7275/20096 [00:19<00:34, 369.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 7201: query      [Flight] use opaque config for flight in `dom-...\n",
      "passage    @@ -10,3 +10,4 @@\\n export * from 'react-clien...\n",
      "label                                                      1\n",
      "Name: 7201, dtype: object\n",
      "Assertion failed at index 7207: query      [Flight] use opaque config for flight in `dom-...\n",
      "passage    @@ -9,3 +9,4 @@\\n \\n export * from '../ReactFl...\n",
      "label                                                      1\n",
      "Name: 7207, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 10390/20096 [00:28<00:26, 368.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 10321: query      [Flight] Taint APIs (#27445)\\n\\nThis lets a re...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 10321, dtype: object\n",
      "Dropped row at index 10321\n",
      "Assertion failed at index 10322: query      [Flight] Taint APIs (#27445)\\n\\nThis lets a re...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 10322, dtype: object\n",
      "Dropped row at index 10322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 10844/20096 [00:29<00:25, 369.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 10776: query      Move unstable_scheduleHydration to ReactDOMHyd...\n",
      "passage    @@ -11,6 +11,7 @@ export {\\n   __SECRET_INTERN...\n",
      "label                                                      0\n",
      "Name: 10776, dtype: object\n",
      "Dropped row at index 10776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 12971/20096 [00:35<00:19, 369.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 12905: query      Track nearest Suspense handler on stack (#2458...\n",
      "passage    @@ -7,6 +7,7 @@\\n  * @flow\\n  */\\n \\n+import t...\n",
      "label                                                      0\n",
      "Name: 12905, dtype: object\n",
      "Dropped row at index 12905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 16007/20096 [00:43<00:11, 369.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 15936: query      Always skip unmounted/unmounting error boundar...\n",
      "passage    @@ -45,6 +45,7 @@ export const disableTextarea...\n",
      "label                                                      0\n",
      "Name: 15936, dtype: object\n",
      "Dropped row at index 15936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 18588/20096 [00:50<00:04, 367.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 18546: query      Don't prerender siblings of suspended componen...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 18546, dtype: object\n",
      "Dropped row at index 18546\n",
      "Assertion failed at index 18547: query      Don't prerender siblings of suspended componen...\n",
      "passage    @@ -57,6 +57,7 @@ export const enableUseRefAcc...\n",
      "label                                                      0\n",
      "Name: 18547, dtype: object\n",
      "Dropped row at index 18547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20096/20096 [00:54<00:00, 370.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of problems in sanity check of training data: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "diff_data = sanity_check_triplets(diff_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ -8,7 +8,10 @@\n",
      "  */\n",
      " \n",
      " // TODO: direct imports like some-package/src/* are bad. Fix me.\n",
      "-import ReactDebugCurrentFiber from 'react-reconciler/src/ReactDebugCurrentFiber';\n",
      "+import {\n",
      "+  getCurrentFiberOwnerNameInDevOrNull,\n",
      "+  getCurrentFiberStackInDevOrNull,\n",
      "+} from 'react-reconciler/src/ReactCurrentFiber';\n",
      " import {registrationNameModules} from 'events/EventPluginRegistry';\n",
      " import warning from 'shared/warning';\n",
      " \n",
      "@@ -45,10 +48,6 @@ import {validateProperties as validateARIAProperties} from '../shared/ReactDOMIn\n",
      " import {validateProperties as validateInputProperties} from '../shared/ReactDOMNullInputValuePropHook';\n",
      " import {validateProperties as validateUnknownProperties} from '../shared/ReactDOMUnknownPropertyHook';\n",
      " \n",
      "-const {\n",
      "-  getCurrentFiberOwnerName,\n",
      "-  getCurrentFiberStackAddendum,\n",
      "-} = ReactDebugCurrentFiber;\n",
      " let didWarnInvalidHydration = false;\n",
      " let didWarnShadyDOM = false;\n",
      " \n",
      "@@ -62,7 +61,7 @@ const HTML = '__html';\n",
      " \n",
      " const {html: HTML_NAMESPACE} = Namespaces;\n",
      " \n",
      "-let getStack = () => '';\n",
      "+let getStackInDevOrNull = () => '';\n",
      " \n",
      " let warnedUnknownTags;\n",
      " let suppressHydrationWarning;\n",
      "@@ -77,7 +76,7 @@ let normalizeMarkupForTextOrAttribute;\n",
      " let normalizeHTML;\n",
      " \n",
      " if (__DEV__) {\n",
      "-  getStack = getCurrentFiberStackAddendum;\n",
      "+  getStackInDevOrNull = getCurrentFiberStackInDevOrNull;\n",
      " \n",
      "   warnedUnknownTags = {\n",
      "     // Chrome is the only major browser not shipping <time>. But as of July\n",
      "@@ -181,7 +180,7 @@ if (__DEV__) {\n",
      "         registrationName,\n",
      "         registrationName,\n",
      "         registrationName,\n",
      "-        getCurrentFiberStackAddendum(),\n",
      "+        getCurrentFiberStackInDevOrNull(),\n",
      "       );\n",
      "     } else {\n",
      "       warning(\n",
      "@@ -189,7 +188,7 @@ if (__DEV__) {\n",
      "         'Expected `%s` listener to be a function, instead got a value of `%s` type.%s',\n",
      "         registrationName,\n",
      "         typeof listener,\n",
      "-        getCurrentFiberStackAddendum(),\n",
      "+        getCurrentFiberStackInDevOrNull(),\n",
      "       );\n",
      "     }\n",
      "   };\n",
      "@@ -267,7 +266,11 @@ function setInitialDOMProperties(\n",
      "         }\n",
      "       }\n",
      "       // Relies on `updateStylesByID` not mutating `styleUpdates`.\n",
      "-      CSSPropertyOperations.setValueForStyles(domElement, nextProp, getStack);\n",
      "+      CSSPropertyOperations.setValueForStyles(\n",
      "+        domElement,\n",
      "+        nextProp,\n",
      "+        getStackInDevOrNull,\n",
      "+      );\n",
      "     } else if (propKey === DANGEROUSLY_SET_INNER_HTML) {\n",
      "       const nextHtml = nextProp ? nextProp[HTML] : undefined;\n",
      "       if (nextHtml != null) {\n",
      "@@ -323,7 +326,11 @@ function updateDOMProperties(\n",
      "     const propKey = updatePayload[i];\n",
      "     const propValue = updatePayload[i + 1];\n",
      "     if (propKey === STYLE) {\n",
      "-      CSSPropertyOperations.setValueForStyles(domElement, propValue, getStack);\n",
      "+      CSSPropertyOperations.setValueForStyles(\n",
      "+        domElement,\n",
      "+        propValue,\n",
      "+        getStackInDevOrNull,\n",
      "+      );\n",
      "     } else if (propKey === DANGEROUSLY_SET_INNER_HTML) {\n",
      "       setInnerHTML(domElement, propValue);\n",
      "     } else if (propKey === CHILDREN) {\n",
      "@@ -442,7 +449,7 @@ export function setInitialProperties(\n",
      "         false,\n",
      "         '%s is using shady DOM. Using shady DOM with React can ' +\n",
      "           'cause things to break subtly.',\n",
      "-        getCurrentFiberOwnerName() || 'A component',\n",
      "+        getCurrentFiberOwnerNameInDevOrNull() || 'A component',\n",
      "       );\n",
      "       didWarnShadyDOM = true;\n",
      "     }\n",
      "@@ -516,7 +523,7 @@ export function setInitialProperties(\n",
      "       props = rawProps;\n",
      "   }\n",
      " \n",
      "-  assertValidProps(tag, props, getStack);\n",
      "+  assertValidProps(tag, props, getStackInDevOrNull);\n",
      " \n",
      "   setInitialDOMProperties(\n",
      "     tag,\n",
      "@@ -604,7 +611,7 @@ export function diffProperties(\n",
      "       break;\n",
      "   }\n",
      " \n",
      "-  assertValidProps(tag, nextProps, getStack);\n",
      "+  assertValidProps(tag, nextProps, getStackInDevOrNull);\n",
      " \n",
      "   let propKey;\n",
      "   let styleName;\n",
      "@@ -834,7 +841,7 @@ export function diffHydratedProperties(\n",
      "         false,\n",
      "         '%s is using shady DOM. Using shady DOM with React can ' +\n",
      "           'cause things to break subtly.',\n",
      "-        getCurrentFiberOwnerName() || 'A component',\n",
      "+        getCurrentFiberOwnerNameInDevOrNull() || 'A component',\n",
      "       );\n",
      "       didWarnShadyDOM = true;\n",
      "     }\n",
      "@@ -895,7 +902,7 @@ export function diffHydratedProperties(\n",
      "       break;\n",
      "   }\n",
      " \n",
      "-  assertValidProps(tag, rawProps, getStack);\n",
      "+  assertValidProps(tag, rawProps, getStackInDevOrNull);\n",
      " \n",
      "   if (__DEV__) {\n",
      "     extraAttributeNames = new Set();\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print passage of first row\n",
    "print(diff_data.iloc[0]['passage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     20082.000000\n",
       "mean       4010.196644\n",
       "std       10365.837954\n",
       "min         112.000000\n",
       "25%         563.000000\n",
       "50%        1376.000000\n",
       "75%        4167.000000\n",
       "max      321990.000000\n",
       "Name: passage, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print length of passages in diff_data\n",
    "diff_data['passage'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Label distribution:\n",
      "label\n",
      "0    11863\n",
      "1     8219\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16065/16065 [00:20<00:00, 775.49 examples/s]\n",
      "Map: 100%|██████████| 4017/4017 [00:05<00:00, 775.57 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset features:\n",
      "{'labels': Value(dtype='float64', id=None), '__index_level_0__': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3433' max='5030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3433/5030 26:31 < 12:20, 2.16 it/s, Epoch 6.82/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.198104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.166507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>0.162599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.161759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.162765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.176837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m do_training(diff_data, code_reranker, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(repo_path, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcode_\u001b[39;49m\u001b[39m{\u001b[39;49;00msave_model_name\u001b[39m}\u001b[39;49;00m\u001b[39m_model_output\u001b[39;49m\u001b[39m\"\u001b[39;49m), args)\n",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb Cell 46\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     model \u001b[39m=\u001b[39m reranker\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     args \u001b[39m=\u001b[39m train_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# Step 12: train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Step 13: save the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m best_model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(hf_output_dir, \u001b[39m'\u001b[39m\u001b[39mbest_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2802\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/accelerate/utils/operations.py:659\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 659\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/accelerate/utils/operations.py:647\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1199\u001b[0m     input_ids,\n\u001b[1;32m   1200\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1201\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1202\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1203\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1204\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    837\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    838\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    839\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    843\u001b[0m )\n\u001b[0;32m--> 844\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    845\u001b[0m     embedding_output,\n\u001b[1;32m    846\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    847\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    848\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    849\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    850\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    851\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    852\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    853\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    854\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    855\u001b[0m )\n\u001b[1;32m    856\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:529\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    521\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    522\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    530\u001b[0m         hidden_states,\n\u001b[1;32m    531\u001b[0m         attention_mask,\n\u001b[1;32m    532\u001b[0m         layer_head_mask,\n\u001b[1;32m    533\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    534\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    535\u001b[0m         past_key_value,\n\u001b[1;32m    536\u001b[0m         output_attentions,\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    539\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    540\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    414\u001b[0m         hidden_states,\n\u001b[1;32m    415\u001b[0m         attention_mask,\n\u001b[1;32m    416\u001b[0m         head_mask,\n\u001b[1;32m    417\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    418\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m    343\u001b[0m         head_mask,\n\u001b[1;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    346\u001b[0m         past_key_value,\n\u001b[1;32m    347\u001b[0m         output_attentions,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:197\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    188\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    189\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    196\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 197\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    199\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_training(diff_data, code_reranker, os.path.join(repo_path, f\"code_{save_model_name}_model_output\"), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker.model = AutoModelForSequenceClassification.from_pretrained(os.path.join(repo_path, f\"code_{save_model_name}_model_output\", 'checkpoint-3018'))\n",
    "code_reranker.model.to(code_reranker.device)\n",
    "rerankers = [bert_reranker, code_reranker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 476.73 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_len': 400, 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 16, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 3, 'mlp_lr': 0.001, 'bert_lr': 5e-05, 'hidden_dim': 128, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'weight_decay': 0.01, 'dropout_prob': 0.5, 'train_commits': 1500}\n"
     ]
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(params)\n",
    "rerankers = [bert_reranker]\n",
    "save_model_name = params['model_name'].replace('/', '_')\n",
    "hf_output_dir = os.path.join(repo_path, f'{save_model_name}_model_output')\n",
    "best_model_path = os.path.join(hf_output_dir, 'best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained(best_model_path).to(bert_reranker.device)\n",
    "rerankers = [bert_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.1859,\n",
       " 'P@10': 0.08,\n",
       " 'P@100': 0.028,\n",
       " 'P@1000': 0.0043,\n",
       " 'MRR': 0.2331,\n",
       " 'Recall@100': 0.4443,\n",
       " 'Recall@1000': 0.5752}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator.evaluate_sampling(n=10, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:57<00:00,  5.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0137,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.007,\n",
       " 'P@1000': 0.0043,\n",
       " 'MRR': 0.0059,\n",
       " 'Recall@100': 0.0598,\n",
       " 'Recall@1000': 0.5752}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator.evaluate_sampling(n=10, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], rerankers=rerankers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:54<00:00,  5.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.1497,\n",
       " 'P@10': 0.13,\n",
       " 'P@100': 0.033,\n",
       " 'P@1000': 0.0043,\n",
       " 'MRR': 0.1727,\n",
       " 'Recall@100': 0.4681,\n",
       " 'Recall@1000': 0.5752}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator.evaluate_sampling(n=10, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], rerankers=rerankers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'facebook_react'\n",
    "oai_model = 'gpt-3'\n",
    "\n",
    "gold_file_path = os.path.join('gold', repo_name, f'{repo_name}_{oai_model}_gold.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_id</th>\n",
       "      <th>commit_date</th>\n",
       "      <th>original_message</th>\n",
       "      <th>actual_files_modified</th>\n",
       "      <th>commit_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af1b039bdd5a8b5def5d51acad00b79e9b7b377c</td>\n",
       "      <td>1586481094</td>\n",
       "      <td>ESLint rule to forbid cross fork imports (#185...</td>\n",
       "      <td>[.eslintrc.js, scripts/eslint-rules/__tests__/...</td>\n",
       "      <td>When syncing changes across implementations, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5aa0c5671fdddc46092d46420fff84a82df558ac</td>\n",
       "      <td>1623102438</td>\n",
       "      <td>Fix Issue with Undefined Lazy Imports By Refac...</td>\n",
       "      <td>[packages/react-reconciler/src/__tests__/React...</td>\n",
       "      <td>When lazy importing, there is an issue with un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>af08b5cbcaf4d3e3ad965a9165e41688733a7771</td>\n",
       "      <td>1509740372</td>\n",
       "      <td>Release script follow-up work after 16.1.0-bet...</td>\n",
       "      <td>[scripts/release/build-commands/add-git-tag.js...</td>\n",
       "      <td>When using the release script after the 16.1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24dbe851e8a3a3a5233654183fd80b0d64b99295</td>\n",
       "      <td>1576610956</td>\n",
       "      <td>fix(dev-tools): fix show correct displayName w...</td>\n",
       "      <td>[packages/react-devtools-shared/src/backend/re...</td>\n",
       "      <td>When using `React.forwardRef()`, the displayNa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ddc4b65cfe17b3f08ff9f18f8804ff5b663788c8</td>\n",
       "      <td>1586291681</td>\n",
       "      <td>Clear finished discrete updates during commit ...</td>\n",
       "      <td>[packages/react-reconciler/src/ReactFiberWorkL...</td>\n",
       "      <td>If a root is finished with a priority lower th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>05a55a4b09b7b7c8f63778fb8252a001ca66f8d7</td>\n",
       "      <td>1642620847</td>\n",
       "      <td>Fix change events for custom elements (#22938)...</td>\n",
       "      <td>[packages/react-dom/src/__tests__/DOMPropertyO...</td>\n",
       "      <td>When using custom elements, there may be issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>27b5699694f20220e0448f0ba3eb6bfa0d3a64ed</td>\n",
       "      <td>1644619917</td>\n",
       "      <td>Simplify cache pool contexts (#23280)\n",
       "\n",
       "The `po...</td>\n",
       "      <td>[packages/react-reconciler/src/ReactFiberCache...</td>\n",
       "      <td>Reading from `pooledCache` variable to track c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>09916479219a61ae86d2ec8ce159a161337b9007</td>\n",
       "      <td>1613595642</td>\n",
       "      <td>Use setImmediate when available over MessageCh...</td>\n",
       "      <td>[packages/scheduler/src/SchedulerFeatureFlags....</td>\n",
       "      <td>When available, it is preferable to use setImm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>c826dc50de288758a0b783b2fd37b40a3b512fc4</td>\n",
       "      <td>1681936268</td>\n",
       "      <td>Add (Client) Functions as Form Actions (#26674...</td>\n",
       "      <td>[fixtures/flight/src/Button.js, fixtures/fligh...</td>\n",
       "      <td>When using `&lt;form action={...}&gt;` or `&lt;button f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>990098f88aef825762f72e76ae84cfe1520222c2</td>\n",
       "      <td>1645660156</td>\n",
       "      <td>Re-arrange main ReactFeatureFlags module (#233...</td>\n",
       "      <td>[packages/shared/ReactFeatureFlags.js]</td>\n",
       "      <td>The main ReactFeatureFlags module has been re-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   commit_id  commit_date  \\\n",
       "0   af1b039bdd5a8b5def5d51acad00b79e9b7b377c   1586481094   \n",
       "1   5aa0c5671fdddc46092d46420fff84a82df558ac   1623102438   \n",
       "2   af08b5cbcaf4d3e3ad965a9165e41688733a7771   1509740372   \n",
       "3   24dbe851e8a3a3a5233654183fd80b0d64b99295   1576610956   \n",
       "4   ddc4b65cfe17b3f08ff9f18f8804ff5b663788c8   1586291681   \n",
       "..                                       ...          ...   \n",
       "95  05a55a4b09b7b7c8f63778fb8252a001ca66f8d7   1642620847   \n",
       "96  27b5699694f20220e0448f0ba3eb6bfa0d3a64ed   1644619917   \n",
       "97  09916479219a61ae86d2ec8ce159a161337b9007   1613595642   \n",
       "98  c826dc50de288758a0b783b2fd37b40a3b512fc4   1681936268   \n",
       "99  990098f88aef825762f72e76ae84cfe1520222c2   1645660156   \n",
       "\n",
       "                                     original_message  \\\n",
       "0   ESLint rule to forbid cross fork imports (#185...   \n",
       "1   Fix Issue with Undefined Lazy Imports By Refac...   \n",
       "2   Release script follow-up work after 16.1.0-bet...   \n",
       "3   fix(dev-tools): fix show correct displayName w...   \n",
       "4   Clear finished discrete updates during commit ...   \n",
       "..                                                ...   \n",
       "95  Fix change events for custom elements (#22938)...   \n",
       "96  Simplify cache pool contexts (#23280)\n",
       "\n",
       "The `po...   \n",
       "97  Use setImmediate when available over MessageCh...   \n",
       "98  Add (Client) Functions as Form Actions (#26674...   \n",
       "99  Re-arrange main ReactFeatureFlags module (#233...   \n",
       "\n",
       "                                actual_files_modified  \\\n",
       "0   [.eslintrc.js, scripts/eslint-rules/__tests__/...   \n",
       "1   [packages/react-reconciler/src/__tests__/React...   \n",
       "2   [scripts/release/build-commands/add-git-tag.js...   \n",
       "3   [packages/react-devtools-shared/src/backend/re...   \n",
       "4   [packages/react-reconciler/src/ReactFiberWorkL...   \n",
       "..                                                ...   \n",
       "95  [packages/react-dom/src/__tests__/DOMPropertyO...   \n",
       "96  [packages/react-reconciler/src/ReactFiberCache...   \n",
       "97  [packages/scheduler/src/SchedulerFeatureFlags....   \n",
       "98  [fixtures/flight/src/Button.js, fixtures/fligh...   \n",
       "99             [packages/shared/ReactFeatureFlags.js]   \n",
       "\n",
       "                                       commit_message  \n",
       "0   When syncing changes across implementations, i...  \n",
       "1   When lazy importing, there is an issue with un...  \n",
       "2   When using the release script after the 16.1.0...  \n",
       "3   When using `React.forwardRef()`, the displayNa...  \n",
       "4   If a root is finished with a priority lower th...  \n",
       "..                                                ...  \n",
       "95  When using custom elements, there may be issue...  \n",
       "96  Reading from `pooledCache` variable to track c...  \n",
       "97  When available, it is preferable to use setImm...  \n",
       "98  When using `<form action={...}>` or `<button f...  \n",
       "99  The main ReactFeatureFlags module has been re-...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df = pd.read_parquet(gold_file_path)\n",
    "\n",
    "# rename the column transformed_message_gpt3 to transformed_message_{oai_model}\n",
    "gold_df = gold_df.rename(columns={'transformed_message_gpt3': f'transformed_message_{oai_model}'})\n",
    "# rename commit_message to original_message\n",
    "gold_df = gold_df.rename(columns={'commit_message': 'original_message'})\n",
    "# rename transformed_message to commit_message\n",
    "gold_df = gold_df.rename(columns={f'transformed_message_{oai_model}': 'commit_message'})\n",
    "gold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n",
      "Found gold_df, evaluating on 100 commits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    string\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    string\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(2), string(2)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.1002,\n",
       " 'P@10': 0.044,\n",
       " 'P@100': 0.0192,\n",
       " 'P@1000': 0.003,\n",
       " 'MRR': 0.163,\n",
       " 'Recall@100': 0.3575,\n",
       " 'Recall@1000': 0.5623}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator.evaluate_sampling(k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], gold_df=gold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n",
      "Found gold_df, evaluating on 100 commits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    string\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    string\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(2), string(2)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:19<00:00,  5.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.119,\n",
       " 'P@10': 0.062,\n",
       " 'P@100': 0.0201,\n",
       " 'P@1000': 0.003,\n",
       " 'MRR': 0.1854,\n",
       " 'Recall@100': 0.3989,\n",
       " 'Recall@1000': 0.5623}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator.evaluate_sampling(k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], gold_df=gold_df, rerankers=rerankers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
