{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_7\t  data\t\t logs\t    profiling  requirements.txt  src\n",
      "2_8\t  debug_test.py  misc\t    README.md  scripts\t\t temp.py\n",
      "big_logs  logging.conf\t notebooks  repos      smalldata\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# import pickle\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "from utils import (\n",
    "    AggregatedSearchResult,\n",
    "    TripletDataset,\n",
    "    get_combined_df,\n",
    "    prepare_triplet_data_from_df,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  1\n",
      "Current cuda device:  0\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "# print torch devices available\n",
    "print('Available devices: ', torch.cuda.device_count())\n",
    "print('Current cuda device: ', torch.cuda.current_device())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Adding an intermediate layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTReranker:\n",
    "    # def __init__(self, model_name, psg_len, psg_cnt, psg_stride, agggreagtion_strategy, batch_size, use_gpu=True):\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        # self.model = AutoModel.from_pretrained(self.model_name, num_labels=1)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            # print GPU info\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt']\n",
    "        # self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        # self.max_title_len = parameters.get('max_title_len', 0)\n",
    "        # self.use_title = self.max_title_len > 0\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        # self.max_seq_length = parameters.get('max_seq_length', 512)\n",
    "        self.max_seq_length = self.tokenizer.model_max_length\n",
    "\n",
    "        print(f\"Initialized BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "        # input_dim = parameters['INPUT_DIM']  # Default BERT hidden size\n",
    "        # hidden_dim = parameters['HIDDEN_DIM']   # Example hidden size\n",
    "        # output_dim = parameters['OUTPUT_DIM']  # We want a single score as output\n",
    "\n",
    "        self.mlp = MLP(self.model.config.hidden_size, parameters['hidden_dim'], 1, parameters['dropout_prob']).to(self.device)\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        query_passage_pairs = []\n",
    "        for agg_result in aggregated_results:\n",
    "            query_passage_pairs.extend(\n",
    "                (query, result.commit_msg)\n",
    "                for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "            )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for agg_result in aggregated_results:\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + len(agg_result.contributing_results[: self.psg_cnt])\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get the pooled output from BERT's [CLS] token\n",
    "                # pooled_output = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "\n",
    "                cls_output = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "                # # Pass the pooled output through the MLP to get the scores\n",
    "                # logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "                logits = self.mlp(cls_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # # Collect the scores (detach them from the computation graph and move to CPU)\n",
    "                scores.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "                # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                # logits = outputs.logits\n",
    "                # scores.extend(logits.detach().cpu().numpy().squeeze(-1))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reranker(bertranker, train_dataloader, validation_dataloader, freeze_bert, save_dir):\n",
    "    # Set BERT parameters to not require gradients\n",
    "    save_dir = os.path.join(save_dir, 'models')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    # for param in bertranker.model.parameters():\n",
    "    #     param.requires_grad = False if freeze_bert else True\n",
    "\n",
    "\n",
    "    # if freeze_bert:\n",
    "    #     optimizer = torch.optim.Adam(bertranker.mlp.parameters(), lr=bertranker.parameters['mlp_lr'], weight_decay=bertranker.parameters['weight_decay'])\n",
    "    # else:\n",
    "    #     optimizer = torch.optim.Adam([\n",
    "    #         {'params': bertranker.model.parameters(), 'lr': bertranker.parameters['bert_lr'], 'weight_decay': bertranker.parameters['weight_decay']},\n",
    "    #         {'params': bertranker.mlp.parameters(), 'lr': bertranker.parameters['mlp_lr'], 'weight_decay': bertranker.parameters['weight_decay']}\n",
    "    #             ], lr=bertranker.parameters['mlp_lr'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(bertranker.model.parameters(), lr=bertranker.parameters['bert_lr'])\n",
    "\n",
    "    # one optimizer for both BERT and MLP with same learning rate\n",
    "\n",
    "\n",
    "    print(f'Optimizer: {optimizer}')\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    # Set up the loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()  #\n",
    "\n",
    "    # Set up training variables\n",
    "    num_epochs = bertranker.parameters['num_epochs']\n",
    "    # print train and val dataloader sizes\n",
    "    print(f'Train dataloader size: {len(train_dataloader)}')\n",
    "    print(f'Val dataloader size: {len(validation_dataloader)}')\n",
    "    # Training loop\n",
    "    print('Starting training loop')\n",
    "\n",
    "    if freeze_bert:\n",
    "        print('BERT is frozen, training only MLP')\n",
    "    else:\n",
    "        print('BERT is unfrozen, training BERT and MLP')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    # model_name = 'bert_reranker_frozen' if freeze_bert else 'bert_reranker'\n",
    "    model_name = bertranker.parameters['model_name'].replace('/', '_') + '_frozen' if freeze_bert else bertranker.parameters['model_name'].replace('/', '_')\n",
    "    model_name += '_frozen' if freeze_bert else ''\n",
    "    print(f'Model name: {model_name}')\n",
    "    # for epoch in range(epochs):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # self.model.eval()  # Make sure the BERT model is in evaluation mode\n",
    "        # if freeze_bert:\n",
    "        #     bertranker.model.eval()  # BERT finetuning should be in eval mode\n",
    "        # else:\n",
    "        #     bertranker.model.train()  # BERT finetuning should be in train mode\n",
    "\n",
    "        bertranker.model.train()  # BERT finetuning should be in train mode\n",
    "        bertranker.mlp.train()  # MLP should be in training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            # breakpoint()\n",
    "            b_input_ids, b_attention_mask, b_labels = batch\n",
    "            b_input_ids = b_input_ids.to(bertranker.device)\n",
    "            b_attention_mask = b_attention_mask.to(bertranker.device)\n",
    "            b_labels = b_labels.float().to(bertranker.device)\n",
    "\n",
    "            # Forward pass\n",
    "            if freeze_bert:\n",
    "                with torch.no_grad():\n",
    "                    # pooled_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                    cls_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "            else:\n",
    "                pooled_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "            cls_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "            logits = bertranker.mlp(cls_output).squeeze(-1) # type: ignore\n",
    "\n",
    "            # outputs = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "            # logits = bertranker.mlp(outputs.logits).squeeze(-1) # type: ignore\n",
    "            # logits = outputs.logits.squeeze(-1) # type: ignore\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation step\n",
    "        bertranker.model.eval()\n",
    "        bertranker.mlp.eval()\n",
    "        total_eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_dataloader:\n",
    "                b_input_ids, b_attention_mask, b_labels = batch\n",
    "                b_input_ids = b_input_ids.to(bertranker.device)\n",
    "                b_attention_mask = b_attention_mask.to(bertranker.device)\n",
    "                b_labels = b_labels.float().to(bertranker.device)\n",
    "\n",
    "                pooled_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                cls_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "                logits = bertranker.mlp(cls_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # outputs = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                # logits = outputs.logits.squeeze(-1) # type: ignore\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, b_labels.float())\n",
    "                total_eval_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        # scheduler.step(avg_val_loss)\n",
    "        # Save losses\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss}\")\n",
    "        print(f'Best validation loss: {best_val_loss}')\n",
    "\n",
    "        # save graph of losses\n",
    "        plt.plot(train_losses, label='Training loss', color='blue', linestyle='dashed', linewidth=1, marker='o', markerfacecolor='blue', markersize=3)\n",
    "        plt.plot(val_losses, label='Validation loss', color='red', linestyle='dashed', linewidth=1, marker='o', markerfacecolor='red', markersize=3)\n",
    "        plt.legend(frameon=False)\n",
    "        plt.savefig(os.path.join(save_dir, f'{model_name}_losses.png'))\n",
    "        plt.close()\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # model name with frozen or unfrozen bert\n",
    "            save_path = os.path.join(save_dir, f'{model_name}_best_model.pth')\n",
    "            mlp_save_path = os.path.join(save_dir, f'{model_name}_best_mlp.pth')\n",
    "            torch.save(bertranker.model.state_dict(), save_path)\n",
    "            torch.save(bertranker.mlp.state_dict(), mlp_save_path)\n",
    "\n",
    "            print(f\"Model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "            # evaluate on train set\n",
    "\n",
    "\n",
    "\n",
    "        # Here you can add early stopping based on validation loss\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    repo_path='../smalldata/ftr',\n",
    "    index_path='../smalldata/ftr/index_commit_tokenized/',\n",
    "    k=1000,\n",
    "    n=100,\n",
    "    overwrite_cache=False,\n",
    "    freeze_bert=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']\n",
    "repo_path = args.repo_path\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../smalldata/ftr/index_commit_tokenized/\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 output path: ../smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to ../smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt\n",
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.1542, 'P@10': 0.087, 'P@100': 0.0267, 'P@1000': 0.0041, 'MRR': 0.2133, 'Recall@100': 0.5077, 'Recall@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm25_output_path = os.path.join(eval_path, f'bm25_baseline_N{n}_K{K}_metrics.txt')\n",
    "print(f'BM25 output path: {bm25_output_path}')\n",
    "\n",
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=bm25_output_path, aggregation_strategy=BM25_AGGR_STRAT, repo_path=repo_path)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking with BERT\n",
    "params = {\n",
    "    'model_name': 'microsoft/codebert-base',\n",
    "    'psg_len': 400,\n",
    "    'psg_cnt': 5,\n",
    "    # 'psg_stride': 32,\n",
    "    'aggregation_strategy': 'sump',\n",
    "    'batch_size': 16,\n",
    "    # 'batch_size': 512,\n",
    "    # 'batch_size': 1,\n",
    "    'use_gpu': True,\n",
    "    'rerank_depth': 250,\n",
    "    'num_epochs': 3,\n",
    "    # 'mlp_lr': 1e-2,\n",
    "    'mlp_lr': 1e-3,\n",
    "    'bert_lr': 5e-5,\n",
    "    'hidden_dim': 128,\n",
    "    'num_positives': 10,\n",
    "    'num_negatives': 10,\n",
    "    'train_depth': 1000,\n",
    "    'num_workers': 8,\n",
    "    'weight_decay': 0.01,\n",
    "    'dropout_prob': 0.5,\n",
    "    'train_commits': 1500,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 476.73 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_len': 400, 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 16, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 3, 'mlp_lr': 0.001, 'bert_lr': 5e-05, 'hidden_dim': 128, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'weight_decay': 0.01, 'dropout_prob': 0.5, 'train_commits': 1500}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_reranker = BERTReranker(params)\n",
    "rerankers = [bert_reranker]\n",
    "save_model_name = params['model_name'].replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.66661695926252"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get average length of commit messages\n",
    "combined_df['commit_message'].str.split().str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commits after midpoint date: 5804\n"
     ]
    }
   ],
   "source": [
    "filtered_df = combined_df[['commit_date', 'commit_message', 'commit_id', 'file_path', 'diff']]\n",
    "\n",
    "# Step 2: Group by commit_id\n",
    "grouped_df = filtered_df.groupby(['commit_id', 'commit_date', 'commit_message'])['file_path'].apply(list).reset_index()\n",
    "grouped_df.rename(columns={'file_path': 'actual_files_modified'}, inplace=True)\n",
    "\n",
    "# Step 3: Determine midpoint and filter dataframe\n",
    "midpoint_date = np.median(grouped_df['commit_date'])\n",
    "recent_df = grouped_df[grouped_df['commit_date'] > midpoint_date]\n",
    "print(f'Number of commits after midpoint date: {len(recent_df)}')\n",
    "# sys.exit(0)\n",
    "\n",
    "# recent_df = recent_df.head(2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commits after filtering by commit message length: 1543\n",
      "Number of commits after sampling: 1500\n"
     ]
    }
   ],
   "source": [
    "average_commit_len = recent_df['commit_message'].str.split().str.len().mean()\n",
    "# filter out commits with less than average length\n",
    "recent_df = recent_df[recent_df['commit_message'].str.split().str.len() > average_commit_len]\n",
    "print(f'Number of commits after filtering by commit message length: {len(recent_df)}')\n",
    "\n",
    "# randomly sample 1500 rows from recent_df\n",
    "recent_df = recent_df.sample(params['train_commits'])\n",
    "print(f'Number of commits after sampling: {len(recent_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_print(df):\n",
    "    # randomly print one commit message\n",
    "    print(df['commit_message'].sample().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data first\n",
    "if not os.path.exists(os.path.join(repo_path, 'cache')):\n",
    "    os.makedirs(os.path.join(repo_path, 'cache'))\n",
    "# train_cache = os.path.join(repo_path, 'cache', 'train_data_cache.pkl')\n",
    "# val_cache = os.path.join(repo_path, 'cache', 'val_data_cache.pkl')\n",
    "# test_cache = os.path.join(repo_path, 'cache', 'test_data_cache.pkl')\n",
    "triplet_cache = os.path.join(repo_path, 'cache', 'triplet_data_cache.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data from dataframe of size: 2 with search_depth: 1000\n",
      "Percentage of positives: 0.2, Percentage of negatives: 0.8\n"
     ]
    }
   ],
   "source": [
    "def temp_prep(df, searcher, search_depth, num_positives, num_negatives):\n",
    "\n",
    "    data = []\n",
    "    print(f'Preparing data from dataframe of size: {len(df)} with search_depth: {search_depth}')\n",
    "    total_positives, total_negatives = 0, 0\n",
    "    for _, row in df.iterrows():\n",
    "    # for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "        cur_positives = 0\n",
    "        cur_negatives = 0\n",
    "        pos_commit_ids = set()\n",
    "        neg_commit_ids = set()\n",
    "        commit_message = row['commit_message']\n",
    "        actual_files_modified = row['actual_files_modified']\n",
    "\n",
    "        agg_search_results = searcher.pipeline(commit_message, row['commit_date'], search_depth, 'sump', aggregate_on='commit')\n",
    "\n",
    "        # for each agg_result, find out how many files it has edited are in actual_files_modified and sort by score\n",
    "\n",
    "        for agg_result in agg_search_results:\n",
    "            agg_result_files = set([result.file_path for result in agg_result.contributing_results])\n",
    "            intersection = agg_result_files.intersection(actual_files_modified)\n",
    "            # TODO maybe try this for training\n",
    "            agg_result.score = len(intersection) / len(agg_result_files) # how focused the commit is\n",
    "            # agg_result.score = len(intersection)\n",
    "\n",
    "        agg_search_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        # go from top to bottom, first num_positives non-0 scores are positive samples and the next num_negatives are negative samples\n",
    "        for agg_result in agg_search_results:\n",
    "            cur_commit_msg = agg_result.contributing_results[0].commit_message\n",
    "            if cur_positives < num_positives and agg_result.score > 0:\n",
    "                # meaning there is at least one file in the agg_result that is in actual_files_modified\n",
    "                # pos_commits.append(agg_result)\n",
    "                data.append((commit_message, cur_commit_msg, 1))\n",
    "                cur_positives += 1\n",
    "                pos_commit_ids.add(agg_result.commit_id)\n",
    "            elif cur_negatives < num_negatives:\n",
    "                # neg_commits.append(agg_result)\n",
    "                data.append((commit_message, cur_commit_msg, 0))\n",
    "                cur_negatives += 1\n",
    "                neg_commit_ids.add(agg_result.commit_id)\n",
    "            if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "                break\n",
    "\n",
    "        assert len(pos_commit_ids.intersection(neg_commit_ids)) == 0, 'Positive and negative commit ids should not intersect'\n",
    "        # print(f\"Total positives: {cur_positives}, Total negatives: {cur_negatives}\")\n",
    "        total_positives += cur_positives\n",
    "        total_negatives += cur_negatives\n",
    "\n",
    "    # # Write data to cache file\n",
    "    # with open(cache_file, 'wb') as file:\n",
    "    #     pickle.dump(data, file)\n",
    "    #     print(f\"Saved data to cache file: {cache_file}\")\n",
    "\n",
    "\n",
    "    # print percentage of positives and negatives\n",
    "    denom = total_positives + total_negatives\n",
    "    print(f\"Percentage of positives: {total_positives / denom}, Percentage of negatives: {total_negatives / denom}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     # flatten the contributing results for each aggregated result\n",
    "    #     search_results = [result for agg_result in agg_search_results for result in agg_result.contributing_results]\n",
    "    #     search_results.sort(key=lambda res: res.score, reverse=True)\n",
    "    #     # efficiently get the top num_positives and num_negatives samples\n",
    "    #     positive_samples = []\n",
    "    #     negative_samples = []\n",
    "\n",
    "    #     for result in search_results:\n",
    "    #         if result.file_path in actual_files_modified and len(positive_samples) < num_positives:\n",
    "    #             positive_samples.append(result.commit_msg)\n",
    "    #             total_positives += 1\n",
    "    #         elif result.file_path not in actual_files_modified and len(negative_samples) < num_negatives:\n",
    "    #             negative_samples.append(result.commit_msg)\n",
    "    #             total_negatives += 1\n",
    "\n",
    "    #         if len(positive_samples) == num_positives and len(negative_samples) == num_negatives:\n",
    "    #             break\n",
    "\n",
    "\n",
    "    #     for sample_msg in positive_samples:\n",
    "    #         data.append((commit_message, sample_msg, 1))\n",
    "\n",
    "    #     for sample_msg in negative_samples:\n",
    "    #         data.append((commit_message, sample_msg, 0))\n",
    "    # print(f\"Total positives: {total_positives}, Total negatives: {total_negatives}\")\n",
    "    # # print percentage of positives and negatives\n",
    "    # denom = total_positives + total_negatives\n",
    "    # print(f\"Percentage of positives: {total_positives / denom}, Percentage of negatives: {total_negatives / denom}\")\n",
    "    # return data\n",
    "\n",
    "\n",
    "as43lqmefl=temp_prep(recent_df.head(2), bm25_searcher, num_positives=10, num_negatives=10, search_depth=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data from dataframe of size: 1500 with search_depth: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [06:26<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to cache file: ../smalldata/ftr/cache/triplet_data_cache.pkl\n",
      "Total positives: 6796, Total negatives: 14660\n",
      "Percentage of positives: 0.3167412378821775, Percentage of negatives: 0.6832587621178225\n"
     ]
    }
   ],
   "source": [
    "triplet_data = prepare_triplet_data_from_df(recent_df, bm25_searcher, search_depth=params['train_depth'], num_positives=params['num_positives'], num_negatives=params['num_negatives'], cache_file=triplet_cache, overwrite=args.overwrite_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21456"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split recent dataframe and prepare data\n",
    "# df_train, df_temp = train_test_split(recent_df, test_size=0.2, random_state=42)\n",
    "# df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create train, val, test data from triplet data using train_test_split with stratify\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# stratify based on the third column of the triplet data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m train_data, temp_data \u001b[39m=\u001b[39m train_test_split(triplet_data, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, stratify\u001b[39m=\u001b[39mtriplet_data[:, \u001b[39m2\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m val_data, test_data \u001b[39m=\u001b[39m train_test_split(temp_data, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, stratify\u001b[39m=\u001b[39mtemp_data[:, \u001b[39m2\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# create train, val, test data from triplet data using train_test_split with stratify\n",
    "# stratify based on the third column of the triplet data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 42011\n",
      "Val data size: 5361\n",
      "Test data size: 5354\n",
      "Train data sample: ('Fix typo in comment (Noticable→Noticeable) (#19737)\\n\\n\\n', 'Convert current build system to Rollup and adopt flat bundles (#9327)\\n\\n* WIP\\n\\n* fbjs support\\n\\n* WIP\\n\\n* dev/prod mode WIP\\n\\n* More WIP\\n\\n* builds a cjs bundle\\n\\n* adding forwarding modules\\n\\n* more progress on forwarding modules and FB config\\n\\n* improved how certain modules get inlined for fb and cjs\\n\\n* more forwarding modules\\n\\n* added comments to the module aliasing code\\n\\n* made ReactPerf and ReactTestUtils bundle again\\n\\n* Use -core suffix for all bundles\\n\\nThis makes it easier to override things in www.\\n\\n* Add a lazy shim for ReactPerf\\n\\nThis prevents a circular dependency between ReactGKJSModule and ReactDOM\\n\\n* Fix forwarding module for ReactCurrentOwner\\n\\n* Revert \"Add a lazy shim for ReactPerf\"\\n\\nThis reverts commit 723b402c07116a70ce8ff1e43a1f4d92052e8f43.\\n\\n* Rename -core suffix to -fb for clarity\\n\\n* Change forwarding modules to import from -fb\\n\\nThis is another, more direct fix for ReactPerf circular dependency\\n\\n* should fix fb and cjs bundles for ReactCurrentOwner\\n\\n* added provides module for ReactCurrentOwner\\n\\n* should improve console output\\n\\n* fixed typo with argument passing on functon call\\n\\n* Revert \"should improve console output\"\\n\\nThis breaks the FB bundles.\\n\\nThis reverts commit 65f11ee64f678c387cb3cfef9a8b28b89a6272b9.\\n\\n* Work around internal FB transform require() issue\\n\\n* moved  ReactInstanceMap out of React and into ReactDOM and ReactDOMFiber\\n\\n* Expose more internal modules to www\\n\\n* Add missing modules to Stack ReactDOM to fix UFI\\n\\n* Fix onlyChild module\\n\\n* improved the build tool\\n\\n* Add a rollup npm script\\n\\n* Rename ReactDOM-fb to ReactDOMStack-fb\\n\\n* Fix circular dependencies now that ReactDOM-fb is a GK switch\\n\\n* Revert \"Work around internal FB transform require() issue\"\\n\\nThis reverts commit 0a50b6a90bffc59f8f5416ef36000b5e3a44d253.\\n\\n* Bump rollup-plugin-commonjs to include a fix for rollup/rollup-plugin-commonjs#176\\n\\n* Add more forwarding modules that are used on www\\n\\n* Add even more forwarding modules that are used on www\\n\\n* Add DOMProperty to hidden exports\\n\\n* Externalize feature flags\\n\\nThis lets www specify them dynamically.\\n\\n* Remove forwarding modules with implementations\\n\\nInstead I\\'m adding them to react-fb in my diff.\\n\\n* Add all injection necessary for error logging\\n\\n* Add missing forwarding module (oops)\\n\\n* Add ReactART builds\\n\\n* Add ReactDOMServer bundle\\n\\n* Fix UMD build of ReactDOMFiber\\n\\n* Work in progress: start adding ReactNative bundle\\n\\n* tidied up the options for bundles, so they can define what types they output and exclude\\n\\n* Add a working RN build\\n\\n* further improved and tidied up build process\\n\\n* improved how bundles are built by exposing externals and making the process less \"magical\", also tidied up code and added more comments\\n\\n* better handling of bundling ReactCurrentOwner and accessing it from renderer modules\\n\\n* added NODE_DEV and NODE_PROD\\n\\n* added NPM package creation and copying into build chain\\n\\n* Improved UMD bundles, added better fixture testing and doc plus prod builds\\n\\n* updated internal modules (WIP)\\n\\n* removed all react/lib/* dependencies from appearing in bundles created on build\\n\\n* added react-test-renderer bundles\\n\\n* renamed bundles and paths\\n\\n* fixed fixture path changes\\n\\n* added extract-errors support\\n\\n* added extractErrors warning\\n\\n* moved shims to shims directory in rollup scripts\\n\\n* changed pathing to use build rather than build/rollup\\n\\n* updated release doc to reflect some rollup changes\\n\\n* Updated ReactNative findNodeHandle() to handle number case (#9238)\\n\\n* Add dynamic injection to ReactErrorUtils (#9246)\\n\\n* Fix ReactErrorUtils injection (#9247)\\n\\n* Fix Haste name\\n\\n* Move files around\\n\\n* More descriptive filenames\\n\\n* Add missing ReactErrorUtils shim\\n\\n* Tweak reactComponentExpect to make it standalone-ish in www\\n\\n* Unflowify shims\\n\\n* facebook-www shims now get copied over correctly to build\\n\\n* removed unnecessary resolve\\n\\n* building facebook-www/build is now all sync to prevent IO issues plus handles extra facebook-www src assets\\n\\n* removed react-native-renderer package and made build make a react-native build dir instead\\n\\n* 😭😭😭\\n\\n* Add more SSR unit tests for elements and children. (#9221)\\n\\n* Adding more SSR unit tests for elements and children.\\n\\n* Some of my SSR tests were testing for react-text and react-empty elements that no longer exist in Fiber. Fixed the tests so that they expect correct markup in Fiber.\\n\\n* Tweaked some test names after @gaearon review comment https://github.com/facebook/react/pull/9221#discussion_r107045673 . Also realized that one of the tests was essentially a direct copy of another, so deleted it.\\n\\n* Responding to code review https://github.com/facebook/react/pull/9221#pullrequestreview-28996315 . Thanks @spicyj!\\n\\n* ReactElementValidator uses temporary ReactNative View propTypes getter (#9256)\\n\\n* Updating packages for 16.0.0-alpha.6 release\\n\\n* Revert \"😭😭😭\"\\n\\nThis reverts commit 7dba33b2cfc67246881f6d57633a80e628ea05ec.\\n\\n* Work around Jest issue with CurrentOwner shared state in www\\n\\n* updated error codes\\n\\n* splits FB into FB_DEV and FB_PROD\\n\\n* Remove deps on specific builds from shims\\n\\n* should no longer mangle FB_PROD output\\n\\n* Added init() dev block to ReactTestUtils\\n\\n* added shims for DEV only code so it does not get included in prod bundles\\n\\n* added a __DEV__ wrapping code to FB_DEV\\n\\n* added __DEV__ flag behind a footer/header\\n\\n* Use right haste names\\n\\n* keeps comments in prod\\n\\n* added external babel helpers plugin\\n\\n* fixed fixtures and updated cjs/umd paths\\n\\n* Fixes Jest so it run tests correctly\\n\\n* fixed an issue with stubbed modules not properly being replaced due to greedy replacement\\n\\n* added a WIP solution for ReactCurrentOwner on FB DEV\\n\\n* adds a FB_TEST bundle\\n\\n* allows both ReactCurrentOwner and react/lib/ReactCurrentOwner\\n\\n* adds -test to provides module name\\n\\n* Remove TEST env\\n\\n* Ensure requires stay at the top\\n\\n* added basic mangle support (disbaled by default)\\n\\n* per bundle property mangling added\\n\\n* moved around plugin order to try and fix deadcode requires as per https://github.com/rollup/rollup/issues/855\\n\\n* Fix flow issues\\n\\n* removed gulp and grunt and moved tasks to standalone node script\\n\\n* configured circleci to use new paths\\n\\n* Fix lint\\n\\n* removed gulp-extract-errors\\n\\n* added test_build.sh back in\\n\\n* added missing newline to flow.js\\n\\n* fixed test coverage command\\n\\n* changed permissions on test_build.sh\\n\\n* fixed test_html_generations.sh\\n\\n* temp removed html render test\\n\\n* removed the warning output from test_build, the build should do this instead\\n\\n* fixed test_build\\n\\n* fixed broken npm script\\n\\n* Remove unused ViewportMetrics shim\\n\\n* better error output\\n\\n* updated circleci to node 7 for async/await\\n\\n* Fixes\\n\\n* removed coverage test from circleci run\\n\\n* circleci run tets\\n\\n* removed build from circlci\\n\\n* made a dedicated jest script in a new process\\n\\n* moved order around of circlci tasks\\n\\n* changing path to jest in more circleci tests\\n\\n* re-enabled code coverage\\n\\n* Add file header to prod bundles\\n\\n* Remove react-dom/server.js (WIP: decide on the plan)\\n\\n* Only UMD bundles need version header\\n\\n* Merge with master\\n\\n* disabled const evaluation by uglify for <script></script> string literal\\n\\n* deal with ART modules for UMD bundles\\n\\n* improved how bundle output gets printed\\n\\n* fixed filesize difference reporting\\n\\n* added filesize dep\\n\\n* Update yarn lockfile for some reason\\n\\n* now compares against the last run branch built on\\n\\n* added react-dom-server\\n\\n* removed un-needed comment\\n\\n* results only get saved on full builds\\n\\n* moved the rollup sized plugin into a plugins directory\\n\\n* added a missing commonjs()\\n\\n* fixed missing ignore\\n\\n* Hack around to fix RN bundle\\n\\n* Partially fix RN bundles\\n\\n* added react-art bundle and a fixture for it\\n\\n* Point UMD bundle to Fiber and add EventPluginHub to exported internals\\n\\n* Make it build on Node 4\\n\\n* fixed eslint error with resolve being defined in outer scope\\n\\n* Tweak how build results are calculated and stored\\n\\n* Tweak fixtures build to work on Node 4\\n\\n* Include LICENSE/PATENTS and fix up package.json files\\n\\n* Add Node bundle for react-test-renderer\\n\\n* Revert \"Hack around to fix RN bundle\"\\n\\nWe\\'ll do this later.\\n\\nThis reverts commit 59445a625962d7be4c7c3e98defc8a31f8761ec1.\\n\\n* Revert more RN changes\\n\\nWe\\'ll do them separately later\\n\\n* Revert more unintentional changes\\n\\n* Revert changes to error codes\\n\\n* Add accidentally deleted RN externals\\n\\n* added RN_DEV/RN_PROD bundles\\n\\n* fixed typo where RN_DEV and RN_PROD were the wrong way around\\n\\n* Delete/ignore fixture build outputs\\n\\n* Format scripts/ with Prettier\\n\\n* tidied up the Rollup build process and split functions into various different files to improve readability\\n\\n* Copy folder before files\\n\\n* updated yarn.lock\\n\\n* updated results and yarn dependencies to the latest versions\\n\\n', 0)\n",
      "Train label distribution: (array([0, 1]), array([32000, 10011]))\n",
      "Val label distribution: (array([0, 1]), array([4000, 1361]))\n",
      "Test label distribution: (array([0, 1]), array([4000, 1354]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get distribution of labels\n",
    "train_labels = [label for _, _, label in train_data]\n",
    "val_labels = [label for _, _, label in val_data]\n",
    "test_labels = [label for _, _, label in test_data]\n",
    "\n",
    "# print size of data\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Val data size: {len(val_data)}')\n",
    "print(f'Test data size: {len(test_data)}')\n",
    "\n",
    "print(f'Train data sample: {train_data[0]}')\n",
    "\n",
    "print(f'Train label distribution: {np.unique(train_labels, return_counts=True)}')\n",
    "print(f'Val label distribution: {np.unique(val_labels, return_counts=True)}')\n",
    "print(f'Test label distribution: {np.unique(test_labels, return_counts=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = TripletDataset(train_data, bert_reranker.tokenizer, bert_reranker.max_seq_length)\n",
    "val_dataset = TripletDataset(val_data, bert_reranker.tokenizer, bert_reranker.max_seq_length)\n",
    "test_dataset = TripletDataset(test_data, bert_reranker.tokenizer, bert_reranker.max_seq_length)\n",
    "\n",
    "# Step 5: train the MLP\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bert_reranker.batch_size, shuffle=True, num_workers=params['num_workers'])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=bert_reranker.batch_size, shuffle=False, num_workers=params['num_workers'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bert_reranker.batch_size, shuffle=False, num_workers=params['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bert_reranker.train_mlp(train_dataloader, val_dataloader)\n",
    "train_reranker(bert_reranker, train_dataloader, val_dataloader, freeze_bert=args.freeze_bert, save_dir=repo_path)\n",
    "\n",
    "reranker_output_file = f\"925_bert_reranker_{save_model_name}_N{args.n}_K{args.k}_non_frozen_metrics.txt\" if not args.freeze_bert else f\"bert_reranker_{save_model_name}_N{args.n}_K{args.k}_frozen_metrics.txt\"\n",
    "\n",
    "# reranker_output_file = f\"bert_reranker_{save_model_name}_N{args.n}_K{args.k}_without_mlp_metrics.txt\"\n",
    "reranker_output_path = os.path.join(eval_path, reranker_output_file)\n",
    "\n",
    "bert_reranker_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=reranker_output_path, aggregation_strategy='sump', rerankers=rerankers, repo_path=repo_path)\n",
    "\n",
    "print(\"BERT Reranker Evaluation\")\n",
    "print(bert_reranker_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
