{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "from utils import AggregatedSearchResult, get_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_from_df(df, searcher, depth, n_positive, n_negative):\n",
    "    data = []\n",
    "    print(f'Preparing data from dataframe of size: {len(df)}')\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        commit_message = row['commit_message']\n",
    "        actual_files_modified = row['actual_files_modified']\n",
    "        # search_results = search(searcher, commit_message, row['commit_date'], 1000)\n",
    "\n",
    "        # search_results = searcher.search(commit_message, row['commit_date'], 100)\n",
    "        search_results = searcher.pipeline(commit_message, row['commit_date'], depth, 'sump')\n",
    "\n",
    "        # flatten the contributing results for each aggregated result\n",
    "        search_results = [result for agg_result in search_results for result in agg_result.contributing_results]\n",
    "\n",
    "        # efficiently get the top n_positive and n_negative samples\n",
    "        positive_samples = []\n",
    "        negative_samples = []\n",
    "\n",
    "        for result in search_results:\n",
    "            if result.file_path in actual_files_modified and len(positive_samples) < n_positive:\n",
    "                positive_samples.append(result.commit_msg)\n",
    "            elif result.file_path not in actual_files_modified and len(negative_samples) < n_negative:\n",
    "                negative_samples.append(result.commit_msg)\n",
    "\n",
    "            if len(positive_samples) == n_positive and len(negative_samples) == n_negative:\n",
    "                break\n",
    "\n",
    "        # Get positive and negative samples\n",
    "        # positive_samples = [res.commit_msg for res in search_results if res.file_path in actual_files_modified][:n_positive]\n",
    "        # negative_samples = [res.commit_msg for res in search_results if res.file_path not in actual_files_modified][:n_negative]\n",
    "\n",
    "        for sample_msg in positive_samples:\n",
    "            # sample_msg  = reverse_tokenize(json.loads(sample.raw)['contents'])\n",
    "            data.append((commit_message, sample_msg, 1))\n",
    "\n",
    "        for sample_msg in negative_samples:\n",
    "            # sample_msg  = reverse_tokenize(json.loads(sample.raw)['contents'])\n",
    "            data.append((commit_message, sample_msg, 0))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTReranker:\n",
    "    # def __init__(self, model_name, psg_len, psg_cnt, psg_stride, agggreagtion_strategy, batch_size, use_gpu=True):\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        # self.model = AutoModel.from_pretrained(self.model_name, num_labels=1)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            # print GPU info\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt']\n",
    "        # self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        # self.max_title_len = parameters.get('max_title_len', 0)\n",
    "        # self.use_title = self.max_title_len > 0\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        # self.max_seq_length = parameters.get('max_seq_length', 512)\n",
    "        self.max_seq_length = self.tokenizer.model_max_length\n",
    "\n",
    "        print(f\"Initialized BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "        input_dim = parameters['INPUT_DIM']  # Default BERT hidden size\n",
    "        hidden_dim = parameters['HIDDEN_DIM']  # Example hidden size\n",
    "        output_dim = parameters['OUTPUT_DIM']  # We want a single score as output\n",
    "\n",
    "        self.mlp = MLP(input_dim, hidden_dim, output_dim).to(self.device)\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        aggregated_results = aggregated_results[:self.rerank_depth]\n",
    "        print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        query_passage_pairs = []\n",
    "        for agg_result in aggregated_results:\n",
    "            query_passage_pairs.extend(\n",
    "                (query, result.commit_msg)\n",
    "                for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "            )\n",
    "        print(f'Flattened query passage pairs: {len(query_passage_pairs)}')\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank')\n",
    "            return aggregated_results\n",
    "        # query_passage_pairs = [(query, result.commit_msg) for aggregated_result in aggregated_results for result in aggregated_result.contributing_results]\n",
    "\n",
    "        # print('Flattened query passage pairs')\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # print('Encoded query passage pairs')\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.cat([encoded_pair['input_ids'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.cat([encoded_pair['attention_mask'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "\n",
    "        # print('Created dataloader')\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        for agg_result in aggregated_results:\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has\n",
    "            end_index = score_index + len(agg_result.contributing_results)\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get the pooled output from BERT's [CLS] token\n",
    "                pooled_output = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "\n",
    "                # Pass the pooled output through the MLP to get the scores\n",
    "                logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # Collect the scores (detach them from the computation graph and move to CPU)\n",
    "                scores.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def train_mlp(self, train_dataloader, validation_dataloader):\n",
    "        # Set BERT parameters to not require gradients\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # Set up the optimizer. Only parameters of the MLP will be updated.\n",
    "        optimizer = torch.optim.Adam(self.mlp.parameters(), lr=self.parameters['lr'])\n",
    "\n",
    "        # Set up the loss function\n",
    "        criterion = nn.BCEWithLogitsLoss()  #\n",
    "\n",
    "        # Set up training variables\n",
    "        num_epochs = self.parameters['num_epochs']\n",
    "        # Training loop\n",
    "\n",
    "        print('Starting training loop')\n",
    "        # for epoch in range(epochs):\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # self.model.eval()  # Make sure the BERT model is in evaluation mode\n",
    "            self.model.train()  # BERT finetuning should be in train mode\n",
    "            self.mlp.train()  # MLP should be in training mode\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch in train_dataloader:\n",
    "                b_input_ids, b_attention_mask, b_labels = batch\n",
    "                # queries, commits, b_labels = batch\n",
    "\n",
    "                # # tokenize the query passage pairs and create tensors for the input ids, attention masks, and token type ids\n",
    "                # encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in zip(queries, commits)]\n",
    "\n",
    "                # b_input_ids = torch.cat([encoded_pair['input_ids'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "                # b_attention_mask = torch.cat([encoded_pair['attention_mask'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "                # b_input_ids = b_input_ids.to(self.device)\n",
    "                # b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # tokenize the query passage pairs\n",
    "\n",
    "                # b_labels = b_labels.to(self.device)\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "                b_labels = b_labels.float().to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                # with torch.no_grad():  # No need to calculate gradients for BERT\n",
    "                pooled_output = self.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, b_labels.float())\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Calculate average loss over the training data.\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "            # Validation step\n",
    "            self.model.eval()\n",
    "            self.mlp.eval()\n",
    "            total_eval_loss = 0\n",
    "            for batch in validation_dataloader:\n",
    "                b_input_ids, b_attention_mask, b_labels = batch\n",
    "                # queries, commits, b_labels = batch\n",
    "\n",
    "                # # tokenize the query passage pairs and create tensors for the input ids, attention masks, and token type ids\n",
    "                # encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in zip(queries, commits)]\n",
    "\n",
    "                # b_input_ids = torch.cat([encoded_pair['input_ids'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "                # b_attention_mask = torch.cat([encoded_pair['attention_mask'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "                # b_input_ids = b_input_ids.to(self.device)\n",
    "                # b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # tokenize the query passage pairs\n",
    "\n",
    "                # b_labels = b_labels.to(self.device)\n",
    "\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "                b_labels = b_labels.float().to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pooled_output = self.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                    logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, b_labels.float())\n",
    "                total_eval_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Average training loss: {avg_train_loss}\")\n",
    "            print(f\"Validation Loss: {avg_val_loss}\")\n",
    "            break\n",
    "\n",
    "            # Here you can add early stopping based on validation loss\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        reranked_results = self.rerank(query, aggregated_results)\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']\n",
    "repo_path = '../smalldata/fbr/'\n",
    "index_path = '../smalldata/fbr/index_commit_tokenized/'\n",
    "K = 1000\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69835 entries, 0 to 69834\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   owner                  69835 non-null  string  \n",
      " 1   repo_name              69835 non-null  string  \n",
      " 2   commit_date            69835 non-null  int64   \n",
      " 3   commit_id              69835 non-null  string  \n",
      " 4   commit_message         69835 non-null  string  \n",
      " 5   file_path              69835 non-null  string  \n",
      " 6   cur_file_content       67179 non-null  string  \n",
      " 7   previous_commit_id     64247 non-null  string  \n",
      " 8   previous_file_path     4140 non-null   string  \n",
      " 9   previous_file_content  64247 non-null  string  \n",
      " 10  diff                   61590 non-null  string  \n",
      " 11  status                 69835 non-null  category\n",
      " 12  is_merge_request       69835 non-null  bool    \n",
      " 13  file_extension         69835 non-null  object  \n",
      "dtypes: bool(1), category(1), int64(1), object(1), string(10)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df = get_combined_df(repo_path)\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../smalldata/fbr/index_commit_tokenized/\n",
      "Index Stats: {'total_terms': 8061856, 'documents': 69835, 'non_empty_documents': 69835, 'unique_terms': 14589}\n"
     ]
    }
   ],
   "source": [
    "BM25_AGGR_STRAT = 'sump'\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to ../smalldata/fbr/BM25_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file='BM25_metrics.txt', aggregation_strategy=BM25_AGGR_STRAT, repo_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.2247, 'P@10': 0.09, 'P@100': 0.029, 'P@1000': 0.0033, 'MRR': 0.2406, 'Recall@100': 0.6583, 'Recall@1000': 0.7417}\n"
     ]
    }
   ],
   "source": [
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POSITIVE = 10\n",
    "NUM_NEGATIVE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking with BERT\n",
    "params = {\n",
    "    'model_name': 'microsoft/codebert-base',\n",
    "    'psg_len': 400,\n",
    "    'psg_cnt': 3,\n",
    "    # 'psg_stride': 32,\n",
    "    'aggregation_strategy': 'sump',\n",
    "    'batch_size': 32,\n",
    "    'use_gpu': True,\n",
    "    'rerank_depth': 500,\n",
    "    'num_epochs': 10,\n",
    "    'lr': 2e-5,\n",
    "    'INPUT_DIM': 768,\n",
    "    'HIDDEN_DIM': 100,\n",
    "    'OUTPUT_DIM': 1,\n",
    "    # 'max_seq_length': 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_len': 400, 'psg_cnt': 3, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 500, 'num_epochs': 10, 'lr': 2e-05, 'INPUT_DIM': 768, 'HIDDEN_DIM': 100, 'OUTPUT_DIM': 1}\n"
     ]
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(params)\n",
    "rerankers = [bert_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commits after midpoint date: 5795\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter necessary columns\n",
    "filtered_df = combined_df[['commit_date', 'commit_message', 'commit_id', 'file_path']]\n",
    "\n",
    "# Step 2: Group by commit_id\n",
    "grouped_df = filtered_df.groupby(['commit_id', 'commit_date', 'commit_message'])['file_path'].apply(list).reset_index()\n",
    "grouped_df.rename(columns={'file_path': 'actual_files_modified'}, inplace=True)\n",
    "\n",
    "# Step 3: Determine midpoint and filter dataframe\n",
    "midpoint_date = np.median(grouped_df['commit_date'])\n",
    "recent_df = grouped_df[grouped_df['commit_date'] > midpoint_date]\n",
    "print(f'Number of commits after midpoint date: {len(recent_df)}')\n",
    "# sys.exit(0)\n",
    "\n",
    "recent_df = recent_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 0 to 2015\n",
      "Data columns (total 4 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              1000 non-null   string\n",
      " 1   commit_date            1000 non-null   int64 \n",
      " 2   commit_message         1000 non-null   string\n",
      " 3   actual_files_modified  1000 non-null   object\n",
      "dtypes: int64(1), object(1), string(2)\n",
      "memory usage: 39.1+ KB\n"
     ]
    }
   ],
   "source": [
    "recent_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Preparing data from dataframe of size: 800\n",
      "Preparing data from dataframe of size: 100\n",
      "Preparing data from dataframe of size: 100\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split recent dataframe and prepare data\n",
    "df_train, df_temp = train_test_split(recent_df, test_size=0.2, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print('Preparing data...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_depth = 1000\n",
    "train_data = prepare_data_from_df(df_train, bm25_searcher, depth=train_depth, n_positive=NUM_POSITIVE, n_negative=NUM_NEGATIVE)\n",
    "val_data = prepare_data_from_df(df_val, bm25_searcher, depth=train_depth, n_positive=NUM_POSITIVE, n_negative=NUM_NEGATIVE)\n",
    "test_data = prepare_data_from_df(df_test, bm25_searcher, depth=train_depth, n_positive=NUM_POSITIVE, n_negative=NUM_NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 83414\n",
      "Val data size: 10372\n",
      "Test data size: 10486\n",
      "Train data sample: (\"Clean up enableSyncDefaultUpdates flag a bit (#26858)\\n\\n## Overview\\r\\n\\r\\nDoes a few things:\\r\\n- Renames `enableSyncDefaultUpdates` to\\r\\n`forceConcurrentByDefaultForTesting`\\r\\n- Changes the way it's used so it's dead-code eliminated separate from\\r\\n`allowConcurrentByDefault`\\r\\n- Deletes a bunch of the gated code\\r\\n\\r\\nThe gates that are deleted are unnecessary now. We were keeping them\\r\\nwhen we originally thought we would come back to being concurrent by\\r\\ndefault. But we've shifted and now sync-by default is the desired\\r\\nbehavior long term, so there's no need to keep all these forked tests\\r\\naround.\\r\\n\\r\\nI'll follow up to delete more of the forked behavior if possible.\\r\\nIdeally we wouldn't need this flag even if we're still using\\r\\n`allowConcurrentByDefault`.\", 'Revert \"Re-arrange slightly to prevent refactor hazard (#16743)\" (#16769)\\n\\nThis reverts commit ab4951fc03750a726e412b90126e0737fbd04014.\\r\\n\\r\\n* Track \"pending\" and \"suspended\" ranges\\r\\n\\r\\nA FiberRoot can have pending work at many distinct priorities. (Note: we\\r\\nrefer to these levels as \"expiration times\" to distinguish the concept\\r\\nfrom Scheduler\\'s notion of priority levels, which represent broad\\r\\ncategories of work. React expiration times are more granualar. They\\'re\\r\\nmore like a concurrent thread ID, which also happens to correspond to a\\r\\nmoment on a timeline. It\\'s an overloaded concept and I\\'m handwaving over\\r\\nsome of the details.)\\r\\n\\r\\nGiven a root, there\\'s no convenient way to read all the pending levels\\r\\nin the entire tree, i.e. there\\'s no single queue-like structure that\\r\\ntracks all the levels, because that granularity of information is not\\r\\nneeded by our algorithms. Instead we track the subset of information\\r\\nthat we actually need — most importantly, the highest priority level\\r\\nthat exists in the entire tree.\\r\\n\\r\\nAside from that, the other information we track includes the range of\\r\\npending levels that are known to be suspended, and therefore should not\\r\\nbe worked on.\\r\\n\\r\\nThis is a refactor of how that information is tracked, and what each\\r\\nfield represents:\\r\\n\\r\\n- A *pending* level is work that is unfinished, or not yet committed.\\r\\n  This includes work that is suspended from committing.\\r\\n  `firstPendingTime` and `lastPendingTime` represent the range of\\r\\n  pending work. (Previously, \"pending\" was the same as \"not suspended.\")\\r\\n- A *suspended* level is work that did not complete because data was\\r\\n  missing. `firstSuspendedTime` and `lastSuspendedTime` represent the\\r\\n  range of suspended work. It is a subset of the pending range. (These\\r\\n  fields are new to this commit.)\\r\\n- `nextAfterSuspendedTime` represents the next known level that comes\\r\\n  after the suspended range.\\r\\n\\r\\nThis commit doesn\\'t change much in terms of observable behavior. The one\\r\\nchange is that, when a level is suspended, React will continue working\\r\\non the next known level instead of jumping straight to the last pending\\r\\nlevel. Subsequent commits will use this new structure for a more\\r\\nsubstantial refactor for how tasks are scheduled per root.\\r\\n\\r\\n* Get next expiration time from FiberRoot\\r\\n\\r\\nGiven a FiberRoot, we should be able to determine the next expiration\\r\\ntime that needs to be worked on, taking into account the levels that\\r\\nare pending, suspended, pinged, and so on.\\r\\n\\r\\nThis removes the `expirationTime` argument from\\r\\n`scheduleCallbackForRoot`, and renames it to `ensureRootIsScheduled` to\\r\\nreflect the new signature. The expiration time is instead read from the\\r\\nroot using a new function, `getNextExpirationTimeToWorkOn`.\\r\\n\\r\\nThe next step will be to remove the `expirationTime` argument from\\r\\n`renderRoot`, too.\\r\\n\\r\\n* Don\\'t bind expiration time to render callback\\r\\n\\r\\nThis is a fragile pattern because there\\'s only meant to be a single\\r\\ntask per root, running at a single expiration time. Instead of binding\\r\\nthe expiration time to the render task, or closing over it, we should\\r\\ndetermine the correct expiration time to work on using fields we\\r\\nstore on the root object itself.\\r\\n\\r\\nThis removes the \"return a continuation\" pattern from the\\r\\n`renderRoot` function. Continuation handling is now handled by\\r\\nthe wrapper function, which I\\'ve renamed from `runRootCallback` to\\r\\n`performWorkOnRoot`. That function is merely an entry point to\\r\\n`renderRoot`, so I\\'ve also removed the callback argument.\\r\\n\\r\\nSo to sum up, at at the beginning of each task, `performWorkOnRoot`\\r\\ndetermines which expiration time to work on, then calls `renderRoot`.\\r\\nAnd before exiting, it checks if it needs to schedule another task.\\r\\n\\r\\n* Update error recovery test to match new semantics\\r\\n\\r\\n* Remove `lastPendingTime` field\\r\\n\\r\\nIt\\'s no longer used anywhere\\r\\n\\r\\n* Restart on update to already suspended root\\r\\n\\r\\nIf the work-in-progress root already suspended with a delay, then the\\r\\ncurrent render definitely won\\'t finish. We should interrupt the render\\r\\nand switch to the incoming update.\\r\\n\\r\\n* Restart on suspend if return path has an update\\r\\n\\r\\nSimilar to the previous commit, if we suspend with a delay, and\\r\\nsomething in the return path has a pending update, we should abort\\r\\nthe current render and switch to the update instead.\\r\\n\\r\\n* Track the next unprocessed level globally\\r\\n\\r\\nInstead of backtracking the return path. The main advantage over the\\r\\nbacktracking approach is that we don\\'t have to backtrack from the source\\r\\nfiber. (The main disadvantages are that it requires another module-level\\r\\nvariable, and that it could include updates from unrelated\\r\\nsibling paths.)\\r\\n\\r\\n* Re-arrange slightly to prevent refactor hazard\\r\\n\\r\\nIt should not be possible to perform any work on a root without\\r\\ncalling `ensureRootIsScheduled` before exiting. Otherwise, we could\\r\\nfail to schedule a callback for pending work and the app could freeze.\\r\\n\\r\\nTo help prevent a future refactor from introducing such a bug, this\\r\\nchange makes it so that `renderRoot` is always wrapped in try-finally,\\r\\nand the `finally` block calls `ensureRootIsScheduled`.\\r\\n\\r\\n* Remove recursive calls to `renderRoot`.\\r\\n\\r\\nThere are a few leftover cases where `renderRoot` is called recursively.\\r\\nAll of them are related to synchronously flushing work before its\\r\\nexpiration time.\\r\\n\\r\\nWe can remove these calls by tracking the last expired level on the\\r\\nroot, similar to what we do for other types of pending work, like pings.\\r\\n\\r\\n* Remove argument from performSyncWorkOnRoot\\r\\n\\r\\nRead the expiration time from the root, like we do\\r\\nin performConcurrentWorkOnRoot.', 1)\n",
      "Train label distribution: (array([0, 1]), array([79932,  3482]))\n",
      "Val label distribution: (array([0, 1]), array([9938,  434]))\n",
      "Test label distribution: (array([0, 1]), array([10000,   486]))\n"
     ]
    }
   ],
   "source": [
    "# get distribution of labels\n",
    "train_labels = [label for _, _, label in train_data]\n",
    "val_labels = [label for _, _, label in val_data]\n",
    "test_labels = [label for _, _, label in test_data]\n",
    "\n",
    "# print size of data\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Val data size: {len(val_data)}')\n",
    "print(f'Test data size: {len(test_data)}')\n",
    "\n",
    "print(f'Train data sample: {train_data[0]}')\n",
    "\n",
    "print(f'Train label distribution: {np.unique(train_labels, return_counts=True)}')\n",
    "print(f'Val label distribution: {np.unique(val_labels, return_counts=True)}')\n",
    "print(f'Test label distribution: {np.unique(test_labels, return_counts=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query, passage, label = self.data[index]\n",
    "\n",
    "        # tokenize the query passage pairs and create tensors for the input ids, attention masks, and token type ids\n",
    "        encoded_pair = self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "        input_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TripletDataset(train_data, bert_reranker.tokenizer, bert_reranker.max_seq_length)\n",
    "val_dataset = TripletDataset(val_data, bert_reranker.tokenizer, bert_reranker.max_seq_length)\n",
    "test_dataset = TripletDataset(test_data, bert_reranker.tokenizer, bert_reranker.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=bert_reranker.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=bert_reranker.batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bert_reranker.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_reranker.train_mlp(train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
