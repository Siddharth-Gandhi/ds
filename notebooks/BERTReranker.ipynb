{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "from utils import AggregatedSearchResult, get_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_from_df(df, searcher, depth, n_positive, n_negative):\n",
    "    data = []\n",
    "    print(f'Preparing data from dataframe of size: {len(df)}')\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        commit_message = row['commit_message']\n",
    "        actual_files_modified = row['actual_files_modified']\n",
    "        # search_results = search(searcher, commit_message, row['commit_date'], 1000)\n",
    "\n",
    "        # search_results = searcher.search(commit_message, row['commit_date'], 100)\n",
    "        search_results = searcher.pipeline(commit_message, row['commit_date'], depth, 'sump')\n",
    "\n",
    "        # flatten the contributing results for each aggregated result\n",
    "        search_results = [result for agg_result in search_results for result in agg_result.contributing_results]\n",
    "\n",
    "        # efficiently get the top n_positive and n_negative samples\n",
    "        positive_samples = []\n",
    "        negative_samples = []\n",
    "\n",
    "        for result in search_results:\n",
    "            if result.file_path in actual_files_modified and len(positive_samples) < n_positive:\n",
    "                positive_samples.append(result.commit_msg)\n",
    "            elif result.file_path not in actual_files_modified and len(negative_samples) < n_negative:\n",
    "                negative_samples.append(result.commit_msg)\n",
    "\n",
    "            if len(positive_samples) == n_positive and len(negative_samples) == n_negative:\n",
    "                break\n",
    "\n",
    "        # Get positive and negative samples\n",
    "        # positive_samples = [res.commit_msg for res in search_results if res.file_path in actual_files_modified][:n_positive]\n",
    "        # negative_samples = [res.commit_msg for res in search_results if res.file_path not in actual_files_modified][:n_negative]\n",
    "\n",
    "        for sample_msg in positive_samples:\n",
    "            # sample_msg  = reverse_tokenize(json.loads(sample.raw)['contents'])\n",
    "            data.append((commit_message, sample_msg, 1))\n",
    "\n",
    "        for sample_msg in negative_samples:\n",
    "            # sample_msg  = reverse_tokenize(json.loads(sample.raw)['contents'])\n",
    "            data.append((commit_message, sample_msg, 0))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTReranker:\n",
    "    # def __init__(self, model_name, psg_len, psg_cnt, psg_stride, agggreagtion_strategy, batch_size, use_gpu=True):\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        # self.model = AutoModel.from_pretrained(self.model_name, num_labels=1)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            # print GPU info\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt']\n",
    "        # self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        # self.max_title_len = parameters.get('max_title_len', 0)\n",
    "        # self.use_title = self.max_title_len > 0\n",
    "        self.rerank_depth = parameters.get['rerank_depth']\n",
    "        # self.max_seq_length = parameters.get('max_seq_length', 512)\n",
    "        self.max_seq_length = self.tokenizer.model_max_length\n",
    "\n",
    "        print(f\"Initialized BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "        input_dim = parameters['INPUT_DIM']  # Default BERT hidden size\n",
    "        hidden_dim = parameters['HIDDEN_DIM']  # Example hidden size\n",
    "        output_dim = parameters['OUTPUT_DIM']  # We want a single score as output\n",
    "\n",
    "        self.mlp = MLP(input_dim, hidden_dim, output_dim).to(self.device)\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        aggregated_results = aggregated_results[:self.rerank_depth]\n",
    "        print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        query_passage_pairs = []\n",
    "        for agg_result in aggregated_results:\n",
    "            for result in agg_result.contributing_results[:self.psg_cnt]:\n",
    "                query_passage_pairs.append((query, result.commit_msg))\n",
    "\n",
    "        print(f'Flattened query passage pairs: {len(query_passage_pairs)}')\n",
    "\n",
    "        if len(query_passage_pairs) == 0:\n",
    "            print('WARNING: No query passage pairs to rerank')\n",
    "            return aggregated_results\n",
    "        # query_passage_pairs = [(query, result.commit_msg) for aggregated_result in aggregated_results for result in aggregated_result.contributing_results]\n",
    "\n",
    "        # print('Flattened query passage pairs')\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # print('Encoded query passage pairs')\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.cat([encoded_pair['input_ids'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.cat([encoded_pair['attention_mask'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "\n",
    "        # print('Created dataloader')\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        for agg_result in aggregated_results:\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has\n",
    "            end_index = score_index + len(agg_result.contributing_results)\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get the pooled output from BERT's [CLS] token\n",
    "                pooled_output = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "\n",
    "                # Pass the pooled output through the MLP to get the scores\n",
    "                logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # Collect the scores (detach them from the computation graph and move to CPU)\n",
    "                scores.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def train_mlp(self, train_dataloader, validation_dataloader):\n",
    "        # Set BERT parameters to not require gradients\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Set up the optimizer. Only parameters of the MLP will be updated.\n",
    "        optimizer = torch.optim.Adam(self.mlp.parameters(), lr=self.parameters.get('LEARNING_RATE', 1e-4))\n",
    "\n",
    "        # Set up the loss function\n",
    "        criterion = nn.BCEWithLogitsLoss()  #\n",
    "\n",
    "        # Set up training variables\n",
    "        epochs = self.parameters.get('EPOCHS', 10)\n",
    "        # Training loop\n",
    "\n",
    "        print('Starting training loop')\n",
    "        # for epoch in range(epochs):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.model.eval()  # Make sure the BERT model is in evaluation mode\n",
    "            self.mlp.train()  # MLP should be in training mode\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch in train_dataloader:\n",
    "                # b_input_ids, b_attention_mask, b_labels = batch\n",
    "                queries, commits, b_labels = batch\n",
    "\n",
    "                # tokenize the query passage pairs and create tensors for the input ids, attention masks, and token type ids\n",
    "                encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in zip(queries, commits)]\n",
    "\n",
    "                b_input_ids = torch.cat([encoded_pair['input_ids'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "                b_attention_mask = torch.cat([encoded_pair['attention_mask'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "                # b_input_ids = b_input_ids.to(self.device)\n",
    "                # b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # tokenize the query passage pairs\n",
    "\n",
    "                # b_labels = b_labels.to(self.device)\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "                b_labels = b_labels.float().to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.no_grad():  # No need to calculate gradients for BERT\n",
    "                    pooled_output = self.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, b_labels.float())\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Calculate average loss over the training data.\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "            # Validation step\n",
    "            self.model.eval()\n",
    "            self.mlp.eval()\n",
    "            total_eval_loss = 0\n",
    "            for batch in validation_dataloader:\n",
    "                queries, commits, b_labels = batch\n",
    "\n",
    "                # tokenize the query passage pairs and create tensors for the input ids, attention masks, and token type ids\n",
    "                encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in zip(queries, commits)]\n",
    "\n",
    "                b_input_ids = torch.cat([encoded_pair['input_ids'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "                b_attention_mask = torch.cat([encoded_pair['attention_mask'] for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "                # b_input_ids = b_input_ids.to(self.device)\n",
    "                # b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # tokenize the query passage pairs\n",
    "\n",
    "                # b_labels = b_labels.to(self.device)\n",
    "\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "                b_labels = b_labels.float().to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pooled_output = self.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                    logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, b_labels.float())\n",
    "                total_eval_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Average training loss: {avg_train_loss}\")\n",
    "            print(f\"Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "            # Here you can add early stopping based on validation loss\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        reranked_results = self.rerank(query, aggregated_results)\n",
    "        return reranked_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
