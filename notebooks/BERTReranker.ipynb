{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# import pickle\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "from utils import (\n",
    "    AggregatedSearchResult,\n",
    "    TripletDataset,\n",
    "    get_combined_df,\n",
    "    prepare_triplet_data_from_df,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  1\n",
      "Current cuda device:  0\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "# print torch devices available\n",
    "print('Available devices: ', torch.cuda.device_count())\n",
    "print('Current cuda device: ', torch.cuda.current_device())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1\n",
    "# # class BERTReranker:\n",
    "#     def __init__(self, parameters):\n",
    "#         self.parameters = parameters\n",
    "#         self.model_name = parameters['model_name']\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "#         # self.model = AutoModel.from_pretrained(self.model_name, num_labels=1)\n",
    "#         self.model = AutoModel.from_pretrained(self.model_name)\n",
    "#         # self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1)\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "#         self.model.to(self.device)\n",
    "\n",
    "#         print(f'Using device: {self.device}')\n",
    "\n",
    "#         if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "#             # print GPU info\n",
    "#             print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "#             print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "#             print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "#         self.psg_len = parameters['psg_len']\n",
    "#         self.psg_cnt = parameters['psg_cnt']\n",
    "#         # self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "#         self.aggregation_strategy = parameters['aggregation_strategy']\n",
    "#         self.batch_size = parameters['batch_size']\n",
    "#         # self.max_title_len = parameters.get('max_title_len', 0)\n",
    "#         # self.use_title = self.max_title_len > 0\n",
    "#         self.rerank_depth = parameters['rerank_depth']\n",
    "#         # self.max_seq_length = parameters.get('max_seq_length', 512)\n",
    "#         self.max_seq_length = self.tokenizer.model_max_length\n",
    "\n",
    "#         print(f\"Initialized BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "#         # input_dim = parameters['INPUT_DIM']  # Default BERT hidden size\n",
    "#         # hidden_dim = parameters['HIDDEN_DIM']   # Example hidden size\n",
    "#         # output_dim = parameters['OUTPUT_DIM']  # We want a single score as output\n",
    "\n",
    "#         self.mlp = MLP(self.model.config.hidden_size, parameters['hidden_dim'], 1, parameters['dropout_prob']).to(self.device)\n",
    "\n",
    "#     def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "#         \"\"\"\n",
    "#         Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "#         query: The issue query string.\n",
    "#         aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "#         \"\"\"\n",
    "#         # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "#         # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "#         # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "#         query_passage_pairs = []\n",
    "#         for agg_result in aggregated_results:\n",
    "#             query_passage_pairs.extend(\n",
    "#                 (query, result.commit_msg)\n",
    "#                 for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "#             )\n",
    "\n",
    "#         if not query_passage_pairs:\n",
    "#             print('WARNING: No query passage pairs to rerank')\n",
    "#             print(query, aggregated_results, self.psg_cnt)\n",
    "#             return aggregated_results\n",
    "\n",
    "#         # tokenize the query passage pairs\n",
    "#         encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "#         # create tensors for the input ids, attention masks\n",
    "#         input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "#         attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "#         # Create a dataloader for feeding the data to the model\n",
    "#         dataset = TensorDataset(input_ids, attention_masks)\n",
    "#         dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "#         scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "#         score_index = 0\n",
    "#         # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "#         for agg_result in aggregated_results:\n",
    "#             # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "#             assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "#             end_index = score_index + len(agg_result.contributing_results[: self.psg_cnt])\n",
    "#             cur_passage_scores = scores[score_index:end_index]\n",
    "#             score_index = end_index\n",
    "\n",
    "\n",
    "#             # Aggregate the scores for the current aggregated result\n",
    "#             agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "#             agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "#         assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "#         # Sort by the new aggregated score\n",
    "#         aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "#         return aggregated_results\n",
    "\n",
    "#     def get_scores(self, dataloader, model):\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in dataloader:\n",
    "#                 # Unpack the batch and move it to GPU\n",
    "#                 b_input_ids, b_attention_mask = batch\n",
    "#                 b_input_ids = b_input_ids.to(self.device)\n",
    "#                 b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "#                 # Get the pooled output from BERT's [CLS] token\n",
    "#                 # pooled_output = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "\n",
    "#                 cls_output = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "#                 # # Pass the pooled output through the MLP to get the scores\n",
    "#                 # logits = self.mlp(pooled_output).squeeze(-1) # type: ignore\n",
    "#                 logits = self.mlp(cls_output).squeeze(-1) # type: ignore\n",
    "\n",
    "#                 # # Collect the scores (detach them from the computation graph and move to CPU)\n",
    "#                 scores.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "#                 # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "#                 # logits = outputs.logits\n",
    "#                 # scores.extend(logits.detach().cpu().numpy().squeeze(-1))\n",
    "\n",
    "#         return scores\n",
    "\n",
    "#     def aggregate_scores(self, passage_scores):\n",
    "#         \"\"\"\n",
    "#         Aggregate passage scores based on the specified strategy.\n",
    "#         \"\"\"\n",
    "#         if len(passage_scores) == 0:\n",
    "#             return 0.0\n",
    "\n",
    "\n",
    "#         if self.aggregation_strategy == 'firstp':\n",
    "#             return passage_scores[0]\n",
    "#         if self.aggregation_strategy == 'maxp':\n",
    "#             return max(passage_scores)\n",
    "#         if self.aggregation_strategy == 'avgp':\n",
    "#             return sum(passage_scores) / len(passage_scores)\n",
    "#         if self.aggregation_strategy == 'sump':\n",
    "#             return sum(passage_scores)\n",
    "#         # else:\n",
    "#         raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "#     def rerank_pipeline(self, query, aggregated_results):\n",
    "#         if len(aggregated_results) == 0:\n",
    "#             return aggregated_results\n",
    "#         top_results = aggregated_results[:self.rerank_depth]\n",
    "#         bottom_results = aggregated_results[self.rerank_depth:]\n",
    "#         reranked_results = self.rerank(query, top_results)\n",
    "#         min_top_score = reranked_results[-1].score\n",
    "#         # now adjust the scores of bottom_results\n",
    "#         for i, result in enumerate(bottom_results):\n",
    "#             result.score = min_top_score - i - 1\n",
    "#         # combine the results\n",
    "#         reranked_results.extend(bottom_results)\n",
    "#         assert(len(reranked_results) == len(aggregated_results))\n",
    "#         return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    # repo_path='smalldata/ftr',\n",
    "    # index_path='smalldata/ftr/index_commit_tokenized/',\n",
    "    repo_path='2_7/apache_spark/',\n",
    "    index_path='2_7/apache_spark/index_commit_tokenized/',\n",
    "    k=1000,\n",
    "    n=100,\n",
    "    overwrite_cache=False,\n",
    "    freeze_bert=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']\n",
    "repo_path = args.repo_path\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at 2_7/apache_spark/index_commit_tokenized/\n",
      "Index Stats: {'total_terms': 58140950, 'documents': 188006, 'non_empty_documents': 188006, 'unique_terms': 24952}\n"
     ]
    }
   ],
   "source": [
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 output path: ../smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to ../smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt\n",
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.1542, 'P@10': 0.087, 'P@100': 0.0267, 'P@1000': 0.0041, 'MRR': 0.2133, 'Recall@100': 0.5077, 'Recall@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm25_output_path = os.path.join(eval_path, f'bm25_baseline_N{n}_K{K}_metrics.txt')\n",
    "print(f'BM25 output path: {bm25_output_path}')\n",
    "\n",
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=bm25_output_path, aggregation_strategy=BM25_AGGR_STRAT, repo_path=repo_path)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking with BERT\n",
    "params = {\n",
    "    'model_name': 'microsoft/codebert-base',\n",
    "    'psg_len': 400,\n",
    "    'psg_cnt': 5,\n",
    "    # 'psg_stride': 32,\n",
    "    'aggregation_strategy': 'sump',\n",
    "    'batch_size': 16,\n",
    "    # 'batch_size': 512,\n",
    "    # 'batch_size': 1,\n",
    "    'use_gpu': True,\n",
    "    'rerank_depth': 250,\n",
    "    'num_epochs': 3,\n",
    "    # 'mlp_lr': 1e-2,\n",
    "    'mlp_lr': 1e-3,\n",
    "    'bert_lr': 5e-5,\n",
    "    'hidden_dim': 128,\n",
    "    'num_positives': 10,\n",
    "    'num_negatives': 10,\n",
    "    'train_depth': 1000,\n",
    "    'num_workers': 8,\n",
    "    'weight_decay': 0.01,\n",
    "    'dropout_prob': 0.5,\n",
    "    'train_commits': 1500,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.66661695926252"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get average length of commit messages\n",
    "combined_df['commit_message'].str.split().str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commits after midpoint date: 16839\n",
      "Number of commits after filtering by commit message length: 5280\n",
      "Number of commits after sampling: 1500\n"
     ]
    }
   ],
   "source": [
    "filtered_df = combined_df[['commit_date', 'commit_message', 'commit_id', 'file_path', 'diff']]\n",
    "\n",
    "# Step 2: Group by commit_id\n",
    "grouped_df = filtered_df.groupby(['commit_id', 'commit_date', 'commit_message'])['file_path'].apply(list).reset_index()\n",
    "grouped_df.rename(columns={'file_path': 'actual_files_modified'}, inplace=True)\n",
    "\n",
    "# Step 3: Determine midpoint and filter dataframe\n",
    "midpoint_date = np.median(grouped_df['commit_date'])\n",
    "recent_df = grouped_df[grouped_df['commit_date'] > midpoint_date]\n",
    "print(f'Number of commits after midpoint date: {len(recent_df)}')\n",
    "\n",
    "# Step 4: Filter out commits with less than average length commit messages\n",
    "average_commit_len = recent_df['commit_message'].str.split().str.len().mean()\n",
    "# filter out commits with less than average length\n",
    "recent_df = recent_df[recent_df['commit_message'].str.split().str.len() > average_commit_len]\n",
    "print(f'Number of commits after filtering by commit message length: {len(recent_df)}')\n",
    "\n",
    "# Step 5: randomly sample 1500 rows from recent_df\n",
    "recent_df = recent_df.sample(params['train_commits'])\n",
    "print(f'Number of commits after sampling: {len(recent_df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data first\n",
    "if not os.path.exists(os.path.join(repo_path, 'cache')):\n",
    "    os.makedirs(os.path.join(repo_path, 'cache'))\n",
    "triplet_cache = os.path.join(repo_path, 'cache', 'triplet_data_cache.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data from dataframe of size: 2 with search_depth: 1000\n",
      "Percentage of positives: 0.2, Percentage of negatives: 0.8\n"
     ]
    }
   ],
   "source": [
    "# def temp_prep(df, searcher, search_depth, num_positives, num_negatives):\n",
    "\n",
    "#     data = []\n",
    "#     print(f'Preparing data from dataframe of size: {len(df)} with search_depth: {search_depth}')\n",
    "#     total_positives, total_negatives = 0, 0\n",
    "#     for _, row in df.iterrows():\n",
    "#     # for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "#         cur_positives = 0\n",
    "#         cur_negatives = 0\n",
    "#         pos_commit_ids = set()\n",
    "#         neg_commit_ids = set()\n",
    "#         commit_message = row['commit_message']\n",
    "#         actual_files_modified = row['actual_files_modified']\n",
    "\n",
    "#         agg_search_results = searcher.pipeline(commit_message, row['commit_date'], search_depth, 'sump', aggregate_on='commit')\n",
    "\n",
    "#         # for each agg_result, find out how many files it has edited are in actual_files_modified and sort by score\n",
    "\n",
    "#         for agg_result in agg_search_results:\n",
    "#             agg_result_files = set([result.file_path for result in agg_result.contributing_results])\n",
    "#             intersection = agg_result_files.intersection(actual_files_modified)\n",
    "#             # TODO maybe try this for training\n",
    "#             agg_result.score = len(intersection) / len(agg_result_files) # how focused the commit is\n",
    "#             # agg_result.score = len(intersection)\n",
    "\n",
    "#         agg_search_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "#         # go from top to bottom, first num_positives non-0 scores are positive samples and the next num_negatives are negative samples\n",
    "#         for agg_result in agg_search_results:\n",
    "#             cur_commit_msg = agg_result.contributing_results[0].commit_message\n",
    "#             if cur_positives < num_positives and agg_result.score > 0:\n",
    "#                 # meaning there is at least one file in the agg_result that is in actual_files_modified\n",
    "#                 # pos_commits.append(agg_result)\n",
    "#                 data.append((commit_message, cur_commit_msg, 1))\n",
    "#                 cur_positives += 1\n",
    "#                 pos_commit_ids.add(agg_result.commit_id)\n",
    "#             elif cur_negatives < num_negatives:\n",
    "#                 # neg_commits.append(agg_result)\n",
    "#                 data.append((commit_message, cur_commit_msg, 0))\n",
    "#                 cur_negatives += 1\n",
    "#                 neg_commit_ids.add(agg_result.commit_id)\n",
    "#             if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "#                 break\n",
    "\n",
    "#         assert len(pos_commit_ids.intersection(neg_commit_ids)) == 0, 'Positive and negative commit ids should not intersect'\n",
    "#         # print(f\"Total positives: {cur_positives}, Total negatives: {cur_negatives}\")\n",
    "#         total_positives += cur_positives\n",
    "#         total_negatives += cur_negatives\n",
    "\n",
    "#     # # Write data to cache file\n",
    "#     # with open(cache_file, 'wb') as file:\n",
    "#     #     pickle.dump(data, file)\n",
    "#     #     print(f\"Saved data to cache file: {cache_file}\")\n",
    "\n",
    "\n",
    "#     # print percentage of positives and negatives\n",
    "#     denom = total_positives + total_negatives\n",
    "#     print(f\"Percentage of positives: {total_positives / denom}, Percentage of negatives: {total_negatives / denom}\")\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache file: 2_7/apache_spark/cache/triplet_data_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "triplet_data = prepare_triplet_data_from_df(recent_df, bm25_searcher, search_depth=params['train_depth'], num_positives=params['num_positives'], num_negatives=params['num_negatives'], cache_file=triplet_cache, overwrite=args.overwrite_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    14973\n",
       "1     7670\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see distribution of labels\n",
    "triplet_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20533/22643 [01:44<00:10, 196.27it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39massert\u001b[39;00m data[(data[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m&\u001b[39m (data[\u001b[39m'\u001b[39m\u001b[39mpassage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mpassage\u001b[39m\u001b[39m'\u001b[39m])][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m sanity_check(triplet_data)\n",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m tqdm(data\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m row[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39massert\u001b[39;00m data[(data[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m&\u001b[39m (data[\u001b[39m'\u001b[39m\u001b[39mpassage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mpassage\u001b[39m\u001b[39m'\u001b[39m])][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39massert\u001b[39;00m data[(data[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m&\u001b[39m (data[\u001b[39m'\u001b[39m\u001b[39mpassage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mpassage\u001b[39m\u001b[39m'\u001b[39m])][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sanity_check(data):\n",
    "    # check that there no (query, passage) pairs with both labels 0 and 1\n",
    "    # for i, row in data.iterrows():\n",
    "    for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        if row['label'] == 0:\n",
    "            assert data[(data['query'] == row['query']) & (data['passage'] == row['passage'])]['label'].values[0] == 0\n",
    "        else:\n",
    "            assert data[(data['query'] == row['query']) & (data['passage'] == row['passage'])]['label'].values[0] == 1\n",
    "\n",
    "sanity_check(triplet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"[SPARK-25496][SQL] Deprecate from_utc_timestamp and to_utc_timestamp\\n\\n## What changes were proposed in this pull request?\\n\\nIn the PR, I propose to deprecate the `from_utc_timestamp()` and `to_utc_timestamp`, and disable them by default. The functions can be enabled back via the SQL config `spark.sql.legacy.utcTimestampFunc.enabled`. By default, any calls of the functions throw an analysis exception.\\n\\nOne of the reason for deprecation is functions violate semantic of `TimestampType` which is number of microseconds since epoch in UTC time zone. Shifting microseconds since epoch by time zone offset doesn't make sense because the result doesn't represent microseconds since epoch in UTC time zone any more, and cannot be considered as `TimestampType`.\\n\\n## How was this patch tested?\\n\\nThe changes were tested by `DateExpressionsSuite` and `DateFunctionsSuite`.\\n\\nCloses #24195 from MaxGekk/conv-utc-timestamp-deprecate.\\n\\nLead-authored-by: Maxim Gekk <max.gekk@gmail.com>\\nCo-authored-by: Maxim Gekk <maxim.gekk@databricks.com>\\nCo-authored-by: Hyukjin Kwon <gurwls223@apache.org>\\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\\n\"}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# check that there is no intersection between the two sets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(passages0)\u001b[39m.\u001b[39mintersection(\u001b[39mset\u001b[39m(passages1))) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m test()\n",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mset\u001b[39m(passages0)\u001b[39m.\u001b[39mintersection(\u001b[39mset\u001b[39m(passages1)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# check that there is no intersection between the two sets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(passages0)\u001b[39m.\u001b[39mintersection(\u001b[39mset\u001b[39m(passages1))) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    problem_query = triplet_data.iloc[20533]['query']\n",
    "    # find all passages with this query and label 0\n",
    "    passages0 = triplet_data[(triplet_data['query'] == problem_query) & (triplet_data['label'] == 0)]['passage'].values\n",
    "    # find all passages with this query and label 1\n",
    "    passages1 = triplet_data[(triplet_data['query'] == problem_query) & (triplet_data['label'] == 1)]['passage'].values\n",
    "\n",
    "    # print common passages\n",
    "    print(set(passages0).intersection(set(passages1)))\n",
    "    # check that there is no intersection between the two sets\n",
    "    assert len(set(passages0).intersection(set(passages1))) == 0\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20557/22643 [01:49<00:11, 185.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion failed at index 20533: query      [SPARK-31318][SQL] Split Parquet/Avro configs ...\n",
      "passage    [SPARK-25496][SQL] Deprecate from_utc_timestam...\n",
      "label                                                      0\n",
      "Name: 20533, dtype: object\n",
      "Dropped row at index 20533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22643/22643 [02:00<00:00, 187.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of problems: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sanity_check(data):\n",
    "    problems = 0\n",
    "    for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        try:\n",
    "            if row['label'] == 0:\n",
    "                assert data[(data['query'] == row['query']) & (data['passage'] == row['passage'])]['label'].values[0] == 0\n",
    "            else:\n",
    "                assert data[(data['query'] == row['query']) & (data['passage'] == row['passage'])]['label'].values[0] == 1\n",
    "        except AssertionError:\n",
    "            print(f\"Assertion failed at index {i}: {row}\")\n",
    "            # break  # Optional: break after the first failure, remove if you want to see all failures\n",
    "            # remove the row with label 0\n",
    "\n",
    "            if row['label'] == 0:\n",
    "                problems += 1\n",
    "                data.drop(i, inplace=True)\n",
    "                print(f\"Dropped row at index {i}\")\n",
    "\n",
    "    print(f\"Total number of problems: {problems}\")\n",
    "    return data\n",
    "# Call the function with your data\n",
    "triplet_data = sanity_check(triplet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = triplet_data[['query', 'passage']], triplet_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query = str(self.data.iloc[index]['query'])\n",
    "        passage = str(self.data.iloc[index]['passage'])\n",
    "        label = self.labels.iloc[index]\n",
    "\n",
    "        # encode_plus returns a dictionary with the following keys\n",
    "        # ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "        encoded_pair = self.tokenizer.encode_plus([query, passage], max_length=self.max_len, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True)\n",
    "        input_ids = encoded_pair['input_ids'].squeeze()\n",
    "        # token_type_ids = encoded_pair['token_type_ids'].squeeze()\n",
    "        attention_mask = encoded_pair['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            # 'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TripletDataset(X_train, y_train, bert_reranker.tokenizer, bert_reranker.max_seq_length)\n",
    "val_dataset = TripletDataset(X_val, y_val, bert_reranker.tokenizer, bert_reranker.max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTReranker:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        # self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        # self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        print(f\"Initialized BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        query_passage_pairs = []\n",
    "        for agg_result in aggregated_results:\n",
    "            query_passage_pairs.extend(\n",
    "                (query, result.commit_message)\n",
    "                for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "            )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for agg_result in aggregated_results:\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + len(agg_result.contributing_results[: self.psg_cnt]) # only use psg_cnt contributing_results\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self, train_hf_dataset, val_hf_dataset, train_args, save_dir=None):\n",
    "        train_hf_dataset = train_dataset.map(tokenize_hf, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 476.73 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_len': 400, 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 16, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 3, 'mlp_lr': 0.001, 'bert_lr': 5e-05, 'hidden_dim': 128, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'weight_decay': 0.01, 'dropout_prob': 0.5, 'train_commits': 1500}\n"
     ]
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(params)\n",
    "rerankers = [bert_reranker]\n",
    "save_model_name = params['model_name'].replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge pull request #1362 from spicyj/cb-queue\n",
      "\n",
      "Make MountReady more reusable, reduce allocations\n",
      "\n",
      "BM25 baseline: {'MAP': 0.3271, 'P@10': 0.1, 'P@100': 0.04, 'P@1000': 0.004, 'MRR': 1.0, 'Recall@100': 1.0, 'Recall@1000': 1.0}\n",
      "BERT reranker: {'MAP': 0.306, 'P@10': 0.1, 'P@100': 0.04, 'P@1000': 0.004, 'MRR': 1.0, 'Recall@100': 1.0, 'Recall@1000': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def test1():\n",
    "    # get a sample commit from combined_df\n",
    "    random_commit = combined_df.sample(1, random_state=6456)\n",
    "    actual_files_modified = combined_df[combined_df['commit_id'] == random_commit['commit_id'].values[0]]['file_path'].values\n",
    "    print(random_commit['commit_message'].values[0])\n",
    "\n",
    "    bm25_results = bm25_searcher.pipeline(random_commit['commit_message'].values[0], random_commit['commit_date'].values[0], args.k, 'sump')\n",
    "    baseline = evaluator.evaluate(bm25_results, actual_files_modified)\n",
    "    print(f'BM25 baseline: {baseline}')\n",
    "\n",
    "    # rerank with BERT\n",
    "    bert_results = bert_reranker.rerank_pipeline(random_commit['commit_message'].values[0], bm25_results)\n",
    "    bert_eval = evaluator.evaluate(bert_results, actual_files_modified)\n",
    "    print(f'BERT reranker: {bert_eval}')\n",
    "\n",
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Baseline\n",
    "# {'MAP': 0.1542, 'P@10': 0.087, 'P@100': 0.0267, 'P@1000': 0.0041, 'MRR': 0.2133, 'Recall@100': 0.5077, 'Recall@1000': 0.6845}\n",
    "\n",
    "# BERT reranker with training\n",
    "# {'MAP': 0.0842, 'P@10': 0.054, 'P@100': 0.0231, 'P@1000': 0.0041, 'MRR': 0.1404, 'Recall@100': 0.4266, 'Recall@1000': 0.6845}\n",
    "\n",
    "# BERT reranker with training on 1 epoch\n",
    "# {'MAP': 0.1471, 'P@10': 0.071, 'P@100': 0.0266, 'P@1000': 0.0041, 'MRR': 0.2243, 'Recall@100': 0.4841, 'Recall@1000': 0.6845}\n",
    "\n",
    "# BERT reranker with training on 10 epochs\n",
    "# {MAP: 0.1934, P@10: 0.104, P@100: 0.0282, P@1000: 0.0041, MRR: 0.2704, Recall@100: 0.499, Recall@1000: 0.6845}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, using default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:58<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to smalldata/ftr/BM25Searcher_results.txt\n",
      "Results without training: {'MAP': 0.1471, 'P@10': 0.071, 'P@100': 0.0266, 'P@1000': 0.0041, 'MRR': 0.2243, 'Recall@100': 0.4841, 'Recall@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_without_training = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], repo_path=repo_path, rerankers=rerankers)\n",
    "print(f'Results without training: {bert_without_training}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reranker(bertranker, train_dataloader, validation_dataloader, freeze_bert, save_dir):\n",
    "    save_dir = os.path.join(save_dir, 'models')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Set BERT parameters to not require gradients\n",
    "    # for param in bertranker.model.parameters():\n",
    "    #     param.requires_grad = False if freeze_bert else True\n",
    "\n",
    "\n",
    "    # if freeze_bert:\n",
    "    #     optimizer = torch.optim.Adam(bertranker.mlp.parameters(), lr=bertranker.parameters['mlp_lr'], weight_decay=bertranker.parameters['weight_decay'])\n",
    "    # else:\n",
    "    #     optimizer = torch.optim.Adam([\n",
    "    #         {'params': bertranker.model.parameters(), 'lr': bertranker.parameters['bert_lr'], 'weight_decay': bertranker.parameters['weight_decay']},\n",
    "    #         {'params': bertranker.mlp.parameters(), 'lr': bertranker.parameters['mlp_lr'], 'weight_decay': bertranker.parameters['weight_decay']}\n",
    "    #             ], lr=bertranker.parameters['mlp_lr'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(bertranker.model.parameters(), lr=bertranker.parameters['bert_lr'])\n",
    "\n",
    "    # one optimizer for both BERT and MLP with same learning rate\n",
    "\n",
    "\n",
    "    print(f'Optimizer: {optimizer}')\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    # Set up the loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()  #\n",
    "\n",
    "    # Set up training variables\n",
    "    num_epochs = bertranker.parameters['num_epochs']\n",
    "    # print train and val dataloader sizes\n",
    "    print(f'Train dataloader size: {len(train_dataloader)}')\n",
    "    print(f'Val dataloader size: {len(validation_dataloader)}')\n",
    "    # Training loop\n",
    "    print('Starting training loop')\n",
    "\n",
    "    if freeze_bert:\n",
    "        print('BERT is frozen, training only MLP')\n",
    "    else:\n",
    "        print('BERT is unfrozen, training BERT and MLP')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    # model_name = 'bert_reranker_frozen' if freeze_bert else 'bert_reranker'\n",
    "    model_name = bertranker.parameters['model_name'].replace('/', '_') + '_frozen' if freeze_bert else bertranker.parameters['model_name'].replace('/', '_')\n",
    "    model_name += '_frozen' if freeze_bert else ''\n",
    "    print(f'Model name: {model_name}')\n",
    "    # for epoch in range(epochs):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # self.model.eval()  # Make sure the BERT model is in evaluation mode\n",
    "        # if freeze_bert:\n",
    "        #     bertranker.model.eval()  # BERT finetuning should be in eval mode\n",
    "        # else:\n",
    "        #     bertranker.model.train()  # BERT finetuning should be in train mode\n",
    "\n",
    "        bertranker.model.train()  # BERT finetuning should be in train mode\n",
    "        bertranker.mlp.train()  # MLP should be in training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            # breakpoint()\n",
    "            b_input_ids, b_attention_mask, b_labels = batch\n",
    "            b_input_ids = b_input_ids.to(bertranker.device)\n",
    "            b_attention_mask = b_attention_mask.to(bertranker.device)\n",
    "            b_labels = b_labels.float().to(bertranker.device)\n",
    "\n",
    "            # Forward pass\n",
    "            if freeze_bert:\n",
    "                with torch.no_grad():\n",
    "                    # pooled_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                    cls_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "            else:\n",
    "                pooled_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "            cls_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "            logits = bertranker.mlp(cls_output).squeeze(-1) # type: ignore\n",
    "\n",
    "            # outputs = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "            # logits = bertranker.mlp(outputs.logits).squeeze(-1) # type: ignore\n",
    "            # logits = outputs.logits.squeeze(-1) # type: ignore\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation step\n",
    "        bertranker.model.eval()\n",
    "        bertranker.mlp.eval()\n",
    "        total_eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_dataloader:\n",
    "                b_input_ids, b_attention_mask, b_labels = batch\n",
    "                b_input_ids = b_input_ids.to(bertranker.device)\n",
    "                b_attention_mask = b_attention_mask.to(bertranker.device)\n",
    "                b_labels = b_labels.float().to(bertranker.device)\n",
    "\n",
    "                pooled_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).pooler_output\n",
    "                cls_output = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask).last_hidden_state[:, 0, :]\n",
    "                logits = bertranker.mlp(cls_output).squeeze(-1) # type: ignore\n",
    "\n",
    "                # outputs = bertranker.model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                # logits = outputs.logits.squeeze(-1) # type: ignore\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, b_labels.float())\n",
    "                total_eval_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        # scheduler.step(avg_val_loss)\n",
    "        # Save losses\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss}\")\n",
    "        print(f'Best validation loss: {best_val_loss}')\n",
    "\n",
    "        # save graph of losses\n",
    "        plt.plot(train_losses, label='Training loss', color='blue', linestyle='dashed', linewidth=1, marker='o', markerfacecolor='blue', markersize=3)\n",
    "        plt.plot(val_losses, label='Validation loss', color='red', linestyle='dashed', linewidth=1, marker='o', markerfacecolor='red', markersize=3)\n",
    "        plt.legend(frameon=False)\n",
    "        plt.savefig(os.path.join(save_dir, f'{model_name}_losses.png'))\n",
    "        plt.close()\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # model name with frozen or unfrozen bert\n",
    "            save_path = os.path.join(save_dir, f'{model_name}_best_model.pth')\n",
    "            mlp_save_path = os.path.join(save_dir, f'{model_name}_best_mlp.pth')\n",
    "            torch.save(bertranker.model.state_dict(), save_path)\n",
    "            torch.save(bertranker.mlp.state_dict(), mlp_save_path)\n",
    "\n",
    "            print(f\"Model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "            # evaluate on train set\n",
    "\n",
    "\n",
    "\n",
    "        # Here you can add early stopping based on validation loss\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from_pandas method\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert triplet_data to HuggingFace Dataset\n",
    "triplet_data['label'] = triplet_data['label'].astype(float)\n",
    "train_df, val_df = train_test_split(triplet_data, test_size=0.2, random_state=42, stratify=triplet_data['label'])\n",
    "train_hf_dataset = HFDataset.from_pandas(train_df, split='train')\n",
    "val_hf_dataset = HFDataset.from_pandas(val_df, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training methods\n",
    "def tokenize_hf(example):\n",
    "    return bert_reranker.tokenizer(example['query'], example['passage'], truncation=True, padding='max_length', max_length=bert_reranker.max_seq_length, return_tensors='pt', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/dill/_dill.py:412: PicklingWarning: Cannot locate reference to <class '__main__.BERTReranker'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/dill/_dill.py:412: PicklingWarning: Cannot pickle <class '__main__.BERTReranker'>: __main__.BERTReranker has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Map: 100%|██████████| 17164/17164 [00:15<00:00, 1074.31 examples/s]\n",
      "Map: 100%|██████████| 4292/4292 [00:03<00:00, 1093.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_hf_dataset.map(tokenize_hf, batched=True)\n",
    "tokenized_val_dataset = val_hf_dataset.map(tokenize_hf, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['query', 'passage'])\n",
    "tokenized_val_dataset = tokenized_val_dataset.remove_columns(['query', 'passage'])\n",
    "\n",
    "# rename label column to labels\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column('label', 'labels')\n",
    "tokenized_val_dataset = tokenized_val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "# set format to pytorch\n",
    "tokenized_train_dataset = tokenized_train_dataset.with_format('torch')\n",
    "tokenized_val_dataset = tokenized_val_dataset.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': Value(dtype='float64', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_output_dir = os.path.join(repo_path, f'{save_model_name}_model_output')\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=hf_output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=5,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train_dataset.shuffle(seed=42).select(range(100))\n",
    "small_val_dataset = tokenized_val_dataset.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = bert_reranker.model,\n",
    "    args = train_args,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    eval_dataset = tokenized_val_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2685' max='2685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2685/2685 48:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.180959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.212182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.175978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.168036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.165875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2685, training_loss=0.18268427253879427, metrics={'train_runtime': 2889.3115, 'train_samples_per_second': 29.703, 'train_steps_per_second': 0.929, 'total_flos': 2.257998803257344e+16, 'train_loss': 0.18268427253879427, 'epoch': 5.0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = os.path.join(hf_output_dir, 'best_model')\n",
    "trainer.save_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7fd00c68dc70>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained('smalldata/ftr/microsoft_codebert-base_model_output/checkpoint-2685')\n",
    "bert_reranker.model.to(bert_reranker.device)\n",
    "rerankers = [bert_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, using default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:59<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to smalldata/ftr/BM25Searcher_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_with_training = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], repo_path=repo_path, rerankers=rerankers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_reranker.train_mlp(train_dataloader, val_dataloader)\n",
    "train_reranker(bert_reranker, train_dataloader, val_dataloader, freeze_bert=args.freeze_bert, save_dir=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reranker_output_file = f\"925_bert_reranker_{save_model_name}_N{args.n}_K{args.k}_non_frozen_metrics.txt\" if not args.freeze_bert else f\"bert_reranker_{save_model_name}_N{args.n}_K{args.k}_frozen_metrics.txt\"\n",
    "\n",
    "# reranker_output_file = f\"bert_reranker_{save_model_name}_N{args.n}_K{args.k}_without_mlp_metrics.txt\"\n",
    "reranker_output_path = os.path.join(eval_path, reranker_output_file)\n",
    "\n",
    "bert_reranker_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=reranker_output_path, aggregation_strategy='sump', rerankers=rerankers, repo_path=repo_path)\n",
    "\n",
    "print(\"BERT Reranker Evaluation\")\n",
    "print(bert_reranker_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
