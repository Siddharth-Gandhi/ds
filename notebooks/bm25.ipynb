{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            }
         ],
         "source": [
            "import pandas as pd\n",
            "import glob\n",
            "import os\n",
            "import json\n",
            "from pyserini.search.lucene import LuceneSearcher\n",
            "from pyserini.index.lucene import IndexReader\n",
            "import tiktoken\n",
            "import datetime\n",
            "import time\n",
            "import numpy as np"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "import glob\n",
            "# pd.read_parquet('data/angular_angular/angular_angular_commit_data_0.parquet' ).info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "# encoding = 'cl100k_base'\n",
            "encoding = 'p50k_base'\n",
            "enc = tiktoken.get_encoding(encoding)\n",
            "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "def tokenize(text):\n",
            "    return ' '.join(map(str,enc.encode(text, disallowed_special=())))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "total 2.4M\n",
                  "   0 drwxr-xr-x 10 siddharth  320 Oct  5 22:02 ./\n",
                  "   0 drwxr-xr-x 12 siddharth  384 Oct  5 22:04 ../\n",
                  "8.0K -rw-r--r--  1 siddharth 6.1K Oct  5 22:02 .DS_Store\n",
                  "   0 drwxr-xr-x  3 siddharth   96 Oct  5 22:02 jsonl/\n",
                  "868K -rw-r--r--  1 siddharth 868K Oct  2 22:02 karpathy_llama2.c_commit_data_0.parquet\n",
                  "616K -rw-r--r--  1 siddharth 615K Oct  2 22:02 karpathy_llama2.c_commit_data_1.parquet\n",
                  "372K -rw-r--r--  1 siddharth 372K Oct  2 22:03 karpathy_llama2.c_commit_data_2.parquet\n",
                  "284K -rw-r--r--  1 siddharth 283K Oct  2 22:03 karpathy_llama2.c_commit_data_3.parquet\n",
                  "240K -rw-r--r--  1 siddharth 238K Oct  2 22:03 karpathy_llama2.c_commit_data_4.parquet\n",
                  "   0 drwxr-xr-x 21 siddharth  672 Oct  3 00:52 searcher/\n"
               ]
            }
         ],
         "source": [
            "!ls -GFlash data/karpathy_llama2.c/"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 57,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 1774 entries, 0 to 1773\n",
                  "Data columns (total 13 columns):\n",
                  " #   Column                 Non-Null Count  Dtype              \n",
                  "---  ------                 --------------  -----              \n",
                  " 0   owner                  1774 non-null   string             \n",
                  " 1   repo_name              1774 non-null   string             \n",
                  " 2   commit_date            1774 non-null   datetime64[ns, UTC]\n",
                  " 3   commit_id              1774 non-null   string             \n",
                  " 4   commit_message         1774 non-null   string             \n",
                  " 5   file_path              1774 non-null   string             \n",
                  " 6   previous_commit_id     1774 non-null   string             \n",
                  " 7   previous_file_content  1774 non-null   string             \n",
                  " 8   cur_file_content       1774 non-null   string             \n",
                  " 9   diff                   1626 non-null   string             \n",
                  " 10  status                 1774 non-null   category           \n",
                  " 11  is_merge_request       1774 non-null   bool               \n",
                  " 12  file_extension         1774 non-null   category           \n",
                  "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
                  "memory usage: 144.7 KB\n"
               ]
            }
         ],
         "source": [
            "pd.read_parquet('data/ggerganov_llama.cpp/ggerganov_llama.cpp_commit_data_0.parquet').info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 9418 entries, 0 to 9417\n",
                  "Data columns (total 14 columns):\n",
                  " #   Column                 Non-Null Count  Dtype              \n",
                  "---  ------                 --------------  -----              \n",
                  " 0   owner                  9418 non-null   string             \n",
                  " 1   repo_name              9418 non-null   string             \n",
                  " 2   commit_date            9418 non-null   datetime64[ns, UTC]\n",
                  " 3   commit_id              9418 non-null   string             \n",
                  " 4   commit_message         9418 non-null   string             \n",
                  " 5   file_path              9418 non-null   string             \n",
                  " 6   cur_file_content       9181 non-null   string             \n",
                  " 7   previous_commit_id     9062 non-null   string             \n",
                  " 8   previous_file_path     234 non-null    string             \n",
                  " 9   previous_file_content  9062 non-null   string             \n",
                  " 10  diff                   8825 non-null   string             \n",
                  " 11  status                 9418 non-null   category           \n",
                  " 12  is_merge_request       9418 non-null   bool               \n",
                  " 13  file_extension         9418 non-null   category           \n",
                  "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(10)\n",
                  "memory usage: 837.5 KB\n"
               ]
            }
         ],
         "source": [
            "pd.read_parquet('../data/facebook_react/facebook_react_commit_data_0.parquet').info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 9418 entries, 0 to 9417\n",
                  "Data columns (total 13 columns):\n",
                  " #   Column                 Non-Null Count  Dtype              \n",
                  "---  ------                 --------------  -----              \n",
                  " 0   owner                  9418 non-null   string             \n",
                  " 1   repo_name              9418 non-null   string             \n",
                  " 2   commit_date            9418 non-null   datetime64[ns, UTC]\n",
                  " 3   commit_id              9418 non-null   string             \n",
                  " 4   commit_message         9418 non-null   string             \n",
                  " 5   file_path              9418 non-null   string             \n",
                  " 6   previous_commit_id     9418 non-null   string             \n",
                  " 7   previous_file_content  9418 non-null   string             \n",
                  " 8   cur_file_content       9418 non-null   string             \n",
                  " 9   diff                   8591 non-null   string             \n",
                  " 10  status                 9418 non-null   category           \n",
                  " 11  is_merge_request       9418 non-null   bool               \n",
                  " 12  file_extension         9418 non-null   category           \n",
                  "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
                  "memory usage: 763.8 KB\n"
               ]
            }
         ],
         "source": [
            "pd.read_parquet('../data/facebook_react/facebook_react_commit_data_0.parquet').info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 9418 entries, 0 to 9417\n",
                  "Data columns (total 13 columns):\n",
                  " #   Column                 Non-Null Count  Dtype              \n",
                  "---  ------                 --------------  -----              \n",
                  " 0   owner                  9418 non-null   string             \n",
                  " 1   repo_name              9418 non-null   string             \n",
                  " 2   commit_date            9418 non-null   datetime64[ns, UTC]\n",
                  " 3   commit_id              9418 non-null   string             \n",
                  " 4   commit_message         9418 non-null   string             \n",
                  " 5   file_path              9418 non-null   string             \n",
                  " 6   previous_commit_id     9062 non-null   string             \n",
                  " 7   previous_file_content  9062 non-null   string             \n",
                  " 8   cur_file_content       8947 non-null   string             \n",
                  " 9   diff                   8591 non-null   string             \n",
                  " 10  status                 9418 non-null   category           \n",
                  " 11  is_merge_request       9418 non-null   bool               \n",
                  " 12  file_extension         9418 non-null   category           \n",
                  "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
                  "memory usage: 763.8 KB\n"
               ]
            }
         ],
         "source": [
            "pd.read_parquet('../data/facebook_react/facebook_react_commit_data_0.parquet').info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 54,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 5948 entries, 0 to 5947\n",
                  "Data columns (total 12 columns):\n",
                  " #   Column                 Non-Null Count  Dtype              \n",
                  "---  ------                 --------------  -----              \n",
                  " 0   owner                  5948 non-null   string             \n",
                  " 1   repo_name              5948 non-null   string             \n",
                  " 2   commit_date            5948 non-null   datetime64[ns, UTC]\n",
                  " 3   commit_id              5948 non-null   string             \n",
                  " 4   commit_message         5948 non-null   string             \n",
                  " 5   file_path              5948 non-null   string             \n",
                  " 6   previous_commit_id     5948 non-null   string             \n",
                  " 7   previous_file_content  5159 non-null   string             \n",
                  " 8   cur_file_content       5860 non-null   string             \n",
                  " 9   diff                   5072 non-null   string             \n",
                  " 10  status                 5948 non-null   category           \n",
                  " 11  is_merge_request       5948 non-null   bool               \n",
                  "dtypes: bool(1), category(1), datetime64[ns, UTC](1), string(9)\n",
                  "memory usage: 476.6 KB\n"
               ]
            }
         ],
         "source": [
            "pd.read_parquet('data/apache_kafka/apache_kafka_commit_data_0.parquet').info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 119,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Load the parquet file\n",
            "# tempdf = pd.read_parquet('data/karpathy_llama2.c/karpathy_llama2.c_commit_data_0.parquet')\n",
            "tempdf = pd.read_parquet('data/apache_kafka/apache_kafka_commit_data_0.parquet')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 120,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 5948 entries, 0 to 5947\n",
                  "Data columns (total 12 columns):\n",
                  " #   Column                 Non-Null Count  Dtype              \n",
                  "---  ------                 --------------  -----              \n",
                  " 0   owner                  5948 non-null   string             \n",
                  " 1   repo_name              5948 non-null   string             \n",
                  " 2   commit_date            5948 non-null   datetime64[ns, UTC]\n",
                  " 3   commit_id              5948 non-null   string             \n",
                  " 4   commit_message         5948 non-null   string             \n",
                  " 5   file_path              5948 non-null   string             \n",
                  " 6   previous_commit_id     5948 non-null   string             \n",
                  " 7   previous_file_content  5159 non-null   string             \n",
                  " 8   cur_file_content       5860 non-null   string             \n",
                  " 9   diff                   5072 non-null   string             \n",
                  " 10  status                 5948 non-null   category           \n",
                  " 11  is_merge_request       5948 non-null   bool               \n",
                  "dtypes: bool(1), category(1), datetime64[ns, UTC](1), string(9)\n",
                  "memory usage: 476.6 KB\n"
               ]
            }
         ],
         "source": [
            "tempdf.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = pd.read_parquet('../data/facebook_react/facebook_react_commit_data_0.parquet')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 203,
         "metadata": {},
         "outputs": [],
         "source": [
            "# get commit 7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
            "# df[df['commit_id'] == '7022e8d6a3222c97d287dfa0f2361acc8a30683a']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "0   2023-09-29 22:24:38+00:00\n",
                     "Name: commit_date, dtype: datetime64[ns, UTC]"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# (df.head(1)['commit_date'].astype('int64')/1e6).astype('int64')\n",
            "df.head(1)['commit_date']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 9418 entries, 0 to 9417\n",
                  "Data columns (total 13 columns):\n",
                  " #   Column                 Non-Null Count  Dtype              \n",
                  "---  ------                 --------------  -----              \n",
                  " 0   owner                  9418 non-null   string             \n",
                  " 1   repo_name              9418 non-null   string             \n",
                  " 2   commit_date            9418 non-null   datetime64[ns, UTC]\n",
                  " 3   commit_id              9418 non-null   string             \n",
                  " 4   commit_message         9418 non-null   string             \n",
                  " 5   file_path              9418 non-null   string             \n",
                  " 6   previous_commit_id     9062 non-null   string             \n",
                  " 7   previous_file_content  9062 non-null   string             \n",
                  " 8   cur_file_content       8947 non-null   string             \n",
                  " 9   diff                   8591 non-null   string             \n",
                  " 10  status                 9418 non-null   category           \n",
                  " 11  is_merge_request       9418 non-null   bool               \n",
                  " 12  file_extension         9418 non-null   category           \n",
                  "dtypes: bool(1), category(2), datetime64[ns, UTC](1), string(9)\n",
                  "memory usage: 763.8 KB\n"
               ]
            }
         ],
         "source": [
            "df.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2699.89 MB\n"
               ]
            }
         ],
         "source": [
            " # print just the memory usage in human readable format (MB) to 2 decimal places\n",
            "print(f'{df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Number of unique commits stored (others excluded for not being code commits): 11595\n"
               ]
            }
         ],
         "source": [
            "print('Number of unique commits stored (others excluded for not being code commits):', df.commit_id.nunique())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 60,
         "metadata": {},
         "outputs": [],
         "source": [
            "# BASE_DIR = 'data/karpathy_llama2.c/'\n",
            "# REPO_LIST = ['karpathy_llama2.c', 'facebook_react', 'apache_kafka', 'ggerganov_llama.cpp', 'nodejs_node']\n",
            "REPO_LIST = ['karpathy_llama2.c', 'apache_kafka', 'ggerganov_llama.cpp', 'nodejs_node']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 47,
         "metadata": {},
         "outputs": [],
         "source": [
            "# REPO_LIST = ['karpathy_llama2.c']\n",
            "REPONAME = ['facebook_react']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 61,
         "metadata": {},
         "outputs": [],
         "source": [
            "# def convert_data_to_jsonl(data_dir, output_file):\n",
            "#     all_files = glob.glob(os.path.join(data_dir, '*.parquet'))\n",
            "#     all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
            "#     combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
            "#     # replace NaN with empty string\n",
            "#     combined_df.fillna('', inplace=True)\n",
            "\n",
            "#     with open(output_file, 'w') as f:\n",
            "#         for index, row in combined_df.iterrows():\n",
            "#             doc = {\n",
            "#                 'id': row['commit_id'],\n",
            "#                 'contents': row['commit_message'] + '\\n' + row['cur_file_content'],\n",
            "#                 # Optionally include source code\n",
            "#                 # 'source_code': row['cur_file_content']\n",
            "#             }\n",
            "#             f.write(json.dumps(doc) + '\\n')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "def count_commits(repo_dir):\n",
            "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
            "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
            "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
            "\n",
            "    # number of unique commit_id columns\n",
            "    return combined_df.commit_id.nunique()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 119,
         "metadata": {},
         "outputs": [],
         "source": [
            "total_commits = 0\n",
            "for repo in REPO_LIST:\n",
            "    total_commits += count_commits('data/' + repo + '/')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 120,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total number of commits: 11595\n"
               ]
            }
         ],
         "source": [
            "print('Total number of commits:', total_commits)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_combined_df(repo_dir):\n",
            "    all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
            "    all_files.sort()\n",
            "    print(all_files)\n",
            "    all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
            "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
            "    combined_df['commit_date'] = (combined_df['commit_date'].astype('int64') / 1e9).astype('int64')\n",
            "    # replace NaN with empty string\n",
            "    # combined_df.fillna('', inplace=True)\n",
            "    return combined_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "def convert_repo_to_jsonl(repo_dir, output_file, use_tokenizer=False):\n",
            "    # all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
            "    # print(f\"Found {len(all_files)} parquet files\")\n",
            "    # print(all_files)\n",
            "    # all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
            "    # combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
            "    combined_df = get_combined_df(repo_dir)\n",
            "    # replace NaN with empty string in non-category columns\n",
            "    # combined_df.fillna('', inplace=True)\n",
            "\n",
            "    combined_df['commit_message'] = combined_df['commit_message'].fillna('')\n",
            "    combined_df['cur_file_content'] = combined_df['cur_file_content'].fillna('')\n",
            "    # convert commit_date to int64 (unix timestamp in milliseconds)\n",
            "    # 1e9 is very important AND it HAS to be in UTC\n",
            "    # combined_df['commit_date'] = (combined_df['commit_date'].astype('int64') / 1e9).astype('int64')\n",
            "    # combined_df['commit_date'] = combined_df['commit_date'].astype(str)\n",
            "    # df['commit_date'] = df['commit_date'].astype(str)\n",
            "    # print(type(df['commit_date'][0]))\n",
            "    # print combined_df memory usage\n",
            "    # print(combined_df.info(memory_usage='deep'))\n",
            "    print(f'Combined Memory Usage: {combined_df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB for {len(combined_df)} rows')\n",
            "    with open(output_file, 'x') as f:\n",
            "        for index, row in combined_df.iterrows():\n",
            "            doc = {\n",
            "                'id': row['commit_id'],\n",
            "                'contents': row['commit_message'] if not use_tokenizer else tokenize(row['commit_message']),\n",
            "                # 'source_code': row['cur_file_content'],  # Optionally include source code\n",
            "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']),\n",
            "                # 'contents': tokenize(row['commit_message']) + '\\n' + tokenize(row['cur_file_content']) if use_tokenizer else row['commit_message'] + '\\n' + row['cur_file_content'],\n",
            "                'repo_name': row['repo_name'],\n",
            "                'file_path': row['file_path'],\n",
            "                'commit_date': row['commit_date'],\n",
            "            }\n",
            "            f.write(json.dumps(doc) + '\\n')\n",
            "    print(f'Wrote {len(combined_df)} rows to {output_file}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "# empty data/jsonl if it has data\n",
            "# !rm -rf data/jsonl_tiktoken"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [],
         "source": [
            "# jsonl_dir_name = 'jsonl_6'\n",
            "# for repo_name in REPO_LIST:\n",
            "#     repo_dir = os.path.join('data', repo_name)\n",
            "#     # create data/jsonl directory if it doesn't exist\n",
            "#     os.makedirs(os.path.join('data', jsonl_dir_name), exist_ok=True)\n",
            "\n",
            "#     # store in data/jsonl\n",
            "#     output_jsonl_file = os.path.join('data', jsonl_dir_name, f'{repo_name}.jsonl')\n",
            "#     convert_repo_to_jsonl(repo_dir, output_jsonl_file)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "REPO_LIST = ['facebook_react']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 48,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "facebook_react\n",
                  "Combined Memory Usage: 2758.40 MB for 69860 rows\n",
                  "Wrote 69860 rows to ../data/facebook_react/jsonl_no_tk/facebook_react_commit_only_tk.jsonl\n"
               ]
            }
         ],
         "source": [
            "# store in data/repo_dir/jsonl\n",
            "jsonl_dir_name = 'jsonl_no_tk'\n",
            "for repo_name in REPO_LIST:\n",
            "    print(repo_name)\n",
            "    repo_dir = os.path.join('../data', repo_name)\n",
            "    # create data/jsonl directory if it doesn't exist\n",
            "    os.makedirs(os.path.join(repo_dir, jsonl_dir_name), exist_ok=True)\n",
            "    output_name = f'{repo_name}_commit_only_tk.jsonl'\n",
            "    # store in data/jsonl\n",
            "    output_jsonl_file = os.path.join(repo_dir, jsonl_dir_name, output_name)\n",
            "    # if file exists, delete it\n",
            "    if os.path.exists(output_jsonl_file):\n",
            "        os.remove(output_jsonl_file)\n",
            "    convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=False)\n",
            "    # if not os.path.exists(output_jsonl_file):\n",
            "    #     convert_repo_to_jsonl(repo_dir, output_jsonl_file, use_tokenizer=True)\n",
            "    # else:\n",
            "    #     print('File already exists:', output_jsonl_file)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 62,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Usage\n",
            "# jsonl_file_path = f'{BASE_DIR}/jsonl/llama2.jsonl'\n",
            "# convert_data_to_jsonl(BASE_DIR, jsonl_file_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 121,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # get list of jsonl files which are present in data/repo_name/jsonl/repo_name.jsonl\n",
            "# jsonl_files = glob.glob('data/*/*/*.jsonl')\n",
            "# print(jsonl_files)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "For normal untokenized\n",
            "- Parquet -> JSONL 22s\n",
            "- Index build 1m26s\n",
            "- 6 repos\n",
            "    Parquet -> JSONL 1m11s\n",
            "    Same mem usage as before, just lower time since no need for tokenization\n",
            "    Index Build 3m51s\n",
            "    Index Size 5Gb\n",
            "\n",
            "For tokenized\n",
            "- Parquet -> JSONL 8m3s\n",
            "- Index Build 2m12s\n",
            "- 6 repos:\n",
            "    Parquert -> JSONL 24m\n",
            "        - Combined Memory Usage: 18.29 MB for 402 rows data/isonl_6/karpathy_llama2.c.jsonl\n",
            "        - Combined Memory Usage: 0.94 MB for 108 rows data/json1_6/siddharth-gandhi_refpred.jsonl \\\\\n",
            "        - Combined Memory Usage: 2699.89 MB for 73551 rows data/jsonl_6/facebook_react.jsonl \\\\\n",
            "        - Combined Memory Usage: 3645.70 MB for 75870 rows data/jsonl_6/apache_kafka. jsonl \\\\\n",
            "        - Combined Memory Usage: 605.11 MB for 2111 rows data/jsonl_6/ggerganov_llama.cpp.jsonl \\\\\n",
            "        - Combined Memory Usage: 11010.96 MB for 208188 rows data/jsonl_6/nodejs_node.json\n",
            "        - 36731 total commits \n",
            "        - Total ~360K rows\n",
            "        - Interesting heuristic, on avg 10 files edited per commit?\n",
            "    Index build 6m42s\n",
            "    Index Size 10GB"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Building the index\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "# REPO_LIST = ['facebook_react']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 49,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[0.009s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016d55c000-0x000000016d568000).\n",
                  "[0.009s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
                  "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
                  "2023-10-17 13:52:38,502 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
                  "2023-10-17 13:52:38,504 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
                  "2023-10-17 13:52:38,504 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
                  "2023-10-17 13:52:38,504 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: ../data/facebook_react/jsonl_no_tk\n",
                  "2023-10-17 13:52:38,504 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
                  "2023-10-17 13:52:38,504 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
                  "2023-10-17 13:52:38,504 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
                  "2023-10-17 13:52:38,504 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
                  "2023-10-17 13:52:38,505 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
                  "2023-10-17 13:52:38,506 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
                  "2023-10-17 13:52:38,506 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: ../data/facebook_react/index_no_tk\n",
                  "2023-10-17 13:52:38,508 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
                  "2023-10-17 13:52:38,581 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
                  "2023-10-17 13:52:38,581 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in ../data/facebook_react/jsonl_no_tk\n",
                  "2023-10-17 13:52:38,582 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
                  "2023-10-17 13:52:38,582 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
                  "2023-10-17 13:52:41,171 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl_no_tk/facebook_react_commit_no_tk.jsonl: 69860 docs added.\n",
                  "2023-10-17 13:52:41,578 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 69,860 documents indexed\n",
                  "2023-10-17 13:52:41,578 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
                  "2023-10-17 13:52:41,578 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:           69,860\n",
                  "2023-10-17 13:52:41,578 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
                  "2023-10-17 13:52:41,578 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
                  "2023-10-17 13:52:41,578 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
                  "2023-10-17 13:52:41,579 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
                  "2023-10-17 13:52:41,583 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 69,860 documents indexed in 00:00:03\n",
                  "Processing facebook_react\n"
               ]
            }
         ],
         "source": [
            "%%bash\n",
            "\n",
            "# Specify the repository list here\n",
            "# REPO_LIST=(\"karpathy_llama2.c\" \"facebook_react\" \"apache_kafka\" \"ggerganov_llama.cpp\" \"nodejs_node\")\n",
            "# REPO_LIST=(\"karpathy_llama2.c\" \"apache_kafka\" \"ggerganov_llama.cpp\" \"nodejs_node\")\n",
            "REPO_LIST=(\"facebook_react\")\n",
            "# Loop over each repo in the REPO_LIST array\n",
            "for repo in \"${REPO_LIST[@]}\"\n",
            "do\n",
            "    # Directory paths\n",
            "    repo_dir=\"../data/$repo\"\n",
            "    index_dir=\"$repo_dir/index_no_tk\"\n",
            "    jsonl_dir_name=\"$repo_dir/jsonl_no_tk\"\n",
            "\n",
            "    # Check if the index directory already exists\n",
            "    # if [ -d \"$index_dir\" ]; then\n",
            "    #     echo \"Index directory $index_dir already exists. Not doing $repo.\"\n",
            "    #     continue  # Skip to the next iteration of the loop\n",
            "    # fi\n",
            "\n",
            "    # remove all fiiles in the index directory\n",
            "    rm -rf \"$index_dir\"\n",
            "\n",
            "    # Create the directory if it doesn't exist\n",
            "    mkdir -p \"$index_dir\"\n",
            "\n",
            "    # Build the index from data/jsonl\n",
            "    python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
            "     -threads 4 -input \"$jsonl_dir_name\" -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized\n",
            "\n",
            "    # Log the repo being processed\n",
            "    echo \"Processing $repo\"\n",
            "done"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "jsonl_dir_name: data/karpathy_llama2.c/jsonl\n",
                  "index_dir: data/karpathy_llama2.c/index_tk\n",
                  "total 100\n",
                  "-rw-r--r-- 1 siddharth staff 101236 Oct 10 03:02 karpathy_llama2.c_commit_only_tk.jsonl\n",
                  "[0.003s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000169900000-0x000000016990c000).\n",
                  "[0.003s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
                  "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
                  "2023-10-10 03:04:25,567 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/karpathy_llama2.c/jsonl\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
                  "2023-10-10 03:04:25,569 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
                  "2023-10-10 03:04:25,570 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: true\n",
                  "2023-10-10 03:04:25,571 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/karpathy_llama2.c/index_tk\n",
                  "2023-10-10 03:04:25,573 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
                  "2023-10-10 03:04:25,641 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
                  "2023-10-10 03:04:25,641 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/karpathy_llama2.c/jsonl\n",
                  "2023-10-10 03:04:25,642 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
                  "2023-10-10 03:04:25,642 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
                  "2023-10-10 03:04:25,766 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - jsonl/karpathy_llama2.c_commit_only_tk.jsonl: 402 docs added.\n",
                  "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 402 documents indexed\n",
                  "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
                  "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:              402\n",
                  "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
                  "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
                  "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
                  "2023-10-10 03:04:25,832 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
                  "2023-10-10 03:04:25,838 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 402 documents indexed in 00:00:00\n"
               ]
            }
         ],
         "source": [
            "%%bash\n",
            "# Directory to store the index\n",
            "# index_dir=\"./bm25_index_6/\"\n",
            "# jsonl_dir_name=\"jsonl_6\"\n",
            "repo_dir=\"data/karpathy_llama2.c\"\n",
            "index_dir=\"$repo_dir/index_tk\"\n",
            "# jsonl_dir_name=\"jsonl_tiktoken_6\"\n",
            "jsonl_dir_name=\"$repo_dir/jsonl\"\n",
            "# Create the directory if it doesn't exist\n",
            "mkdir -p \"$index_dir\"\n",
            "\n",
            "# Remove any existing indexes\n",
            "rm -rf \"$index_dir/*\"\n",
            "\n",
            "echo jsonl_dir_name: \"$jsonl_dir_name\"\n",
            "echo index_dir: \"$index_dir\"\n",
            "ls -l \"$jsonl_dir_name\"\n",
            "\n",
            "# build the index from data/jsonl\n",
            "python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
            " -threads 4 -input \"$jsonl_dir_name\" -index \"$index_dir\" -storePositions -storeDocvectors -storeRaw -impact -pretokenized"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 73,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'total_terms': 8062861,\n",
                     " 'documents': 69860,\n",
                     " 'non_empty_documents': 69860,\n",
                     " 'unique_terms': 14589}"
                  ]
               },
               "execution_count": 73,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "IndexReader('../data/facebook_react/index_tk/').stats()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 51,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'total_terms': 4445676,\n",
                     " 'documents': 69860,\n",
                     " 'non_empty_documents': 69860,\n",
                     " 'unique_terms': 42926}"
                  ]
               },
               "execution_count": 51,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "IndexReader('../data/facebook_react/index_no_tk/').stats()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 53,
         "metadata": {},
         "outputs": [],
         "source": [
            "query = 'Refactors Resources to have a more compact and memory efficient struture.'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 69,
         "metadata": {},
         "outputs": [],
         "source": [
            "query = \"\"\"[SPARK-9516][UI] Improvement of Thread Dump Page\n",
            "https://issues.apache.org/jira/browse/SPARK-9516\n",
            "\n",
            "- [x] new look of Thread Dump Page\n",
            "\n",
            "- [x] click column title to sort\n",
            "\n",
            "- [x] grep\n",
            "\n",
            "- [x] search as you type\n",
            "\n",
            "squito JoshRosen It's ready for the review now\n",
            "\n",
            "Author: CodingCat <zhunansjtu@gmail.com>\n",
            "\n",
            "Closes #7910 from CodingCat/SPARK-9516.\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 70,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "36\n"
               ]
            }
         ],
         "source": [
            "# unique terms in query.split()\n",
            "print(len(set(query.split())))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 72,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'58 4303 14175 12 3865 1433 7131 10080 60 33764 286 14122 360 931 7873 198 5450 1378 37165 13 43073 13 2398 14 73 8704 14 25367 325 14 4303 14175 12 3865 1433 198 198 12 685 87 60 649 804 286 14122 360 931 7873 198 198 12 685 87 60 3904 5721 3670 284 3297 198 198 12 685 87 60 42717 198 198 12 685 87 60 2989 355 345 2099 198 198 16485 10094 8518 49 5233 632 338 3492 329 262 2423 783 198 198 13838 25 327 7656 21979 1279 89 20088 504 73 28047 31 14816 13 785 29 198 198 2601 4629 1303 3720 940 422 327 7656 21979 14 4303 14175 12 3865 1433 13'"
                  ]
               },
               "execution_count": 72,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenize(query)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 71,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "73\n"
               ]
            }
         ],
         "source": [
            "print(len(set(tokenize(query).split())))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "1692975245\n"
               ]
            }
         ],
         "source": [
            "# def convert_date_to_timestamp(date_str):\n",
            "#     date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
            "\n",
            "#     # Convert the datetime object to a UNIX timestamp\n",
            "#     # Method 1: Using timestamp() method\n",
            "#     unix_timestamp_1 = int(date_obj.timestamp())\n",
            "#     return unix_timestamp_1\n",
            "\n",
            "import datetime\n",
            "\n",
            "def convert_date_to_timestamp(date_str, precise_timestamp=True):\n",
            "    if precise_timestamp:\n",
            "        # Parse date string with time and timezone information\n",
            "        date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
            "    else:\n",
            "        # Parse date string without time information\n",
            "        date_obj = datetime.datetime.strptime(date_str.split()[0], '%Y-%m-%d')\n",
            "\n",
            "    # Convert the datetime object to a UNIX timestamp\n",
            "    unix_timestamp = int(date_obj.timestamp())\n",
            "    return unix_timestamp\n",
            "\n",
            "# Example Usage:\n",
            "date_str = \"2023-08-25 14:54:05+00:00\"\n",
            "timestamp = convert_date_to_timestamp(date_str)\n",
            "print(timestamp)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "# https://github.com/facebook/react/commit/7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
            "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls. I've encountered an issue in Firefox where multiple calls to chrome.panels.create result in the creation of duplicate panels. This seems to happen every time chrome.panels.create is called, even if a panel already exists. This leads to a cluttered interface with many duplicate panels. Ideally, chrome.panels.create should only create a new panel if there isn't one already existing. I believe a check should be implemented to ensure that chrome.panels.create is only called if no panels have been created yet to prevent this duplication issue.\"\n",
            "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls.\"\n",
            "query_date = \"2023-08-31\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 118,
         "metadata": {},
         "outputs": [],
         "source": [
            "# convert_date_to_timestamp(query_date, precise_timestamp=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "def reverse_tokenize(text):\n",
            "    text = json.loads(text)\n",
            "    # print(list(text['contents'].split(' ')))\n",
            "    text['contents'] = enc.decode([int(i) for i in text['contents'].split(' ')])\n",
            "    # return string\n",
            "    return json.dumps(text, indent=2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 147,
         "metadata": {},
         "outputs": [
            {
               "ename": "JavaException",
               "evalue": "JVM exception occurred: no segments* file found in MMapDirectory@/Users/siddharth/dev/ds/data/nodejs_node/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@5477a1ca: files: [] org.apache.lucene.index.IndexNotFoundException",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
                  "\u001b[1;32m/Users/siddharth/dev/ds/bm25.ipynb Cell 42\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lst \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_dir\u001b[39m}\u001b[39;00m\u001b[39m/index/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_dir\u001b[39m}\u001b[39;00m\u001b[39m/index_tk/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_dir\u001b[39m}\u001b[39;00m\u001b[39m/index_nf/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_dir\u001b[39m}\u001b[39;00m\u001b[39m/index_tk_nf/\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m lst:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     index_reader \u001b[39m=\u001b[39m IndexReader(i)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     search \u001b[39m=\u001b[39m LuceneSearcher(i)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharth/dev/ds/bm25.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
                  "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.9/site-packages/pyserini/index/lucene/_base.py:194\u001b[0m, in \u001b[0;36mIndexReader.__init__\u001b[0;34m(self, index_dir)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, index_dir):\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobject \u001b[39m=\u001b[39m JIndexReader()\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobject\u001b[39m.\u001b[39;49mgetReader(index_dir)\n",
                  "File \u001b[0;32mjnius/jnius_export_class.pxi:877\u001b[0m, in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
                  "File \u001b[0;32mjnius/jnius_export_class.pxi:1043\u001b[0m, in \u001b[0;36mjnius.JavaMethod.call_staticmethod\u001b[0;34m()\u001b[0m\n",
                  "File \u001b[0;32mjnius/jnius_utils.pxi:79\u001b[0m, in \u001b[0;36mjnius.check_exception\u001b[0;34m()\u001b[0m\n",
                  "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: no segments* file found in MMapDirectory@/Users/siddharth/dev/ds/data/nodejs_node/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@5477a1ca: files: [] org.apache.lucene.index.IndexNotFoundException"
               ]
            }
         ],
         "source": [
            "lst = [f'{repo_dir}/index/', f'{repo_dir}/index_tk/', f'{repo_dir}/index_nf/', f'{repo_dir}/index_tk_nf/']\n",
            "for i in lst:\n",
            "    index_reader = IndexReader(i)\n",
            "    search = LuceneSearcher(i)\n",
            "    print(i)\n",
            "    print(index_reader.stats())\n",
            "    search_res = search.search(query, k=10) if 'tk' not in i else search.search(tokenize(query), k=10)\n",
            "    if 'tk' in i:\n",
            "        print(reverse_tokenize(search_res[0].raw))\n",
            "    else:\n",
            "        print(search_res[0].raw)\n",
            "    print(f'Score: {search_res[0].score}')\n",
            "    print()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # https://github.com/facebook/react/commit/7022e8d6a3222c97d287dfa0f2361acc8a30683a\n",
            "query = \"Duplicate Panels Created in Firefox on Multiple chrome.panels.create Calls.\"\n",
            "query_date = \"2023-08-31\"\n",
            "\n",
            "\n",
            "modified_query = \"I've encountered an issue in Firefox where multiple calls to chrome.panels.create result in the creation of duplicate panels. This seems to happen every time chrome.panels.create is called, even if a panel already exists. This leads to a cluttered interface with many duplicate panels. Ideally, chrome.panels.create should only create a new panel if there isn't one already existing. I believe a check should be implemented to ensure that chrome.panels.create is only called if no panels have been created yet to prevent this duplication issue.\"\n",
            "\n",
            "# actual_modified_files = ['packages/react-devtools-extensions/src/main/index.js']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [],
         "source": [
            "# df = pd.read_parquet('data/facebook_react/facebook_react_commit_data_0.parquet')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 188,
         "metadata": {},
         "outputs": [],
         "source": [
            "# https://github.com/facebook/react/commit/d9e00f795b77676fb14f2a3c6f421f48f73bec2a\n",
            "query = \"Stop flowing and then abort if a stream is cancelled\"\n",
            "query_date = \"2023-09-22\"\n",
            "query_commit_id = 'd9e00f795b77676fb14f2a3c6f421f48f73bec2a'\n",
            "actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 59,
         "metadata": {},
         "outputs": [],
         "source": [
            "# filter df to only include commits with commit_id d9e00f795b77676fb14f2a3c6f421f48f73bec2a & get the file_path column as a list to get actual_modified_files\n",
            "# df[df['commit_id'] == 'd9e00f795b77676fb14f2a3c6f421f48f73bec2a']\n",
            "# actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 72,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['packages/react-dom/src/__tests__/ReactDOMFizzServerBrowser-test.js',\n",
                     " 'packages/react-dom/src/server/ReactDOMFizzServerBrowser.js',\n",
                     " 'packages/react-dom/src/server/ReactDOMFizzServerBun.js',\n",
                     " 'packages/react-dom/src/server/ReactDOMFizzServerEdge.js',\n",
                     " 'packages/react-dom/src/server/ReactDOMFizzServerNode.js',\n",
                     " 'packages/react-dom/src/server/ReactDOMFizzStaticBrowser.js',\n",
                     " 'packages/react-dom/src/server/ReactDOMFizzStaticEdge.js',\n",
                     " 'packages/react-server-dom-esm/src/ReactFlightDOMServerNode.js',\n",
                     " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerBrowser.js',\n",
                     " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerEdge.js',\n",
                     " 'packages/react-server-dom-webpack/src/ReactFlightDOMServerNode.js',\n",
                     " 'packages/react-server-dom-webpack/src/__tests__/ReactFlightDOMBrowser-test.js',\n",
                     " 'packages/react-server/src/ReactFizzServer.js',\n",
                     " 'packages/react-server/src/ReactFlightServer.js']"
                  ]
               },
               "execution_count": 72,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "actual_modified_files"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "tokenized with or without flag is the same, so let's just use with flag to avoid recomputing tokens"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 148,
         "metadata": {},
         "outputs": [],
         "source": [
            "repo_dir = f\"data/facebook_react/\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 145,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "data/nodejs_node/index_tk/\n"
               ]
            }
         ],
         "source": [
            "print(idx_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_repo = get_combined_df('../data/facebook_react/')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>owner</th>\n",
                     "      <th>repo_name</th>\n",
                     "      <th>commit_date</th>\n",
                     "      <th>commit_id</th>\n",
                     "      <th>commit_message</th>\n",
                     "      <th>file_path</th>\n",
                     "      <th>cur_file_content</th>\n",
                     "      <th>previous_commit_id</th>\n",
                     "      <th>previous_file_path</th>\n",
                     "      <th>previous_file_content</th>\n",
                     "      <th>diff</th>\n",
                     "      <th>status</th>\n",
                     "      <th>is_merge_request</th>\n",
                     "      <th>file_extension</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>416</th>\n",
                     "      <td>facebook</td>\n",
                     "      <td>react</td>\n",
                     "      <td>1693502666</td>\n",
                     "      <td>7022e8d6a3222c97d287dfa0f2361acc8a30683a</td>\n",
                     "      <td>fix[devtools/extension]: fixed duplicating pan...</td>\n",
                     "      <td>packages/react-devtools-extensions/src/main/in...</td>\n",
                     "      <td>/* global chrome */\n",
                     "\n",
                     "import {createElement} fr...</td>\n",
                     "      <td>b70a0d70224ceb4e277bd8ac535a2caafa5c075a</td>\n",
                     "      <td>&lt;NA&gt;</td>\n",
                     "      <td>/* global chrome */\n",
                     "\n",
                     "import {createElement} fr...</td>\n",
                     "      <td>@@ -340,16 +340,24 @@ function ensureInitialHT...</td>\n",
                     "      <td>modified</td>\n",
                     "      <td>False</td>\n",
                     "      <td>js</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "        owner repo_name  commit_date  \\\n",
                     "416  facebook     react   1693502666   \n",
                     "\n",
                     "                                    commit_id  \\\n",
                     "416  7022e8d6a3222c97d287dfa0f2361acc8a30683a   \n",
                     "\n",
                     "                                        commit_message  \\\n",
                     "416  fix[devtools/extension]: fixed duplicating pan...   \n",
                     "\n",
                     "                                             file_path  \\\n",
                     "416  packages/react-devtools-extensions/src/main/in...   \n",
                     "\n",
                     "                                      cur_file_content  \\\n",
                     "416  /* global chrome */\n",
                     "\n",
                     "import {createElement} fr...   \n",
                     "\n",
                     "                           previous_commit_id previous_file_path  \\\n",
                     "416  b70a0d70224ceb4e277bd8ac535a2caafa5c075a               <NA>   \n",
                     "\n",
                     "                                 previous_file_content  \\\n",
                     "416  /* global chrome */\n",
                     "\n",
                     "import {createElement} fr...   \n",
                     "\n",
                     "                                                  diff    status  \\\n",
                     "416  @@ -340,16 +340,24 @@ function ensureInitialHT...  modified   \n",
                     "\n",
                     "     is_merge_request file_extension  \n",
                     "416             False             js  "
                  ]
               },
               "execution_count": 20,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "test_repo[test_repo['commit_id'] == test_query_commit_id]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>owner</th>\n",
                     "      <th>repo_name</th>\n",
                     "      <th>commit_date</th>\n",
                     "      <th>commit_id</th>\n",
                     "      <th>commit_message</th>\n",
                     "      <th>file_path</th>\n",
                     "      <th>previous_commit_id</th>\n",
                     "      <th>previous_file_content</th>\n",
                     "      <th>cur_file_content</th>\n",
                     "      <th>diff</th>\n",
                     "      <th>status</th>\n",
                     "      <th>is_merge_request</th>\n",
                     "      <th>file_extension</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>416</th>\n",
                     "      <td>facebook</td>\n",
                     "      <td>react</td>\n",
                     "      <td>1693502666</td>\n",
                     "      <td>7022e8d6a3222c97d287dfa0f2361acc8a30683a</td>\n",
                     "      <td>fix[devtools/extension]: fixed duplicating pan...</td>\n",
                     "      <td>packages/react-devtools-extensions/src/main/in...</td>\n",
                     "      <td>b70a0d70224ceb4e277bd8ac535a2caafa5c075a</td>\n",
                     "      <td>/* global chrome */\n",
                     "\n",
                     "import {createElement} fr...</td>\n",
                     "      <td>/* global chrome */\n",
                     "\n",
                     "import {createElement} fr...</td>\n",
                     "      <td>@@ -340,16 +340,24 @@ function ensureInitialHT...</td>\n",
                     "      <td>modified</td>\n",
                     "      <td>False</td>\n",
                     "      <td>js</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "        owner repo_name  commit_date  \\\n",
                     "416  facebook     react   1693502666   \n",
                     "\n",
                     "                                    commit_id  \\\n",
                     "416  7022e8d6a3222c97d287dfa0f2361acc8a30683a   \n",
                     "\n",
                     "                                        commit_message  \\\n",
                     "416  fix[devtools/extension]: fixed duplicating pan...   \n",
                     "\n",
                     "                                             file_path  \\\n",
                     "416  packages/react-devtools-extensions/src/main/in...   \n",
                     "\n",
                     "                           previous_commit_id  \\\n",
                     "416  b70a0d70224ceb4e277bd8ac535a2caafa5c075a   \n",
                     "\n",
                     "                                 previous_file_content  \\\n",
                     "416  /* global chrome */\n",
                     "\n",
                     "import {createElement} fr...   \n",
                     "\n",
                     "                                      cur_file_content  \\\n",
                     "416  /* global chrome */\n",
                     "\n",
                     "import {createElement} fr...   \n",
                     "\n",
                     "                                                  diff    status  \\\n",
                     "416  @@ -340,16 +340,24 @@ function ensureInitialHT...  modified   \n",
                     "\n",
                     "     is_merge_request file_extension  \n",
                     "416             False             js  "
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "test_repo[test_repo['commit_id'] == test_query_commit_id]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [],
         "source": [
            "# k_q_d = '2023-08-25'\n",
            "test_query_commit_id = '7022e8d6a3222c97d287dfa0f2361acc8a30683a'\n",
            "# test_query = test_repo[test_repo['commit_id'] == test_query_commit_id]['commit_message'].tolist()[0]\n",
            "test_query = 'fix[devtools/extension]: fixed duplicating panels in firefox'\n",
            "test_query_date = test_repo[test_repo['commit_id'] == test_query_commit_id]['commit_date'].tolist()[0]\n",
            "actual_modified_files = test_repo[test_repo['commit_id'] == test_query_commit_id]['file_path'].tolist()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "1693502666"
                  ]
               },
               "execution_count": 23,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "test_query_date"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "1693502666"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "test_query_date"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 70,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "1000\n",
                  "13 29b405b2de6b4abaa67ff53f3b5e067f80b106d3 21.66010 react/packages/react-devtools-extensions/src/main/index.js 1693420278\n",
                  "172 8fbd3079425eaff85ec1b52b0eefecdd44ba7289 10.85599 react/packages/react-devtools-extensions/src/main/index.js 1693307366\n"
               ]
            }
         ],
         "source": [
            "# idx_path = f'{repo_dir}/index_tk/'\n",
            "test_idx_path = '../data/facebook_react/index_tk'\n",
            "bm25searcher = LuceneSearcher(test_idx_path)\n",
            "hits = bm25searcher.search(tokenize(test_query), k=1000)\n",
            "print(len(hits))\n",
            "# unix_date = convert_date_to_timestamp(k_q_d, precise_timestamp=False)\n",
            "unix_date = test_query_date\n",
            "for i in range(len(hits)):\n",
            "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
            "    # print with repo name and file name\n",
            "    obj = json.loads(hits[i].raw)\n",
            "    # print(obj)\n",
            "    commit_date = int(obj[\"commit_date\"])\n",
            "\n",
            "    if commit_date >= unix_date:\n",
            "        continue\n",
            "    # if obj[\"file_path\"] in actual_modified_files:\n",
            "    #     print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')\n",
            "    if hits[i].docid == test_query_commit_id:\n",
            "        print('BAD BAD future leaking, BM25 can see the future')\n",
            "\n",
            "    if obj[\"file_path\"] in actual_modified_files:\n",
            "        print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')\n",
            "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "# we have to write a function to evaluate this behaviour. For now just focus on the perfomance of normal query (without modification)\n",
            "# the way we do this is by randomly sampling 1000 queries from df and then running the query on the index and then checking if the file is present in the actual_modified_files list. We want to store all hits and return IR metrics like MAP, MRR, P@10, P@100, P@1K, P@10K, NDCG@10, NDCG@100, NDCG@1K, NDCG@10K\n",
            "\n",
            "# write 2 functions, one for searching and one for evaluating\n",
            "\n",
            "def search(query, idx_path, query_date, k=1000):\n",
            "    bm25searcher = LuceneSearcher(idx_path)\n",
            "    hits = bm25searcher.search(tokenize(query), k)\n",
            "    # filter hits based on date\n",
            "    # unix_date = convert_date_to_timestamp(query_date, precise_timestamp=precise_timestamp)\n",
            "    unix_date = query_date\n",
            "    filtered_hits = []\n",
            "    for i in range(len(hits)):\n",
            "        obj = json.loads(hits[i].raw)\n",
            "        commit_date = int(obj[\"commit_date\"])\n",
            "        if commit_date >= unix_date:\n",
            "            continue\n",
            "        filtered_hits.append(hits[i])\n",
            "    return filtered_hits"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_repo_path = '../data/facebook_react/'\n",
            "test_idx_path = f'{test_repo_path}/index_tk'\n",
            "test_df = get_combined_df(test_repo_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_query_commit = '7022e8d6a3222c97d287dfa0f2361acc8a30683a'\n",
            "test_query = 'fix[devtools/extension]: fixed duplicating panels in firefox'\n",
            "test_query_date = test_df[test_df['commit_id'] == test_query_commit]['commit_date'].tolist()[0]\n",
            "actual_modified_files = test_df[test_df['commit_id'] == test_query_commit]['file_path'].tolist()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# test_query_commit = 'd8f358facc2a5405d08977f922bc0b1dae8f114e'\n",
            "# test_query = 'In TestSslUtils set SubjectAlternativeNames to null if there are no hostnames'\n",
            "# test_query_date = test_df[test_df['commit_id'] == test_query_commit]['commit_date'].tolist()[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 46,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "965\n",
                  " 6 29b405b2de6b4abaa67ff53f3b5e067f80b106d3 21.67930 react/packages/react-devtools-extensions/src/main/index.js 1693420278\n",
                  "137 8fbd3079425eaff85ec1b52b0eefecdd44ba7289 10.86729 react/packages/react-devtools-extensions/src/main/index.js 1693307366\n"
               ]
            }
         ],
         "source": [
            "# test_hits = search(test_query, test_idx_path, test_query_date, k=1000)\n",
            "# print(len(test_hits))\n",
            "\n",
            "# for i in range(len(test_hits)):\n",
            "#     # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
            "#     # print with repo name and file name\n",
            "#     obj = json.loads(test_hits[i].raw)\n",
            "#     # print(obj)\n",
            "#     commit_date = int(obj[\"commit_date\"])\n",
            "#     if obj[\"file_path\"] in actual_modified_files:\n",
            "#         print(f'{i+1:2} {test_hits[i].docid:4} {test_hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')\n",
            "#     # print(f'{i+1:2} {test_hits[i].docid:4} {test_hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 64,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "965\n",
                  " 6 29b405b2de6b4abaa67ff53f3b5e067f80b106d3 21.67930 react/packages/react-devtools-extensions/src/main/index.js 1693420278\n",
                  "137 8fbd3079425eaff85ec1b52b0eefecdd44ba7289 10.86729 react/packages/react-devtools-extensions/src/main/index.js 1693307366\n"
               ]
            }
         ],
         "source": [
            "test_hits = search(test_query, test_idx_path, test_query_date, k=1000)\n",
            "print(len(test_hits))\n",
            "\n",
            "for i in range(len(test_hits)):\n",
            "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
            "    # print with repo name and file name\n",
            "    obj = json.loads(test_hits[i].raw)\n",
            "    # print(obj)\n",
            "    commit_date = int(obj[\"commit_date\"])\n",
            "    if obj[\"file_path\"] in actual_modified_files:\n",
            "        print(f'{i+1:2} {test_hits[i].docid:4} {test_hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')\n",
            "    # print(f'{i+1:2} {test_hits[i].docid:4} {test_hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]} {commit_date}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "from sklearn.metrics import average_precision_score, ndcg_score\n",
            "\n",
            "def evaluate(query, idx_path, query_date, actual_modified_files, k=1000):\n",
            "    hits = search(query, idx_path, query_date, k)\n",
            "\n",
            "    # Convert the hits to a list of filenames\n",
            "    retrieved_files = [json.loads(hit.raw)['file_path'] for hit in hits]\n",
            "\n",
            "    # Generate binary relevance judgments based on the actual_modified_files\n",
            "    relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
            "\n",
            "    if sum(relevant) == 0:\n",
            "        return {\n",
            "            'MAP': 0,\n",
            "            'P@10': 0,\n",
            "            'P@100': 0,\n",
            "            'P@1K': 0,\n",
            "            # 'P@10K': 0,\n",
            "            'MRR': 0,\n",
            "            'Recall@1K': 0\n",
            "            # 'NDCG@10': 0,\n",
            "            # 'NDCG@100': 0,\n",
            "            # 'NDCG@1K': 0,\n",
            "            # 'NDCG@10K': 0\n",
            "        }\n",
            "    # Calculate the metrics\n",
            "    MAP = average_precision_score(relevant, [1]*len(relevant))\n",
            "    unique_relevant_files = {\n",
            "        file for idx, file in enumerate(retrieved_files) if relevant[idx] == 1\n",
            "    }\n",
            "    recall = len(unique_relevant_files) / len(actual_modified_files)\n",
            "    # recall = sum(relevant) / len(actual_modified_files)\n",
            "    # also calculate MRR\n",
            "    MRR = mean_reciprocal_rank(relevant)\n",
            "    precision_values = [precision_at_k(relevant, k_val) for k_val in [10, 100, 1000]]\n",
            "\n",
            "    #todo NDCG calculations - no multi-label support as of now\n",
            "    # true_relevance = [[rel] for rel in relevant]\n",
            "    # scores = [[1] for _ in relevant]  # assuming all the retrieved files are equally relevant\n",
            "    # NDCG_values = [ndcg_score(true_relevance, scores, k=k_val) for k_val in [10, 100, 1000, 10000]]\n",
            "\n",
            "    metrics = {\n",
            "        'MAP': MAP,\n",
            "        'P@10': precision_values[0],\n",
            "        'P@100': precision_values[1],\n",
            "        'P@1K': precision_values[2],\n",
            "        # 'P@10K': precision_values[3],\n",
            "        'MRR': MRR,\n",
            "        'Recall@1K': recall\n",
            "        # 'NDCG@10': NDCG_values[0],\n",
            "        # 'NDCG@100': NDCG_values[1],\n",
            "        # 'NDCG@1K': NDCG_values[2],\n",
            "        # 'NDCG@10K': NDCG_values[3]\n",
            "    }\n",
            "    # round all the values to 4 decimal places\n",
            "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
            "    return metrics\n",
            "\n",
            "def precision_at_k(relevant, k):\n",
            "    # if k > len(relevant):\n",
            "    #     return -1\n",
            "    return sum(relevant[:k]) / k\n",
            "\n",
            "def mean_reciprocal_rank(relevant):\n",
            "    for idx, value in enumerate(relevant):\n",
            "        if value == 1:\n",
            "            return 1 / (idx + 1)\n",
            "    return 0"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 140,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'data/facebook_react//index_tk_nf/'"
                  ]
               },
               "execution_count": 140,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "idx_path"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 86,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.0154,\n",
                     " 'P@10': 0.1,\n",
                     " 'P@100': 0.01,\n",
                     " 'P@1K': 0.001,\n",
                     " 'MRR': 0.1667,\n",
                     " 'Recall@1K': 1.0}"
                  ]
               },
               "execution_count": 86,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate(test_query, test_idx_path, test_query_date, actual_modified_files, k=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 195,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'MAP': 0.0219, 'MRR': 0.121, 'P@10': 0.052, 'P@100': 0.0188, 'P@1K': 0.0068, 'Recall@1K': 0.5148}\n"
               ]
            }
         ],
         "source": [
            "# # Assuming df is your data frame\n",
            "# sampled_commits = df.drop_duplicates(subset='commit_id').sample(100, replace=False, random_state=42)\n",
            "\n",
            "# results = []\n",
            "\n",
            "# for index, row in sampled_commits.iterrows():\n",
            "#     query = row['commit_message']\n",
            "#     query_date = row['commit_date'].strftime('%Y-%m-%d')\n",
            "#     query_commit_id = row['commit_id']\n",
            "#     actual_modified_files = df[df['commit_id'] == query_commit_id]['file_path'].tolist()\n",
            "\n",
            "#     result = evaluate(query, idx_path, query_date, actual_modified_files)\n",
            "#     results.append(result)\n",
            "\n",
            "# # Compute average scores\n",
            "# avg_scores = {}\n",
            "# metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
            "# for metric in metrics:\n",
            "#     avg_scores[metric] = np.mean([result[metric] for result in results])\n",
            "\n",
            "# # round all the values to 4 decimal places\n",
            "# avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
            "# print(avg_scores)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "# generalize sampling across all repos by making a function which does it for each repo_name in REPO_LIST\n",
            "\n",
            "def evaluate_sampling(repo_dir, idx_path, n=100):\n",
            "    metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
            "    # all_files = glob.glob(os.path.join(repo_dir, '*.parquet'))\n",
            "    # all_dataframes = [pd.read_parquet(file) for file in all_files]\n",
            "    # combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
            "\n",
            "    combined_df = get_combined_df(repo_dir)\n",
            "    print(f'Total rows: {combined_df.shape[0]}')\n",
            "    print(f'Index path: {idx_path}')\n",
            "    print(IndexReader(idx_path).stats())\n",
            "    total_commits = combined_df.commit_id.nunique()\n",
            "    # print(f'Total commits: {total_commits}')\n",
            "    if total_commits < n:\n",
            "        print(f'Not enough commits to sample for {repo_dir}, skipping...')\n",
            "        return {metric: 0 for metric in metrics}\n",
            "    # n = total_commits // 10 if total_commits > 10 else 1\n",
            "    print(f'Processing {repo_dir} with {n} samples')\n",
            "\n",
            "    sampled_commits = combined_df.drop_duplicates(subset='commit_id').sample(n, replace=False, random_state=42)\n",
            "    print(f'Number of commits sampled: {len(sampled_commits)}')\n",
            "    results = []\n",
            "    for index, row in sampled_commits.iterrows():\n",
            "        query = row['commit_message']\n",
            "        # query_date = row['commit_date'].strftime('%Y-%m-%d')\n",
            "        query_date = row['commit_date']\n",
            "        # query_date = row['commit_date'].strftime('%Y-%m-%d %H:%M:%S%z')\n",
            "        query_commit_id = row['commit_id']\n",
            "        actual_modified_files = combined_df[combined_df['commit_id'] == query_commit_id]['file_path'].tolist()\n",
            "\n",
            "        result = evaluate(query, idx_path, query_date, actual_modified_files)\n",
            "        results.append(result)\n",
            "    avg_scores = {\n",
            "        metric: np.mean([result[metric] for result in results])\n",
            "        for metric in metrics\n",
            "    }\n",
            "    # round all the values to 4 decimal places\n",
            "    avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
            "    return avg_scores"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['../smalldata/ftr/facebook_react_commit_data_0.parquet', '../smalldata/ftr/facebook_react_commit_data_1.parquet', '../smalldata/ftr/facebook_react_commit_data_10.parquet', '../smalldata/ftr/facebook_react_commit_data_11.parquet', '../smalldata/ftr/facebook_react_commit_data_12.parquet', '../smalldata/ftr/facebook_react_commit_data_13.parquet', '../smalldata/ftr/facebook_react_commit_data_14.parquet', '../smalldata/ftr/facebook_react_commit_data_15.parquet', '../smalldata/ftr/facebook_react_commit_data_16.parquet', '../smalldata/ftr/facebook_react_commit_data_2.parquet', '../smalldata/ftr/facebook_react_commit_data_3.parquet', '../smalldata/ftr/facebook_react_commit_data_4.parquet', '../smalldata/ftr/facebook_react_commit_data_5.parquet', '../smalldata/ftr/facebook_react_commit_data_6.parquet', '../smalldata/ftr/facebook_react_commit_data_7.parquet', '../smalldata/ftr/facebook_react_commit_data_8.parquet', '../smalldata/ftr/facebook_react_commit_data_9.parquet']\n",
                  "Total rows: 73765\n",
                  "Index path: ../smalldata/ftr/index_commit_tokenized\n",
                  "{'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n",
                  "Processing ../smalldata/ftr/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.0494,\n",
                     " 'MRR': 0.2464,\n",
                     " 'P@10': 0.093,\n",
                     " 'P@100': 0.0209,\n",
                     " 'P@1K': 0.0056,\n",
                     " 'Recall@1K': 0.5429}"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate_sampling('../smalldata/ftr/', '../smalldata/ftr/index_commit_tokenized', n=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 127,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total rows: 73551\n",
                  "Index path: data/facebook_react/index_tk\n",
                  "Processing data/facebook_react/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.0314,\n",
                     " 'MRR': 0.2445,\n",
                     " 'P@10': 0.088,\n",
                     " 'P@100': 0.0269,\n",
                     " 'P@1K': 0.0066,\n",
                     " 'Recall@1K': 0.592}"
                  ]
               },
               "execution_count": 127,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate_sampling('data/facebook_react/', 'data/facebook_react/index_tk', n=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 62,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total rows: 69860\n",
                  "Index path: ../data/facebook_react/index_tk\n",
                  "{'total_terms': 8062861, 'documents': 69860, 'non_empty_documents': 69860, 'unique_terms': 14589}\n",
                  "Processing ../data/facebook_react/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.0427,\n",
                     " 'MRR': 0.2676,\n",
                     " 'P@10': 0.079,\n",
                     " 'P@100': 0.0327,\n",
                     " 'P@1K': 0.0084,\n",
                     " 'Recall@1K': 0.6351}"
                  ]
               },
               "execution_count": 62,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate_sampling('../data/facebook_react/', '../data/facebook_react/index_tk', n=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 63,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total rows: 69860\n",
                  "Index path: ../data/facebook_react/index_no_tk\n",
                  "{'total_terms': 4445676, 'documents': 69860, 'non_empty_documents': 69860, 'unique_terms': 42926}\n",
                  "Processing ../data/facebook_react/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.0009,\n",
                     " 'MRR': 0.0013,\n",
                     " 'P@10': 0.0,\n",
                     " 'P@100': 0.0005,\n",
                     " 'P@1K': 0.0002,\n",
                     " 'Recall@1K': 0.0199}"
                  ]
               },
               "execution_count": 63,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate_sampling('../data/facebook_react/', '../data/facebook_react/index_no_tk', n=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 89,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total rows: 69631\n",
                  "Index path: ../data/facebook_react/index_tk\n",
                  "Processing ../data/facebook_react/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.0266,\n",
                     " 'MRR': 0.1858,\n",
                     " 'P@10': 0.061,\n",
                     " 'P@100': 0.0253,\n",
                     " 'P@1K': 0.0082,\n",
                     " 'Recall@1K': 0.4956}"
                  ]
               },
               "execution_count": 89,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate_sampling('../data/facebook_react/', '../data/facebook_react/index_tk', n=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 131,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total rows: 402\n",
                  "Index path: data/karpathy_llama2.c/index_tk\n",
                  "Processing data/karpathy_llama2.c/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.2585,\n",
                     " 'MRR': 0.6278,\n",
                     " 'P@10': 0.287,\n",
                     " 'P@100': -0.1793,\n",
                     " 'P@1K': -0.98,\n",
                     " 'Recall@1K': 0.9717}"
                  ]
               },
               "execution_count": 131,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate_sampling('data/karpathy_llama2.c/', 'data/karpathy_llama2.c/index_tk', n=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 128,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total rows: 75870\n",
                  "Index path: data/apache_kafka/index_tk\n",
                  "Processing data/apache_kafka/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'MAP': 0.025,\n",
                     " 'MRR': 0.2009,\n",
                     " 'P@10': 0.065,\n",
                     " 'P@100': 0.0315,\n",
                     " 'P@1K': 0.0105,\n",
                     " 'Recall@1K': 0.6829}"
                  ]
               },
               "execution_count": 128,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "evaluate_sampling('data/apache_kafka/', 'data/apache_kafka/index_tk', n=100)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Index path: data/apache_kafka/index_tk\n",
            "Total commits: 10438\n",
            "Processing data/apache_kafka/ with 100 samples\n",
            "Number of commits sampled: 100\n",
            "{'MAP': 0.0284,\n",
            " 'MRR': 0.3324,\n",
            " 'P@10': 0.142,\n",
            " 'P@100': 0.0422,\n",
            " 'P@1K': 0.0116,\n",
            " 'Recall@1K': 0.7336}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 79,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm import tqdm"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 76,
         "metadata": {},
         "outputs": [],
         "source": [
            "REPO_LIST = ['karpathy_llama2.c',\n",
            "#  'facebook_react',\n",
            " 'apache_kafka',\n",
            " 'ggerganov_llama.cpp',\n",
            " 'nodejs_node']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 129,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "  0%|          | 0/4 [00:00<?, ?it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Processing data/karpathy_llama2.c/\n",
                  "Total rows: 402\n",
                  "Index path: data/karpathy_llama2.c//index_tk/\n",
                  "Processing data/karpathy_llama2.c/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  " 25%|       | 1/4 [00:00<00:01,  1.64it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'MAP': 0.2585, 'MRR': 0.6278, 'P@10': 0.335, 'P@100': 0.2065, 'P@1K': 0.0388, 'Recall@1K': 0.9717}\n",
                  "\n",
                  "Processing data/apache_kafka/\n",
                  "Total rows: 75870\n",
                  "Index path: data/apache_kafka//index_tk/\n",
                  "Processing data/apache_kafka/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  " 50%|     | 2/4 [00:28<00:32, 16.38s/it]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'MAP': 0.025, 'MRR': 0.2009, 'P@10': 0.065, 'P@100': 0.0315, 'P@1K': 0.0105, 'Recall@1K': 0.6829}\n",
                  "\n",
                  "Processing data/ggerganov_llama.cpp/\n",
                  "Total rows: 2111\n",
                  "Index path: data/ggerganov_llama.cpp//index_tk/\n",
                  "Processing data/ggerganov_llama.cpp/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  " 75%|  | 3/4 [00:30<00:10, 10.16s/it]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'MAP': 0.1405, 'MRR': 0.3986, 'P@10': 0.157, 'P@100': 0.1192, 'P@1K': 0.0466, 'Recall@1K': 0.9172}\n",
                  "\n",
                  "Processing data/nodejs_node/\n",
                  "Total rows: 208188\n",
                  "Index path: data/nodejs_node//index_tk/\n",
                  "Processing data/nodejs_node/ with 100 samples\n",
                  "Number of commits sampled: 100\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|| 4/4 [00:55<00:00, 13.77s/it]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'MAP': 0.0321, 'MRR': 0.2734, 'P@10': 0.087, 'P@100': 0.0423, 'P@1K': 0.0099, 'Recall@1K': 0.6001}\n",
                  "\n",
                  "Average scores for all repos:\n",
                  " {'MAP': 0.114, 'MRR': 0.3752, 'P@10': 0.161, 'P@100': 0.0999, 'P@1K': 0.0265, 'Recall@1K': 0.793}\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "metrics = ['MAP', 'MRR', 'P@10', 'P@100', 'P@1K', 'Recall@1K']\n",
            "res = []\n",
            "for repo_name in tqdm(REPO_LIST):\n",
            "    repo_dir = f'data/{repo_name}/'\n",
            "    idx_path = f'{repo_dir}/index_tk/'\n",
            "    print(f'Processing {repo_dir}')\n",
            "    avg_scores = evaluate_sampling(repo_dir, idx_path)\n",
            "    res.append(avg_scores)\n",
            "    print(avg_scores)\n",
            "    print()\n",
            "\n",
            "# avg scores for all repos\n",
            "avg_scores = {}\n",
            "for metric in metrics:\n",
            "    avg_scores[metric] = np.mean([result[metric] for result in res])\n",
            "# round all the values to 4 decimal places\n",
            "avg_scores = {k: round(v, 4) for k, v in avg_scores.items()}\n",
            "print(f'Average scores for all repos:\\n {avg_scores}')\n",
            "# evaluate_sampling(repo_dir)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 57,
         "metadata": {},
         "outputs": [],
         "source": [
            "# llama2.c\n",
            "# query = 'nInference for Llama-2 Transformer model in pure C'\n",
            "\n",
            "# refpred\n",
            "# query = 'if is_arxiv:\\n return f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{paper_id}/references?fields=title,\n",
            "# abstract,url,venue,publicationVenue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess'\n",
            "\n",
            "# react\n",
            "# query = \"export {default} from './npm/Circle';\"\n",
            "\n",
            "# kafka\n",
            "# public class MockKafkaLog4jAppender extends KafkaLog4jAppender {\n",
            "#     private MockProducer<byte[], byte[]> mockProducer =\n",
            "#             new MockProducer<>(false, new MockSerializer(), new MockSerializer());\n",
            "\n",
            "#     private Properties producerProperties;\n",
            "\n",
            "#     @Override\n",
            "#     protected Producer<byte[], byte[]> getKafkaProducer(Properties props) {\n",
            "#         producerProperties = props;\n",
            "#         return mockProducer;\n",
            "#     }\n",
            "\n",
            "#     void setKafkaProducer(MockProducer<byte[], byte[]> producer) {\n",
            "#         this.mockProducer = producer;\n",
            "#     }\n",
            "# \"\"\"\n",
            "\n",
            "# Kakfa\n",
            "# query = \"\"\"\n",
            "# /**\n",
            "#  * Local file based quorum state store. It takes the JSON format of {@link QuorumStateData}\n",
            "#  * with an extra data version number as part of the data for easy deserialization.\n",
            "#  *\n",
            "#  * Example format:\n",
            "#  * <pre>\n",
            "#  * {\"clusterId\":\"\",\n",
            "#  *   \"leaderId\":1,\n",
            "#  *   \"leaderEpoch\":2,\n",
            "#  *   \"votedId\":-1,\n",
            "#  *   \"appliedOffset\":0,\n",
            "#  *   \"currentVoters\":[],\n",
            "#  *   \"data_version\":0}\n",
            "#  * </pre>\n",
            "#  * */\n",
            "\n",
            "# \"\"\"\n",
            "\n",
            "# kakfa\n",
            "query = \"\"\"Convert coordinator retriable errors to a known producer\n",
            " response error (#14378)\n",
            "\n",
            "KIP-890 Part 1 tries to address hanging transactions on old clients. Thus, the produce version can not be bumped and no new errors can be added. Before we used the java client's notion of retriable and abortable errors -- retriable errors are defined as such by extending the retriable error class, fatal errors are defined explicitly, and abortable errors are the remaining. However, many other clients treat non specified errors as fatal and that means many retriable errors kill the application.\"\"\"\n",
            "\n",
            "# kakfa\n",
            "# query = \"\"\"Fix flaky TopicAdminTest::retryEndOffsetsShouldRetryWhenTopicNotFound test case\"\"\"\n",
            "\n",
            "# nodejs\n",
            "# query = \"\"\"bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
            "#   DebugSealHandleScope scope(isolate);\n",
            "#   Environment* env = Environment::GetCurrent(isolate);\n",
            "#   return env != nullptr &&\n",
            "#          (env->is_main_thread() || !env->is_stopping()) &&\n",
            "#          env->abort_on_uncaught_exception() &&\n",
            "#          env->should_abort_on_uncaught_toggle()[0] &&\n",
            "#          !env->inside_should_not_abort_on_uncaught_scope();\n",
            "# }\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 58,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  " 1 5aecd2825644728f68a26558c957f5dfd4643423 99.51060 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
                  " 2 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 82.57980 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 81.72260 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 4 5aecd2825644728f68a26558c957f5dfd4643423 81.36090 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
                  " 5 ef09a2e3fc11a738f6681fd57fb84ad109593fd3 80.57710 kafka/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala\n",
                  " 6 f5d5f654db359af077088685e29fbe5ea69616cf 79.69870 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 7 2b6365c78b6e659f8df0651a24013d028f39edd9 79.64400 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 8 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 78.68580 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 9 1a10c3445e157da1d2fd670c043f19c385465eb0 78.48480 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  "10 69d2a177101eb1c29b59b4c64d8c22f6d5e3d281 78.27240 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
               ]
            }
         ],
         "source": [
            "bm25searcher = LuceneSearcher('bm25_index_6/')\n",
            "hits = bm25searcher.search(query, k=10)\n",
            "# print(hits[0])\n",
            "for i in range(len(hits)):\n",
            "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
            "    # print with repo name and file name\n",
            "    obj = json.loads(hits[i].raw)\n",
            "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'total_terms': 696778,\n",
                     " 'documents': 402,\n",
                     " 'non_empty_documents': 402,\n",
                     " 'unique_terms': 6840}"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "index_reader = IndexReader('idx_karpathy/')\n",
            "index_reader.stats()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pyserini.index import IndexReader"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [],
         "source": [
            "index_reader = IndexReader('idx_karpathy_double_token/')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|| 402/402 [00:02<00:00, 190.05it/s]\n"
               ]
            }
         ],
         "source": [
            "index_reader.dump_documents_BM25('tmp/idx_karpathy_double.jsonl')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'total_terms': 578447,\n",
                     " 'documents': 402,\n",
                     " 'non_empty_documents': 402,\n",
                     " 'unique_terms': 3034}"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "index_reader = IndexReader('idx_karpathy_double_token/')\n",
            "index_reader.stats()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 59,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  " 1 5aecd2825644728f68a26558c957f5dfd4643423 141.63670 kafka/core/src/main/scala/kafka/server/ReplicaManager.scala\n",
                  " 2 5aecd2825644728f68a26558c957f5dfd4643423 112.99820 kafka/core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala\n",
                  " 3 5aad085a8e7514c14a17121d316a2e2b2add8bcc 111.59350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 4 ff77b3ad041c1a4c80119f960e1f87c07b9e93dd 111.57550 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 5 29a1a16668d76a1cc04ec9e39ea13026f2dce1de 110.54000 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 6 ea0bb001262320bc9233221955a2be31c85993b9 109.68660 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 7 f5d5f654db359af077088685e29fbe5ea69616cf 109.62250 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 8 b937ec75677f8af13bf6fda686f07e9c62cdd20f 109.10350 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  " 9 a81f35c1c8f9dc594aa585618c36f92ade0f86e2 109.03760 kafka/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java\n",
                  "10 b49013b73efa25466652d8d8122974e60c927ec4 108.96060 kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java\n"
               ]
            }
         ],
         "source": [
            "tiktoken_searcher = LuceneSearcher('bm25_index_tiktoken_6/')\n",
            "# get tokenized query with enc.encode\n",
            "tokeninzed_query = tokenize(query)\n",
            "hits = tiktoken_searcher.search(tokeninzed_query, k=10)\n",
            "# print(hits[0])\n",
            "for i in range(len(hits)):\n",
            "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
            "    # print with repo name and file name\n",
            "    obj = json.loads(hits[i].raw)\n",
            "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {obj[\"repo_name\"]}/{obj[\"file_path\"]}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 46,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'total_terms': 2698903862,\n",
                     " 'documents': 360230,\n",
                     " 'non_empty_documents': 360230,\n",
                     " 'unique_terms': -1}"
                  ]
               },
               "execution_count": 46,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tiktoken_index_reader = IndexReader('bm25_index_tiktoken_6/')\n",
            "tiktoken_index_reader.stats()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 47,
         "metadata": {},
         "outputs": [],
         "source": [
            "# print the document source code inside the first hit raw\n",
            "content = json.loads(hits[0].raw)['contents']\n",
            "\n",
            "# print the document source code inside the first hit raw by decoding the tokenized string with enc.decode (convert to array of int and then decode)\n",
            "# print(enc.decode(json.loads(hits[0].raw)['contents']))\n",
            "\n",
            "# convert content to array of int\n",
            "content_arr = [int(i) for i in content.split()]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 48,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "worker: fix --abort-on-uncaught-exception handling\n",
                  "\n",
                  "The `set_abort_on_uncaught_exception(false)` line was supposed to\n",
                  "prevent aborting when running Workers in\n",
                  "`--abort-on-uncaught-exception` mode, but it was incorrectly set\n",
                  "and not checked properly in the should-abort callback.\n",
                  "\n",
                  "PR-URL: https://github.com/nodejs/node/pull/34724\n",
                  "Reviewed-By: Colin Ihrig <cjihrig@gmail.com>\n",
                  "Reviewed-By: Richard Lau <riclau@uk.ibm.com>\n",
                  "Reviewed-By: James M Snell <jasnell@gmail.com>\n",
                  "Reviewed-By: Mary Marchini <oss@mmarchini.me>\n",
                  "\n",
                  "#include \"node.h\"\n",
                  "#include \"node_context_data.h\"\n",
                  "#include \"node_errors.h\"\n",
                  "#include \"node_internals.h\"\n",
                  "#include \"node_native_module_env.h\"\n",
                  "#include \"node_platform.h\"\n",
                  "#include \"node_v8_platform-inl.h\"\n",
                  "#include \"uv.h\"\n",
                  "\n",
                  "#if HAVE_INSPECTOR\n",
                  "#include \"inspector/worker_inspector.h\"  // ParentInspectorHandle\n",
                  "#endif\n",
                  "\n",
                  "namespace node {\n",
                  "using errors::TryCatchScope;\n",
                  "using v8::Array;\n",
                  "using v8::Context;\n",
                  "using v8::EscapableHandleScope;\n",
                  "using v8::Function;\n",
                  "using v8::FunctionCallbackInfo;\n",
                  "using v8::HandleScope;\n",
                  "using v8::Isolate;\n",
                  "using v8::Local;\n",
                  "using v8::MaybeLocal;\n",
                  "using v8::Null;\n",
                  "using v8::Object;\n",
                  "using v8::ObjectTemplate;\n",
                  "using v8::Private;\n",
                  "using v8::PropertyDescriptor;\n",
                  "using v8::SealHandleScope;\n",
                  "using v8::String;\n",
                  "using v8::Value;\n",
                  "\n",
                  "static bool AllowWasmCodeGenerationCallback(Local<Context> context,\n",
                  "                                            Local<String>) {\n",
                  "  Local<Value> wasm_code_gen =\n",
                  "      context->GetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration);\n",
                  "  return wasm_code_gen->IsUndefined() || wasm_code_gen->IsTrue();\n",
                  "}\n",
                  "\n",
                  "static bool ShouldAbortOnUncaughtException(Isolate* isolate) {\n",
                  "  DebugSealHandleScope scope(isolate);\n",
                  "  Environment* env = Environment::GetCurrent(isolate);\n",
                  "  return env != nullptr &&\n",
                  "         (env->is_main_thread() || !env->is_stopping()) &&\n",
                  "         env->abort_on_uncaught_exception() &&\n",
                  "         env->should_abort_on_uncaught_toggle()[0] &&\n",
                  "         !env->inside_should_not_abort_on_uncaught_scope();\n",
                  "}\n",
                  "\n",
                  "static MaybeLocal<Value> PrepareStackTraceCallback(Local<Context> context,\n",
                  "                                      Local<Value> exception,\n",
                  "                                      Local<Array> trace) {\n",
                  "  Environment* env = Environment::GetCurrent(context);\n",
                  "  if (env == nullptr) {\n",
                  "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
                  "  }\n",
                  "  Local<Function> prepare = env->prepare_stack_trace_callback();\n",
                  "  if (prepare.IsEmpty()) {\n",
                  "    return exception->ToString(context).FromMaybe(Local<Value>());\n",
                  "  }\n",
                  "  Local<Value> args[] = {\n",
                  "      context->Global(),\n",
                  "      exception,\n",
                  "      trace,\n",
                  "  };\n",
                  "  // This TryCatch + Rethrow is required by V8 due to details around exception\n",
                  "  // handling there. For C++ callbacks, V8 expects a scheduled exception (which\n",
                  "  // is what ReThrow gives us). Just returning the empty MaybeLocal would leave\n",
                  "  // us with a pending exception.\n",
                  "  TryCatchScope try_catch(env);\n",
                  "  MaybeLocal<Value> result = prepare->Call(\n",
                  "      context, Undefined(env->isolate()), arraysize(args), args);\n",
                  "  if (try_catch.HasCaught() && !try_catch.HasTerminated()) {\n",
                  "    try_catch.ReThrow();\n",
                  "  }\n",
                  "  return result;\n",
                  "}\n",
                  "\n",
                  "void* NodeArrayBufferAllocator::Allocate(size_t size) {\n",
                  "  void* ret;\n",
                  "  if (zero_fill_field_ || per_process::cli_options->zero_fill_all_buffers)\n",
                  "    ret = UncheckedCalloc(size);\n",
                  "  else\n",
                  "    ret = UncheckedMalloc(size);\n",
                  "  if (LIKELY(ret != nullptr))\n",
                  "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
                  "  return ret;\n",
                  "}\n",
                  "\n",
                  "void* NodeArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
                  "  void* ret = node::UncheckedMalloc(size);\n",
                  "  if (LIKELY(ret != nullptr))\n",
                  "    total_mem_usage_.fetch_add(size, std::memory_order_relaxed);\n",
                  "  return ret;\n",
                  "}\n",
                  "\n",
                  "void* NodeArrayBufferAllocator::Reallocate(\n",
                  "    void* data, size_t old_size, size_t size) {\n",
                  "  void* ret = UncheckedRealloc<char>(static_cast<char*>(data), size);\n",
                  "  if (LIKELY(ret != nullptr) || UNLIKELY(size == 0))\n",
                  "    total_mem_usage_.fetch_add(size - old_size, std::memory_order_relaxed);\n",
                  "  return ret;\n",
                  "}\n",
                  "\n",
                  "void NodeArrayBufferAllocator::Free(void* data, size_t size) {\n",
                  "  total_mem_usage_.fetch_sub(size, std::memory_order_relaxed);\n",
                  "  free(data);\n",
                  "}\n",
                  "\n",
                  "DebuggingArrayBufferAllocator::~DebuggingArrayBufferAllocator() {\n",
                  "  CHECK(allocations_.empty());\n",
                  "}\n",
                  "\n",
                  "void* DebuggingArrayBufferAllocator::Allocate(size_t size) {\n",
                  "  Mutex::ScopedLock lock(mutex_);\n",
                  "  void* data = NodeArrayBufferAllocator::Allocate(size);\n",
                  "  RegisterPointerInternal(data, size);\n",
                  "  return data;\n",
                  "}\n",
                  "\n",
                  "void* DebuggingArrayBufferAllocator::AllocateUninitialized(size_t size) {\n",
                  "  Mutex::ScopedLock lock(mutex_);\n",
                  "  void* data = NodeArrayBufferAllocator::AllocateUninitialized(size);\n",
                  "  RegisterPointerInternal(data, size);\n",
                  "  return data;\n",
                  "}\n",
                  "\n",
                  "void DebuggingArrayBufferAllocator::Free(void* data, size_t size) {\n",
                  "  Mutex::ScopedLock lock(mutex_);\n",
                  "  UnregisterPointerInternal(data, size);\n",
                  "  NodeArrayBufferAllocator::Free(data, size);\n",
                  "}\n",
                  "\n",
                  "void* DebuggingArrayBufferAllocator::Reallocate(void* data,\n",
                  "                                                size_t old_size,\n",
                  "                                                size_t size) {\n",
                  "  Mutex::ScopedLock lock(mutex_);\n",
                  "  void* ret = NodeArrayBufferAllocator::Reallocate(data, old_size, size);\n",
                  "  if (ret == nullptr) {\n",
                  "    if (size == 0)  // i.e. equivalent to free().\n",
                  "      UnregisterPointerInternal(data, old_size);\n",
                  "    return nullptr;\n",
                  "  }\n",
                  "\n",
                  "  if (data != nullptr) {\n",
                  "    auto it = allocations_.find(data);\n",
                  "    CHECK_NE(it, allocations_.end());\n",
                  "    allocations_.erase(it);\n",
                  "  }\n",
                  "\n",
                  "  RegisterPointerInternal(ret, size);\n",
                  "  return ret;\n",
                  "}\n",
                  "\n",
                  "void DebuggingArrayBufferAllocator::RegisterPointer(void* data, size_t size) {\n",
                  "  Mutex::ScopedLock lock(mutex_);\n",
                  "  NodeArrayBufferAllocator::RegisterPointer(data, size);\n",
                  "  RegisterPointerInternal(data, size);\n",
                  "}\n",
                  "\n",
                  "void DebuggingArrayBufferAllocator::UnregisterPointer(void* data, size_t size) {\n",
                  "  Mutex::ScopedLock lock(mutex_);\n",
                  "  NodeArrayBufferAllocator::UnregisterPointer(data, size);\n",
                  "  UnregisterPointerInternal(data, size);\n",
                  "}\n",
                  "\n",
                  "void DebuggingArrayBufferAllocator::UnregisterPointerInternal(void* data,\n",
                  "                                                              size_t size) {\n",
                  "  if (data == nullptr) return;\n",
                  "  auto it = allocations_.find(data);\n",
                  "  CHECK_NE(it, allocations_.end());\n",
                  "  if (size > 0) {\n",
                  "    // We allow allocations with size 1 for 0-length buffers to avoid having\n",
                  "    // to deal with nullptr values.\n",
                  "    CHECK_EQ(it->second, size);\n",
                  "  }\n",
                  "  allocations_.erase(it);\n",
                  "}\n",
                  "\n",
                  "void DebuggingArrayBufferAllocator::RegisterPointerInternal(void* data,\n",
                  "                                                            size_t size) {\n",
                  "  if (data == nullptr) return;\n",
                  "  CHECK_EQ(allocations_.count(data), 0);\n",
                  "  allocations_[data] = size;\n",
                  "}\n",
                  "\n",
                  "std::unique_ptr<ArrayBufferAllocator> ArrayBufferAllocator::Create(bool debug) {\n",
                  "  if (debug || per_process::cli_options->debug_arraybuffer_allocations)\n",
                  "    return std::make_unique<DebuggingArrayBufferAllocator>();\n",
                  "  else\n",
                  "    return std::make_unique<NodeArrayBufferAllocator>();\n",
                  "}\n",
                  "\n",
                  "ArrayBufferAllocator* CreateArrayBufferAllocator() {\n",
                  "  return ArrayBufferAllocator::Create().release();\n",
                  "}\n",
                  "\n",
                  "void FreeArrayBufferAllocator(ArrayBufferAllocator* allocator) {\n",
                  "  delete allocator;\n",
                  "}\n",
                  "\n",
                  "void SetIsolateCreateParamsForNode(Isolate::CreateParams* params) {\n",
                  "  const uint64_t constrained_memory = uv_get_constrained_memory();\n",
                  "  const uint64_t total_memory = constrained_memory > 0 ?\n",
                  "      std::min(uv_get_total_memory(), constrained_memory) :\n",
                  "      uv_get_total_memory();\n",
                  "  if (total_memory > 0) {\n",
                  "    // V8 defaults to 700MB or 1.4GB on 32 and 64 bit platforms respectively.\n",
                  "    // This default is based on browser use-cases. Tell V8 to configure the\n",
                  "    // heap based on the actual physical memory.\n",
                  "    params->constraints.ConfigureDefaults(total_memory, 0);\n",
                  "  }\n",
                  "  params->embedder_wrapper_object_index = BaseObject::InternalFields::kSlot;\n",
                  "  params->embedder_wrapper_type_index = std::numeric_limits<int>::max();\n",
                  "}\n",
                  "\n",
                  "void SetIsolateErrorHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
                  "  if (s.flags & MESSAGE_LISTENER_WITH_ERROR_LEVEL)\n",
                  "    isolate->AddMessageListenerWithErrorLevel(\n",
                  "            errors::PerIsolateMessageListener,\n",
                  "            Isolate::MessageErrorLevel::kMessageError |\n",
                  "                Isolate::MessageErrorLevel::kMessageWarning);\n",
                  "\n",
                  "  auto* abort_callback = s.should_abort_on_uncaught_exception_callback ?\n",
                  "      s.should_abort_on_uncaught_exception_callback :\n",
                  "      ShouldAbortOnUncaughtException;\n",
                  "  isolate->SetAbortOnUncaughtExceptionCallback(abort_callback);\n",
                  "\n",
                  "  auto* fatal_error_cb = s.fatal_error_callback ?\n",
                  "      s.fatal_error_callback : OnFatalError;\n",
                  "  isolate->SetFatalErrorHandler(fatal_error_cb);\n",
                  "\n",
                  "  auto* prepare_stack_trace_cb = s.prepare_stack_trace_callback ?\n",
                  "      s.prepare_stack_trace_callback : PrepareStackTraceCallback;\n",
                  "  isolate->SetPrepareStackTraceCallback(prepare_stack_trace_cb);\n",
                  "}\n",
                  "\n",
                  "void SetIsolateMiscHandlers(v8::Isolate* isolate, const IsolateSettings& s) {\n",
                  "  isolate->SetMicrotasksPolicy(s.policy);\n",
                  "\n",
                  "  auto* allow_wasm_codegen_cb = s.allow_wasm_code_generation_callback ?\n",
                  "    s.allow_wasm_code_generation_callback : AllowWasmCodeGenerationCallback;\n",
                  "  isolate->SetAllowWasmCodeGenerationCallback(allow_wasm_codegen_cb);\n",
                  "\n",
                  "  if ((s.flags & SHOULD_NOT_SET_PROMISE_REJECTION_CALLBACK) == 0) {\n",
                  "    auto* promise_reject_cb = s.promise_reject_callback ?\n",
                  "      s.promise_reject_callback : task_queue::PromiseRejectCallback;\n",
                  "    isolate->SetPromiseRejectCallback(promise_reject_cb);\n",
                  "  }\n",
                  "\n",
                  "  if (s.flags & DETAILED_SOURCE_POSITIONS_FOR_PROFILING)\n",
                  "    v8::CpuProfiler::UseDetailedSourcePositionsForProfiling(isolate);\n",
                  "}\n",
                  "\n",
                  "void SetIsolateUpForNode(v8::Isolate* isolate,\n",
                  "                         const IsolateSettings& settings) {\n",
                  "  SetIsolateErrorHandlers(isolate, settings);\n",
                  "  SetIsolateMiscHandlers(isolate, settings);\n",
                  "}\n",
                  "\n",
                  "void SetIsolateUpForNode(v8::Isolate* isolate) {\n",
                  "  IsolateSettings settings;\n",
                  "  SetIsolateUpForNode(isolate, settings);\n",
                  "}\n",
                  "\n",
                  "Isolate* NewIsolate(ArrayBufferAllocator* allocator, uv_loop_t* event_loop) {\n",
                  "  return NewIsolate(allocator, event_loop, GetMainThreadMultiIsolatePlatform());\n",
                  "}\n",
                  "\n",
                  "// TODO(joyeecheung): we may want to expose this, but then we need to be\n",
                  "// careful about what we override in the params.\n",
                  "Isolate* NewIsolate(Isolate::CreateParams* params,\n",
                  "                    uv_loop_t* event_loop,\n",
                  "                    MultiIsolatePlatform* platform) {\n",
                  "  Isolate* isolate = Isolate::Allocate();\n",
                  "  if (isolate == nullptr) return nullptr;\n",
                  "\n",
                  "  // Register the isolate on the platform before the isolate gets initialized,\n",
                  "  // so that the isolate can access the platform during initialization.\n",
                  "  platform->RegisterIsolate(isolate, event_loop);\n",
                  "\n",
                  "  SetIsolateCreateParamsForNode(params);\n",
                  "  Isolate::Initialize(isolate, *params);\n",
                  "  SetIsolateUpForNode(isolate);\n",
                  "\n",
                  "  return isolate;\n",
                  "}\n",
                  "\n",
                  "Isolate* NewIsolate(ArrayBufferAllocator* allocator,\n",
                  "                    uv_loop_t* event_loop,\n",
                  "                    MultiIsolatePlatform* platform) {\n",
                  "  Isolate::CreateParams params;\n",
                  "  if (allocator != nullptr) params.array_buffer_allocator = allocator;\n",
                  "  return NewIsolate(&params, event_loop, platform);\n",
                  "}\n",
                  "\n",
                  "Isolate* NewIsolate(std::shared_ptr<ArrayBufferAllocator> allocator,\n",
                  "                    uv_loop_t* event_loop,\n",
                  "                    MultiIsolatePlatform* platform) {\n",
                  "  Isolate::CreateParams params;\n",
                  "  if (allocator) params.array_buffer_allocator_shared = allocator;\n",
                  "  return NewIsolate(&params, event_loop, platform);\n",
                  "}\n",
                  "\n",
                  "IsolateData* CreateIsolateData(Isolate* isolate,\n",
                  "                               uv_loop_t* loop,\n",
                  "                               MultiIsolatePlatform* platform,\n",
                  "                               ArrayBufferAllocator* allocator) {\n",
                  "  return new IsolateData(isolate, loop, platform, allocator);\n",
                  "}\n",
                  "\n",
                  "void FreeIsolateData(IsolateData* isolate_data) {\n",
                  "  delete isolate_data;\n",
                  "}\n",
                  "\n",
                  "InspectorParentHandle::~InspectorParentHandle() {}\n",
                  "\n",
                  "// Hide the internal handle class from the public API.\n",
                  "#if HAVE_INSPECTOR\n",
                  "struct InspectorParentHandleImpl : public InspectorParentHandle {\n",
                  "  std::unique_ptr<inspector::ParentInspectorHandle> impl;\n",
                  "\n",
                  "  explicit InspectorParentHandleImpl(\n",
                  "      std::unique_ptr<inspector::ParentInspectorHandle>&& impl)\n",
                  "    : impl(std::move(impl)) {}\n",
                  "};\n",
                  "#endif\n",
                  "\n",
                  "Environment* CreateEnvironment(IsolateData* isolate_data,\n",
                  "                               Local<Context> context,\n",
                  "                               int argc,\n",
                  "                               const char* const* argv,\n",
                  "                               int exec_argc,\n",
                  "                               const char* const* exec_argv) {\n",
                  "  return CreateEnvironment(\n",
                  "      isolate_data, context,\n",
                  "      std::vector<std::string>(argv, argv + argc),\n",
                  "      std::vector<std::string>(exec_argv, exec_argv + exec_argc));\n",
                  "}\n",
                  "\n",
                  "Environment* CreateEnvironment(\n",
                  "    IsolateData* isolate_data,\n",
                  "    Local<Context> context,\n",
                  "    const std::vector<std::string>& args,\n",
                  "    const std::vector<std::string>& exec_args,\n",
                  "    EnvironmentFlags::Flags flags,\n",
                  "    ThreadId thread_id,\n",
                  "    std::unique_ptr<InspectorParentHandle> inspector_parent_handle) {\n",
                  "  Isolate* isolate = context->GetIsolate();\n",
                  "  HandleScope handle_scope(isolate);\n",
                  "  Context::Scope context_scope(context);\n",
                  "  // TODO(addaleax): This is a much better place for parsing per-Environment\n",
                  "  // options than the global parse call.\n",
                  "  Environment* env = new Environment(\n",
                  "      isolate_data, context, args, exec_args, nullptr, flags, thread_id);\n",
                  "#if HAVE_INSPECTOR\n",
                  "  if (inspector_parent_handle) {\n",
                  "    env->InitializeInspector(\n",
                  "        std::move(static_cast<InspectorParentHandleImpl*>(\n",
                  "            inspector_parent_handle.get())->impl));\n",
                  "  } else {\n",
                  "    env->InitializeInspector({});\n",
                  "  }\n",
                  "#endif\n",
                  "\n",
                  "  if (env->RunBootstrapping().IsEmpty()) {\n",
                  "    FreeEnvironment(env);\n",
                  "    return nullptr;\n",
                  "  }\n",
                  "\n",
                  "  return env;\n",
                  "}\n",
                  "\n",
                  "void FreeEnvironment(Environment* env) {\n",
                  "  Isolate::DisallowJavascriptExecutionScope disallow_js(env->isolate(),\n",
                  "      Isolate::DisallowJavascriptExecutionScope::THROW_ON_FAILURE);\n",
                  "  {\n",
                  "    HandleScope handle_scope(env->isolate());  // For env->context().\n",
                  "    Context::Scope context_scope(env->context());\n",
                  "    SealHandleScope seal_handle_scope(env->isolate());\n",
                  "\n",
                  "    env->set_stopping(true);\n",
                  "    env->stop_sub_worker_contexts();\n",
                  "    env->RunCleanup();\n",
                  "    RunAtExit(env);\n",
                  "  }\n",
                  "\n",
                  "  // This call needs to be made while the `Environment` is still alive\n",
                  "  // because we assume that it is available for async tracking in the\n",
                  "  // NodePlatform implementation.\n",
                  "  MultiIsolatePlatform* platform = env->isolate_data()->platform();\n",
                  "  if (platform != nullptr)\n",
                  "    platform->DrainTasks(env->isolate());\n",
                  "\n",
                  "  delete env;\n",
                  "}\n",
                  "\n",
                  "NODE_EXTERN std::unique_ptr<InspectorParentHandle> GetInspectorParentHandle(\n",
                  "    Environment* env,\n",
                  "    ThreadId thread_id,\n",
                  "    const char* url) {\n",
                  "  CHECK_NOT_NULL(env);\n",
                  "  CHECK_NE(thread_id.id, static_cast<uint64_t>(-1));\n",
                  "#if HAVE_INSPECTOR\n",
                  "  return std::make_unique<InspectorParentHandleImpl>(\n",
                  "      env->inspector_agent()->GetParentHandle(thread_id.id, url));\n",
                  "#else\n",
                  "  return {};\n",
                  "#endif\n",
                  "}\n",
                  "\n",
                  "void LoadEnvironment(Environment* env) {\n",
                  "  USE(LoadEnvironment(env,\n",
                  "                      StartExecutionCallback{},\n",
                  "                      {}));\n",
                  "}\n",
                  "\n",
                  "MaybeLocal<Value> LoadEnvironment(\n",
                  "    Environment* env,\n",
                  "    StartExecutionCallback cb,\n",
                  "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
                  "  env->InitializeLibuv();\n",
                  "  env->InitializeDiagnostics();\n",
                  "\n",
                  "  return StartExecution(env, cb);\n",
                  "}\n",
                  "\n",
                  "MaybeLocal<Value> LoadEnvironment(\n",
                  "    Environment* env,\n",
                  "    const char* main_script_source_utf8,\n",
                  "    std::unique_ptr<InspectorParentHandle> removeme) {\n",
                  "  CHECK_NOT_NULL(main_script_source_utf8);\n",
                  "  return LoadEnvironment(\n",
                  "      env,\n",
                  "      [&](const StartExecutionCallbackInfo& info) -> MaybeLocal<Value> {\n",
                  "        // This is a slightly hacky way to convert UTF-8 to UTF-16.\n",
                  "        Local<String> str =\n",
                  "            String::NewFromUtf8(env->isolate(),\n",
                  "                                main_script_source_utf8).ToLocalChecked();\n",
                  "        auto main_utf16 = std::make_unique<String::Value>(env->isolate(), str);\n",
                  "\n",
                  "        // TODO(addaleax): Avoid having a global table for all scripts.\n",
                  "        std::string name = \"embedder_main_\" + std::to_string(env->thread_id());\n",
                  "        native_module::NativeModuleEnv::Add(\n",
                  "            name.c_str(),\n",
                  "            UnionBytes(**main_utf16, main_utf16->length()));\n",
                  "        env->set_main_utf16(std::move(main_utf16));\n",
                  "        std::vector<Local<String>> params = {\n",
                  "            env->process_string(),\n",
                  "            env->require_string()};\n",
                  "        std::vector<Local<Value>> args = {\n",
                  "            env->process_object(),\n",
                  "            env->native_module_require()};\n",
                  "        return ExecuteBootstrapper(env, name.c_str(), &params, &args);\n",
                  "      });\n",
                  "}\n",
                  "\n",
                  "Environment* GetCurrentEnvironment(Local<Context> context) {\n",
                  "  return Environment::GetCurrent(context);\n",
                  "}\n",
                  "\n",
                  "MultiIsolatePlatform* GetMainThreadMultiIsolatePlatform() {\n",
                  "  return per_process::v8_platform.Platform();\n",
                  "}\n",
                  "\n",
                  "MultiIsolatePlatform* GetMultiIsolatePlatform(Environment* env) {\n",
                  "  return GetMultiIsolatePlatform(env->isolate_data());\n",
                  "}\n",
                  "\n",
                  "MultiIsolatePlatform* GetMultiIsolatePlatform(IsolateData* env) {\n",
                  "  return env->platform();\n",
                  "}\n",
                  "\n",
                  "MultiIsolatePlatform* CreatePlatform(\n",
                  "    int thread_pool_size,\n",
                  "    node::tracing::TracingController* tracing_controller) {\n",
                  "  return CreatePlatform(\n",
                  "      thread_pool_size,\n",
                  "      static_cast<v8::TracingController*>(tracing_controller));\n",
                  "}\n",
                  "\n",
                  "MultiIsolatePlatform* CreatePlatform(\n",
                  "    int thread_pool_size,\n",
                  "    v8::TracingController* tracing_controller) {\n",
                  "  return MultiIsolatePlatform::Create(thread_pool_size, tracing_controller)\n",
                  "      .release();\n",
                  "}\n",
                  "\n",
                  "void FreePlatform(MultiIsolatePlatform* platform) {\n",
                  "  delete platform;\n",
                  "}\n",
                  "\n",
                  "std::unique_ptr<MultiIsolatePlatform> MultiIsolatePlatform::Create(\n",
                  "    int thread_pool_size,\n",
                  "    v8::TracingController* tracing_controller) {\n",
                  "  return std::make_unique<NodePlatform>(thread_pool_size, tracing_controller);\n",
                  "}\n",
                  "\n",
                  "MaybeLocal<Object> GetPerContextExports(Local<Context> context) {\n",
                  "  Isolate* isolate = context->GetIsolate();\n",
                  "  EscapableHandleScope handle_scope(isolate);\n",
                  "\n",
                  "  Local<Object> global = context->Global();\n",
                  "  Local<Private> key = Private::ForApi(isolate,\n",
                  "      FIXED_ONE_BYTE_STRING(isolate, \"node:per_context_binding_exports\"));\n",
                  "\n",
                  "  Local<Value> existing_value;\n",
                  "  if (!global->GetPrivate(context, key).ToLocal(&existing_value))\n",
                  "    return MaybeLocal<Object>();\n",
                  "  if (existing_value->IsObject())\n",
                  "    return handle_scope.Escape(existing_value.As<Object>());\n",
                  "\n",
                  "  Local<Object> exports = Object::New(isolate);\n",
                  "  if (context->Global()->SetPrivate(context, key, exports).IsNothing() ||\n",
                  "      !InitializePrimordials(context))\n",
                  "    return MaybeLocal<Object>();\n",
                  "  return handle_scope.Escape(exports);\n",
                  "}\n",
                  "\n",
                  "// Any initialization logic should be performed in\n",
                  "// InitializeContext, because embedders don't necessarily\n",
                  "// call NewContext and so they will experience breakages.\n",
                  "Local<Context> NewContext(Isolate* isolate,\n",
                  "                          Local<ObjectTemplate> object_template) {\n",
                  "  auto context = Context::New(isolate, nullptr, object_template);\n",
                  "  if (context.IsEmpty()) return context;\n",
                  "\n",
                  "  if (!InitializeContext(context)) {\n",
                  "    return Local<Context>();\n",
                  "  }\n",
                  "\n",
                  "  return context;\n",
                  "}\n",
                  "\n",
                  "void ProtoThrower(const FunctionCallbackInfo<Value>& info) {\n",
                  "  THROW_ERR_PROTO_ACCESS(info.GetIsolate());\n",
                  "}\n",
                  "\n",
                  "// This runs at runtime, regardless of whether the context\n",
                  "// is created from a snapshot.\n",
                  "void InitializeContextRuntime(Local<Context> context) {\n",
                  "  Isolate* isolate = context->GetIsolate();\n",
                  "  HandleScope handle_scope(isolate);\n",
                  "\n",
                  "  // Delete `Intl.v8BreakIterator`\n",
                  "  // https://github.com/nodejs/node/issues/14909\n",
                  "  Local<String> intl_string = FIXED_ONE_BYTE_STRING(isolate, \"Intl\");\n",
                  "  Local<String> break_iter_string =\n",
                  "    FIXED_ONE_BYTE_STRING(isolate, \"v8BreakIterator\");\n",
                  "  Local<Value> intl_v;\n",
                  "  if (context->Global()->Get(context, intl_string).ToLocal(&intl_v) &&\n",
                  "      intl_v->IsObject()) {\n",
                  "    Local<Object> intl = intl_v.As<Object>();\n",
                  "    intl->Delete(context, break_iter_string).Check();\n",
                  "  }\n",
                  "\n",
                  "  // Delete `Atomics.wake`\n",
                  "  // https://github.com/nodejs/node/issues/21219\n",
                  "  Local<String> atomics_string = FIXED_ONE_BYTE_STRING(isolate, \"Atomics\");\n",
                  "  Local<String> wake_string = FIXED_ONE_BYTE_STRING(isolate, \"wake\");\n",
                  "  Local<Value> atomics_v;\n",
                  "  if (context->Global()->Get(context, atomics_string).ToLocal(&atomics_v) &&\n",
                  "      atomics_v->IsObject()) {\n",
                  "    Local<Object> atomics = atomics_v.As<Object>();\n",
                  "    atomics->Delete(context, wake_string).Check();\n",
                  "  }\n",
                  "\n",
                  "  // Remove __proto__\n",
                  "  // https://github.com/nodejs/node/issues/31951\n",
                  "  Local<String> object_string = FIXED_ONE_BYTE_STRING(isolate, \"Object\");\n",
                  "  Local<String> prototype_string = FIXED_ONE_BYTE_STRING(isolate, \"prototype\");\n",
                  "  Local<Object> prototype = context->Global()\n",
                  "                                ->Get(context, object_string)\n",
                  "                                .ToLocalChecked()\n",
                  "                                .As<Object>()\n",
                  "                                ->Get(context, prototype_string)\n",
                  "                                .ToLocalChecked()\n",
                  "                                .As<Object>();\n",
                  "  Local<String> proto_string = FIXED_ONE_BYTE_STRING(isolate, \"__proto__\");\n",
                  "  if (per_process::cli_options->disable_proto == \"delete\") {\n",
                  "    prototype->Delete(context, proto_string).ToChecked();\n",
                  "  } else if (per_process::cli_options->disable_proto == \"throw\") {\n",
                  "    Local<Value> thrower =\n",
                  "        Function::New(context, ProtoThrower).ToLocalChecked();\n",
                  "    PropertyDescriptor descriptor(thrower, thrower);\n",
                  "    descriptor.set_enumerable(false);\n",
                  "    descriptor.set_configurable(true);\n",
                  "    prototype->DefineProperty(context, proto_string, descriptor).ToChecked();\n",
                  "  } else if (per_process::cli_options->disable_proto != \"\") {\n",
                  "    // Validated in ProcessGlobalArgs\n",
                  "    FatalError(\"InitializeContextRuntime()\", \"invalid --disable-proto mode\");\n",
                  "  }\n",
                  "}\n",
                  "\n",
                  "bool InitializeContextForSnapshot(Local<Context> context) {\n",
                  "  Isolate* isolate = context->GetIsolate();\n",
                  "  HandleScope handle_scope(isolate);\n",
                  "\n",
                  "  context->SetEmbedderData(ContextEmbedderIndex::kAllowWasmCodeGeneration,\n",
                  "                           True(isolate));\n",
                  "  return InitializePrimordials(context);\n",
                  "}\n",
                  "\n",
                  "bool InitializePrimordials(Local<Context> context) {\n",
                  "  // Run per-context JS files.\n",
                  "  Isolate* isolate = context->GetIsolate();\n",
                  "  Context::Scope context_scope(context);\n",
                  "  Local<Object> exports;\n",
                  "\n",
                  "  Local<String> primordials_string =\n",
                  "      FIXED_ONE_BYTE_STRING(isolate, \"primordials\");\n",
                  "  Local<String> global_string = FIXED_ONE_BYTE_STRING(isolate, \"global\");\n",
                  "  Local<String> exports_string = FIXED_ONE_BYTE_STRING(isolate, \"exports\");\n",
                  "\n",
                  "  // Create primordials first and make it available to per-context scripts.\n",
                  "  Local<Object> primordials = Object::New(isolate);\n",
                  "  if (!primordials->SetPrototype(context, Null(isolate)).FromJust() ||\n",
                  "      !GetPerContextExports(context).ToLocal(&exports) ||\n",
                  "      !exports->Set(context, primordials_string, primordials).FromJust()) {\n",
                  "    return false;\n",
                  "  }\n",
                  "\n",
                  "  static const char* context_files[] = {\"internal/per_context/primordials\",\n",
                  "                                        \"internal/per_context/domexception\",\n",
                  "                                        \"internal/per_context/messageport\",\n",
                  "                                        nullptr};\n",
                  "\n",
                  "  for (const char** module = context_files; *module != nullptr; module++) {\n",
                  "    std::vector<Local<String>> parameters = {\n",
                  "        global_string, exports_string, primordials_string};\n",
                  "    Local<Value> arguments[] = {context->Global(), exports, primordials};\n",
                  "    MaybeLocal<Function> maybe_fn =\n",
                  "        native_module::NativeModuleEnv::LookupAndCompile(\n",
                  "            context, *module, &parameters, nullptr);\n",
                  "    Local<Function> fn;\n",
                  "    if (!maybe_fn.ToLocal(&fn)) {\n",
                  "      return false;\n",
                  "    }\n",
                  "    MaybeLocal<Value> result =\n",
                  "        fn->Call(context, Undefined(isolate), arraysize(arguments), arguments);\n",
                  "    // Execution failed during context creation.\n",
                  "    // TODO(joyeecheung): deprecate this signature and return a MaybeLocal.\n",
                  "    if (result.IsEmpty()) {\n",
                  "      return false;\n",
                  "    }\n",
                  "  }\n",
                  "\n",
                  "  return true;\n",
                  "}\n",
                  "\n",
                  "bool InitializeContext(Local<Context> context) {\n",
                  "  if (!InitializeContextForSnapshot(context)) {\n",
                  "    return false;\n",
                  "  }\n",
                  "\n",
                  "  InitializeContextRuntime(context);\n",
                  "  return true;\n",
                  "}\n",
                  "\n",
                  "uv_loop_t* GetCurrentEventLoop(Isolate* isolate) {\n",
                  "  HandleScope handle_scope(isolate);\n",
                  "  Local<Context> context = isolate->GetCurrentContext();\n",
                  "  if (context.IsEmpty()) return nullptr;\n",
                  "  Environment* env = Environment::GetCurrent(context);\n",
                  "  if (env == nullptr) return nullptr;\n",
                  "  return env->event_loop();\n",
                  "}\n",
                  "\n",
                  "void AddLinkedBinding(Environment* env, const node_module& mod) {\n",
                  "  CHECK_NOT_NULL(env);\n",
                  "  Mutex::ScopedLock lock(env->extra_linked_bindings_mutex());\n",
                  "\n",
                  "  node_module* prev_head = env->extra_linked_bindings_head();\n",
                  "  env->extra_linked_bindings()->push_back(mod);\n",
                  "  if (prev_head != nullptr)\n",
                  "    prev_head->nm_link = &env->extra_linked_bindings()->back();\n",
                  "}\n",
                  "\n",
                  "void AddLinkedBinding(Environment* env,\n",
                  "                      const char* name,\n",
                  "                      addon_context_register_func fn,\n",
                  "                      void* priv) {\n",
                  "  node_module mod = {\n",
                  "    NODE_MODULE_VERSION,\n",
                  "    NM_F_LINKED,\n",
                  "    nullptr,  // nm_dso_handle\n",
                  "    nullptr,  // nm_filename\n",
                  "    nullptr,  // nm_register_func\n",
                  "    fn,\n",
                  "    name,\n",
                  "    priv,\n",
                  "    nullptr   // nm_link\n",
                  "  };\n",
                  "  AddLinkedBinding(env, mod);\n",
                  "}\n",
                  "\n",
                  "static std::atomic<uint64_t> next_thread_id{0};\n",
                  "\n",
                  "ThreadId AllocateEnvironmentThreadId() {\n",
                  "  return ThreadId { next_thread_id++ };\n",
                  "}\n",
                  "\n",
                  "void DefaultProcessExitHandler(Environment* env, int exit_code) {\n",
                  "  env->set_can_call_into_js(false);\n",
                  "  env->stop_sub_worker_contexts();\n",
                  "  DisposePlatform();\n",
                  "  exit(exit_code);\n",
                  "}\n",
                  "\n",
                  "\n",
                  "void SetProcessExitHandler(Environment* env,\n",
                  "                           std::function<void(Environment*, int)>&& handler) {\n",
                  "  env->set_process_exit_handler(std::move(handler));\n",
                  "}\n",
                  "\n",
                  "}  // namespace node\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "print(enc.decode(content_arr))"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "ds",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.12"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
