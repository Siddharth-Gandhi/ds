{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36960f25-25ee-42b1-bb41-939e55249fcc",
   "metadata": {},
   "source": [
    "Goal is, given an input ranking of AggregatedCodeResult\n",
    "\n",
    "# Changes\n",
    "1. Eval code files must be split on the same function as the original train splits (need a Split Class)\n",
    "2. So your eval list will contain code snippets instead of files\n",
    "\n",
    "## before \n",
    "actual = set(f2,f4,f8)\n",
    "predictions = [f1, f4, f2, f7, f8]\n",
    "\n",
    "\n",
    "## now\n",
    "actual = set(p2.1, p2.2, p2.3, p4.1, p4.2, p8.1)\n",
    "precictions = [p1.1, p7.8, p8.3, p4.1, p2.3, ....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efd078e-00c2-4bee-9815-e215d1b31e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3ab404-f767-4d5f-b281-9ea6fa962c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from typing import List\n",
    "from utils import AggregatedSearchResult, get_combined_df, full_tokenize\n",
    "from bm25_v2 import BM25Searcher\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from BERTReranker_v4 import BERTReranker\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "# from CodeReranker import BERTCodeReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42b53ca-5345-46f4-9e69-c3400b474582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729c1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEvaluator:\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, k):\n",
    "        return sum(relevant[:k]) / k\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(relevant):\n",
    "        for idx, value in enumerate(relevant):\n",
    "            if value == 1:\n",
    "                return 1 / (idx + 1)\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_average_precision(relevant):\n",
    "        pred_rel = [1] * len(relevant)\n",
    "        relevant_documents_count = 0\n",
    "        cumulative_precision = 0.0\n",
    "\n",
    "        # We iterate through the predicted relevance scores\n",
    "        for i in range(len(pred_rel)):\n",
    "            # Check if the prediction at this rank is correct (i.e., if it is a relevant document)\n",
    "            if pred_rel[i] == 1 and relevant[i] == 1:\n",
    "                relevant_documents_count += 1\n",
    "                precision_at_i = relevant_documents_count / (i + 1)\n",
    "                cumulative_precision += precision_at_i\n",
    "\n",
    "        # The average precision is the cumulative precision divided by the number of relevant documents\n",
    "        average_precision = cumulative_precision / sum(relevant) if sum(relevant) > 0 else 0\n",
    "        return average_precision\n",
    "\n",
    "    # @staticmethod\n",
    "    # def calculate_recall(relevant, total_modified_files, k):\n",
    "    #   # Does not work for commit based approach as it can have multiple mentions of the same file across commits leading to a higher than 1 recall\n",
    "    #     print(total_modified_files)\n",
    "    #     print(relevant)\n",
    "    #     return sum(relevant[:k]) / total_modified_files\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_recall(retrieved_files, actual_modified, relevant, k):\n",
    "        # this complicated mess is required as compared to the above much simpler code to support both commit-based and file-based approaches\n",
    "        # in file-based approach, this is equivalent to the above code\n",
    "        # in code-based approach, duplicates could be present in retrieved_files, which is why we need to filter them out (the above code would not work in this case)\n",
    "\n",
    "        return len({file for idx, file in enumerate(retrieved_files[:k])\n",
    "                        if relevant[idx] == 1\n",
    "                    }) / len(actual_modified) if len(actual_modified) > 0 else 0\n",
    "\n",
    "\n",
    "    def evaluate(self, search_results, actual_modified, eval_type='file'):\n",
    "        if eval_type == 'patch':\n",
    "            retrieved = [result.passage for result in search_results]\n",
    "        else:\n",
    "            retrieved = [result.file_path for result in search_results]\n",
    "        relevant = [1 if file in actual_modified else 0 for file in retrieved]\n",
    "\n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric == 'MAP':\n",
    "                evaluations[metric] = self.calculate_average_precision(relevant)\n",
    "            elif metric == 'MRR':\n",
    "                evaluations[metric] = self.mean_reciprocal_rank(relevant)\n",
    "            elif metric.startswith('P@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.precision_at_k(relevant, k)\n",
    "            elif metric.startswith('R@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.calculate_recall(retrieved, actual_modified, relevant, k)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90325ff-e479-4411-9d69-a01c671a14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, eval_model, combined_df, seed=42):\n",
    "        self.model = model\n",
    "        self.eval_model = eval_model\n",
    "        self.combined_df = combined_df\n",
    "        self.seed = seed\n",
    "\n",
    "    def sample_commits(self, n):\n",
    "        if self.combined_df.commit_id.nunique() < n:\n",
    "            raise ValueError(f'Not enough commits to sample. Required: {n}, available: {self.combined_df.commit_id.nunique()}')\n",
    "\n",
    "        midpoint_date = np.median(self.combined_df['commit_date'])\n",
    "        recent_df = self.combined_df[self.combined_df['commit_date'] > midpoint_date]\n",
    "\n",
    "        return recent_df.drop_duplicates(subset='commit_id').sample(n=n, replace=False, random_state=self.seed)\n",
    "\n",
    "    def evaluate_df(self, df, k=1000, aggregation_strategy=None, rerankers=None):\n",
    "        results = []\n",
    "        for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            cur_query = row['commit_message']\n",
    "            search_results = self.model.pipeline(cur_query, row['commit_date'], ranking_depth=k, aggregation_method=aggregation_strategy)\n",
    "            for reranker in rerankers:\n",
    "                if reranker.__class__.__name__ == 'BERTCodeReranker':\n",
    "                    search_results = reranker.rerank_pipeline(cur_query, search_results, row['commit_id'])\n",
    "                else:\n",
    "                    search_results = reranker.rerank_pipeline(cur_query, search_results)\n",
    "\n",
    "\n",
    "            if 'actual_modified_files' in df.columns:\n",
    "                actual_modified_files = row['actual_modified_files']\n",
    "            else:\n",
    "                actual_modified_files = self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist()\n",
    "            evaluation = self.eval_model.evaluate(search_results, actual_modified_files)\n",
    "            results.append(evaluation)\n",
    "        return results\n",
    "\n",
    "    def evaluate_sampling(self, n=100, k=1000, output_file_path=None, overwrite_eval=False, aggregation_strategy=None, rerankers=None, gold_df=None): #, repo_path=None):\n",
    "        # if repo_path is None:\n",
    "        #     print(\"Repo path not provided, using current working directory\")\n",
    "            # repo_path = os.getcwd()\n",
    "        if rerankers is None:\n",
    "            rerankers = []\n",
    "\n",
    "        if output_file_path is None:\n",
    "            print(\"WARNING: Output file path not provided, not writing results to file\")\n",
    "            # output_file_path = os.path.join(repo_path, f'{self.model.__class__.__name__}_results.txt')\n",
    "\n",
    "        # output_file_path = os.path.join(repo_path, output_file)\n",
    "        model_name = self.model.__class__.__name__\n",
    "\n",
    "        if not overwrite_eval and output_file_path and os.path.exists(output_file_path):\n",
    "            print(f'Output file {output_file_path} already exists - not writing to file, set overwrite_eval flag to True for that...')\n",
    "            # print the contents of the file\n",
    "            # with open(output_file_path, \"r\") as file:\n",
    "            #     print(file.read())\n",
    "            # return\n",
    "            output_file_path=None\n",
    "\n",
    "        if gold_df is None:\n",
    "            sampled_commits = self.sample_commits(n)\n",
    "            results = self.evaluate_df(sampled_commits, k, aggregation_strategy, rerankers)\n",
    "        else:\n",
    "            print(f'Found gold_df, evaluating on {len(gold_df)} commits')\n",
    "            print(gold_df.info())\n",
    "            results = self.evaluate_df(gold_df, k, aggregation_strategy, rerankers)\n",
    "\n",
    "        avg_scores = {metric: round(np.mean([result[metric] for result in results]), 4) for metric in results[0]}\n",
    "\n",
    "        if output_file_path is not None:\n",
    "            with open(output_file_path, \"w\") as file:\n",
    "                file.write(f\"Model Name: {model_name}\\n\")\n",
    "                # write name of each reranker\n",
    "                if len(rerankers) > 0:\n",
    "                    file.write(\"Rerankers:\\n\")\n",
    "                    for reranker in rerankers:\n",
    "                        reranker_model_name = reranker.model.config.name_or_path\n",
    "                        # replace / with _\n",
    "                        reranker_model_name = reranker_model_name.replace('/', '_')\n",
    "                        file.write(f\"{reranker.__class__.__name__} ({reranker_model_name}) @ {reranker.rerank_depth}\\n\")\n",
    "\n",
    "\n",
    "                file.write(f\"Sample Size: {n}\\n\")\n",
    "                file.write(\"Evaluation Metrics:\\n\")\n",
    "                for key, value in avg_scores.items():\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "            print(f'Evaluation results written to {output_file_path}')\n",
    "\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e11d01-ec22-4a7b-9ec7-e26a207b44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        print(f'Using device: {self.device}')\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f540cf18-e95f-4e03-9b16-ef540cd431a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCodeReranker:\n",
    "    def __init__(self, parameters, combined_df):\n",
    "        self.combined_df = combined_df\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        print(f\"Initialized Code File BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult], train_commit_id):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        query_passage_pairs, per_result_contribution = self.split_into_query_passage_pairs(query, aggregated_results, train_commit_id)\n",
    "\n",
    "\n",
    "        # for agg_result in aggregated_results:\n",
    "        #     query_passage_pairs.extend(\n",
    "        #         (query, result.commit_message)\n",
    "        #         for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "        #     )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for i, agg_result in enumerate(aggregated_results):\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + per_result_contribution[i] # only use psg_cnt contributing_results\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def split_into_query_passage_pairs(self, query, aggregated_results, train_commit_id):\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        def full_tokenize(s):\n",
    "            return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        query_passage_pairs = []\n",
    "        per_result_contribution = []\n",
    "        if self.combined_df is not None:\n",
    "            combined_df = self.combined_df\n",
    "\n",
    "\n",
    "        for agg_result in aggregated_results:\n",
    "            # agg_result.contributing_results.sort(key=lambda res: res.commit_date, reverse=True)\n",
    "            # get most recent file version\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            # get the file_path and commit_id\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            # commit_id = most_recent_search_result.commit_id\n",
    "            # get the file content from combined_df\n",
    "            # file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['cur_file_content'].values[0]\n",
    "\n",
    "            file_content = get_file_at_commit_from_git(file_path, train_commit_id)\n",
    "\n",
    "            # file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['previous_file_content'].values[0]\n",
    "\n",
    "            # now need to split this file content into psg_cnt passages\n",
    "            # first tokenize the file content\n",
    "\n",
    "            # warning these asserts are useless since we are using NaNs\n",
    "            # assert file_content is not None, f'file_content is None for commit_id: {commit_id}, file_path: {file_path}'\n",
    "            # assert file_path is not None, f'file_path is None for commit_id: {commit_id}'\n",
    "            assert query is not None, 'query is None'\n",
    "\n",
    "            # query_tokens = full_tokenize(query)\n",
    "            path_tokens = full_tokenize(file_path)\n",
    "\n",
    "            if pd.isna(file_content):\n",
    "                # if file_content is NaN, then we can just set file_content to empty string\n",
    "                print(f'WARNING: file_content is NaN for commit_id: {train}, file_path: {file_path}, setting file_content to empty string')\n",
    "                file_content = ''\n",
    "\n",
    "            file_tokens = full_tokenize(file_content)\n",
    "\n",
    "\n",
    "            # now split the file content into psg_cnt passages\n",
    "            cur_result_passages = []\n",
    "            # get the input ids\n",
    "            # input_ids = file_content['input_ids'].squeeze()\n",
    "            # get the number of tokens in the file content\n",
    "            total_tokens = len(file_tokens)\n",
    "\n",
    "            for cur_start in range(0, total_tokens, self.psg_stride):\n",
    "                cur_passage = []\n",
    "                # add query tokens and path tokens\n",
    "                # cur_passage.extend(query_tokens)\n",
    "                cur_passage.extend(path_tokens)\n",
    "\n",
    "                # add the file tokens\n",
    "                cur_passage.extend(file_tokens[cur_start:cur_start+self.psg_len])\n",
    "\n",
    "                # now convert cur_passage into a string\n",
    "                cur_passage_decoded = self.tokenizer.decode(cur_passage)\n",
    "\n",
    "                # add the cur_passage to cur_result_passages\n",
    "                cur_result_passages.append(cur_passage_decoded)\n",
    "\n",
    "                # if len(cur_result_passages) == self.psg_cnt:\n",
    "                #     break\n",
    "\n",
    "            # now add the query, passage pairs to query_passage_pairs\n",
    "            per_result_contribution.append(len(cur_result_passages))\n",
    "            query_passage_pairs.extend((query, passage) for passage in cur_result_passages)\n",
    "        return query_passage_pairs, per_result_contribution\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results, train_commit_id):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results, train_commit_id)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9adda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd96bd1-0bac-46b6-a8ad-85c746483efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f5a761-be74-4ffb-8b2f-585449866911",
   "metadata": {},
   "source": [
    "# Loading some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6b218a-37ae-4b3d-ba55-903afe13188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../data/2_7/facebook_react/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='../data/2_7/facebook_react/index_commit_tokenized',\n",
    "    repo_path='../data/2_7/facebook_react', k=1000, n=100,\n",
    "    model_path='microsoft/codebert-base', overwrite_cache=False,\n",
    "    batch_size=32, num_epochs=10, learning_rate=5e-05,\n",
    "    run_name='debug',\n",
    "    notes='debug (ignore)',\n",
    "    num_positives=10, num_negatives=10, train_depth=1000, num_workers=8,\n",
    "    train_commits=1000, psg_cnt=25, use_gpu=True,\n",
    "    rerank_depth=100, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4',\n",
    "    overwrite_eval=False, sanity_check=True, debug=False,\n",
    "    psg_len=350, psg_stride=250, ignore_gold_in_training=False,\n",
    "    eval_folder='repr_0.1663', use_gpt_train=True,\n",
    "    aggregation_strategy='sump',\n",
    "    bert_best_model='../data/combined_commit_train/best_model',\n",
    "    best_model_path='../data/2_7/facebook_react/models/bce/best_model'\n",
    "\n",
    ")\n",
    "\n",
    "metrics =['MAP', 'P@1', 'P@10', 'P@20', 'P@30', 'MRR', 'R@1', 'R@10', 'R@100', 'R@1000']\n",
    "repo_path = args.repo_path\n",
    "repo_name = repo_path.split('/')[-1]\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'\n",
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)\n",
    "\n",
    "test_path = os.path.join('..', 'gold', 'facebook_react', 'v2_facebook_react_gpt4_gold.parquet')\n",
    "# test_path = os.path.join('gold', 'facebook_react', 'v2_facebook_react_gpt4_gold.parquet')\n",
    "gold_df = pd.read_parquet(test_path)\n",
    "gold_df = gold_df.rename(columns={'commit_message': 'original_message', f'transformed_message_{args.openai_model}': 'commit_message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d248d6ba-2f1c-4f82-8015-1edc87a4948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6cb73f8-b377-4646-bdf6-1446fec8d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../repos/facebook_react'\n",
    "repo = git.Repo(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c62e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'aggregation_strategy': 'maxp',\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': args.rerank_depth,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': BM25_AGGR_STRAT,\n",
    "        'psg_len': args.psg_len,\n",
    "        'psg_stride': args.psg_stride,\n",
    "        'psg_cnt': args.psg_cnt,\n",
    "    }\n",
    "\n",
    "bert_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': 5,\n",
    "        'aggregation_strategy': 'sump',\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': 250,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': 'sump',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feaddd01-a7a7-46ad-9b03-40ecfc6b06c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "commit_id                         323efbc33c27a602a4aab8519f58feba1e0a216c\n",
       "commit_date                                                     1512398372\n",
       "original_message         Ensure value and defaultValue do not assign fu...\n",
       "actual_files_modified    [packages/react-dom/src/__tests__/ReactDOMInpu...\n",
       "commit_message           Input properties 'value' and 'defaultValue' ac...\n",
       "Name: 6, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_train_row = gold_df.iloc[6]\n",
    "dummy_commit_id = dummy_train_row.commit_id\n",
    "dummy_train_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a573cc-5441-4a4f-9d59-dacc753acab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train_query = dummy_train_row.commit_message\n",
    "dummy_file_path_list = dummy_train_row.actual_files_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9d47a8-478a-4eab-a7e6-e3e91774456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['packages/react-dom/src/__tests__/ReactDOMInput-test.js',\n",
       "       'packages/react-dom/src/client/ReactDOMFiberInput.js',\n",
       "       'packages/react-dom/src/events/ChangeEventPlugin.js',\n",
       "       'packages/react-dom/src/shared/DOMProperty.js'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40496248-feae-4349-abd8-964005203772",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "def split_random_chunks(file, tokenizer, stride=250, psg_len=350):\n",
    "    file_tokens = full_tokenize(file, tokenizer)\n",
    "    total_tokens = len(file_tokens)\n",
    "    res = []\n",
    "    for cur_start in range(0, total_tokens, stride):\n",
    "        # get tokens for current passage\n",
    "        res.append(tokenizer.decode(file_tokens[cur_start:cur_start+psg_len]))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e965a22-67e5-4537-ae0c-ff65f58872bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_file = combined_df.query(f\"commit_id=='{dummy_commit_id}' & file_path=='{dummy_file_path_list[0]}'\")['previous_file_content'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "880da63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21407 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dummy_file_list = [combined_df.query(f\"commit_id=='{dummy_commit_id}' & file_path=='{x}'\")['previous_file_content'].values[0] for x in dummy_file_path_list]\n",
    "dummy_file_patch_list = [chunk for x in dummy_file_list for chunk in split_random_chunks(x, tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bffe1a22-73a2-4d80-901f-a0df4ccdc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_diff_list = [combined_df.query(f\"commit_id=='{dummy_commit_id}' & file_path=='{x}'\")['diff'].values[0] for x in dummy_file_path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "167ab9fd-faf2-44bd-bf4e-d06bfebe611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_diff = dummy_diff_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b7629-b66f-4560-9604-0e33a4273115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5af688f4-ca46-4037-ab42-3fb5dd681987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_modified_lines(diff):\n",
    "    \"\"\"\n",
    "    Extracts line numbers of modified lines from a diff string.\n",
    "\n",
    "    Args:\n",
    "    - diff (str): The diff string in Linux diff format.\n",
    "\n",
    "    Returns:\n",
    "    - List[int]: A list of line numbers that were modified in the previous file state.\n",
    "    \"\"\"\n",
    "    modified_lines = []\n",
    "\n",
    "    # Regular expression to find all instances of line number indicators in the diff\n",
    "    line_indicator_regex = re.compile(r'@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@')\n",
    "\n",
    "    for match in line_indicator_regex.finditer(diff):\n",
    "        start_line = int(match.group(1))\n",
    "        line_count = int(match.group(2))\n",
    "\n",
    "        # Adding all affected lines by this change to the list\n",
    "        # for i in range(line_count):\n",
    "            # modified_lines.append(start_line + i)\n",
    "\n",
    "        modified_lines.append((start_line, line_count))\n",
    "\n",
    "    return modified_lines\n",
    "    \n",
    "class LineSpanPassage:\n",
    "    def __init__(self, si, ei, passage):\n",
    "        self.start_index = si\n",
    "        self.end_index = ei\n",
    "        self.passage = passage\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'LineSpanPassage(start_index={self.start_index}, end_index={self.end_index}, passage=\\n{self.passage})'\n",
    "\n",
    "def pluck_diff_sections(file_content, modified_lines, context_lines=0):\n",
    "    \"\"\"\n",
    "    Extracts specific sections from the original file content based on modified lines\n",
    "    and a specified number of context lines before and after each section.\n",
    "\n",
    "    Args:\n",
    "    - file_content (str): The content of the original file.\n",
    "    - modified_lines (List[Tuple[int, int]]): A list of tuples, each representing\n",
    "      the starting line number and the count of consecutive modified lines.\n",
    "    - context_lines (int): The number of extra lines to include before and after\n",
    "      each section of modified lines.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted sections joined together by newlines.\n",
    "    \"\"\"\n",
    "    lines = file_content.split('\\n')\n",
    "    passages = []\n",
    "\n",
    "    for start_line, line_count in modified_lines:\n",
    "        # Adjusting the start line to be zero-indexed and accounting for context lines\n",
    "        start_index = max(0, start_line - 1 - context_lines)\n",
    "        # Ensuring we don't go beyond the file's length\n",
    "        end_index = min(len(lines), start_line - 1 + line_count + context_lines)\n",
    "\n",
    "        # Extracting the specified section with context lines\n",
    "        section = lines[start_index:end_index]\n",
    "        lsp = LineSpanPassage(start_index, end_index, '\\n'.join(section))\n",
    "        passages.append(lsp)\n",
    "\n",
    "    # Joining all passages with two newlines for clarity\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74e329fd-7f29-4bc9-a117-d8634b968e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LineSpanPassage(start_index=242, end_index=258, passage=\n",
      "      expect(console.error.calls.count()).toBe(1);\n",
      "      expect(console.error.calls.argsFor(0)[0]).toContain(\n",
      "        'You provided a `value` prop to a form field ' +\n",
      "          'without an `onChange` handler.',\n",
      "      );\n",
      "    }\n",
      "  });\n",
      "\n",
      "  it('distinguishes precision for extra zeroes in string number values', () => {\n",
      "    spyOnDev(console, 'error');\n",
      "    class Stub extends React.Component {\n",
      "      state = {\n",
      "        value: '3.0000',\n",
      "      };\n",
      "      render() {\n",
      "        return <input type=\"number\" value={this.state.value} />;)\n"
     ]
    }
   ],
   "source": [
    "ml = extract_modified_lines(sample_diff)\n",
    "print(pluck_diff_sections(dummy_file_list[0], modified_lines=ml, context_lines=5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3ce79-cbe8-4d30-89c6-faee21f0f7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab25d4-fa4b-448a-84d6-f574952ac24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9318ded-4211-4195-a3f2-d76d8f1ac281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a468045-16d7-4a73-8e83-7a9a692d2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_file_at_commit_from_git(file, commit_id):\n",
    "#     # Access the specified commit\n",
    "#     commit = repo.commit(commit_id)\n",
    "\n",
    "#     # Check if the commit has parents\n",
    "#     if commit.parents:\n",
    "#         # Access the first parent of the commit\n",
    "#         parent_commit = commit.parents[0]\n",
    "\n",
    "#         # Attempt to get the file content from the parent commit\n",
    "#         try:\n",
    "#             blob = parent_commit.tree / file\n",
    "#             file_content = blob.data_stream.read().decode('utf-8')\n",
    "#             return file_content\n",
    "#         except KeyError:\n",
    "#             # Handle the case where the file does not exist in the parent commit\n",
    "#             return \"The file was not present in the parent commit.\"\n",
    "#     else:\n",
    "#         # Handle the case where the specified commit is the initial commit and has no parents\n",
    "#         return \"The specified commit has no parents (it might be the initial commit).\"\n",
    "\n",
    "\n",
    "from git import Repo, exc\n",
    "\n",
    "def get_file_at_commit_from_git(file_path, commit_id):\n",
    "    # Access the specified commit\n",
    "    try:\n",
    "        # Initialize the Repo object        \n",
    "        # The '^' symbol is used to refer to the commit immediately before the specified commit_id\n",
    "        # Concatenate the commit_id with '^' and the file_path with a ':' separator\n",
    "        file_content_before_commit = repo.git.show(f\"{commit_id}^:{file_path}\")\n",
    "        \n",
    "        return file_content_before_commit\n",
    "    except exc.GitCommandError:\n",
    "        # Return an empty string if the file does not exist in the commit\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74bc9979-b194-487e-a936-e0eee9b49863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file_at_commit_from_git('packages/react-dom/src/__tests__/ReactDOMInput-test.js', '59763bf7f3ab3b06cd8ab5a5a83ae3dafc667aa9') == tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39318b4e-33fb-4d73-aabd-7ae541378db8",
   "metadata": {},
   "source": [
    "# Getting BM25 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a79f6388-8f94-4404-8cab-df72ba68212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results = bm25_searcher.pipeline(dummy_train_query, dummy_train_row['commit_date'], ranking_depth=K, aggregation_method=BM25_AGGR_STRAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "679ed57a-da98-425e-bcd7-977ffd108605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0867,\n",
       " 'P@1': 0.0,\n",
       " 'P@10': 0.1,\n",
       " 'P@20': 0.05,\n",
       " 'P@30': 0.0333,\n",
       " 'MRR': 0.1429,\n",
       " 'R@1': 0.0,\n",
       " 'R@10': 0.25,\n",
       " 'R@100': 0.75,\n",
       " 'R@1000': 0.75}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(bm25_results, dummy_file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18987c-bdcb-4619-a1db-14def735a288",
   "metadata": {},
   "source": [
    "# Getting BERT Rerank @ 250 on top of BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94b30ce8-d517-45db-885a-78406f948cdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 476.73 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(bert_params)\n",
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained(args.bert_best_model, num_labels=1, problem_type='regression')\n",
    "bert_reranker.model.to(bert_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27e049fa-b497-419d-a3ad-b5bbe696d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_rerank_results = bert_reranker.rerank_pipeline(dummy_train_query, bm25_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1af019ad-6152-448e-a0d4-e18f3123ea6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.1421,\n",
       " 'P@1': 0.0,\n",
       " 'P@10': 0.0,\n",
       " 'P@20': 0.15,\n",
       " 'P@30': 0.1,\n",
       " 'MRR': 0.0833,\n",
       " 'R@1': 0.0,\n",
       " 'R@10': 0.0,\n",
       " 'R@100': 0.75,\n",
       " 'R@1000': 0.75}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(bert_rerank_results, dummy_file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e64dd4b-4f4d-46b1-bfd3-0dd5117ed295",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def aside():\n",
    "#     results = [sorted(x.contributing_results, key=lambda res: res.commit_date, reverse=True)[0] for x in bert_rerank_results]\n",
    "#     files_content = [combined_df.query(f\"commit_id=='{x.commit_id}' & file_path=='{x.file_path}'\")['cur_file_content'].values[0] for x in results]\n",
    "#     patches = [chunk for x in files_content for chunk in split_random_chunks(x, tokenizer)]\n",
    "#     print(len(set(patches).intersection(set(dummy_file_patch_list))))\n",
    "\n",
    "# aside()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f61911-82c5-4825-9c76-0d5e6df69c7d",
   "metadata": {},
   "source": [
    "# File Code Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "806c8b76-15a6-44b8-99bc-cf6b15103207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 965.97 MB\n",
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'aggregation_strategy': 'maxp', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 350, 'psg_stride': 250, 'psg_cnt': 25}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_reranker_params['aggregation_strategy'] = 'maxp'\n",
    "file_code_reranker = BERTCodeReranker(code_reranker_params, combined_df)\n",
    "cur_best_model_path = '../data/2_7/facebook_react/models/combined_diffs/best_model'\n",
    "# cur_best_model_path = '../data/2_7/facebook_react/models/X/best_model'\n",
    "\n",
    "\n",
    "file_code_reranker.model = AutoModelForSequenceClassification.from_pretrained(cur_best_model_path, num_labels=1, problem_type='regression')\n",
    "file_code_reranker.model.to(file_code_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83d11f84-94c2-45d1-a818-73cd2b9c81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2672 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "file_code_reranker_results = file_code_reranker.rerank_pipeline(dummy_train_query, bert_rerank_results, dummy_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ff390d6-313f-4f63-bf13-3b71273a6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.187,\n",
       " 'P@1': 0.0,\n",
       " 'P@10': 0.2,\n",
       " 'P@20': 0.15,\n",
       " 'P@30': 0.1,\n",
       " 'MRR': 0.1111,\n",
       " 'R@1': 0.0,\n",
       " 'R@10': 0.5,\n",
       " 'R@100': 0.75,\n",
       " 'R@1000': 0.75}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(file_code_reranker_results, dummy_file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ec2ba-e3df-4751-9764-a7596aefee22",
   "metadata": {},
   "source": [
    "After latest file version (gold df iloc 6, combined_Df, sump codereranker)\n",
    "\n",
    "\n",
    "{'MAP': 0.7,\n",
    " 'P@1': 1.0,\n",
    " 'P@10': 0.3,\n",
    " 'P@20': 0.15,\n",
    " 'P@30': 0.1,\n",
    " 'MRR': 1.0,\n",
    " 'R@1': 0.25,\n",
    " 'R@10': 0.75,\n",
    " 'R@100': 0.75,\n",
    " 'R@1000': 0.75}\n",
    "\n",
    "{'MAP': 0.5536,\n",
    " 'P@1': 1.0,\n",
    " 'P@10': 0.3,\n",
    " 'P@20': 0.15,\n",
    " 'P@30': 0.1,\n",
    " 'MRR': 1.0,\n",
    " 'R@1': 0.25,\n",
    " 'R@10': 0.75,\n",
    " 'R@100': 0.75,\n",
    " 'R@1000': 0.75}\n",
    "\n",
    "\n",
    "Pre latest version of file\n",
    "\n",
    "{'MAP': 0.3869,\n",
    " 'P@1': 0.0,\n",
    " 'P@10': 0.3,\n",
    " 'P@20': 0.15,\n",
    " 'P@30': 0.1,\n",
    " 'MRR': 0.5,\n",
    " 'R@1': 0.0,\n",
    " 'R@10': 0.75,\n",
    " 'R@100': 0.75,\n",
    " 'R@1000': 0.75}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c8c85-a9f1-4b3f-9001-a38618bf0790",
   "metadata": {},
   "source": [
    "# Patch Code Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430733ae-b851-4934-8c8f-ceaea20a3852",
   "metadata": {},
   "source": [
    "## refactoring a bunch of stuff to be nice and split focused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4e57951-7e93-4344-b882-f676cba8c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "\n",
    "class SplitStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def split(self, file_content):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79533956-b93b-4a0e-b1d6-bb680760afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSpanPassage:\n",
    "        def __init__(self, si, ei, passage):\n",
    "            self.start_line = si\n",
    "            self.end_line = ei\n",
    "            self.passage = passage\n",
    "    \n",
    "        def __repr__(self):\n",
    "            return f'LineSpanPassage(start_line={self.start_line}, end_line={self.end_line}, passage=\\n{self.passage})'\n",
    "\n",
    "class DiffSplitStrategy(SplitStrategy):\n",
    "    def __init__(self, context_lines=5):\n",
    "        super().__init__()\n",
    "        self.context_lines = context_lines\n",
    "\n",
    "    @classmethod\n",
    "    def extract_modified_lines(diff):\n",
    "        \"\"\"\n",
    "        Extracts line numbers of modified lines from a diff string.\n",
    "        Args:\n",
    "        - diff (str): The diff string in Linux diff format.\n",
    "        Returns:\n",
    "        - List[int]: A list of line numbers that were modified in the previous file state.\n",
    "        \"\"\"\n",
    "        modified_lines = []\n",
    "    \n",
    "        # Regular expression to find all instances of line number indicators in the diff\n",
    "        line_indicator_regex = re.compile(r'@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@')\n",
    "    \n",
    "        for match in line_indicator_regex.finditer(diff):\n",
    "            start_line = int(match.group(1))\n",
    "            line_count = int(match.group(2))\n",
    "    \n",
    "            # Adding all affected lines by this change to the list\n",
    "            # for i in range(line_count):\n",
    "                # modified_lines.append(start_line + i)\n",
    "    \n",
    "            modified_lines.append((start_line, line_count))\n",
    "    \n",
    "        return modified_lines\n",
    "\n",
    "    def split(self, file_content, diff, *args, **kwargs):\n",
    "        modified_lines = extract_modified_lines(diff)\n",
    "        lines = file_content.split('\\n')\n",
    "        passages = []\n",
    "        for start_line, line_count in modified_lines:\n",
    "            start_index = max(0, start_line - 1 - self.context_lines)\n",
    "            end_index = min(len(lines), start_line - 1 + line_count + self.context_lines)\n",
    "            section = lines[start_index:end_index]\n",
    "            lsp = LineSpanPassage(start_index, end_index, '\\n'.join(section))\n",
    "            passages.append(lsp)\n",
    "        return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4d1b6c1-5b16-4501-9ba2-4e098a05d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedPassage:\n",
    "        def __init__(self, st, et, passage):\n",
    "            self.start_token = st\n",
    "            self.end_token = et\n",
    "            self.passage = passage\n",
    "\n",
    "    \n",
    "        def __repr__(self):\n",
    "            return f'TokenizedPassage(start_token_index={self.start_token}, end_token_index={self.end_token}, passage=\\n{self.passage})'\n",
    "            \n",
    "class TokenizedSplitStrategy(SplitStrategy):\n",
    "\n",
    "    def __init__(self, tokenizer, psg_len, psg_stride):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.psg_len = psg_len\n",
    "        self.psg_stride = psg_stride\n",
    "\n",
    "    # def full_tokenize(self, s):\n",
    "    #         return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "    def full_tokenize(self, s):\n",
    "        tokens = self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        # Ensure tokens is always a list\n",
    "        if not isinstance(tokens, list):\n",
    "            tokens = [tokens]\n",
    "        return tokens\n",
    "\n",
    "    def split(self, file_content, *args, **kwargs):\n",
    "        # Tokenize the entire file content\n",
    "        file_tokens = self.full_tokenize(file_content)\n",
    "        total_tokens = len(file_tokens)\n",
    "        res = []\n",
    "        for cur_start in range(0, total_tokens, self.psg_stride):\n",
    "            # get tokens for current passage\n",
    "            cur_psg = tokenizer.decode(file_tokens[cur_start:cur_start+self.psg_len])\n",
    "            res.append(TokenizedPassage(cur_start, min(total_tokens-1, cur_start+self.psg_len-1), cur_psg))\n",
    "    \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d6f874f-191d-4bb5-95e1-fa67c3459889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb09b48f-991e-426c-a263-155fb61a4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassageSplitter:\n",
    "    def __init__(self, strategy: SplitStrategy):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def split_passages(self, file_content, *args, **kwargs):\n",
    "        return self.strategy.split(file_content=file_content, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d858358-ab7e-425e-a27c-2c2a2b25abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to understand the output\n",
    "# tmp = PassageSplitter(DiffSplitStrategy())\n",
    "# tmp.split_passages(file_content=dummy_file_list[0], diff=sample_diff)[0]\n",
    "\n",
    "# tmp = PassageSplitter(TokenizedSplitStrategy(tokenizer=tokenizer, psg_len=args.psg_len, psg_stride=args.psg_stride))\n",
    "\n",
    "# dummy_file_list[0].split('\\n')[:45]\n",
    "# tmp = PassageSplitter(TokenizedLineSplitStrategy(tokenizer=tokenizer, psg_len=args.psg_len, psg_stride=args.psg_stride))\n",
    "# tmp.split_passages(file_content=dummy_file_list[0], diff=sample_diff)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc0c89fc-95ff-4f60-9960-d97260fc907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchResult:\n",
    "    def __init__(self, file_path, passage, score):\n",
    "        self.file_path = file_path\n",
    "        self.score = score\n",
    "        self.passage = passage\n",
    "\n",
    "    def __repr__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        return f'{class_name}(file_path={self.file_path}, passage={self.passage}, score={self.score})'\n",
    "        \n",
    "class PatchCodeReranker(Reranker):\n",
    "    def __init__(self, parameters, split_strategy: SplitStrategy):\n",
    "        super().__init__(parameters)\n",
    "\n",
    "        # specific to CodeReranker type\n",
    "\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        print(f\"Initialized Patch Code Reranker with parameters: {parameters}\")\n",
    "\n",
    "        # Passage/patch splitting\n",
    "        self.split_strategy = split_strategy\n",
    "        self.passage_splitter = PassageSplitter(split_strategy)\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult], train_commit_id):\n",
    "        \"\"\"\n",
    "        Rerank a aggregated search result list by splitting into patches and getting model scores for each patch.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "\n",
    "        Returns:\n",
    "        - List[PatchResult]         [IMP: DOES NOT return file_list]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        query_passage_pairs = self.split_into_query_passage_pairs(query, aggregated_results, train_commit_id)\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning []')\n",
    "            return []\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, obj.passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, file_path, obj in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        # convert the scores to PatchResult objects\n",
    "        patch_results = [PatchResult(file_path, obj, score) for (query, file_path, obj), score in zip(query_passage_pairs, scores)]\n",
    "\n",
    "        # sort patch_results by the scores\n",
    "        sorted_patch_results = sorted(patch_results, key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return sorted_patch_results\n",
    "\n",
    "    def split_into_query_passage_pairs(self, query, aggregated_results, train_commit_id):        \n",
    "        query_passage_pairs = []\n",
    "        for agg_result in aggregated_results:\n",
    "            # get any file result\n",
    "            most_recent_search_result = agg_result.contributing_results[0] # doesn't matter which version we take, we only care about file_path\n",
    "            \n",
    "            # get the file_path\n",
    "            file_path = most_recent_search_result.file_path\n",
    "\n",
    "            file_content = get_file_at_commit_from_git(file_path, train_commit_id)\n",
    "            if not file_content:\n",
    "                # useless file\n",
    "                continue\n",
    "\n",
    "            # warning these asserts are useless since we are using NaNs\n",
    "            assert file_content is not None, f'file_content is None for commit_id: {train_commit_id}, file_path: {file_path}'\n",
    "            assert file_path is not None, f'file_path is None for commit_id: {train_commit_id}'\n",
    "            assert query is not None, 'query is None'\n",
    "\n",
    "            if pd.isna(file_content):\n",
    "                # if file_content is NaN, then we can just set file_content to empty string\n",
    "                print(f'WARNING: file_content is NaN for commit_id: {train_commit_id}, file_path: {file_path}, setting file_content to empty string')\n",
    "                file_content = ''\n",
    "\n",
    "            cur_result_passages = self.passage_splitter.split_passages(file_content)\n",
    "\n",
    "            query_passage_pairs.extend((query, file_path, obj) for obj in cur_result_passages)\n",
    "            \n",
    "        return query_passage_pairs\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results, train_commit_id):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        # bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results, train_commit_id)\n",
    "        # min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        # for i, result in enumerate(bottom_results):\n",
    "            # result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        # reranked_results.extend(bottom_results)\n",
    "        # assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d307c52a-8ab1-4317-926c-1f759344d471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 965.97 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Patch Code Reranker with parameters: {'model_name': 'microsoft/codebert-base', 'aggregation_strategy': 'maxp', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 350, 'psg_stride': 250, 'psg_cnt': 25}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_strategy = TokenizedLineSplitStrategy(tokenizer=tokenizer, psg_len=args.psg_len, psg_stride=args.psg_stride)\n",
    "code_reranker = PatchCodeReranker(code_reranker_params, split_strategy)\n",
    "# cur_best_model_path = 'data/2_7/facebook_react/models/X/best_model'\n",
    "cur_best_model_path = '../data/2_7/facebook_react/models/combined_diffs/best_model'\n",
    "\n",
    "code_reranker.model = AutoModelForSequenceClassification.from_pretrained(cur_best_model_path, num_labels=1, problem_type='regression')\n",
    "code_reranker.model.to(code_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57449b79-f245-4f52-82c1-490a0b29202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_reranker_results = code_reranker.rerank_pipeline(dummy_train_query, bert_rerank_results, dummy_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22113e8d-7b1e-479e-8098-1195b2210c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_results = {}\n",
    "\n",
    "# for i, row in tqdm(gold_df.iterrows(), total=gold_df.shape[0]):\n",
    "#     qcid = row['commit_id']\n",
    "#     qcdate = row['commit_date']\n",
    "#     qcmsg = row['commit_message']\n",
    "#     res = bm25_searcher.pipeline(qcmsg, qcdate, ranking_depth=K, aggregation_method=BM25_AGGR_STRAT)\n",
    "#     res = bert_reranker.rerank_pipeline(qcmsg, res)    \n",
    "#     res = code_reranker.rerank_pipeline(qcmsg, res, qcid)\n",
    "\n",
    "#     query_results[qcid] = res\n",
    "\n",
    "#     with open('query_results.pkl', 'wb') as file:\n",
    "#         pickle.dump(query_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90aa5005-4e9c-4661-afc1-02085264f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = {}\n",
    "with open('query_results.pkl', 'rb') as file:\n",
    "    query_results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "157e5c63-10a4-498b-b3e3-40c04d5288d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 142.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.143,\n",
       " 'P@1': 0.2,\n",
       " 'P@10': 0.221,\n",
       " 'P@20': 0.1995,\n",
       " 'P@30': 0.1787,\n",
       " 'MRR': 0.3194,\n",
       " 'R@1': 0.0639,\n",
       " 'R@10': 0.2758,\n",
       " 'R@100': 0.4473,\n",
       " 'R@1000': 0.4756}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_results = []\n",
    "for i, row in tqdm(gold_df.iterrows(), total=gold_df.shape[0]):\n",
    "    qcid = row['commit_id']\n",
    "    # qcdate = row['commit_date']\n",
    "    # qcmsg = row['commit_message']\n",
    "    qactual_files = row['actual_files_modified']\n",
    "    # qfile_list = [combined_df.query(f\"commit_id=='{qcid}' & file_path=='{x}'\")['previous_file_content'].values[0] for x in qactual_files]\n",
    "\n",
    "    file_results.append(evaluator.evaluate(query_results[qcid], qactual_files))\n",
    "\n",
    "avg_file_scores = {metric: round(np.mean([result[metric] for result in file_results]), 4) for metric in file_results[0]}\n",
    "avg_file_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b6457aa-476b-4744-85c4-6d1de64a5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file code reranker\n",
    "# {\n",
    "#   \"MAP.max\": 0.2378,\n",
    "#   \"P@1.max\": 0.21,\n",
    "#   \"P@10.max\": 0.103,\n",
    "#   \"P@20.max\": 0.074,\n",
    "#   \"P@30.max\": 0.0563\n",
    "#   \"MRR.max\": 0.3114,\n",
    "#   \"R@1.max\": 0.0804,\n",
    "#   \"R@10.max\": 0.2716,\n",
    "#   \"R@100.max\": 0.4756,\n",
    "#   \"R@1000.max\": 0.5452,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e0aae-6d1e-48a8-ad33-9d7d81aad5f1",
   "metadata": {},
   "source": [
    "## patch eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2322eaee-22f2-45b4-84c4-78d63d15a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_splitter = PassageSplitter(DiffSplitStrategy())\n",
    "# \n",
    "# sample_file, sample_diff = dummy_file_list[0], dummy_diff_list[0]\n",
    "# ground_truth = {}\n",
    "# for gt_file_path, gt_file_content, gt_diff in zip(dummy_file_path_list, dummy_file_list, dummy_diff_list):\n",
    "#     patch_list = diff_splitter.split_passages(file_content=gt_file_content, diff=gt_diff)\n",
    "#     ground_truth[gt_file_path] = []\n",
    "#     for lsp in patch_list:\n",
    "#         ground_truth[gt_file_path].append(PatchResult(file_path=gt_file_path, score=-1, passage=lsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "436a3912-489f-44fa-a3cd-9188a97087a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_line_overlap(predicted_passage, ground_truth_list):\n",
    "    \"\"\"\n",
    "    Calculate the overlap score between a predicted passage and a set of actual passages.\n",
    "    \n",
    "    Args:\n",
    "    - actual_passages (List[LineSpanPassage]): The actual ground truth passages.\n",
    "    - predicted_passage (LineSpanPassage): The predicted passage.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The best overlap score between the predicted passage and any of the actual passages.\n",
    "    \"\"\"\n",
    "    best_overlap_score = 0\n",
    "    actual_passages = [x.passage for x in ground_truth_list]\n",
    "    for actual in actual_passages:\n",
    "        if predicted_passage.start_line <= actual.end_line and actual.start_line <= predicted_passage.end_line:\n",
    "            # Calculate overlap\n",
    "            overlap_start = max(predicted_passage.start_line, actual.start_line)\n",
    "            overlap_end = min(predicted_passage.end_line, actual.end_line)\n",
    "            overlap = overlap_end - overlap_start + 1\n",
    "\n",
    "            # Calculate the score as the proportion of overlap over the actual passage length\n",
    "            actual_length = actual.end_line - actual.start_line + 1\n",
    "            overlap_score = overlap / actual_length\n",
    "\n",
    "            best_overlap_score = max(best_overlap_score, overlap_score)\n",
    "    \n",
    "    return best_overlap_score\n",
    "\n",
    "class SearchEvaluator:\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, k):\n",
    "        return sum(relevant[:k]) / k\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(relevant):\n",
    "        for idx, value in enumerate(relevant):\n",
    "            if value == 1:\n",
    "                return 1 / (idx + 1)\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_average_precision(relevant):\n",
    "        pred_rel = [1] * len(relevant)\n",
    "        relevant_documents_count = 0\n",
    "        cumulative_precision = 0.0\n",
    "\n",
    "        # We iterate through the predicted relevance scores\n",
    "        for i in range(len(pred_rel)):\n",
    "            # Check if the prediction at this rank is correct (i.e., if it is a relevant document)\n",
    "            if pred_rel[i] == 1 and relevant[i] == 1:\n",
    "                relevant_documents_count += 1\n",
    "                precision_at_i = relevant_documents_count / (i + 1)\n",
    "                cumulative_precision += precision_at_i\n",
    "\n",
    "        # The average precision is the cumulative precision divided by the number of relevant documents\n",
    "        average_precision = cumulative_precision / sum(relevant) if sum(relevant) > 0 else 0\n",
    "        return average_precision\n",
    "\n",
    "    # @staticmethod\n",
    "    # def calculate_recall(relevant, total_modified_files, k):\n",
    "    #   # Does not work for commit based approach as it can have multiple mentions of the same file across commits leading to a higher than 1 recall\n",
    "    #     print(total_modified_files)\n",
    "    #     print(relevant)\n",
    "    #     return sum(relevant[:k]) / total_modified_files\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_recall(retrieved_files, total_modified, relevant, k):\n",
    "        # this complicated mess is required as compared to the above much simpler code to support both commit-based and file-based approaches\n",
    "        # in file-based approach, this is equivalent to the above code\n",
    "        # in code-based approach, duplicates could be present in retrieved_files, which is why we need to filter them out (the above code would not work in this case)\n",
    "\n",
    "        return len({file for idx, file in enumerate(retrieved_files[:k])\n",
    "                        if relevant[idx] == 1\n",
    "                    }) / total_modified if total_modified > 0 else 0\n",
    "\n",
    "\n",
    "    def evaluate(self, search_results, actual_modified, eval_type='file'):\n",
    "        if eval_type == 'random_split_patch':\n",
    "            retrieved = [result.passage for result in search_results]\n",
    "            relevant = [1 if file in actual_modified else 0 for file in retrieved]\n",
    "            total_modified = len(actual_modified)\n",
    "        elif eval_type == 'file':\n",
    "            retrieved = [result.file_path for result in search_results]\n",
    "            relevant = [1 if file in actual_modified else 0 for file in retrieved]\n",
    "            total_modified = len(actual_modified)\n",
    "        elif eval_type == 'patch2':\n",
    "            # search results = List[PatchResults] and actual_modified = Dict[file_path: List[PatchResults]]\n",
    "            relevant = [0 for _ in range(len(search_results))]\n",
    "            retrieved = [0 for i in range(len(search_results))]\n",
    "            for i, pr in enumerate(search_results):\n",
    "                if pr.file_path not in actual_modified:\n",
    "                    continue\n",
    "                # file_path matches, now compute max similarity of passages based on line numbers\n",
    "                relevant[i] = calculate_line_overlap(pr.passage, actual_modified[pr.file_path])\n",
    "            threshold = 0.5\n",
    "            retrieved = [i if val >= threshold else 0 for i, val in enumerate(relevant)]\n",
    "            relevant = [1 if i >= threshold else 0 for i in relevant]\n",
    "            total_modified = sum([len(x) for k, x in actual_modified.items()])\n",
    "            \n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric == 'MAP':\n",
    "                evaluations[metric] = self.calculate_average_precision(relevant)\n",
    "            elif metric == 'MRR':\n",
    "                evaluations[metric] = self.mean_reciprocal_rank(relevant)\n",
    "            elif metric.startswith('P@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.precision_at_k(relevant, k)\n",
    "            elif metric.startswith('R@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.calculate_recall(retrieved, total_modified, relevant, k)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}\n",
    "\n",
    "\n",
    "\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a25efb5-d11f-4b87-a75f-9068b279a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator.evaluate(code_reranker_results, ground_truth, eval_type='patch2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f098613-511d-4e2e-afbb-9dc1e7f91df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.082,\n",
       " 'P@1': 0.07,\n",
       " 'P@10': 0.061,\n",
       " 'P@20': 0.0485,\n",
       " 'P@30': 0.0427,\n",
       " 'MRR': 0.1321,\n",
       " 'R@1': 0.0044,\n",
       " 'R@10': 0.0633,\n",
       " 'R@100': 0.1924,\n",
       " 'R@1000': 0.4068}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_splitter = PassageSplitter(DiffSplitStrategy())\n",
    "def get_ground_truth(qfile_path_list, qfile_list, qdiff_list):\n",
    "    ground_truth = {}\n",
    "    for gt_file_path, gt_file_content, gt_diff in zip(qfile_path_list, qfile_list, qdiff_list):\n",
    "        patch_list = diff_splitter.split_passages(file_content=gt_file_content, diff=gt_diff)\n",
    "        ground_truth[gt_file_path] = []\n",
    "        for lsp in patch_list:\n",
    "            ground_truth[gt_file_path].append(PatchResult(file_path=gt_file_path, score=-1, passage=lsp))\n",
    "\n",
    "    return ground_truth\n",
    "\n",
    "patch_results = []\n",
    "for i, row in tqdm(gold_df.iterrows(), total=gold_df.shape[0]):\n",
    "    qcid = row['commit_id']\n",
    "    # qcdate = row['commit_date']\n",
    "    # qcmsg = row['commit_message']\n",
    "    qfile_path_list = row['actual_files_modified']\n",
    "    # qfile_list = [combined_df.query(f\"commit_id=='{qcid}' & file_path=='{x}'\")['previous_file_content'].values[0] for x in qfile_path_list]\n",
    "    # qdiff_list = [combined_df.query(f\"commit_id=='{qcid}' & file_path=='{x}'\")['diff'].values[0] for x in qfile_path_list]\n",
    "\n",
    "    qfile_list = [combined_df.query(f\"commit_id=='{qcid}' & file_path=='{x}'\")['previous_file_content'].values[0] if not pd.isna(combined_df.query(f\"commit_id=='{qcid}' & file_path=='{x}'\")['previous_file_content'].values[0]) else '' for x in qfile_path_list]\n",
    "    qdiff_list = [combined_df.query(f\"commit_id=='{qcid}' & file_path=='{x}'\")['diff'].values[0] if not pd.isna(combined_df.query(f\"commit_id=='{qcid}' & file_path=='{x}'\")['diff'].values[0]) else '' for x in qfile_path_list]\n",
    "\n",
    "    qground = get_ground_truth(qfile_path_list, qfile_list, qdiff_list)\n",
    "\n",
    "    patch_results.append(evaluator.evaluate(query_results[qcid], qground, eval_type='patch2'))\n",
    "    \n",
    "\n",
    "avg_patch_scores = {metric: round(np.mean([result[metric] for result in patch_results]), 4) for metric in patch_results[0]}\n",
    "avg_patch_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
