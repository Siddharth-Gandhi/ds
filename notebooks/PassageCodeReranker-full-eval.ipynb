{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36960f25-25ee-42b1-bb41-939e55249fcc",
   "metadata": {},
   "source": [
    "Goal is, given an input ranking of AggregatedCodeResult\n",
    "\n",
    "# Changes\n",
    "1. Eval code files must be split on the same function as the original train splits (need a Split Class)\n",
    "2. So your eval list will contain code snippets instead of files\n",
    "\n",
    "## before \n",
    "actual = set(f2,f4,f8)\n",
    "predictions = [f1, f4, f2, f7, f8]\n",
    "\n",
    "\n",
    "## now\n",
    "actual = set(p2.1, p2.2, p2.3, p4.1, p4.2, p8.1)\n",
    "precictions = [p1.1, p7.8, p8.3, p4.1, p2.3, ....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efd078e-00c2-4bee-9815-e215d1b31e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3ab404-f767-4d5f-b281-9ea6fa962c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from utils import AggregatedSearchResult, get_combined_df, full_tokenize\n",
    "from bm25_v2 import BM25Searcher\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from BERTReranker_v4 import BERTReranker\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "# from CodeReranker import BERTCodeReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729c1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEvaluator:\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, k):\n",
    "        return sum(relevant[:k]) / k\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(relevant):\n",
    "        for idx, value in enumerate(relevant):\n",
    "            if value == 1:\n",
    "                return 1 / (idx + 1)\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_average_precision(relevant):\n",
    "        pred_rel = [1] * len(relevant)\n",
    "        relevant_documents_count = 0\n",
    "        cumulative_precision = 0.0\n",
    "\n",
    "        # We iterate through the predicted relevance scores\n",
    "        for i in range(len(pred_rel)):\n",
    "            # Check if the prediction at this rank is correct (i.e., if it is a relevant document)\n",
    "            if pred_rel[i] == 1 and relevant[i] == 1:\n",
    "                relevant_documents_count += 1\n",
    "                precision_at_i = relevant_documents_count / (i + 1)\n",
    "                cumulative_precision += precision_at_i\n",
    "\n",
    "        # The average precision is the cumulative precision divided by the number of relevant documents\n",
    "        average_precision = cumulative_precision / sum(relevant) if sum(relevant) > 0 else 0\n",
    "        return average_precision\n",
    "\n",
    "    # @staticmethod\n",
    "    # def calculate_recall(relevant, total_modified_files, k):\n",
    "    #   # Does not work for commit based approach as it can have multiple mentions of the same file across commits leading to a higher than 1 recall\n",
    "    #     print(total_modified_files)\n",
    "    #     print(relevant)\n",
    "    #     return sum(relevant[:k]) / total_modified_files\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_recall(retrieved_files, actual_modified, relevant, k):\n",
    "        # this complicated mess is required as compared to the above much simpler code to support both commit-based and file-based approaches\n",
    "        # in file-based approach, this is equivalent to the above code\n",
    "        # in code-based approach, duplicates could be present in retrieved_files, which is why we need to filter them out (the above code would not work in this case)\n",
    "\n",
    "        return len({file for idx, file in enumerate(retrieved_files[:k])\n",
    "                        if relevant[idx] == 1\n",
    "                    }) / len(actual_modified) if len(actual_modified) > 0 else 0\n",
    "\n",
    "\n",
    "    def evaluate(self, search_results, actual_modified, eval_type='file'):\n",
    "        if eval_type == 'patch':\n",
    "            retrieved = [result.passage for result in search_results]\n",
    "        else:\n",
    "            retrieved = [result.file_path for result in search_results]\n",
    "        relevant = [1 if file in actual_modified else 0 for file in retrieved]\n",
    "\n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric == 'MAP':\n",
    "                evaluations[metric] = self.calculate_average_precision(relevant)\n",
    "            elif metric == 'MRR':\n",
    "                evaluations[metric] = self.mean_reciprocal_rank(relevant)\n",
    "            elif metric.startswith('P@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.precision_at_k(relevant, k)\n",
    "            elif metric.startswith('R@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.calculate_recall(retrieved, actual_modified, relevant, k)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90325ff-e479-4411-9d69-a01c671a14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, eval_model, combined_df, seed=42):\n",
    "        self.model = model\n",
    "        self.eval_model = eval_model\n",
    "        self.combined_df = combined_df\n",
    "        self.seed = seed\n",
    "\n",
    "    def sample_commits(self, n):\n",
    "        if self.combined_df.commit_id.nunique() < n:\n",
    "            raise ValueError(f'Not enough commits to sample. Required: {n}, available: {self.combined_df.commit_id.nunique()}')\n",
    "\n",
    "        midpoint_date = np.median(self.combined_df['commit_date'])\n",
    "        recent_df = self.combined_df[self.combined_df['commit_date'] > midpoint_date]\n",
    "\n",
    "        return recent_df.drop_duplicates(subset='commit_id').sample(n=n, replace=False, random_state=self.seed)\n",
    "\n",
    "    def evaluate_df(self, df, k=1000, aggregation_strategy=None, rerankers=None):\n",
    "        results = []\n",
    "        for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            cur_query = row['commit_message']\n",
    "            search_results = self.model.pipeline(cur_query, row['commit_date'], ranking_depth=k, aggregation_method=aggregation_strategy)\n",
    "            for reranker in rerankers:\n",
    "                if reranker.__class__.__name__ == 'BERTCodeReranker':\n",
    "                    search_results = reranker.rerank_pipeline(cur_query, search_results, row['commit_id'])\n",
    "                else:\n",
    "                    search_results = reranker.rerank_pipeline(cur_query, search_results)\n",
    "\n",
    "\n",
    "            if 'actual_modified_files' in df.columns:\n",
    "                actual_modified_files = row['actual_modified_files']\n",
    "            else:\n",
    "                actual_modified_files = self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist()\n",
    "            evaluation = self.eval_model.evaluate(search_results, actual_modified_files)\n",
    "            results.append(evaluation)\n",
    "        return results\n",
    "\n",
    "    def evaluate_sampling(self, n=100, k=1000, output_file_path=None, overwrite_eval=False, aggregation_strategy=None, rerankers=None, gold_df=None): #, repo_path=None):\n",
    "        # if repo_path is None:\n",
    "        #     print(\"Repo path not provided, using current working directory\")\n",
    "            # repo_path = os.getcwd()\n",
    "        if rerankers is None:\n",
    "            rerankers = []\n",
    "\n",
    "        if output_file_path is None:\n",
    "            print(\"WARNING: Output file path not provided, not writing results to file\")\n",
    "            # output_file_path = os.path.join(repo_path, f'{self.model.__class__.__name__}_results.txt')\n",
    "\n",
    "        # output_file_path = os.path.join(repo_path, output_file)\n",
    "        model_name = self.model.__class__.__name__\n",
    "\n",
    "        if not overwrite_eval and output_file_path and os.path.exists(output_file_path):\n",
    "            print(f'Output file {output_file_path} already exists - not writing to file, set overwrite_eval flag to True for that...')\n",
    "            # print the contents of the file\n",
    "            # with open(output_file_path, \"r\") as file:\n",
    "            #     print(file.read())\n",
    "            # return\n",
    "            output_file_path=None\n",
    "\n",
    "        if gold_df is None:\n",
    "            sampled_commits = self.sample_commits(n)\n",
    "            results = self.evaluate_df(sampled_commits, k, aggregation_strategy, rerankers)\n",
    "        else:\n",
    "            print(f'Found gold_df, evaluating on {len(gold_df)} commits')\n",
    "            print(gold_df.info())\n",
    "            results = self.evaluate_df(gold_df, k, aggregation_strategy, rerankers)\n",
    "\n",
    "        avg_scores = {metric: round(np.mean([result[metric] for result in results]), 4) for metric in results[0]}\n",
    "\n",
    "        if output_file_path is not None:\n",
    "            with open(output_file_path, \"w\") as file:\n",
    "                file.write(f\"Model Name: {model_name}\\n\")\n",
    "                # write name of each reranker\n",
    "                if len(rerankers) > 0:\n",
    "                    file.write(\"Rerankers:\\n\")\n",
    "                    for reranker in rerankers:\n",
    "                        reranker_model_name = reranker.model.config.name_or_path\n",
    "                        # replace / with _\n",
    "                        reranker_model_name = reranker_model_name.replace('/', '_')\n",
    "                        file.write(f\"{reranker.__class__.__name__} ({reranker_model_name}) @ {reranker.rerank_depth}\\n\")\n",
    "\n",
    "\n",
    "                file.write(f\"Sample Size: {n}\\n\")\n",
    "                file.write(\"Evaluation Metrics:\\n\")\n",
    "                for key, value in avg_scores.items():\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "            print(f'Evaluation results written to {output_file_path}')\n",
    "\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e11d01-ec22-4a7b-9ec7-e26a207b44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        print(f'Using device: {self.device}')\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f540cf18-e95f-4e03-9b16-ef540cd431a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCodeReranker:\n",
    "    def __init__(self, parameters, combined_df):\n",
    "        self.combined_df = combined_df\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        print(f\"Initialized Code File BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult], train_commit_id):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        query_passage_pairs, per_result_contribution = self.split_into_query_passage_pairs(query, aggregated_results, train_commit_id)\n",
    "\n",
    "\n",
    "        # for agg_result in aggregated_results:\n",
    "        #     query_passage_pairs.extend(\n",
    "        #         (query, result.commit_message)\n",
    "        #         for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "        #     )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for i, agg_result in enumerate(aggregated_results):\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + per_result_contribution[i] # only use psg_cnt contributing_results\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "    def split_into_query_passage_pairs(self, query, aggregated_results, train_commit_id):\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        def full_tokenize(s):\n",
    "            return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        query_passage_pairs = []\n",
    "        per_result_contribution = []\n",
    "        if self.combined_df is not None:\n",
    "            combined_df = self.combined_df\n",
    "\n",
    "\n",
    "        for agg_result in aggregated_results:\n",
    "            # agg_result.contributing_results.sort(key=lambda res: res.commit_date, reverse=True)\n",
    "            # get most recent file version\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            # get the file_path and commit_id\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            # commit_id = most_recent_search_result.commit_id\n",
    "            # get the file content from combined_df\n",
    "            # file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['cur_file_content'].values[0]\n",
    "\n",
    "            file_content = get_file_at_commit_from_git(file_path, train_commit_id)\n",
    "\n",
    "            # file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['previous_file_content'].values[0]\n",
    "\n",
    "            # now need to split this file content into psg_cnt passages\n",
    "            # first tokenize the file content\n",
    "\n",
    "            # warning these asserts are useless since we are using NaNs\n",
    "            # assert file_content is not None, f'file_content is None for commit_id: {commit_id}, file_path: {file_path}'\n",
    "            # assert file_path is not None, f'file_path is None for commit_id: {commit_id}'\n",
    "            assert query is not None, 'query is None'\n",
    "\n",
    "            # query_tokens = full_tokenize(query)\n",
    "            path_tokens = full_tokenize(file_path)\n",
    "\n",
    "            if pd.isna(file_content):\n",
    "                # if file_content is NaN, then we can just set file_content to empty string\n",
    "                print(f'WARNING: file_content is NaN for commit_id: {train}, file_path: {file_path}, setting file_content to empty string')\n",
    "                file_content = ''\n",
    "\n",
    "            file_tokens = full_tokenize(file_content)\n",
    "\n",
    "\n",
    "            # now split the file content into psg_cnt passages\n",
    "            cur_result_passages = []\n",
    "            # get the input ids\n",
    "            # input_ids = file_content['input_ids'].squeeze()\n",
    "            # get the number of tokens in the file content\n",
    "            total_tokens = len(file_tokens)\n",
    "\n",
    "            for cur_start in range(0, total_tokens, self.psg_stride):\n",
    "                cur_passage = []\n",
    "                # add query tokens and path tokens\n",
    "                # cur_passage.extend(query_tokens)\n",
    "                cur_passage.extend(path_tokens)\n",
    "\n",
    "                # add the file tokens\n",
    "                cur_passage.extend(file_tokens[cur_start:cur_start+self.psg_len])\n",
    "\n",
    "                # now convert cur_passage into a string\n",
    "                cur_passage_decoded = self.tokenizer.decode(cur_passage)\n",
    "\n",
    "                # add the cur_passage to cur_result_passages\n",
    "                cur_result_passages.append(cur_passage_decoded)\n",
    "\n",
    "                # if len(cur_result_passages) == self.psg_cnt:\n",
    "                #     break\n",
    "\n",
    "            # now add the query, passage pairs to query_passage_pairs\n",
    "            per_result_contribution.append(len(cur_result_passages))\n",
    "            query_passage_pairs.extend((query, passage) for passage in cur_result_passages)\n",
    "        return query_passage_pairs, per_result_contribution\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results, train_commit_id):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results, train_commit_id)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9adda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchResult:\n",
    "    def __init__(self, passage, score):\n",
    "        self.score = score\n",
    "        self.passage = passage\n",
    "\n",
    "    def __repr__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        return f'{class_name}({self.passage}, {self.score})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd96bd1-0bac-46b6-a8ad-85c746483efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCodeReranker(Reranker):\n",
    "    def __init__(self, parameters):\n",
    "        super().__init__(parameters)\n",
    "\n",
    "        # specific to CodeReranker type\n",
    "\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        \n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        print(f\"Initialized Code File BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult], train_commit_id):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        query_passage_pairs, _ = self.split_into_query_passage_pairs(query, aggregated_results, train_commit_id)\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        # convert the scores to PatchResult objects\n",
    "        patch_results = [PatchResult(passage[1], score) for passage, score in zip(query_passage_pairs, scores)]\n",
    "\n",
    "        # sort patch_results by the scores\n",
    "        sorted_patch_results = sorted(patch_results, key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return sorted_patch_results\n",
    "\n",
    "    def split_into_query_passage_pairs(self, query, aggregated_results, train_commit_id):\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        def full_tokenize(s):\n",
    "            return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        query_passage_pairs = []\n",
    "        per_result_contribution = []\n",
    "        for agg_result in aggregated_results:\n",
    "            agg_result.contributing_results.sort(key=lambda res: res.commit_date, reverse=True)\n",
    "            # get most recent file version\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            # get the file_path and commit_id\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            commit_id = most_recent_search_result.commit_id\n",
    "            # file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['cur_file_content'].values[0]\n",
    "\n",
    "            # file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['previous_file_content'].values[0]\n",
    "\n",
    "            file_content = get_file_at_commit_from_git(file_path, train_commit_id)\n",
    "\n",
    "            # warning these asserts are useless since we are using NaNs\n",
    "            assert file_content is not None, f'file_content is None for commit_id: {commit_id}, file_path: {file_path}'\n",
    "            assert file_path is not None, f'file_path is None for commit_id: {commit_id}'\n",
    "            assert query is not None, 'query is None'\n",
    "\n",
    "            if pd.isna(file_content):\n",
    "                # if file_content is NaN, then we can just set file_content to empty string\n",
    "                print(f'WARNING: file_content is NaN for commit_id: {commit_id}, file_path: {file_path}, setting file_content to empty string')\n",
    "                file_content = ''\n",
    "\n",
    "            cur_result_passages = split_random_chunks(file_content, tokenizer)\n",
    "\n",
    "            query_passage_pairs.extend((query, passage) for passage in cur_result_passages)\n",
    "        return query_passage_pairs, per_result_contribution\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results, train_commit_id):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        # bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results, train_commit_id)\n",
    "        # min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        # for i, result in enumerate(bottom_results):\n",
    "            # result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        # reranked_results.extend(bottom_results)\n",
    "        # assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5a761-be74-4ffb-8b2f-585449866911",
   "metadata": {},
   "source": [
    "# Loading some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6b218a-37ae-4b3d-ba55-903afe13188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../data/2_7/facebook_react/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='../data/2_7/facebook_react/index_commit_tokenized',\n",
    "    repo_path='../data/2_7/facebook_react', k=1000, n=100,\n",
    "    model_path='microsoft/codebert-base', overwrite_cache=False,\n",
    "    batch_size=32, num_epochs=10, learning_rate=5e-05,\n",
    "    run_name='debug',\n",
    "    notes='debug (ignore)',\n",
    "    num_positives=10, num_negatives=10, train_depth=1000, num_workers=8,\n",
    "    train_commits=1000, psg_cnt=25, use_gpu=True,\n",
    "    rerank_depth=100, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4',\n",
    "    overwrite_eval=False, sanity_check=True, debug=False,\n",
    "    psg_len=350, psg_stride=250, ignore_gold_in_training=False,\n",
    "    eval_folder='repr_0.1663', use_gpt_train=True,\n",
    "    aggregation_strategy='sump',\n",
    "    bert_best_model='../data/combined_commit_train/best_model',\n",
    "    best_model_path='../data/2_7/facebook_react/models/bce/best_model'\n",
    "\n",
    ")\n",
    "\n",
    "metrics =['MAP', 'P@1', 'P@10', 'P@20', 'P@30', 'MRR', 'R@1', 'R@10', 'R@100', 'R@1000']\n",
    "repo_path = args.repo_path\n",
    "repo_name = repo_path.split('/')[-1]\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'\n",
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)\n",
    "\n",
    "test_path = os.path.join('..', 'gold', 'facebook_react', 'v2_facebook_react_gpt4_gold.parquet')\n",
    "# test_path = os.path.join('gold', 'facebook_react', 'v2_facebook_react_gpt4_gold.parquet')\n",
    "gold_df = pd.read_parquet(test_path)\n",
    "gold_df = gold_df.rename(columns={'commit_message': 'original_message', f'transformed_message_{args.openai_model}': 'commit_message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d248d6ba-2f1c-4f82-8015-1edc87a4948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6cb73f8-b377-4646-bdf6-1446fec8d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../repos/facebook_react'\n",
    "repo = git.Repo(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c62e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'aggregation_strategy': 'maxp',\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': args.rerank_depth,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': BM25_AGGR_STRAT,\n",
    "        'psg_len': args.psg_len,\n",
    "        'psg_stride': args.psg_stride,\n",
    "        'psg_cnt': args.psg_cnt,\n",
    "    }\n",
    "\n",
    "bert_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': 5,\n",
    "        'aggregation_strategy': 'sump',\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': 250,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': 'sump',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feaddd01-a7a7-46ad-9b03-40ecfc6b06c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "commit_id                         323efbc33c27a602a4aab8519f58feba1e0a216c\n",
       "commit_date                                                     1512398372\n",
       "original_message         Ensure value and defaultValue do not assign fu...\n",
       "actual_files_modified    [packages/react-dom/src/__tests__/ReactDOMInpu...\n",
       "commit_message           Input properties 'value' and 'defaultValue' ac...\n",
       "Name: 6, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_train_row = gold_df.iloc[6]\n",
    "dummy_commit_id = dummy_train_row.commit_id\n",
    "dummy_train_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a573cc-5441-4a4f-9d59-dacc753acab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train_query = dummy_train_row.commit_message\n",
    "dummy_file_path_list = dummy_train_row.actual_files_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9d47a8-478a-4eab-a7e6-e3e91774456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['packages/react-dom/src/__tests__/ReactDOMInput-test.js',\n",
       "       'packages/react-dom/src/client/ReactDOMFiberInput.js',\n",
       "       'packages/react-dom/src/events/ChangeEventPlugin.js',\n",
       "       'packages/react-dom/src/shared/DOMProperty.js'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40496248-feae-4349-abd8-964005203772",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "def split_random_chunks(file, tokenizer, stride=250, psg_len=350):\n",
    "    file_tokens = full_tokenize(file, tokenizer)\n",
    "    total_tokens = len(file_tokens)\n",
    "    res = []\n",
    "    for cur_start in range(0, total_tokens, stride):\n",
    "        # get tokens for current passage\n",
    "        res.append(tokenizer.decode(file_tokens[cur_start:cur_start+psg_len]))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e965a22-67e5-4537-ae0c-ff65f58872bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_file = combined_df.query(f\"commit_id=='{dummy_commit_id}' & file_path=='{dummy_file_path_list[0]}'\")['previous_file_content'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1154b09-064d-4798-b9f2-4af651d66072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "880da63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21407 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dummy_file_list = [combined_df.query(f\"commit_id=='{dummy_commit_id}' & file_path=='{x}'\")['previous_file_content'].values[0] for x in dummy_file_path_list]\n",
    "dummy_file_patch_list = [chunk for x in dummy_file_list for chunk in split_random_chunks(x, tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bffe1a22-73a2-4d80-901f-a0df4ccdc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_diff_list = [combined_df.query(f\"commit_id=='{dummy_commit_id}' & file_path=='{x}'\")['diff'].values[0] for x in dummy_file_path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "167ab9fd-faf2-44bd-bf4e-d06bfebe611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_diff = dummy_diff_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a468045-16d7-4a73-8e83-7a9a692d2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_at_commit_from_git(file, commit_id):\n",
    "    # Access the specified commit\n",
    "    commit = repo.commit(commit_id)\n",
    "    \n",
    "    # Check if the commit has parents\n",
    "    if commit.parents:\n",
    "        # Access the first parent of the commit\n",
    "        parent_commit = commit.parents[0]\n",
    "        \n",
    "        # Attempt to get the file content from the parent commit\n",
    "        try:\n",
    "            blob = parent_commit.tree / file\n",
    "            file_content = blob.data_stream.read().decode('utf-8')\n",
    "            return file_content\n",
    "        except KeyError:\n",
    "            # Handle the case where the file does not exist in the parent commit\n",
    "            return \"The file was not present in the parent commit.\"\n",
    "    else:\n",
    "        # Handle the case where the specified commit is the initial commit and has no parents\n",
    "        return \"The specified commit has no parents (it might be the initial commit).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74bc9979-b194-487e-a936-e0eee9b49863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_file_at_commit_from_git(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpackages/react-dom/src/__tests__/ReactDOMInput-test.js\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m59763bf7f3ab3b06cd8ab5a5a83ae3dafc667aa9\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[43mtmp\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "get_file_at_commit_from_git('packages/react-dom/src/__tests__/ReactDOMInput-test.js', '59763bf7f3ab3b06cd8ab5a5a83ae3dafc667aa9') == tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39318b4e-33fb-4d73-aabd-7ae541378db8",
   "metadata": {},
   "source": [
    "# Getting BM25 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a79f6388-8f94-4404-8cab-df72ba68212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results = bm25_searcher.pipeline(dummy_train_query, dummy_train_row['commit_date'], ranking_depth=K, aggregation_method=BM25_AGGR_STRAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "679ed57a-da98-425e-bcd7-977ffd108605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0867,\n",
       " 'P@1': 0.0,\n",
       " 'P@10': 0.1,\n",
       " 'P@20': 0.05,\n",
       " 'P@30': 0.0333,\n",
       " 'MRR': 0.1429,\n",
       " 'R@1': 0.0,\n",
       " 'R@10': 0.25,\n",
       " 'R@100': 0.75,\n",
       " 'R@1000': 0.75}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(bm25_results, dummy_file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18987c-bdcb-4619-a1db-14def735a288",
   "metadata": {},
   "source": [
    "# Getting BERT Rerank @ 250 on top of BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94b30ce8-d517-45db-885a-78406f948cdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 476.73 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(bert_params)\n",
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained(args.bert_best_model, num_labels=1, problem_type='regression')\n",
    "bert_reranker.model.to(bert_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e049fa-b497-419d-a3ad-b5bbe696d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_rerank_results = bert_reranker.rerank_pipeline(dummy_train_query, bm25_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af019ad-6152-448e-a0d4-e18f3123ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(bert_rerank_results, dummy_file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64dd4b-4f4d-46b1-bfd3-0dd5117ed295",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def aside():\n",
    "#     results = [sorted(x.contributing_results, key=lambda res: res.commit_date, reverse=True)[0] for x in bert_rerank_results]\n",
    "#     files_content = [combined_df.query(f\"commit_id=='{x.commit_id}' & file_path=='{x.file_path}'\")['cur_file_content'].values[0] for x in results]\n",
    "#     patches = [chunk for x in files_content for chunk in split_random_chunks(x, tokenizer)]\n",
    "#     print(len(set(patches).intersection(set(dummy_file_patch_list))))\n",
    "    \n",
    "# aside()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f61911-82c5-4825-9c76-0d5e6df69c7d",
   "metadata": {},
   "source": [
    "# File Code Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "806c8b76-15a6-44b8-99bc-cf6b15103207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n",
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 350, 'psg_stride': 250, 'psg_cnt': 25}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_reranker_params['aggregation_strategy'] = 'sump'\n",
    "file_code_reranker = BERTCodeReranker(code_reranker_params, combined_df)\n",
    "cur_best_model_path = '../data/2_7/facebook_react/models/combined_diffs/best_model'\n",
    "# cur_best_model_path = '../data/2_7/facebook_react/models/X/best_model'\n",
    "\n",
    "\n",
    "file_code_reranker.model = AutoModelForSequenceClassification.from_pretrained(cur_best_model_path, num_labels=1, problem_type='regression')\n",
    "file_code_reranker.model.to(file_code_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d11f84-94c2-45d1-a818-73cd2b9c81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_code_reranker_results = file_code_reranker.rerank_pipeline(dummy_train_query, bert_rerank_results, dummy_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff390d6-313f-4f63-bf13-3b71273a6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(file_code_reranker_results, dummy_file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ec2ba-e3df-4751-9764-a7596aefee22",
   "metadata": {},
   "source": [
    "After latest file version (gold df iloc 6, combined_Df, sump codereranker)\n",
    "\n",
    "\n",
    "{'MAP': 0.7,\n",
    " 'P@1': 1.0,\n",
    " 'P@10': 0.3,\n",
    " 'P@20': 0.15,\n",
    " 'P@30': 0.1,\n",
    " 'MRR': 1.0,\n",
    " 'R@1': 0.25,\n",
    " 'R@10': 0.75,\n",
    " 'R@100': 0.75,\n",
    " 'R@1000': 0.75}\n",
    "\n",
    "{'MAP': 0.5536,\n",
    " 'P@1': 1.0,\n",
    " 'P@10': 0.3,\n",
    " 'P@20': 0.15,\n",
    " 'P@30': 0.1,\n",
    " 'MRR': 1.0,\n",
    " 'R@1': 0.25,\n",
    " 'R@10': 0.75,\n",
    " 'R@100': 0.75,\n",
    " 'R@1000': 0.75}\n",
    "\n",
    "\n",
    "Pre latest version of file\n",
    "\n",
    "{'MAP': 0.3869,\n",
    " 'P@1': 0.0,\n",
    " 'P@10': 0.3,\n",
    " 'P@20': 0.15,\n",
    " 'P@30': 0.1,\n",
    " 'MRR': 0.5,\n",
    " 'R@1': 0.0,\n",
    " 'R@10': 0.75,\n",
    " 'R@100': 0.75,\n",
    " 'R@1000': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574c026-bb90-435b-bbd6-38a7123f96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [sorted(x.contributing_results, key=lambda res: res.commit_date, reverse=True)[0] for x in file_code_reranker_results]\n",
    "# def aside():\n",
    "#     tmp = [results[1]]\n",
    "#     print(tmp)\n",
    "#     # print(dummy_train_row.commit_date >= results[1].commit_date)\n",
    "#     files_content = [combined_df.query(f\"commit_id=='{x.commit_id}' & file_path=='{x.file_path}'\")['cur_file_content'].values[0] for x in tmp]\n",
    "#     return files_content[0]\n",
    "#     # patches = [chunk for x in files_content for chunk in split_random_chunks(x, tokenizer)]\n",
    "#     # print(len(set(patches).intersection(set(dummy_file_patch_list))))\n",
    "    \n",
    "# tmp = aside()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aeee0d-257a-4aa2-b7bf-fdbfbc90428e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b549142-fcc1-4882-879b-7bd41bbced28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f6a32-c82b-4f16-afc3-f6249f02727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=BM25_AGGR_STRAT, rerankers=[bert_reranker], gold_df=gold_df, overwrite_eval=False)\n",
    "# bert_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac4005-9511-4858-b1a0-284437851c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=BM25_AGGR_STRAT, rerankers=[bert_reranker], gold_df=gold_df.head(10), overwrite_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fca12c-1535-49db-bf99-c97f3b191f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of code rereank on 10 gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cf3c0-0735-4026-99e3-ec93a584609c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c331f3-db05-463a-ab0d-fc5bb2859212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f65629-b7bc-46bc-bd56-06ad78962630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd864555-b6ea-42be-817f-333742e680fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0bdc8-0c18-4ca3-80d0-02ff7882874d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "296c8c85-a9f1-4b3f-9001-a38618bf0790",
   "metadata": {},
   "source": [
    "# Patch Code Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26484313-1856-47a0-8a99-85c77198e6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d307c52a-8ab1-4317-926c-1f759344d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 350, 'psg_stride': 250, 'psg_cnt': 25}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_reranker = PatchCodeReranker(code_reranker_params)\n",
    "# cur_best_model_path = 'data/2_7/facebook_react/models/X/best_model'\n",
    "cur_best_model_path = '../data/2_7/facebook_react/models/combined_diffs/best_model'\n",
    "\n",
    "code_reranker.model = AutoModelForSequenceClassification.from_pretrained(cur_best_model_path, num_labels=1, problem_type='regression')\n",
    "code_reranker.model.to(code_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1184c6-3b25-463e-8f5a-661dc0214ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker_results = code_reranker.rerank_pipeline(dummy_train_query, bert_rerank_results, dummy_commit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e0efd-6cf5-4b3c-a409-7f16420a92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266edbfa-8bdf-4cc3-8d99-a678def2a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dummy_file_patch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd7500-0981-4635-8924-87b06d140ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dummy_file_patch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34263154",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker_results[0].passage in dummy_file_patch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f47b2-f24b-4391-bac6-389cf078f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(code_reranker_results, dummy_file_patch_list, eval_type='patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d8e3f-b534-4d7c-826b-a494a538c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d56738fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_df(df, k=1000, aggregation_strategy='sump', rerankers=None):\n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        cur_query = row['commit_message']\n",
    "        search_results = bm25_searcher.pipeline(cur_query, row['commit_date'], ranking_depth=k, aggregation_method=aggregation_strategy)\n",
    "        for reranker in rerankers:\n",
    "            if reranker.__class__.__name__ in ['BERTCodeReranker', 'PatchCodeReranker']:\n",
    "                search_results = reranker.rerank_pipeline(cur_query, search_results, row['commit_id'])\n",
    "            else:\n",
    "                search_results = reranker.rerank_pipeline(cur_query, search_results)\n",
    "\n",
    "\n",
    "        # if 'actual_modified_files' in df.columns:\n",
    "        actual_modified_files = row['actual_files_modified']\n",
    "        # else:\n",
    "        #     actual_modified_files = combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist()\n",
    "        if rerankers[-1].__class__.__name__ == 'PatchCodeReranker':\n",
    "            dummy_file_list = [combined_df.query(f\"commit_id=='{row['commit_id']}' & file_path=='{x}'\")['previous_file_content'].values[0] for x in actual_modified_files]\n",
    "            dummy_file_patch_list = [chunk for x in dummy_file_list for chunk in split_random_chunks(x, tokenizer)]\n",
    "            actual_modified_files = dummy_file_patch_list\n",
    "\n",
    "            evaluation = evaluator.evaluate(search_results, actual_modified_files, eval_type='patch')\n",
    "        else:\n",
    "            evaluation = evaluator.evaluate(search_results, actual_modified_files)\n",
    "        results.append(evaluation)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73d99b60-76eb-4498-8bc4-6c171e4957b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerankers = [bert_reranker, code_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bbe3f-31c3-4153-9ebb-8be6da7f7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerankers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71c0f35b-195e-4688-a41a-3cd4b6c8ba64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    string\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    string\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(2), string(2)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "gold_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141199b1-6d9d-4c8f-8092-4e3c26d83d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 58/100 [31:41<27:28, 39.26s/it]  "
     ]
    }
   ],
   "source": [
    "results = evaluate_df(gold_df, k=K, aggregation_strategy='sump', rerankers=rerankers)\n",
    "avg_scores = {metric: round(np.mean([result[metric] for result in results]), 4) for metric in results[0]}\n",
    "avg_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
