{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016d07c000-0x000000016d088000).\n",
      "[0.006s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "from utils import count_commits, get_combined_df, tokenize, reverse_tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResult:\n",
    "    def __init__(self, commit_id, file_path, score, commit_date, commit_msg):\n",
    "        self.commit_id = commit_id\n",
    "        self.file_path = file_path\n",
    "        self.score = score\n",
    "        self.commit_date = commit_date\n",
    "        self.commit_msg = commit_msg\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        return f\"{class_name}(score={self.score:.5f}, file_path={self.file_path!r}, commit_id={self.commit_id!r}, commit_date={self.commit_date})\"\n",
    "\n",
    "    def is_actual_modified(self, actual_modified_files):\n",
    "        return self.file_path in actual_modified_files\n",
    "\n",
    "    @staticmethod\n",
    "    def print_results(query, search_results, show_only_actual_modified=False):\n",
    "        actual_modified_files = query['actual_files_modified']\n",
    "        for i, result in enumerate(search_results):\n",
    "            if show_only_actual_modified and not result.is_actual_modified(actual_modified_files):\n",
    "                continue\n",
    "            print(f\"{i+1:2} {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatedSearchResult:\n",
    "    def __init__(self, file_path, aggregated_score, contributing_results):\n",
    "        self.file_path = file_path\n",
    "        self.score = aggregated_score\n",
    "        self.contributing_results = contributing_results\n",
    "\n",
    "    def __repr__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        return f\"{class_name}(file_path={self.file_path!r}, score={self.score}, \" \\\n",
    "               f\"contributing_results={self.contributing_results})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Search:\n",
    "    def __init__(self, index_path):\n",
    "        if not os.path.exists(index_path):\n",
    "            raise FileNotFoundError(f\"Index at {index_path} does not exist!\")\n",
    "        self.searcher = LuceneSearcher(index_path)\n",
    "        print(f\"Loaded index at {index_path}\")\n",
    "        print(f'Index Stats: {IndexReader(index_path).stats()}')\n",
    "        # self.ranking_depth = ranking_depth\n",
    "\n",
    "    def search(self, query, query_date, ranking_depth):\n",
    "        # TODO maybe change this to mean returning reranking_depths total results instead of being pruned by the query date\n",
    "        hits = self.searcher.search(tokenize(query), ranking_depth)\n",
    "        unix_date = query_date\n",
    "        filtered_hits = [\n",
    "            SearchResult(hit.docid, json.loads(hit.raw)['file_path'], hit.score, int(json.loads(hit.raw)[\"commit_date\"]), reverse_tokenize(json.loads(hit.raw)['contents']))\n",
    "            for hit in hits if int(json.loads(hit.raw)[\"commit_date\"]) < unix_date\n",
    "        ]\n",
    "        return filtered_hits\n",
    "\n",
    "    def search_full(self, query, query_date, ranking_depth):\n",
    "        filtered_hits = []\n",
    "        step_size = ranking_depth  # Initial search window\n",
    "        total_hits_retrieved = 0\n",
    "\n",
    "        while len(filtered_hits) < ranking_depth and step_size > 0:\n",
    "            current_hits = self.searcher.search(tokenize(query), total_hits_retrieved + step_size)\n",
    "            if not current_hits:\n",
    "                break  # No more results to retrieve\n",
    "\n",
    "            # Filter hits by query date\n",
    "            for hit in current_hits:\n",
    "                if int(json.loads(hit.raw)[\"commit_date\"]) < query_date:\n",
    "                    filtered_hits.append(\n",
    "                        SearchResult(hit.docid, json.loads(hit.raw)['file_path'], hit.score,\n",
    "                                     int(json.loads(hit.raw)[\"commit_date\"]),\n",
    "                                     reverse_tokenize(json.loads(hit.raw)['contents']))\n",
    "                    )\n",
    "                if len(filtered_hits) == ranking_depth:\n",
    "                    break  # We have enough results\n",
    "\n",
    "            total_hits_retrieved += step_size\n",
    "            step_size = ranking_depth - len(filtered_hits)  # Decrease step size to only get as many as needed\n",
    "\n",
    "        return filtered_hits[:ranking_depth]  # Return up to ranking_depth results\n",
    "\n",
    "    def aggregate_file_scores(self, search_results, aggregation_method='sump'):\n",
    "        file_to_results = defaultdict(list)\n",
    "        for result in search_results:\n",
    "            file_to_results[result.file_path].append(result)\n",
    "\n",
    "        aggregated_results = []\n",
    "        for file_path, results in file_to_results.items():\n",
    "            # aggregated_score = sum(result.score for result in results)\n",
    "            if aggregation_method == 'sump':\n",
    "                aggregated_score = sum(result.score for result in results)\n",
    "            elif aggregation_method == 'maxp':\n",
    "                aggregated_score = max(result.score for result in results)\n",
    "            # elif aggregation_method == 'firstp':\n",
    "            #     aggregated_score = results[0].score\n",
    "            elif aggregation_method == 'avgp':\n",
    "                aggregated_score = np.mean([result.score for result in results])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown aggregation method {aggregation_method}\")\n",
    "\n",
    "            aggregated_results.append(AggregatedSearchResult(file_path, aggregated_score, results))\n",
    "\n",
    "        aggregated_results.sort(key=lambda result: result.score, reverse=True)\n",
    "        return aggregated_results\n",
    "\n",
    "    def pipeline(self, query, query_date, ranking_depth, aggregation_method):\n",
    "        search_results = self.search(query, query_date, ranking_depth)\n",
    "        if aggregation_method is not None:\n",
    "            aggregated_results = self.aggregate_file_scores(search_results, aggregation_method)\n",
    "            return aggregated_results\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEvaluator:\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, k):\n",
    "        return sum(relevant[:k]) / k\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(relevant):\n",
    "        for idx, value in enumerate(relevant):\n",
    "            if value == 1:\n",
    "                return 1 / (idx + 1)\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_average_precision(relevant):\n",
    "        pred_rel = [1] * len(relevant)\n",
    "        relevant_documents_count = 0\n",
    "        cumulative_precision = 0.0\n",
    "\n",
    "        # We iterate through the predicted relevance scores\n",
    "        for i in range(len(pred_rel)):\n",
    "            # Check if the prediction at this rank is correct (i.e., if it is a relevant document)\n",
    "            if pred_rel[i] == 1 and relevant[i] == 1:\n",
    "                relevant_documents_count += 1\n",
    "                precision_at_i = relevant_documents_count / (i + 1)\n",
    "                cumulative_precision += precision_at_i\n",
    "\n",
    "        # The average precision is the cumulative precision divided by the number of relevant documents\n",
    "        average_precision = cumulative_precision / sum(relevant) if sum(relevant) > 0 else 0\n",
    "        return average_precision\n",
    "\n",
    "    # @staticmethod\n",
    "    # def calculate_recall(relevant, total_modified_files, k):\n",
    "    #   # Does not work for commit based approach as it can have multiple mentions of the same file across commits leading to a higher than 1 recall\n",
    "    #     print(total_modified_files)\n",
    "    #     print(relevant)\n",
    "    #     return sum(relevant[:k]) / total_modified_files\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_recall(retrieved_files, actual_modified_files, relevant, k):\n",
    "        # this complicated mess is required as compared to the above much simpler code to support both commit-based and file-based approaches\n",
    "        # in file-based approach, this is equivalent to the above code\n",
    "        # in code-based approach, duplicates could be present in retrieved_files, which is why we need to filter them out (the above code would not work in this case)\n",
    "\n",
    "        return len({file for idx, file in enumerate(retrieved_files[:k])\n",
    "                        if relevant[idx] == 1\n",
    "                    }) / len(actual_modified_files) if len(actual_modified_files) > 0 else 0\n",
    "\n",
    "\n",
    "    def evaluate(self, search_results, actual_modified_files):\n",
    "        retrieved_files = [result.file_path for result in search_results]\n",
    "        relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "\n",
    "\n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric == 'MAP':\n",
    "                evaluations[metric] = self.calculate_average_precision(relevant)\n",
    "            elif metric == 'MRR':\n",
    "                evaluations[metric] = self.mean_reciprocal_rank(relevant)\n",
    "            elif metric.startswith('P@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.precision_at_k(relevant, k)\n",
    "            elif metric.startswith('Recall@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "\n",
    "                # evaluations[metric] = len(list({\n",
    "                #         file\n",
    "                #         for idx, file in enumerate(retrieved_files)\n",
    "                #         if relevant[idx] == 1\n",
    "                #     })[:k]) / len(actual_modified_files)\n",
    "                # evaluations[metric] = self.calculate_recall(relevant, len(actual_modified_files), k) # DOES NOT WORK FOR COMMIT-BASED APPROACH\n",
    "\n",
    "                evaluations[metric] = self.calculate_recall(retrieved_files, actual_modified_files, relevant, k)\n",
    "\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}\n",
    "\n",
    "    # def evaluate_file_based(self, search_results, actual_modified_files, aggregation_strategy='sump'):\n",
    "    #     file_relevance = defaultdict(list)\n",
    "\n",
    "    #     # Aggregate relevance scores for each file across all commits\n",
    "    #     for result in search_results:\n",
    "    #         if result.file_path in actual_modified_files:\n",
    "    #             # file_relevance[result.file_path] += 1\n",
    "    #             file_relevance[result.file_path].append(result.score)\n",
    "\n",
    "    #     # Normalize relevance scores based on occurrences in actual modified files\n",
    "    #     # max_relevance = max(file_relevance.values(), default=1)\n",
    "    #     # normalized_relevance = {file: relevance / max_relevance for file, relevance in file_relevance.items()}\n",
    "    #     # sorted_normalized_relevance = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)\n",
    "    #     print(file_relevance)\n",
    "    #     if aggregation_strategy == 'sump':\n",
    "    #         aggregated_scores = {file: sum(relevance) for file, relevance in file_relevance.items()}\n",
    "    #     elif aggregation_strategy == 'firstp':\n",
    "    #         aggregated_scores = {file: relevance[0] for file, relevance in file_relevance.items()}\n",
    "    #     elif aggregation_strategy == 'avgp':\n",
    "    #         aggregated_scores = {file: np.mean(relevance) for file, relevance in file_relevance.items()}\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Unknown aggregation strategy {aggregation_strategy}\")\n",
    "\n",
    "    #     sorted_aggregated_scores = sorted(aggregated_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    #     print(sorted_aggregated_scores)\n",
    "\n",
    "    #     evaluations = {}\n",
    "    #     for metric in self.metrics:\n",
    "    #         if metric.startswith('P@'):\n",
    "    #             # Compute precision at k for files, not individual commit mentions\n",
    "    #             k = int(metric.split('@')[1])\n",
    "    #             # top_k_files = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "    #             top_k_files = sorted_aggregated_scores[:k]\n",
    "    #             precision_at_k = sum(1 for file, relevance in top_k_files if file in actual_modified_files) / k\n",
    "    #             evaluations[metric] = precision_at_k\n",
    "    #         elif metric.startswith('Recall@'):\n",
    "    #             k = int(metric.split('@')[1])\n",
    "    #             # top_k_files = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "    #             top_k_files = sorted_aggregated_scores[:k]\n",
    "    #             recall_at_k = sum(1 for file, relevance in top_k_files if file in actual_modified_files) / len(actual_modified_files)\n",
    "    #             evaluations[metric] = recall_at_k\n",
    "    #         elif metric == 'MAP':\n",
    "    #             # Compute average precision for files, not individual commit mentions\n",
    "    #             average_precision = 0\n",
    "    #             num_relevant_files = 0\n",
    "    #             for idx, (file, relevance) in enumerate(sorted_aggregated_scores):\n",
    "    #                 if file in actual_modified_files:\n",
    "    #                     num_relevant_files += 1\n",
    "    #                     average_precision += num_relevant_files / (idx + 1)\n",
    "    #             average_precision /= len(actual_modified_files)\n",
    "    #             evaluations[metric] = average_precision\n",
    "    #         elif metric == 'MRR':\n",
    "    #             # Compute mean reciprocal rank for files, not individual commit mentions\n",
    "    #             reciprocal_rank = 0\n",
    "    #             for idx, (file, relevance) in enumerate(sorted_aggregated_scores):\n",
    "    #                 if file in actual_modified_files:\n",
    "    #                     reciprocal_rank = 1 / (idx + 1)\n",
    "    #                     break\n",
    "    #             evaluations[metric] = reciprocal_rank\n",
    "\n",
    "    #     return {k: round(v, 4) for k, v in evaluations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, eval_model, combined_df, seed=42):\n",
    "        self.model = model\n",
    "        self.eval_model = eval_model\n",
    "        self.combined_df = combined_df\n",
    "        self.seed = seed\n",
    "\n",
    "    def sample_commits(self, n):\n",
    "        if self.combined_df.commit_id.nunique() < n:\n",
    "            raise ValueError(f'Not enough commits to sample. Required: {n}, available: {self.combined_df.commit_id.nunique()}')\n",
    "        return self.combined_df.drop_duplicates(subset='commit_id').sample(n=n, replace=False, random_state=self.seed)\n",
    "\n",
    "    def evaluate_sampling(self, n=100, k=1000, output_dir='.', skip_existing=False, evaluation_strategy='commit', aggregation_strategy=None):\n",
    "        model_name = self.model.__class__.__name__\n",
    "        output_file = f\"{output_dir}/{model_name}_metrics.txt\"\n",
    "\n",
    "        if skip_existing and os.path.exists(output_file):\n",
    "            print(f'Output file {output_file} already exists, skipping...')\n",
    "            return\n",
    "\n",
    "        sampled_commits = self.sample_commits(n)\n",
    "\n",
    "        results = []\n",
    "        for _, row in sampled_commits.iterrows():\n",
    "            # search_results = self.model.search(row['commit_message'], row['commit_date'], ranking_depth=k)\n",
    "            # TODO: Add ChatGPT based query modification here\n",
    "            search_results = self.model.pipeline(row['commit_message'], row['commit_date'], ranking_depth=k, aggregation_method=aggregation_strategy)\n",
    "            # if evaluation_strategy == 'commit':\n",
    "            evaluation = self.eval_model.evaluate(search_results,\n",
    "                                                       self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist())\n",
    "            # elif evaluation_strategy == 'file':\n",
    "            #     evaluation = self.eval_model.evaluate_file_based(search_results,\n",
    "            #                                                       self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist(), aggregation_strategy=aggregation_strategy)\n",
    "            # else:\n",
    "            #     raise ValueError(f'Invalid evaluation strategy: {evaluation_strategy}')\n",
    "            results.append(evaluation)\n",
    "\n",
    "        avg_scores = {metric: round(np.mean([result[metric] for result in results]), 4) for metric in results[0]}\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "        with open(output_file, \"w\") as file:\n",
    "            file.write(f\"Model Name: {model_name}\\n\")\n",
    "            file.write(f\"Sample Size: {n}\\n\")\n",
    "            file.write(\"Evaluation Metrics:\\n\")\n",
    "            for key, value in avg_scores.items():\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.4861111111111111\n",
      "MRR: 0.5\n",
      "P@1: 0.0\n",
      "Recall@1: 0.0\n",
      "P@2: 0.5\n",
      "Recall@2: 0.25\n",
      "P@3: 0.3333333333333333\n",
      "Recall@3: 0.25\n",
      "P@4: 0.5\n",
      "Recall@4: 0.5\n",
      "P@5: 0.4\n",
      "Recall@5: 0.5\n",
      "P@6: 0.5\n",
      "Recall@6: 0.75\n",
      "P@7: 0.42857142857142855\n",
      "Recall@7: 0.75\n",
      "P@8: 0.375\n",
      "Recall@8: 0.75\n",
      "P@9: 0.4444444444444444\n",
      "Recall@9: 1.0\n"
     ]
    }
   ],
   "source": [
    "def tmp():\n",
    "    def calculate_recall(relevant, total_modified_files, k):\n",
    "        return sum(relevant[:k]) / total_modified_files\n",
    "\n",
    "    def calculate_average_precision(relevant):\n",
    "        pred_rel = [1] * len(relevant)\n",
    "        relevant_documents_count = 0\n",
    "        cumulative_precision = 0.0\n",
    "\n",
    "        # We iterate through the predicted relevance scores\n",
    "        for i in range(len(pred_rel)):\n",
    "            # Check if the prediction at this rank is correct (i.e., if it is a relevant document)\n",
    "            if pred_rel[i] == 1 and relevant[i] == 1:\n",
    "                relevant_documents_count += 1\n",
    "                precision_at_i = relevant_documents_count / (i + 1)\n",
    "                cumulative_precision += precision_at_i\n",
    "\n",
    "        # The average precision is the cumulative precision divided by the number of relevant documents\n",
    "        average_precision = cumulative_precision / sum(relevant)\n",
    "        return average_precision\n",
    "\n",
    "    def mean_reciprocal_rank(relevant):\n",
    "        for idx, value in enumerate(relevant):\n",
    "            if value == 1:\n",
    "                return 1 / (idx + 1)\n",
    "        return 0\n",
    "\n",
    "    def precision_at_k(relevant, k):\n",
    "        return sum(relevant[:k]) / k\n",
    "\n",
    "    rel = [0,1,0,1,0,1,0,0,1]\n",
    "    k = 3\n",
    "    # print(calculate_recall(rel, 4, k))\n",
    "    print(f'MAP: {calculate_average_precision(rel)}')\n",
    "    print(f'MRR: {mean_reciprocal_rank(rel)}')\n",
    "\n",
    "    for k in range(1, 10):\n",
    "        print(f'P@{k}: {precision_at_k(rel, k)}')\n",
    "        print(f'Recall@{k}: {calculate_recall(rel, sum(rel), k)}')\n",
    "\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = '../smalldata/fbr/index_commit_tokenized'\n",
    "repo_path = '../smalldata/fbr/'\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = get_combined_df(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../smalldata/fbr/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 8061856, 'documents': 69835, 'non_empty_documents': 69835, 'unique_terms': 14589}\n"
     ]
    }
   ],
   "source": [
    "bm25_searcher = BM25Search(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "bm25_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner                                                             facebook\n",
       "repo_name                                                            react\n",
       "commit_date                                                     1522876866\n",
       "commit_id                         27535e7bfcb63e8a4d65f273311e380b4ca12eff\n",
       "commit_message           Clarify ReactDOM's case warning for html tags ...\n",
       "file_path                packages/react-dom/src/server/ReactPartialRend...\n",
       "cur_file_content         /**\\n * Copyright (c) 2013-present, Facebook, ...\n",
       "previous_commit_id                8ec0e4a99df76c0ff1779cac4f2eaaaf35a6b5bb\n",
       "previous_file_path                                                    <NA>\n",
       "previous_file_content    /**\\n * Copyright (c) 2013-present, Facebook, ...\n",
       "diff                     @@ -955,8 +955,9 @@ class ReactDOMServerRender...\n",
       "status                                                            modified\n",
       "is_merge_request                                                     False\n",
       "file_extension                                                          js\n",
       "Name: 54170, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample a commit from the combined_df\n",
    "random_commit = combined_df.sample(1).iloc[0]\n",
    "random_commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(score=67.77170, file_path='src/renderers/dom/fiber/ReactDOMFiberEntry.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126),\n",
       " SearchResult(score=67.77170, file_path='src/renderers/dom/fiber/__tests__/ReactDOMFiber-test.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126),\n",
       " SearchResult(score=67.77170, file_path='src/renderers/dom/shared/__tests__/ReactDOMTextComponent-test.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126),\n",
       " SearchResult(score=67.77170, file_path='src/renderers/dom/shared/__tests__/ReactMount-test.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126),\n",
       " SearchResult(score=67.77169, file_path='src/renderers/shared/fiber/ReactFiberReconciler.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126),\n",
       " SearchResult(score=67.77169, file_path='src/renderers/shared/fiber/ReactFiberTreeReflection.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126),\n",
       " SearchResult(score=62.60210, file_path='src/renderers/shared/fiber/ReactChildFiber.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60210, file_path='src/renderers/shared/fiber/ReactDebugCurrentFiber.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60210, file_path='src/renderers/shared/fiber/ReactFiber.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60210, file_path='src/renderers/shared/fiber/ReactFiberBeginWork.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60210, file_path='src/renderers/shared/fiber/ReactReifiedYield.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60210, file_path='src/renderers/shared/fiber/__tests__/ReactIncrementalErrorHandling-test.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/shared/fiber/__tests__/ReactIncrementalSideEffects-test.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/shared/shared/__tests__/ReactComponent-test.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/shared/shared/__tests__/ReactStatelessComponent-test.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/shared/stack/reconciler/ReactCompositeComponent.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/shared/stack/reconciler/ReactCompositeComponentTypes.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/shared/stack/reconciler/ReactInstanceType.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/shared/stack/reconciler/ReactRef.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358),\n",
       " SearchResult(score=62.60209, file_path='src/renderers/testing/__tests__/ReactTestRenderer-test.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get search results for the random commit\n",
    "search_results = bm25_searcher.search(random_commit['commit_message'], random_commit['commit_date'], ranking_depth=K)\n",
    "search_results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AggregatedSearchResult(file_path='src/renderers/shared/fiber/ReactChildFiber.js', score=245.6372833251953, contributing_results=[SearchResult(score=62.60210, file_path='src/renderers/shared/fiber/ReactChildFiber.js', commit_id='90294ead4c627715cb70f20ff448bb0d34ee4c1b', commit_date=1484003358), SearchResult(score=50.19460, file_path='src/renderers/shared/fiber/ReactChildFiber.js', commit_id='309412d8b6d6ed0cfbb50a6be2f1a99cca36fea5', commit_date=1499967692), SearchResult(score=47.07320, file_path='src/renderers/shared/fiber/ReactChildFiber.js', commit_id='3e48422fdc2feaf741d768a6071a87adf2fc107b', commit_date=1492614304), SearchResult(score=43.78109, file_path='src/renderers/shared/fiber/ReactChildFiber.js', commit_id='e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date=1481819818), SearchResult(score=41.98630, file_path='src/renderers/shared/fiber/ReactChildFiber.js', commit_id='3b43f3190bbcc5d4cfe77dbc665e03b7b95d35b2', commit_date=1499899781)]),\n",
       " AggregatedSearchResult(file_path='scripts/rollup/bundles.js', score=245.63184356689453, contributing_results=[SearchResult(score=59.56300, file_path='scripts/rollup/bundles.js', commit_id='4b2eac3de7e1dbf5c2dd742fd9989974a83972cb', commit_date=1491407249), SearchResult(score=50.09090, file_path='scripts/rollup/bundles.js', commit_id='111731dedd1c8ab2e3033f6ebdac45cb1941cbf7', commit_date=1507746566), SearchResult(score=47.62730, file_path='scripts/rollup/bundles.js', commit_id='411e04bd71715298de564a24dbf0304e17471ce0', commit_date=1498368702), SearchResult(score=45.87980, file_path='scripts/rollup/bundles.js', commit_id='8af729231b2e380f7d0f602109c7d6ac632a503d', commit_date=1495058039), SearchResult(score=42.47084, file_path='scripts/rollup/bundles.js', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/shared/__tests__/ReactDOMServerIntegration-test.js', score=213.02236557006836, contributing_results=[SearchResult(score=57.51410, file_path='src/renderers/dom/shared/__tests__/ReactDOMServerIntegration-test.js', commit_id='8d61138186e79e1e719786c8c76186e64b603bd5', commit_date=1498790864), SearchResult(score=55.81689, file_path='src/renderers/dom/shared/__tests__/ReactDOMServerIntegration-test.js', commit_id='2999811b07a5eb9e04680457a0b56195abd7e08c', commit_date=1502812844), SearchResult(score=52.06409, file_path='src/renderers/dom/shared/__tests__/ReactDOMServerIntegration-test.js', commit_id='c1dbc8e4708d1de49d8e6a22e9682987d08a15c7', commit_date=1504142935), SearchResult(score=47.62729, file_path='src/renderers/dom/shared/__tests__/ReactDOMServerIntegration-test.js', commit_id='411e04bd71715298de564a24dbf0304e17471ce0', commit_date=1498368702)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/fiber/ReactDOMFiberComponent.js', score=209.1761932373047, contributing_results=[SearchResult(score=57.51410, file_path='src/renderers/dom/fiber/ReactDOMFiberComponent.js', commit_id='8d61138186e79e1e719786c8c76186e64b603bd5', commit_date=1498790864), SearchResult(score=55.81690, file_path='src/renderers/dom/fiber/ReactDOMFiberComponent.js', commit_id='2999811b07a5eb9e04680457a0b56195abd7e08c', commit_date=1502812844), SearchResult(score=52.06410, file_path='src/renderers/dom/fiber/ReactDOMFiberComponent.js', commit_id='c1dbc8e4708d1de49d8e6a22e9682987d08a15c7', commit_date=1504142935), SearchResult(score=43.78110, file_path='src/renderers/dom/fiber/ReactDOMFiberComponent.js', commit_id='e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date=1481819818)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/shared/hooks/ReactDOMUnknownPropertyHook.js', score=197.54186248779297, contributing_results=[SearchResult(score=55.81689, file_path='src/renderers/dom/shared/hooks/ReactDOMUnknownPropertyHook.js', commit_id='2999811b07a5eb9e04680457a0b56195abd7e08c', commit_date=1502812844), SearchResult(score=52.06409, file_path='src/renderers/dom/shared/hooks/ReactDOMUnknownPropertyHook.js', commit_id='c1dbc8e4708d1de49d8e6a22e9682987d08a15c7', commit_date=1504142935), SearchResult(score=45.87980, file_path='src/renderers/dom/shared/hooks/ReactDOMUnknownPropertyHook.js', commit_id='8af729231b2e380f7d0f602109c7d6ac632a503d', commit_date=1495058039), SearchResult(score=43.78109, file_path='src/renderers/dom/shared/hooks/ReactDOMUnknownPropertyHook.js', commit_id='e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date=1481819818)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/shared/fiber/ReactFiberReconciler.js', score=169.06686401367188, contributing_results=[SearchResult(score=67.77169, file_path='src/renderers/shared/fiber/ReactFiberReconciler.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126), SearchResult(score=57.51409, file_path='src/renderers/shared/fiber/ReactFiberReconciler.js', commit_id='8d61138186e79e1e719786c8c76186e64b603bd5', commit_date=1498790864), SearchResult(score=43.78108, file_path='src/renderers/shared/fiber/ReactFiberReconciler.js', commit_id='e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date=1481819818)]),\n",
       " AggregatedSearchResult(file_path='scripts/rollup/modules.js', score=156.56299591064453, contributing_results=[SearchResult(score=59.56300, file_path='scripts/rollup/modules.js', commit_id='4b2eac3de7e1dbf5c2dd742fd9989974a83972cb', commit_date=1491407249), SearchResult(score=50.09090, file_path='scripts/rollup/modules.js', commit_id='111731dedd1c8ab2e3033f6ebdac45cb1941cbf7', commit_date=1507746566), SearchResult(score=46.90910, file_path='scripts/rollup/modules.js', commit_id='964c263d8fc6e7119f5773f7fa857c9985db40cf', commit_date=1495557342)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/shared/HTMLDOMPropertyConfig.js', score=155.32838439941406, contributing_results=[SearchResult(score=55.81689, file_path='src/renderers/dom/shared/HTMLDOMPropertyConfig.js', commit_id='2999811b07a5eb9e04680457a0b56195abd7e08c', commit_date=1502812844), SearchResult(score=52.06409, file_path='src/renderers/dom/shared/HTMLDOMPropertyConfig.js', commit_id='c1dbc8e4708d1de49d8e6a22e9682987d08a15c7', commit_date=1504142935), SearchResult(score=47.44740, file_path='src/renderers/dom/shared/HTMLDOMPropertyConfig.js', commit_id='29d9710892f773b4d5081f1b77820d40d79f69d3', commit_date=1490632758)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/shared/__tests__/ReactDOMComponent-test.js', score=151.66207885742188, contributing_results=[SearchResult(score=55.81689, file_path='src/renderers/dom/shared/__tests__/ReactDOMComponent-test.js', commit_id='2999811b07a5eb9e04680457a0b56195abd7e08c', commit_date=1502812844), SearchResult(score=52.06409, file_path='src/renderers/dom/shared/__tests__/ReactDOMComponent-test.js', commit_id='c1dbc8e4708d1de49d8e6a22e9682987d08a15c7', commit_date=1504142935), SearchResult(score=43.78109, file_path='src/renderers/dom/shared/__tests__/ReactDOMComponent-test.js', commit_id='e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date=1481819818)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/shared/hooks/ReactDOMInvalidARIAHook.js', score=151.66207122802734, contributing_results=[SearchResult(score=55.81689, file_path='src/renderers/dom/shared/hooks/ReactDOMInvalidARIAHook.js', commit_id='2999811b07a5eb9e04680457a0b56195abd7e08c', commit_date=1502812844), SearchResult(score=52.06409, file_path='src/renderers/dom/shared/hooks/ReactDOMInvalidARIAHook.js', commit_id='c1dbc8e4708d1de49d8e6a22e9682987d08a15c7', commit_date=1504142935), SearchResult(score=43.78109, file_path='src/renderers/dom/shared/hooks/ReactDOMInvalidARIAHook.js', commit_id='e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date=1481819818)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/stack/client/ReactDOMComponent.js', score=151.66205978393555, contributing_results=[SearchResult(score=55.81688, file_path='src/renderers/dom/stack/client/ReactDOMComponent.js', commit_id='2999811b07a5eb9e04680457a0b56195abd7e08c', commit_date=1502812844), SearchResult(score=52.06409, file_path='src/renderers/dom/stack/client/ReactDOMComponent.js', commit_id='c1dbc8e4708d1de49d8e6a22e9682987d08a15c7', commit_date=1504142935), SearchResult(score=43.78109, file_path='src/renderers/dom/stack/client/ReactDOMComponent.js', commit_id='e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date=1481819818)]),\n",
       " AggregatedSearchResult(file_path='scripts/circleci/test_entry_point.sh', score=148.13032913208008, contributing_results=[SearchResult(score=59.56303, file_path='scripts/circleci/test_entry_point.sh', commit_id='4b2eac3de7e1dbf5c2dd742fd9989974a83972cb', commit_date=1491407249), SearchResult(score=46.09644, file_path='scripts/circleci/test_entry_point.sh', commit_id='6041f481b7851d75649630eea489628d399cc3cf', commit_date=1511355746), SearchResult(score=42.47086, file_path='scripts/circleci/test_entry_point.sh', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098)]),\n",
       " AggregatedSearchResult(file_path='scripts/jest/preprocessor.js', score=148.13030624389648, contributing_results=[SearchResult(score=59.56302, file_path='scripts/jest/preprocessor.js', commit_id='4b2eac3de7e1dbf5c2dd742fd9989974a83972cb', commit_date=1491407249), SearchResult(score=46.09644, file_path='scripts/jest/preprocessor.js', commit_id='6041f481b7851d75649630eea489628d399cc3cf', commit_date=1511355746), SearchResult(score=42.47085, file_path='scripts/jest/preprocessor.js', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/shared/stack/reconciler/ReactChildReconciler.js', score=144.33349609375, contributing_results=[SearchResult(score=52.84030, file_path='src/renderers/shared/stack/reconciler/ReactChildReconciler.js', commit_id='83cbc3e5fb700f45c48214c3785d8b44ab5ebdce', commit_date=1466630886), SearchResult(score=49.50690, file_path='src/renderers/shared/stack/reconciler/ReactChildReconciler.js', commit_id='f33f03e3572d11e6810f4ce110eb3af97cbd24a8', commit_date=1476551636), SearchResult(score=41.98630, file_path='src/renderers/shared/stack/reconciler/ReactChildReconciler.js', commit_id='3b43f3190bbcc5d4cfe77dbc665e03b7b95d35b2', commit_date=1499899781)]),\n",
       " AggregatedSearchResult(file_path='packages/react-dom/src/__tests__/ReactDOMServerIntegration-test.js', score=135.62417221069336, contributing_results=[SearchResult(score=47.05680, file_path='packages/react-dom/src/__tests__/ReactDOMServerIntegration-test.js', commit_id='4ce5da7aee90a373f2f36d1beb559097af30952e', commit_date=1509411160), SearchResult(score=46.09648, file_path='packages/react-dom/src/__tests__/ReactDOMServerIntegration-test.js', commit_id='6041f481b7851d75649630eea489628d399cc3cf', commit_date=1511355746), SearchResult(score=42.47089, file_path='packages/react-dom/src/__tests__/ReactDOMServerIntegration-test.js', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098)]),\n",
       " AggregatedSearchResult(file_path='packages/react/src/__tests__/ReactJSXElementValidator-test.js', score=135.6240997314453, contributing_results=[SearchResult(score=47.05679, file_path='packages/react/src/__tests__/ReactJSXElementValidator-test.js', commit_id='4ce5da7aee90a373f2f36d1beb559097af30952e', commit_date=1509411160), SearchResult(score=46.09645, file_path='packages/react/src/__tests__/ReactJSXElementValidator-test.js', commit_id='6041f481b7851d75649630eea489628d399cc3cf', commit_date=1511355746), SearchResult(score=42.47086, file_path='packages/react/src/__tests__/ReactJSXElementValidator-test.js', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098)]),\n",
       " AggregatedSearchResult(file_path='packages/react-dom/src/server/ReactPartialRenderer.js', score=133.43796157836914, contributing_results=[SearchResult(score=47.05680, file_path='packages/react-dom/src/server/ReactPartialRenderer.js', commit_id='4ce5da7aee90a373f2f36d1beb559097af30952e', commit_date=1509411160), SearchResult(score=43.91029, file_path='packages/react-dom/src/server/ReactPartialRenderer.js', commit_id='97e2911508a2a7af6f50cf87ae503abe39842bef', commit_date=1516383406), SearchResult(score=42.47087, file_path='packages/react-dom/src/server/ReactPartialRenderer.js', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098)]),\n",
       " AggregatedSearchResult(file_path='packages/react-dom/src/__tests__/ReactServerRendering-test.js', score=132.47764205932617, contributing_results=[SearchResult(score=46.09647, file_path='packages/react-dom/src/__tests__/ReactServerRendering-test.js', commit_id='6041f481b7851d75649630eea489628d399cc3cf', commit_date=1511355746), SearchResult(score=43.91029, file_path='packages/react-dom/src/__tests__/ReactServerRendering-test.js', commit_id='97e2911508a2a7af6f50cf87ae503abe39842bef', commit_date=1516383406), SearchResult(score=42.47088, file_path='packages/react-dom/src/__tests__/ReactServerRendering-test.js', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098)]),\n",
       " AggregatedSearchResult(file_path='packages/react-dom/src/__tests__/ReactDOMComponent-test.js', score=129.75738143920898, contributing_results=[SearchResult(score=46.09649, file_path='packages/react-dom/src/__tests__/ReactDOMComponent-test.js', commit_id='6041f481b7851d75649630eea489628d399cc3cf', commit_date=1511355746), SearchResult(score=42.47090, file_path='packages/react-dom/src/__tests__/ReactDOMComponent-test.js', commit_id='fa7a97fc46935e1611d52da2fdb7d53f6ab9577d', commit_date=1511459098), SearchResult(score=41.18999, file_path='packages/react-dom/src/__tests__/ReactDOMComponent-test.js', commit_id='d289d4b634749861199556e42174a3f4a3ce2b16', commit_date=1515092250)]),\n",
       " AggregatedSearchResult(file_path='src/renderers/dom/fiber/ReactDOMFiberEntry.js', score=125.28579711914062, contributing_results=[SearchResult(score=67.77170, file_path='src/renderers/dom/fiber/ReactDOMFiberEntry.js', commit_id='1f74eca9937ad6f19b6291d21edfb8747bae88ca', commit_date=1500652126), SearchResult(score=57.51410, file_path='src/renderers/dom/fiber/ReactDOMFiberEntry.js', commit_id='8d61138186e79e1e719786c8c76186e64b603bd5', commit_date=1498790864)])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_results = bm25_searcher.aggregate_file_scores(search_results)\n",
    "aggregated_results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 428)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results), len(aggregated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0097,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.0,\n",
       " 'P@1000': 0.008,\n",
       " 'MRR': 0.0035,\n",
       " 'Recall@100': 0.0,\n",
       " 'Recall@1000': 0.75}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the search results\n",
    "evaluation = evaluator.evaluate(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0052,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.0,\n",
       " 'P@1000': 0.003,\n",
       " 'MRR': 0.003,\n",
       " 'Recall@100': 0.0,\n",
       " 'Recall@1000': 0.75}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_based_evaluation = evaluator.evaluate(aggregated_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "file_based_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Search Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_full = bm25_searcher.search_full(random_commit['commit_message'], random_commit['commit_date'], ranking_depth=K)\n",
    "# search_full[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated_results_full = bm25_searcher.aggregate_file_scores(search_full)\n",
    "# aggregated_results_full[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(search_full), len(aggregated_results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_full = evaluator.evaluate(search_full, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "# evaluation_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_based_evaluation_full = evaluator.evaluate(aggregated_results_full, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "# file_based_evaluation_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Strategy: sump\n",
      "{'MAP': 0.0674, 'P@10': 0.0, 'P@100': 0.03, 'P@1000': 0.003, 'MRR': 0.0588, 'Recall@100': 0.75, 'Recall@1000': 0.75}\n",
      "Aggregation Strategy: maxp\n",
      "{'MAP': 0.0062, 'P@10': 0.0, 'P@100': 0.0, 'P@1000': 0.003, 'MRR': 0.004, 'Recall@100': 0.0, 'Recall@1000': 0.75}\n",
      "Aggregation Strategy: avgp\n",
      "{'MAP': 0.0052, 'P@10': 0.0, 'P@100': 0.0, 'P@1000': 0.003, 'MRR': 0.003, 'Recall@100': 0.0, 'Recall@1000': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# test different aggregation strategies in bm25 search\n",
    "agggreagtion_strategies = ['sump', 'maxp', 'avgp']\n",
    "for strategy in agggreagtion_strategies:\n",
    "    print(f\"Aggregation Strategy: {strategy}\")\n",
    "    aggregated_results = bm25_searcher.aggregate_file_scores(search_results, aggregation_method=strategy)\n",
    "    evaluation = evaluator.evaluate(aggregated_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "    print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.1794,\n",
       " 'P@10': 0.079,\n",
       " 'P@100': 0.0327,\n",
       " 'P@1000': 0.0084,\n",
       " 'MRR': 0.2676,\n",
       " 'Recall@100': 0.4512,\n",
       " 'Recall@1000': 0.6351}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_evaluator.evaluate_sampling(n=100, k=K, output_dir='../tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Strategy: sump\n",
      "{'MAP': 0.2037, 'P@10': 0.074, 'P@100': 0.0202, 'P@1000': 0.0027, 'MRR': 0.2746, 'Recall@100': 0.4826, 'Recall@1000': 0.6351}\n",
      "Aggregation Strategy: maxp\n",
      "{'MAP': 0.2482, 'P@10': 0.06, 'P@100': 0.0181, 'P@1000': 0.0027, 'MRR': 0.2688, 'Recall@100': 0.4629, 'Recall@1000': 0.6351}\n",
      "Aggregation Strategy: avgp\n",
      "{'MAP': 0.1773, 'P@10': 0.048, 'P@100': 0.0149, 'P@1000': 0.0027, 'MRR': 0.1968, 'Recall@100': 0.4011, 'Recall@1000': 0.6351}\n"
     ]
    }
   ],
   "source": [
    "for strategy in agggreagtion_strategies:\n",
    "    print(f\"Aggregation Strategy: {strategy}\")\n",
    "    results = bm25_evaluator.evaluate_sampling(n=100, k=K, output_dir='../tmp/', evaluation_strategy='file', aggregation_strategy=strategy)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_based_evaluation = evaluator.evaluate_file_based(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "# file_based_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate over the list of aggregation strategies and evaluate each one\n",
    "\n",
    "# aggregation_strategies = ['sump', 'maxp', 'firstp', 'avgp']\n",
    "# for strategy in aggregation_strategies:\n",
    "#     file_based_evaluation = evaluator.evaluate_file_based(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist(), aggregation_strategy=strategy)\n",
    "#     print(f\"{strategy}: {file_based_evaluation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do the same for the model evaluator\n",
    "# for strategy in aggregation_strategies:\n",
    "#     model_evaluation = bm25_evaluator.evaluate_sampling(n=1000, k=K, output_dir='../tmp/', evaluation_strategy='file', aggregation_strategy=strategy)\n",
    "#     print(f\"{strategy}: {model_evaluation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class BERTReRanker:\n",
    "    \"\"\"\n",
    "    A class for performing reranking with a BERT-based model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, psgLen=128, psgStride=64, psgCnt=None, scoreAggregation='maxp', batchSize=8):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        # Passage handling parameters\n",
    "        self.psgLen = psgLen\n",
    "        self.psgStride = psgStride\n",
    "        self.psgCnt = psgCnt\n",
    "        self.scoreAggregation = scoreAggregation\n",
    "\n",
    "        if self.scoreAggregation == 'firstp':\n",
    "            self.psgCnt = 1\n",
    "\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "    def rerank(self, query, search_results):\n",
    "        \"\"\"\n",
    "        Rerank the search results using the BERT model.\n",
    "\n",
    "        query: The query string.\n",
    "        search_results: A list of SearchResult objects.\n",
    "        \"\"\"\n",
    "        reranked_results = []\n",
    "        # todo - add batching\n",
    "\n",
    "        # Process each SearchResult to create input for BERT\n",
    "        for result in search_results:\n",
    "            passages = self._split_into_passages(query, result.commit_msg)\n",
    "\n",
    "            # Score each passage with BERT\n",
    "            passage_scores = [self._score_passage(query, passage) for passage in passages]\n",
    "\n",
    "            # Aggregate passage scores to get a single document score\n",
    "            doc_score = self._aggregate_scores(passage_scores)\n",
    "\n",
    "            # Create a new SearchResult with the updated score\n",
    "            reranked_results.append((doc_score, result))\n",
    "\n",
    "        # Sort reranked results by the new score\n",
    "        reranked_results.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [result for _, result in reranked_results]\n",
    "\n",
    "    def _split_into_passages(self, query, commit_msg):\n",
    "        # Tokenize the query and commit message\n",
    "        tokens = self.tokenizer.tokenize(query) + self.tokenizer.tokenize(commit_msg)\n",
    "\n",
    "        # Split the tokens into passages\n",
    "        passages = []\n",
    "        for i in range(0, len(tokens), self.psgStride):\n",
    "            passage = tokens[i:i+self.psgLen]\n",
    "            passages.append(self.tokenizer.convert_tokens_to_string(passage))\n",
    "            if self.psgCnt and len(passages) >= self.psgCnt:\n",
    "                break\n",
    "        return passages\n",
    "\n",
    "    def _score_passage(self, query, passage):\n",
    "        # Encode query and passage for BERT\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            passage,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.psgLen,\n",
    "            truncation=\"only_second\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Score the (query, passage) pair with BERT\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            score = outputs.logits.squeeze().item()\n",
    "        return score\n",
    "\n",
    "    def _aggregate_scores(self, passage_scores):\n",
    "        # Aggregate passage scores based on the specified strategy\n",
    "        if self.scoreAggregation == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        elif self.scoreAggregation == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        elif self.scoreAggregation == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        elif self.scoreAggregation == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid score aggregation method: {self.scoreAggregation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Assuming bm25_searcher is an instance of BM25Search\n",
    "query = \"Fix login page error\"\n",
    "query_date = 1699261235\n",
    "bm25_results = bm25_search.search(query, query_date, K)\n",
    "\n",
    "# Now rerank those results with BERT\n",
    "bert_reranker = BERTReRanker(model_name=\"bert-base-uncased\")\n",
    "reranked_results = bert_reranker.rerank(query, bm25_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(score: 2.69137, file_path: 'packages/react-reconciler/src/ReactFiberBeginWork.new.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/ReactFiberBeginWork.old.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/ReactFiberSuspenseComponent.new.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/ReactFiberSuspenseComponent.old.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/__tests__/ReactCPUSuspense-test.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom-bindings/src/client/ReactFiberConfigDOM.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom-bindings/src/server/ReactFizzConfigDOM.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom-bindings/src/server/ReactFizzConfigDOMLegacy.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom/src/__tests__/ReactDOMFizzServer-test.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-reconciler/src/ReactFiberConfigWithNoHydration.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 results\n",
    "reranked_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.016,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.0,\n",
       " 'P@1000': 0.016,\n",
       " 'MRR': 0.0031,\n",
       " 'Recall@1000': 0.3913}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the reranked results\n",
    "evaluation = evaluator.evaluate(reranked_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'packages/react-reconciler/src/ReactFiberCompleteWork.js': [2.718986988067627, 3.8634390830993652, 3.863529920578003, 4.273975372314453], 'packages/shared/ReactSymbols.js': [3.5164780616760254], 'packages/shared/ReactTypes.js': [3.6565749645233154, 3.8633739948272705, 3.863521099090576], 'scripts/rollup/bundles.js': [4.612883567810059, 5.564578056335449, 2.7962939739227295], 'packages/react-dom/src/shared/assertValidProps.js': [5.564676284790039], 'packages/react-dom/src/events/DOMEventResponderSystem.js': [3.8635449409484863], 'packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js': [3.86354398727417], 'packages/react-events/src/FocusScope.js': [3.863539934158325], 'packages/shared/getComponentName.js': [3.8635189533233643]})\n",
      "[('packages/react-reconciler/src/ReactFiberCompleteWork.js', 14.719931364059448), ('scripts/rollup/bundles.js', 12.973755598068237), ('packages/shared/ReactTypes.js', 11.383470058441162), ('packages/react-dom/src/shared/assertValidProps.js', 5.564676284790039), ('packages/react-dom/src/events/DOMEventResponderSystem.js', 3.8635449409484863), ('packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js', 3.86354398727417), ('packages/react-events/src/FocusScope.js', 3.863539934158325), ('packages/shared/getComponentName.js', 3.8635189533233643), ('packages/shared/ReactSymbols.js', 3.5164780616760254)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.3913,\n",
       " 'P@10': 0.9,\n",
       " 'P@100': 0.09,\n",
       " 'P@1000': 0.009,\n",
       " 'MRR': 1.0,\n",
       " 'Recall@1000': 0.3913}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = evaluator.evaluate_file_based(reranked_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'MAP': 0.0546,\n",
    " 'P@10': 0.0,\n",
    " 'P@100': 0.05,\n",
    " 'P@1000': 0.019,\n",
    " 'MRR': 0.0119,\n",
    " 'Recall@1000': 0.3478}\n",
    "\n",
    "{'MAP': 0.3478,\n",
    " 'P@10': 0.8,\n",
    " 'P@100': 0.08,\n",
    " 'P@1000': 0.008,\n",
    " 'MRR': 1.0,\n",
    " 'Recall@1000': 0.3478}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
