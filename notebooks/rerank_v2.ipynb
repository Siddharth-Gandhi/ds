{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "from utils import count_commits, get_combined_df, tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResult:\n",
    "    def __init__(self, commit_id, file_path, score, commit_date):\n",
    "        self.commit_id = commit_id\n",
    "        self.file_path = file_path\n",
    "        self.score = score\n",
    "        self.commit_date = commit_date\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        # return f\"{self.file_path} {self.score:.5f} {self.commit_date}\"\n",
    "        return f\"{class_name}(score: {self.score:.5f}, file_path: {self.file_path!r}, commit_id: {self.commit_id!r}, commit_date: {self.commit_date})\"\n",
    "\n",
    "    def is_actual_modified(self, actual_modified_files):\n",
    "        return self.file_path in actual_modified_files\n",
    "\n",
    "    @staticmethod\n",
    "    def print_results(query, search_results, show_only_actual_modified=False):\n",
    "        actual_modified_files = query['actual_files_modified']\n",
    "        for i, result in enumerate(search_results):\n",
    "            if show_only_actual_modified and not result.is_actual_modified(actual_modified_files):\n",
    "                continue\n",
    "            print(f\"{i+1:2} {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Search:\n",
    "    def __init__(self, index_path):\n",
    "        if not os.path.exists(index_path):\n",
    "            raise FileNotFoundError(f\"Index at {index_path} does not exist!\")\n",
    "        self.searcher = LuceneSearcher(index_path)\n",
    "        # self.ranking_depth = ranking_depth\n",
    "\n",
    "    def search(self, query, query_date, ranking_depth):\n",
    "        # TODO maybe change this to mean returning reranking_depths total results instead of being pruned by the query date\n",
    "        hits = self.searcher.search(tokenize(query), ranking_depth)\n",
    "        unix_date = query_date\n",
    "        filtered_hits = [\n",
    "            SearchResult(hit.docid, json.loads(hit.raw)['file_path'], hit.score, int(json.loads(hit.raw)[\"commit_date\"]))\n",
    "            for hit in hits if int(json.loads(hit.raw)[\"commit_date\"]) < unix_date\n",
    "        ]\n",
    "        return filtered_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SearchEvaluator:\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, k):\n",
    "        return sum(relevant[:k]) / k\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(relevant):\n",
    "        for idx, value in enumerate(relevant):\n",
    "            if value == 1:\n",
    "                return 1 / (idx + 1)\n",
    "        return 0\n",
    "\n",
    "    def evaluate(self, search_results, actual_modified_files):\n",
    "        retrieved_files = [result.file_path for result in search_results]\n",
    "        relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "\n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric == 'MAP':\n",
    "                evaluations[metric] = average_precision_score(relevant, [1]*len(relevant)) if any(relevant) else 0\n",
    "            elif metric == 'MRR':\n",
    "                evaluations[metric] = self.mean_reciprocal_rank(relevant)\n",
    "            elif metric.startswith('P@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.precision_at_k(relevant, k)\n",
    "            elif metric.startswith('Recall@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = len(\n",
    "                    {\n",
    "                        file\n",
    "                        for idx, file in enumerate(retrieved_files)\n",
    "                        if relevant[idx] == 1\n",
    "                    }\n",
    "                ) / len(actual_modified_files)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}\n",
    "\n",
    "    def evaluate_file_based(self, search_results, actual_modified_files):\n",
    "        file_relevance = defaultdict(int)\n",
    "\n",
    "        # Aggregate relevance scores for each file across all commits\n",
    "        for result in search_results:\n",
    "            if result.file_path in actual_modified_files:\n",
    "                file_relevance[result.file_path] += 1\n",
    "\n",
    "        # Normalize relevance scores based on occurrences in actual modified files\n",
    "        max_relevance = max(file_relevance.values(), default=1)\n",
    "        normalized_relevance = {file: relevance / max_relevance for file, relevance in file_relevance.items()}\n",
    "        sorted_normalized_relevance = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric.startswith('P@'):\n",
    "                # Compute precision at k for files, not individual commit mentions\n",
    "                k = int(metric.split('@')[1])\n",
    "                # top_k_files = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "                top_k_files = sorted_normalized_relevance[:k]\n",
    "                precision_at_k = sum(1 for file, relevance in top_k_files if file in actual_modified_files) / k\n",
    "                evaluations[metric] = precision_at_k\n",
    "            elif metric.startswith('Recall@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                # top_k_files = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "                top_k_files = sorted_normalized_relevance[:k]\n",
    "                recall_at_k = sum(1 for file, relevance in top_k_files if file in actual_modified_files) / len(actual_modified_files)\n",
    "                evaluations[metric] = recall_at_k\n",
    "            elif metric == 'MAP':\n",
    "                # Compute average precision for files, not individual commit mentions\n",
    "                average_precision = 0\n",
    "                num_relevant_files = 0\n",
    "                for idx, (file, relevance) in enumerate(sorted_normalized_relevance):\n",
    "                    if file in actual_modified_files:\n",
    "                        num_relevant_files += 1\n",
    "                        average_precision += num_relevant_files / (idx + 1)\n",
    "                average_precision /= len(actual_modified_files)\n",
    "                evaluations[metric] = average_precision\n",
    "            elif metric == 'MRR':\n",
    "                # Compute mean reciprocal rank for files, not individual commit mentions\n",
    "                reciprocal_rank = 0\n",
    "                for idx, (file, relevance) in enumerate(sorted_normalized_relevance):\n",
    "                    if file in actual_modified_files:\n",
    "                        reciprocal_rank = 1 / (idx + 1)\n",
    "                        break\n",
    "                evaluations[metric] = reciprocal_rank\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = '../smalldata/fbr/index_commit_tokenized'\n",
    "repo_path = '../smalldata/fbr/'\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_search = BM25Search(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', f'Recall@{K}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = SearchEvaluator(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, eval_model, combined_df, seed=42):\n",
    "        self.model = model\n",
    "        self.eval_model = eval_model\n",
    "        self.combined_df = combined_df\n",
    "        self.seed = seed\n",
    "\n",
    "    def sample_commits(self, n):\n",
    "        if self.combined_df.commit_id.nunique() < n:\n",
    "            raise ValueError(f'Not enough commits to sample. Required: {n}, available: {self.combined_df.commit_id.nunique()}')\n",
    "        return self.combined_df.drop_duplicates(subset='commit_id').sample(n=n, replace=False, random_state=self.seed)\n",
    "\n",
    "    def evaluate_sampling(self, n=100, k=1000, output_dir='.', skip_existing=False, evaluation_strategy='commit'):\n",
    "        model_name = self.model.__class__.__name__\n",
    "        output_file = f\"{output_dir}/{model_name}_metrics.txt\"\n",
    "\n",
    "        if skip_existing and os.path.exists(output_file):\n",
    "            print(f'Output file {output_file} already exists, skipping...')\n",
    "            return\n",
    "\n",
    "        sampled_commits = self.sample_commits(n)\n",
    "\n",
    "        results = []\n",
    "        for _, row in sampled_commits.iterrows():\n",
    "            search_results = self.model.search(row['commit_message'], row['commit_date'], ranking_depth=k)\n",
    "            if evaluation_strategy == 'commit':\n",
    "                evaluation = self.eval_model.evaluate(search_results,\n",
    "                                                       self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist())\n",
    "            elif evaluation_strategy == 'file':\n",
    "                evaluation = self.eval_model.evaluate_file_based(search_results,\n",
    "                                                                  self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist())\n",
    "            else:\n",
    "                raise ValueError(f'Invalid evaluation strategy: {evaluation_strategy}')\n",
    "            results.append(evaluation)\n",
    "\n",
    "        avg_scores = {metric: round(np.mean([result[metric] for result in results]), 4) for metric in results[0]}\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "        with open(output_file, \"w\") as file:\n",
    "            file.write(f\"Model Name: {model_name}\\n\")\n",
    "            file.write(f\"Sample Size: {n}\\n\")\n",
    "            file.write(\"Evaluation Metrics:\\n\")\n",
    "            for key, value in avg_scores.items():\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = get_combined_df(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_evaluator = ModelEvaluator(bm25_search, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0427,\n",
       " 'P@10': 0.079,\n",
       " 'P@100': 0.0327,\n",
       " 'P@1000': 0.0084,\n",
       " 'MRR': 0.2676,\n",
       " 'Recall@1000': 0.6351}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_evaluator.evaluate_sampling(n=100, k=K, output_dir='../tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0427,\n",
       " 'P@10': 0.079,\n",
       " 'P@100': 0.0327,\n",
       " 'P@1000': 0.0084,\n",
       " 'MRR': 0.2676,\n",
       " 'Recall@1000': 0.6351}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_evaluator.evaluate_sampling(n=100, k=K, output_dir='../tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.6351,\n",
       " 'P@10': 0.242,\n",
       " 'P@100': 0.0269,\n",
       " 'P@1000': 0.0027,\n",
       " 'MRR': 0.76,\n",
       " 'Recall@1000': 0.6351}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_evaluator.evaluate_sampling(n=100, k=K, output_dir='../tmp/', evaluation_strategy='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner                                                             facebook\n",
       "repo_name                                                            react\n",
       "commit_date                                                     1482357845\n",
       "commit_id                         a27e4f3361caf6461ef51a71855903578604ace0\n",
       "commit_message           [Fiber] Make requestIdleCallback() and request...\n",
       "file_path                                      scripts/jest/environment.js\n",
       "cur_file_content         /* eslint-disable */\\nglobal.__DEV__ = true;\\n...\n",
       "previous_commit_id                2a5fe4c2b021ad9a67b904aceed6bb10fe160a79\n",
       "previous_file_path                                                    <NA>\n",
       "previous_file_content    /* eslint-disable */\\nglobal.__DEV__ = true;\\n...\n",
       "diff                     @@ -1,11 +1,13 @@\\n /* eslint-disable */\\n glo...\n",
       "status                                                            modified\n",
       "is_merge_request                                                     False\n",
       "file_extension                                                          js\n",
       "Name: 63779, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample a commit from the combined_df\n",
    "random_commit = combined_df.sample(1).iloc[0]\n",
    "random_commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(score: 93.78740, file_path: 'src/renderers/art/ReactARTFiber.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78740, file_path: 'src/renderers/dom/fiber/ReactDOMFiber.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78740, file_path: 'src/renderers/dom/fiber/ReactDOMFiberComponent.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78740, file_path: 'src/renderers/dom/fiber/__tests__/ReactDOMFiber-test.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/dom/shared/__tests__/ReactDOMSVG-test.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/noop/ReactNoop.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/shared/fiber/ReactFiberBeginWork.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/shared/fiber/ReactFiberCommitWork.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/shared/fiber/ReactFiberCompleteWork.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/shared/fiber/ReactFiberHostContext.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/shared/fiber/ReactFiberReconciler.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 93.78739, file_path: 'src/renderers/shared/fiber/ReactFiberScheduler.js', commit_id: 'c87ffc0bebaabe69dfbd7b385480da614c5dc0da', commit_date: 1481231447),\n",
       " SearchResult(score: 92.10130, file_path: 'src/renderers/dom/fiber/ReactDOMFiber.js', commit_id: '6144212a8634948faf18cce8211c71e6f9d0667e', commit_date: 1473810392),\n",
       " SearchResult(score: 92.10130, file_path: 'src/renderers/noop/ReactNoop.js', commit_id: '6144212a8634948faf18cce8211c71e6f9d0667e', commit_date: 1473810392),\n",
       " SearchResult(score: 92.10130, file_path: 'src/renderers/shared/fiber/ReactFiberBeginWork.js', commit_id: '6144212a8634948faf18cce8211c71e6f9d0667e', commit_date: 1473810392),\n",
       " SearchResult(score: 92.10130, file_path: 'src/renderers/shared/fiber/ReactFiberReconciler.js', commit_id: '6144212a8634948faf18cce8211c71e6f9d0667e', commit_date: 1473810392),\n",
       " SearchResult(score: 92.10130, file_path: 'src/renderers/shared/fiber/ReactFiberScheduler.js', commit_id: '6144212a8634948faf18cce8211c71e6f9d0667e', commit_date: 1473810392),\n",
       " SearchResult(score: 92.10130, file_path: 'src/renderers/shared/fiber/__tests__/ReactIncremental-test.js', commit_id: '6144212a8634948faf18cce8211c71e6f9d0667e', commit_date: 1473810392),\n",
       " SearchResult(score: 92.10130, file_path: 'src/renderers/shared/fiber/__tests__/ReactIncrementalSideEffects-test.js', commit_id: '6144212a8634948faf18cce8211c71e6f9d0667e', commit_date: 1473810392),\n",
       " SearchResult(score: 86.52950, file_path: 'src/isomorphic/classic/element/__tests__/ReactElementValidator-test.js', commit_id: 'e36b38c1cad22f7dff557736cf4b0280106e937e', commit_date: 1481819818)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get search results for the random commit\n",
    "search_results = bm25_search.search(random_commit['commit_message'], random_commit['commit_date'], ranking_depth=K)\n",
    "search_results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0253,\n",
       " 'P@10': 0.1,\n",
       " 'P@100': 0.02,\n",
       " 'P@1000': 0.002,\n",
       " 'MRR': 0.25,\n",
       " 'Recall@1000': 0.5}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the search results\n",
    "evaluation = evaluator.evaluate(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.5,\n",
       " 'P@10': 0.1,\n",
       " 'P@100': 0.01,\n",
       " 'P@1000': 0.001,\n",
       " 'MRR': 1.0,\n",
       " 'Recall@1000': 0.5}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_based_evaluation = evaluator.evaluate_file_based(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "file_based_evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
