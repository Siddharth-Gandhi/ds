{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "from utils import count_commits, get_combined_df, tokenize, reverse_tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResult:\n",
    "    def __init__(self, commit_id, file_path, score, commit_date, commit_msg):\n",
    "        self.commit_id = commit_id\n",
    "        self.file_path = file_path\n",
    "        self.score = score\n",
    "        self.commit_date = commit_date\n",
    "        self.commit_msg = commit_msg\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        # return f\"{self.file_path} {self.score:.5f} {self.commit_date}\"\n",
    "        return f\"{class_name}(score: {self.score:.5f}, file_path: {self.file_path!r}, commit_id: {self.commit_id!r}, commit_date: {self.commit_date})\"\n",
    "\n",
    "    def is_actual_modified(self, actual_modified_files):\n",
    "        return self.file_path in actual_modified_files\n",
    "\n",
    "    @staticmethod\n",
    "    def print_results(query, search_results, show_only_actual_modified=False):\n",
    "        actual_modified_files = query['actual_files_modified']\n",
    "        for i, result in enumerate(search_results):\n",
    "            if show_only_actual_modified and not result.is_actual_modified(actual_modified_files):\n",
    "                continue\n",
    "            print(f\"{i+1:2} {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Search:\n",
    "    def __init__(self, index_path):\n",
    "        if not os.path.exists(index_path):\n",
    "            raise FileNotFoundError(f\"Index at {index_path} does not exist!\")\n",
    "        self.searcher = LuceneSearcher(index_path)\n",
    "        print(f\"Loaded index at {index_path}\")\n",
    "        print(f'Index Stats: {IndexReader(index_path).index_stats()}')\n",
    "        # self.ranking_depth = ranking_depth\n",
    "\n",
    "    def search(self, query, query_date, ranking_depth):\n",
    "        # TODO maybe change this to mean returning reranking_depths total results instead of being pruned by the query date\n",
    "        hits = self.searcher.search(tokenize(query), ranking_depth)\n",
    "        unix_date = query_date\n",
    "        filtered_hits = [\n",
    "            SearchResult(hit.docid, json.loads(hit.raw)['file_path'], hit.score, int(json.loads(hit.raw)[\"commit_date\"]), reverse_tokenize(json.loads(hit.raw)['contents']))\n",
    "            for hit in hits if int(json.loads(hit.raw)[\"commit_date\"]) < unix_date\n",
    "        ]\n",
    "        return filtered_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEvaluator:\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, k):\n",
    "        return sum(relevant[:k]) / k\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(relevant):\n",
    "        for idx, value in enumerate(relevant):\n",
    "            if value == 1:\n",
    "                return 1 / (idx + 1)\n",
    "        return 0\n",
    "\n",
    "    def evaluate(self, search_results, actual_modified_files):\n",
    "        retrieved_files = [result.file_path for result in search_results]\n",
    "        relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "\n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric == 'MAP':\n",
    "                evaluations[metric] = average_precision_score(relevant, [1]*len(relevant)) if any(relevant) else 0\n",
    "            elif metric == 'MRR':\n",
    "                evaluations[metric] = self.mean_reciprocal_rank(relevant)\n",
    "            elif metric.startswith('P@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = self.precision_at_k(relevant, k)\n",
    "            elif metric.startswith('Recall@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                evaluations[metric] = len(\n",
    "                    {\n",
    "                        file\n",
    "                        for idx, file in enumerate(retrieved_files)\n",
    "                        if relevant[idx] == 1\n",
    "                    }\n",
    "                ) / len(actual_modified_files)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}\n",
    "\n",
    "    def evaluate_file_based(self, search_results, actual_modified_files, aggregation_strategy='sump'):\n",
    "        file_relevance = defaultdict(list)\n",
    "\n",
    "        # Aggregate relevance scores for each file across all commits\n",
    "        for result in search_results:\n",
    "            if result.file_path in actual_modified_files:\n",
    "                # file_relevance[result.file_path] += 1\n",
    "                file_relevance[result.file_path].append(result.score)\n",
    "\n",
    "        # Normalize relevance scores based on occurrences in actual modified files\n",
    "        # max_relevance = max(file_relevance.values(), default=1)\n",
    "        # normalized_relevance = {file: relevance / max_relevance for file, relevance in file_relevance.items()}\n",
    "        # sorted_normalized_relevance = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)\n",
    "        print(file_relevance)\n",
    "        if aggregation_strategy == 'sump':\n",
    "            aggregated_scores = {file: sum(relevance) for file, relevance in file_relevance.items()}\n",
    "        elif aggregation_strategy == 'maxp':\n",
    "            aggregated_scores = {file: max(relevance) for file, relevance in file_relevance.items()}\n",
    "        elif aggregation_strategy == 'firstp':\n",
    "            aggregated_scores = {file: relevance[0] for file, relevance in file_relevance.items()}\n",
    "        elif aggregation_strategy == 'avgp':\n",
    "            aggregated_scores = {file: np.mean(relevance) for file, relevance in file_relevance.items()}\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation strategy {aggregation_strategy}\")\n",
    "\n",
    "        sorted_aggregated_scores = sorted(aggregated_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        print(sorted_aggregated_scores)\n",
    "\n",
    "        evaluations = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric.startswith('P@'):\n",
    "                # Compute precision at k for files, not individual commit mentions\n",
    "                k = int(metric.split('@')[1])\n",
    "                # top_k_files = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "                top_k_files = sorted_aggregated_scores[:k]\n",
    "                precision_at_k = sum(1 for file, relevance in top_k_files if file in actual_modified_files) / k\n",
    "                evaluations[metric] = precision_at_k\n",
    "            elif metric.startswith('Recall@'):\n",
    "                k = int(metric.split('@')[1])\n",
    "                # top_k_files = sorted(normalized_relevance.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "                top_k_files = sorted_aggregated_scores[:k]\n",
    "                recall_at_k = sum(1 for file, relevance in top_k_files if file in actual_modified_files) / len(actual_modified_files)\n",
    "                evaluations[metric] = recall_at_k\n",
    "            elif metric == 'MAP':\n",
    "                # Compute average precision for files, not individual commit mentions\n",
    "                average_precision = 0\n",
    "                num_relevant_files = 0\n",
    "                for idx, (file, relevance) in enumerate(sorted_aggregated_scores):\n",
    "                    if file in actual_modified_files:\n",
    "                        num_relevant_files += 1\n",
    "                        average_precision += num_relevant_files / (idx + 1)\n",
    "                average_precision /= len(actual_modified_files)\n",
    "                evaluations[metric] = average_precision\n",
    "            elif metric == 'MRR':\n",
    "                # Compute mean reciprocal rank for files, not individual commit mentions\n",
    "                reciprocal_rank = 0\n",
    "                for idx, (file, relevance) in enumerate(sorted_aggregated_scores):\n",
    "                    if file in actual_modified_files:\n",
    "                        reciprocal_rank = 1 / (idx + 1)\n",
    "                        break\n",
    "                evaluations[metric] = reciprocal_rank\n",
    "\n",
    "        return {k: round(v, 4) for k, v in evaluations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = '../smalldata/fbr/index_commit_tokenized'\n",
    "repo_path = '../smalldata/fbr/'\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_search = BM25Search(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', f'Recall@{K}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = SearchEvaluator(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, eval_model, combined_df, seed=42):\n",
    "        self.model = model\n",
    "        self.eval_model = eval_model\n",
    "        self.combined_df = combined_df\n",
    "        self.seed = seed\n",
    "\n",
    "    def sample_commits(self, n):\n",
    "        if self.combined_df.commit_id.nunique() < n:\n",
    "            raise ValueError(f'Not enough commits to sample. Required: {n}, available: {self.combined_df.commit_id.nunique()}')\n",
    "        return self.combined_df.drop_duplicates(subset='commit_id').sample(n=n, replace=False, random_state=self.seed)\n",
    "\n",
    "    def evaluate_sampling(self, n=100, k=1000, output_dir='.', skip_existing=False, evaluation_strategy='commit', aggregation_strategy='sump'):\n",
    "        model_name = self.model.__class__.__name__\n",
    "        output_file = f\"{output_dir}/{model_name}_metrics.txt\"\n",
    "\n",
    "        if skip_existing and os.path.exists(output_file):\n",
    "            print(f'Output file {output_file} already exists, skipping...')\n",
    "            return\n",
    "\n",
    "        sampled_commits = self.sample_commits(n)\n",
    "\n",
    "        results = []\n",
    "        for _, row in sampled_commits.iterrows():\n",
    "            search_results = self.model.search(row['commit_message'], row['commit_date'], ranking_depth=k)\n",
    "            if evaluation_strategy == 'commit':\n",
    "                evaluation = self.eval_model.evaluate(search_results,\n",
    "                                                       self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist())\n",
    "            elif evaluation_strategy == 'file':\n",
    "                evaluation = self.eval_model.evaluate_file_based(search_results,\n",
    "                                                                  self.combined_df[self.combined_df['commit_id'] == row['commit_id']]['file_path'].tolist(), aggregation_strategy=aggregation_strategy)\n",
    "            else:\n",
    "                raise ValueError(f'Invalid evaluation strategy: {evaluation_strategy}')\n",
    "            results.append(evaluation)\n",
    "\n",
    "        avg_scores = {metric: round(np.mean([result[metric] for result in results]), 4) for metric in results[0]}\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "        with open(output_file, \"w\") as file:\n",
    "            file.write(f\"Model Name: {model_name}\\n\")\n",
    "            file.write(f\"Sample Size: {n}\\n\")\n",
    "            file.write(\"Evaluation Metrics:\\n\")\n",
    "            for key, value in avg_scores.items():\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = get_combined_df(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_evaluator = ModelEvaluator(bm25_search, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0427,\n",
       " 'P@10': 0.079,\n",
       " 'P@100': 0.0327,\n",
       " 'P@1000': 0.0084,\n",
       " 'MRR': 0.2676,\n",
       " 'Recall@1000': 0.6351}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_evaluator.evaluate_sampling(n=100, k=K, output_dir='../tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.6351,\n",
       " 'P@10': 0.242,\n",
       " 'P@100': 0.0269,\n",
       " 'P@1000': 0.0027,\n",
       " 'MRR': 0.76,\n",
       " 'Recall@1000': 0.6351}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_evaluator.evaluate_sampling(n=100, k=K, output_dir='../tmp/', evaluation_strategy='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner                                                             facebook\n",
       "repo_name                                                            react\n",
       "commit_date                                                     1556154069\n",
       "commit_id                         64e3da286f2c1e66303d2ae1dc06276b2f866e9d\n",
       "commit_message            Event API: Add `FocusScope` surface (#15487)\\n\\n\n",
       "file_path                         packages/react-events/src/ReactEvents.js\n",
       "cur_file_content         /**\\n * Copyright (c) Facebook, Inc. and its a...\n",
       "previous_commit_id                3f058debc29ccb05a47ac8a8d747c5a5b29a6ed3\n",
       "previous_file_path                                                    <NA>\n",
       "previous_file_content    /**\\n * Copyright (c) Facebook, Inc. and its a...\n",
       "diff                     @@ -10,8 +10,6 @@\\n import {\\n   REACT_EVENT_T...\n",
       "status                                                            modified\n",
       "is_merge_request                                                     False\n",
       "file_extension                                                          js\n",
       "Name: 47363, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample a commit from the combined_df\n",
    "random_commit = combined_df.sample(1).iloc[0]\n",
    "random_commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(score: 13.67170, file_path: 'src/core/ReactCompositeComponent.js', commit_id: '8855d6153e252c735de0e6cc373787d22c1a467b', commit_date: 1397094653),\n",
       " SearchResult(score: 13.67170, file_path: 'src/core/ReactPropTypes.js', commit_id: '8855d6153e252c735de0e6cc373787d22c1a467b', commit_date: 1397094653),\n",
       " SearchResult(score: 13.67170, file_path: 'src/core/__tests__/ReactPropTypes-test.js', commit_id: '8855d6153e252c735de0e6cc373787d22c1a467b', commit_date: 1397094653),\n",
       " SearchResult(score: 11.86170, file_path: 'src/event/delegate/DelegateEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86170, file_path: 'src/event/delegate/DelegateFocusEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86170, file_path: 'src/event/delegate/DelegateKeyboardEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86170, file_path: 'src/event/delegate/DelegateMouseEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86170, file_path: 'src/event/delegate/DelegateMutationEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86170, file_path: 'src/event/delegate/DelegateTouchEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86169, file_path: 'src/event/delegate/DelegateUIEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86169, file_path: 'src/event/delegate/DelegateWheelEvent.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86169, file_path: 'src/event/delegate/__tests__/DelegateEvent-test.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86169, file_path: 'src/event/delegate/__tests__/DelegateWheelEvent-test.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.86169, file_path: 'src/utils/PooledClass.js', commit_id: '3eaed5a122a050a6a2852b9f5348685373a9a6f7', commit_date: 1370668112),\n",
       " SearchResult(score: 11.58730, file_path: 'src/browser/server/ReactServerRendering.js', commit_id: 'fc7cf2ff639099538fbfea1bbf1a74907348c9ed', commit_date: 1416431588),\n",
       " SearchResult(score: 11.58730, file_path: 'src/browser/ui/ReactDOMComponent.js', commit_id: 'fc7cf2ff639099538fbfea1bbf1a74907348c9ed', commit_date: 1416431588),\n",
       " SearchResult(score: 11.58730, file_path: 'src/browser/ui/ReactDOMTextComponent.js', commit_id: 'fc7cf2ff639099538fbfea1bbf1a74907348c9ed', commit_date: 1416431588),\n",
       " SearchResult(score: 11.58730, file_path: 'src/browser/ui/ReactMount.js', commit_id: 'fc7cf2ff639099538fbfea1bbf1a74907348c9ed', commit_date: 1416431588),\n",
       " SearchResult(score: 11.58730, file_path: 'src/browser/ui/__tests__/ReactDOMComponent-test.js', commit_id: 'fc7cf2ff639099538fbfea1bbf1a74907348c9ed', commit_date: 1416431588),\n",
       " SearchResult(score: 11.58730, file_path: 'src/browser/ui/dom/__tests__/Danger-test.js', commit_id: 'fc7cf2ff639099538fbfea1bbf1a74907348c9ed', commit_date: 1416431588)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get search results for the random commit\n",
    "search_results = bm25_search.search(random_commit['commit_message'], random_commit['commit_date'], ranking_depth=K)\n",
    "search_results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0546,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.05,\n",
       " 'P@1000': 0.019,\n",
       " 'MRR': 0.0119,\n",
       " 'Recall@1000': 0.3478}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the search results\n",
    "evaluation = evaluator.evaluate(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'packages/react-dom/src/events/DOMEventResponderSystem.js': [9.576800346374512, 9.452381134033203, 8.900697708129883, 8.575098037719727], 'packages/shared/ReactTypes.js': [9.576796531677246, 9.452377319335938, 8.900694847106934, 8.575092315673828], 'packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js': [9.452380180358887, 8.900696754455566, 8.57509708404541], 'packages/react-reconciler/src/ReactFiberCompleteWork.js': [8.90069580078125, 8.575094223022461, 8.168195724487305], 'packages/react-events/src/ReactEvents.js': [8.575096130371094], 'packages/shared/ReactSymbols.js': [8.575093269348145, 8.168185234069824], 'packages/shared/getComponentName.js': [8.575091361999512], 'scripts/rollup/bundles.js': [7.981998920440674]})\n",
      "[('packages/react-dom/src/events/DOMEventResponderSystem.js', 36.504977226257324), ('packages/shared/ReactTypes.js', 36.504961013793945), ('packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js', 26.928174018859863), ('packages/react-reconciler/src/ReactFiberCompleteWork.js', 25.643985748291016), ('packages/shared/ReactSymbols.js', 16.74327850341797), ('packages/react-events/src/ReactEvents.js', 8.575096130371094), ('packages/shared/getComponentName.js', 8.575091361999512), ('scripts/rollup/bundles.js', 7.981998920440674)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.3478,\n",
       " 'P@10': 0.8,\n",
       " 'P@100': 0.08,\n",
       " 'P@1000': 0.008,\n",
       " 'MRR': 1.0,\n",
       " 'Recall@1000': 0.3478}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_based_evaluation = evaluator.evaluate_file_based(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "file_based_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'packages/react-dom/src/events/DOMEventResponderSystem.js': [9.576800346374512, 9.452381134033203, 8.900697708129883, 8.575098037719727], 'packages/shared/ReactTypes.js': [9.576796531677246, 9.452377319335938, 8.900694847106934, 8.575092315673828], 'packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js': [9.452380180358887, 8.900696754455566, 8.57509708404541], 'packages/react-reconciler/src/ReactFiberCompleteWork.js': [8.90069580078125, 8.575094223022461, 8.168195724487305], 'packages/react-events/src/ReactEvents.js': [8.575096130371094], 'packages/shared/ReactSymbols.js': [8.575093269348145, 8.168185234069824], 'packages/shared/getComponentName.js': [8.575091361999512], 'scripts/rollup/bundles.js': [7.981998920440674]})\n",
      "[('packages/react-dom/src/events/DOMEventResponderSystem.js', 36.504977226257324), ('packages/shared/ReactTypes.js', 36.504961013793945), ('packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js', 26.928174018859863), ('packages/react-reconciler/src/ReactFiberCompleteWork.js', 25.643985748291016), ('packages/shared/ReactSymbols.js', 16.74327850341797), ('packages/react-events/src/ReactEvents.js', 8.575096130371094), ('packages/shared/getComponentName.js', 8.575091361999512), ('scripts/rollup/bundles.js', 7.981998920440674)]\n",
      "sump: {'MAP': 0.3478, 'P@10': 0.8, 'P@100': 0.08, 'P@1000': 0.008, 'MRR': 1.0, 'Recall@1000': 0.3478}\n",
      "defaultdict(<class 'list'>, {'packages/react-dom/src/events/DOMEventResponderSystem.js': [9.576800346374512, 9.452381134033203, 8.900697708129883, 8.575098037719727], 'packages/shared/ReactTypes.js': [9.576796531677246, 9.452377319335938, 8.900694847106934, 8.575092315673828], 'packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js': [9.452380180358887, 8.900696754455566, 8.57509708404541], 'packages/react-reconciler/src/ReactFiberCompleteWork.js': [8.90069580078125, 8.575094223022461, 8.168195724487305], 'packages/react-events/src/ReactEvents.js': [8.575096130371094], 'packages/shared/ReactSymbols.js': [8.575093269348145, 8.168185234069824], 'packages/shared/getComponentName.js': [8.575091361999512], 'scripts/rollup/bundles.js': [7.981998920440674]})\n",
      "[('packages/react-dom/src/events/DOMEventResponderSystem.js', 9.576800346374512), ('packages/shared/ReactTypes.js', 9.576796531677246), ('packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js', 9.452380180358887), ('packages/react-reconciler/src/ReactFiberCompleteWork.js', 8.90069580078125), ('packages/react-events/src/ReactEvents.js', 8.575096130371094), ('packages/shared/ReactSymbols.js', 8.575093269348145), ('packages/shared/getComponentName.js', 8.575091361999512), ('scripts/rollup/bundles.js', 7.981998920440674)]\n",
      "maxp: {'MAP': 0.3478, 'P@10': 0.8, 'P@100': 0.08, 'P@1000': 0.008, 'MRR': 1.0, 'Recall@1000': 0.3478}\n",
      "defaultdict(<class 'list'>, {'packages/react-dom/src/events/DOMEventResponderSystem.js': [9.576800346374512, 9.452381134033203, 8.900697708129883, 8.575098037719727], 'packages/shared/ReactTypes.js': [9.576796531677246, 9.452377319335938, 8.900694847106934, 8.575092315673828], 'packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js': [9.452380180358887, 8.900696754455566, 8.57509708404541], 'packages/react-reconciler/src/ReactFiberCompleteWork.js': [8.90069580078125, 8.575094223022461, 8.168195724487305], 'packages/react-events/src/ReactEvents.js': [8.575096130371094], 'packages/shared/ReactSymbols.js': [8.575093269348145, 8.168185234069824], 'packages/shared/getComponentName.js': [8.575091361999512], 'scripts/rollup/bundles.js': [7.981998920440674]})\n",
      "[('packages/react-dom/src/events/DOMEventResponderSystem.js', 9.576800346374512), ('packages/shared/ReactTypes.js', 9.576796531677246), ('packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js', 9.452380180358887), ('packages/react-reconciler/src/ReactFiberCompleteWork.js', 8.90069580078125), ('packages/react-events/src/ReactEvents.js', 8.575096130371094), ('packages/shared/ReactSymbols.js', 8.575093269348145), ('packages/shared/getComponentName.js', 8.575091361999512), ('scripts/rollup/bundles.js', 7.981998920440674)]\n",
      "firstp: {'MAP': 0.3478, 'P@10': 0.8, 'P@100': 0.08, 'P@1000': 0.008, 'MRR': 1.0, 'Recall@1000': 0.3478}\n",
      "defaultdict(<class 'list'>, {'packages/react-dom/src/events/DOMEventResponderSystem.js': [9.576800346374512, 9.452381134033203, 8.900697708129883, 8.575098037719727], 'packages/shared/ReactTypes.js': [9.576796531677246, 9.452377319335938, 8.900694847106934, 8.575092315673828], 'packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js': [9.452380180358887, 8.900696754455566, 8.57509708404541], 'packages/react-reconciler/src/ReactFiberCompleteWork.js': [8.90069580078125, 8.575094223022461, 8.168195724487305], 'packages/react-events/src/ReactEvents.js': [8.575096130371094], 'packages/shared/ReactSymbols.js': [8.575093269348145, 8.168185234069824], 'packages/shared/getComponentName.js': [8.575091361999512], 'scripts/rollup/bundles.js': [7.981998920440674]})\n",
      "[('packages/react-dom/src/events/DOMEventResponderSystem.js', 9.126244306564331), ('packages/shared/ReactTypes.js', 9.126240253448486), ('packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js', 8.976058006286621), ('packages/react-events/src/ReactEvents.js', 8.575096130371094), ('packages/shared/getComponentName.js', 8.575091361999512), ('packages/react-reconciler/src/ReactFiberCompleteWork.js', 8.547995249430338), ('packages/shared/ReactSymbols.js', 8.371639251708984), ('scripts/rollup/bundles.js', 7.981998920440674)]\n",
      "avgp: {'MAP': 0.3478, 'P@10': 0.8, 'P@100': 0.08, 'P@1000': 0.008, 'MRR': 1.0, 'Recall@1000': 0.3478}\n"
     ]
    }
   ],
   "source": [
    "# iterate over the list of aggregation strategies and evaluate each one\n",
    "\n",
    "aggregation_strategies = ['sump', 'maxp', 'firstp', 'avgp']\n",
    "for strategy in aggregation_strategies:\n",
    "    file_based_evaluation = evaluator.evaluate_file_based(search_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist(), aggregation_strategy=strategy)\n",
    "    print(f\"{strategy}: {file_based_evaluation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sump: {'MAP': 0.5755, 'P@10': 0.1981, 'P@100': 0.0237, 'P@1000': 0.0025, 'MRR': 0.698, 'Recall@1000': 0.5755}\n",
      "maxp: {'MAP': 0.5755, 'P@10': 0.1981, 'P@100': 0.0237, 'P@1000': 0.0025, 'MRR': 0.698, 'Recall@1000': 0.5755}\n",
      "firstp: {'MAP': 0.5755, 'P@10': 0.1981, 'P@100': 0.0237, 'P@1000': 0.0025, 'MRR': 0.698, 'Recall@1000': 0.5755}\n",
      "avgp: {'MAP': 0.5755, 'P@10': 0.1981, 'P@100': 0.0237, 'P@1000': 0.0025, 'MRR': 0.698, 'Recall@1000': 0.5755}\n"
     ]
    }
   ],
   "source": [
    "# do the same for the model evaluator\n",
    "for strategy in aggregation_strategies:\n",
    "    model_evaluation = bm25_evaluator.evaluate_sampling(n=1000, k=K, output_dir='../tmp/', evaluation_strategy='file', aggregation_strategy=strategy)\n",
    "    print(f\"{strategy}: {model_evaluation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class BERTReRanker:\n",
    "    \"\"\"\n",
    "    A class for performing reranking with a BERT-based model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, psgLen=128, psgStride=64, psgCnt=None, scoreAggregation='maxp', batchSize=8):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        # Passage handling parameters\n",
    "        self.psgLen = psgLen\n",
    "        self.psgStride = psgStride\n",
    "        self.psgCnt = psgCnt\n",
    "        self.scoreAggregation = scoreAggregation\n",
    "\n",
    "        if self.scoreAggregation == 'firstp':\n",
    "            self.psgCnt = 1\n",
    "\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "    def rerank(self, query, search_results):\n",
    "        \"\"\"\n",
    "        Rerank the search results using the BERT model.\n",
    "\n",
    "        query: The query string.\n",
    "        search_results: A list of SearchResult objects.\n",
    "        \"\"\"\n",
    "        reranked_results = []\n",
    "        # todo - add batching\n",
    "\n",
    "        # Process each SearchResult to create input for BERT\n",
    "        for result in search_results:\n",
    "            passages = self._split_into_passages(query, result.commit_msg)\n",
    "\n",
    "            # Score each passage with BERT\n",
    "            passage_scores = [self._score_passage(query, passage) for passage in passages]\n",
    "\n",
    "            # Aggregate passage scores to get a single document score\n",
    "            doc_score = self._aggregate_scores(passage_scores)\n",
    "\n",
    "            # Create a new SearchResult with the updated score\n",
    "            reranked_results.append((doc_score, result))\n",
    "\n",
    "        # Sort reranked results by the new score\n",
    "        reranked_results.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [result for _, result in reranked_results]\n",
    "\n",
    "    def _split_into_passages(self, query, commit_msg):\n",
    "        # Tokenize the query and commit message\n",
    "        tokens = self.tokenizer.tokenize(query) + self.tokenizer.tokenize(commit_msg)\n",
    "\n",
    "        # Split the tokens into passages\n",
    "        passages = []\n",
    "        for i in range(0, len(tokens), self.psgStride):\n",
    "            passage = tokens[i:i+self.psgLen]\n",
    "            passages.append(self.tokenizer.convert_tokens_to_string(passage))\n",
    "            if self.psgCnt and len(passages) >= self.psgCnt:\n",
    "                break\n",
    "        return passages\n",
    "\n",
    "    def _score_passage(self, query, passage):\n",
    "        # Encode query and passage for BERT\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            passage,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.psgLen,\n",
    "            truncation=\"only_second\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Score the (query, passage) pair with BERT\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            score = outputs.logits.squeeze().item()\n",
    "        return score\n",
    "\n",
    "    def _aggregate_scores(self, passage_scores):\n",
    "        # Aggregate passage scores based on the specified strategy\n",
    "        if self.scoreAggregation == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        elif self.scoreAggregation == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        elif self.scoreAggregation == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        elif self.scoreAggregation == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid score aggregation method: {self.scoreAggregation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Assuming bm25_searcher is an instance of BM25Search\n",
    "query = \"Fix login page error\"\n",
    "query_date = 1699261235\n",
    "bm25_results = bm25_search.search(query, query_date, K)\n",
    "\n",
    "# Now rerank those results with BERT\n",
    "bert_reranker = BERTReRanker(model_name=\"bert-base-uncased\")\n",
    "reranked_results = bert_reranker.rerank(query, bm25_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(score: 2.69137, file_path: 'packages/react-reconciler/src/ReactFiberBeginWork.new.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/ReactFiberBeginWork.old.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/ReactFiberSuspenseComponent.new.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/ReactFiberSuspenseComponent.old.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 2.69136, file_path: 'packages/react-reconciler/src/__tests__/ReactCPUSuspense-test.js', commit_id: '1faf9e3dd5d6492f3607d5c721055819e4106bc6', commit_date: 1601495853),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom-bindings/src/client/ReactFiberConfigDOM.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom-bindings/src/server/ReactFizzConfigDOM.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom-bindings/src/server/ReactFizzConfigDOMLegacy.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-dom/src/__tests__/ReactDOMFizzServer-test.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144),\n",
       " SearchResult(score: 3.26617, file_path: 'packages/react-reconciler/src/ReactFiberConfigWithNoHydration.js', commit_id: '8b26f07a883bb341c20283c0099bf5ee6f87bd1f', commit_date: 1694117144)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 results\n",
    "reranked_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.016,\n",
       " 'P@10': 0.0,\n",
       " 'P@100': 0.0,\n",
       " 'P@1000': 0.016,\n",
       " 'MRR': 0.0031,\n",
       " 'Recall@1000': 0.3913}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the reranked results\n",
    "evaluation = evaluator.evaluate(reranked_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'packages/react-reconciler/src/ReactFiberCompleteWork.js': [2.718986988067627, 3.8634390830993652, 3.863529920578003, 4.273975372314453], 'packages/shared/ReactSymbols.js': [3.5164780616760254], 'packages/shared/ReactTypes.js': [3.6565749645233154, 3.8633739948272705, 3.863521099090576], 'scripts/rollup/bundles.js': [4.612883567810059, 5.564578056335449, 2.7962939739227295], 'packages/react-dom/src/shared/assertValidProps.js': [5.564676284790039], 'packages/react-dom/src/events/DOMEventResponderSystem.js': [3.8635449409484863], 'packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js': [3.86354398727417], 'packages/react-events/src/FocusScope.js': [3.863539934158325], 'packages/shared/getComponentName.js': [3.8635189533233643]})\n",
      "[('packages/react-reconciler/src/ReactFiberCompleteWork.js', 14.719931364059448), ('scripts/rollup/bundles.js', 12.973755598068237), ('packages/shared/ReactTypes.js', 11.383470058441162), ('packages/react-dom/src/shared/assertValidProps.js', 5.564676284790039), ('packages/react-dom/src/events/DOMEventResponderSystem.js', 3.8635449409484863), ('packages/react-dom/src/events/__tests__/DOMEventResponderSystem-test.internal.js', 3.86354398727417), ('packages/react-events/src/FocusScope.js', 3.863539934158325), ('packages/shared/getComponentName.js', 3.8635189533233643), ('packages/shared/ReactSymbols.js', 3.5164780616760254)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.3913,\n",
       " 'P@10': 0.9,\n",
       " 'P@100': 0.09,\n",
       " 'P@1000': 0.009,\n",
       " 'MRR': 1.0,\n",
       " 'Recall@1000': 0.3913}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = evaluator.evaluate_file_based(reranked_results, combined_df[combined_df['commit_id'] == random_commit['commit_id']]['file_path'].tolist())\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'MAP': 0.0546,\n",
    " 'P@10': 0.0,\n",
    " 'P@100': 0.05,\n",
    " 'P@1000': 0.019,\n",
    " 'MRR': 0.0119,\n",
    " 'Recall@1000': 0.3478}\n",
    "\n",
    "{'MAP': 0.3478,\n",
    " 'P@10': 0.8,\n",
    " 'P@100': 0.08,\n",
    " 'P@1000': 0.008,\n",
    " 'MRR': 1.0,\n",
    " 'Recall@1000': 0.3478}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
