{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utils import get_combined_df\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../data/2_7/facebook_react/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='../data/2_7/facebook_react/index_commit_tokenized', repo_path='../data/2_7/facebook_react', k=1000, n=100, model_path='microsoft/codebert-base', overwrite_cache=False, batch_size=32, num_epochs=10, learning_rate=5e-05, run_name='repr_0.1663', notes='reproducing current best 0.1663 MAP result for CodeReranker', num_positives=10, num_negatives=10, train_depth=1000, num_workers=8, train_commits=1000, psg_cnt=25, aggregation_strategy='sump', use_gpu=True, rerank_depth=100, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4', overwrite_eval=False, sanity_check=True, debug=False, best_model_path=None, bert_best_model='data/combined_commit_train/best_model', psg_len=350, psg_stride=250, ignore_gold_in_training=False, eval_folder='repr_0.1663', use_gpt_train=True\n",
    ")\n",
    "\n",
    "metrics =['MAP', 'P@1', 'P@10', 'P@20', 'P@30', 'MRR', 'R@1', 'R@10', 'R@100', 'R@1000']\n",
    "repo_path = args.repo_path\n",
    "repo_name = repo_path.split('/')[-1]\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'\n",
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)\n",
    "\n",
    "test_path = os.path.join('..', 'gold', 'facebook_react', 'v2_facebook_react_gpt4_gold.parquet')\n",
    "gold_df = pd.read_parquet(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = os.path.join(args.repo_path, 'cache', 'repr_0.1663')\n",
    "code_df = pd.read_parquet(os.path.join(cache_path, 'code_df.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.train_commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    5000\n",
       "1    1631\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BERTCodeReranker:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        print(f\"Initialized Code File BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        query_passage_pairs, per_result_contribution = self.split_into_query_passage_pairs(query, aggregated_results)\n",
    "\n",
    "\n",
    "        # for agg_result in aggregated_results:\n",
    "        #     query_passage_pairs.extend(\n",
    "        #         (query, result.commit_message)\n",
    "        #         for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "        #     )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for i, agg_result in enumerate(aggregated_results):\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + per_result_contribution[i] # only use psg_cnt contributing_results\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "\n",
    "    def split_into_query_passage_pairs(self, query, aggregated_results):\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        def full_tokenize(s):\n",
    "            return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        query_passage_pairs = []\n",
    "        per_result_contribution = []\n",
    "        for agg_result in aggregated_results:\n",
    "            agg_result.contributing_results.sort(key=lambda res: res.commit_date, reverse=True)\n",
    "            # get most recent file version\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            # get the file_path and commit_id\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            commit_id = most_recent_search_result.commit_id\n",
    "            # get the file content from combined_df\n",
    "            file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['cur_file_content'].values[0]\n",
    "\n",
    "            # now need to split this file content into psg_cnt passages\n",
    "            # first tokenize the file content\n",
    "\n",
    "            # warning these asserts are useless since we are using NaNs\n",
    "            assert file_content is not None, f'file_content is None for commit_id: {commit_id}, file_path: {file_path}'\n",
    "            assert file_path is not None, f'file_path is None for commit_id: {commit_id}'\n",
    "            assert query is not None, f'query is None'\n",
    "\n",
    "            query_tokens = full_tokenize(query)\n",
    "            path_tokens = full_tokenize(file_path)\n",
    "\n",
    "            if pd.isna(file_content):\n",
    "                # if file_content is NaN, then we can just set file_content to empty string\n",
    "                print(f'WARNING: file_content is NaN for commit_id: {commit_id}, file_path: {file_path}, setting file_content to empty string')\n",
    "                file_content = ''\n",
    "\n",
    "            file_tokens = full_tokenize(file_content)\n",
    "\n",
    "\n",
    "            # now split the file content into psg_cnt passages\n",
    "            cur_result_passages = []\n",
    "            # get the input ids\n",
    "            # input_ids = file_content['input_ids'].squeeze()\n",
    "            # get the number of tokens in the file content\n",
    "            total_tokens = len(file_tokens)\n",
    "\n",
    "            for cur_start in range(0, total_tokens, self.psg_stride):\n",
    "                cur_passage = []\n",
    "                # add query tokens and path tokens\n",
    "                # cur_passage.extend(query_tokens) # ??????????????\n",
    "                cur_passage.extend(path_tokens)\n",
    "\n",
    "                # add the file tokens\n",
    "                cur_passage.extend(file_tokens[cur_start:cur_start+self.psg_len])\n",
    "\n",
    "                # now convert cur_passage into a string\n",
    "                cur_passage_decoded = self.tokenizer.decode(cur_passage)\n",
    "\n",
    "                # add the cur_passage to cur_result_passages\n",
    "                cur_result_passages.append(cur_passage_decoded)\n",
    "\n",
    "                if len(cur_result_passages) == self.psg_cnt:\n",
    "                    break\n",
    "\n",
    "            # now add the query, passage pairs to query_passage_pairs\n",
    "            per_result_contribution.append(len(cur_result_passages))\n",
    "            query_passage_pairs.extend((query, passage) for passage in cur_result_passages)\n",
    "        return query_passage_pairs, per_result_contribution\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.1542, 'P@1': 0.11, 'P@10': 0.087, 'P@20': 0.063, 'P@30': 0.0517, 'MRR': 0.2133, 'R@1': 0.0509, 'R@10': 0.2285, 'R@100': 0.5077, 'R@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=BM25_AGGR_STRAT)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing/Verifying code triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple modes:\n",
    "1. Random splits\n",
    "2. Diff only\n",
    "3. Function split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with 500 gold, 500 gold + 500 normal commits, 500 + 1500 normal commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126 entries, 0 to 125\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   train_commit_id         126 non-null    object\n",
      " 1   train_query             126 non-null    object\n",
      " 2   train_original_message  126 non-null    object\n",
      " 3   SR_file_path            126 non-null    object\n",
      " 4   SR_commit_id            126 non-null    object\n",
      " 5   SR_file_content         126 non-null    object\n",
      " 6   SR_diff                 123 non-null    object\n",
      " 7   label                   126 non-null    int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 8.0+ KB\n"
     ]
    }
   ],
   "source": [
    "code_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_commit_id</th>\n",
       "      <th>train_query</th>\n",
       "      <th>train_original_message</th>\n",
       "      <th>SR_file_path</th>\n",
       "      <th>SR_commit_id</th>\n",
       "      <th>SR_file_content</th>\n",
       "      <th>SR_diff</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76e569992b259b9e636ee68dcc7719539f4b9bb8</td>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>Cleanup profile export/import data types, add ...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>cc24d0ea56b0538d1ac61dc09faedd70ced5bb47</td>\n",
       "      <td>/**\\n * Copyright (c) Facebook, Inc. and its a...</td>\n",
       "      <td>@@ -569,8 +569,6 @@ function resolveLocksOnRoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76e569992b259b9e636ee68dcc7719539f4b9bb8</td>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>Cleanup profile export/import data types, add ...</td>\n",
       "      <td>src/renderers/shared/fiber/ReactChildFiber.js</td>\n",
       "      <td>c22b94f14a809abb376f07a53f36860a7c6a342e</td>\n",
       "      <td>/**\\n * Copyright 2013-present, Facebook, Inc....</td>\n",
       "      <td>@@ -13,8 +13,7 @@\\n 'use strict';\\n \\n import ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            train_commit_id  \\\n",
       "0  76e569992b259b9e636ee68dcc7719539f4b9bb8   \n",
       "1  76e569992b259b9e636ee68dcc7719539f4b9bb8   \n",
       "\n",
       "                                         train_query  \\\n",
       "0  Malformed data types (`commitDetails`, `intera...   \n",
       "1  Malformed data types (`commitDetails`, `intera...   \n",
       "\n",
       "                              train_original_message  \\\n",
       "0  Cleanup profile export/import data types, add ...   \n",
       "1  Cleanup profile export/import data types, add ...   \n",
       "\n",
       "                                        SR_file_path  \\\n",
       "0  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "1      src/renderers/shared/fiber/ReactChildFiber.js   \n",
       "\n",
       "                               SR_commit_id  \\\n",
       "0  cc24d0ea56b0538d1ac61dc09faedd70ced5bb47   \n",
       "1  c22b94f14a809abb376f07a53f36860a7c6a342e   \n",
       "\n",
       "                                     SR_file_content  \\\n",
       "0  /**\\n * Copyright (c) Facebook, Inc. and its a...   \n",
       "1  /**\\n * Copyright 2013-present, Facebook, Inc....   \n",
       "\n",
       "                                             SR_diff  label  \n",
       "0  @@ -569,8 +569,6 @@ function resolveLocksOnRoo...      0  \n",
       "1  @@ -13,8 +13,7 @@\\n 'use strict';\\n \\n import ...      0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n",
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 25, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 350, 'psg_stride': 250}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': args.psg_cnt,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': args.rerank_depth,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': 'sump',\n",
    "        'psg_len': args.psg_len,\n",
    "        'psg_stride': args.psg_stride\n",
    "    }\n",
    "code_reranker = BERTCodeReranker(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_code_triplets(code_df, code_reranker, mode, cache_file, overwrite=False):\n",
    "    print(f\"Preparing code triplets with mode {mode} for {len(code_df)} rows.\")\n",
    "    if cache_file and os.path.exists(cache_file) and not overwrite:\n",
    "        print(f\"Loading data from cache file: {cache_file}\")\n",
    "        return pd.read_parquet(cache_file)\n",
    "\n",
    "    if mode == 'sliding_window':\n",
    "        triplets = prepare_sliding_window_triplets(code_df, code_reranker)\n",
    "    elif mode == 'parse_functions':\n",
    "        triplets = prepare_function_triplets(code_df, code_reranker)\n",
    "    elif mode == 'diff_content':\n",
    "        triplets = prepare_diff_content_triplets(code_df, code_reranker)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "    triplets_df = pd.DataFrame(triplets, columns=['query', 'file_path', 'passage', 'label'])\n",
    "    if cache_file:\n",
    "        print(f\"Saving data to cache file: {cache_file}\")\n",
    "        triplets_df.to_parquet(cache_file)\n",
    "\n",
    "    return triplets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sliding_window_triplets(code_df, code_reranker):\n",
    "\n",
    "    #### Helper functions ####\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        \n",
    "    def prep_line(line):\n",
    "        return line.rstrip().lstrip()\n",
    "\n",
    "    def parse_diff(diff):\n",
    "        return [\n",
    "            line[1:] if line.startswith('+') else line\n",
    "            for line in diff.split('\\n')\n",
    "            if not (line.startswith('-') or len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "            and len(prep_line(line)) > 2\n",
    "        ]\n",
    "        \n",
    "    def count_matching_lines(passage_lines, diff_lines):\n",
    "        # Create a 2D array to store the lengths of the longest common subsequences\n",
    "        dp = [[0] * (len(diff_lines) + 1) for _ in range(len(passage_lines) + 1)]\n",
    "\n",
    "        # Fill the dp array\n",
    "        for i in range(1, len(passage_lines) + 1):\n",
    "            for j in range(1, len(diff_lines) + 1):\n",
    "                if prep_line(passage_lines[i - 1]) == prep_line(diff_lines[j - 1]):\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "\n",
    "        return dp[-1][-1]\n",
    "\n",
    "    #### End of helper functions ####\n",
    "\n",
    "    triplets = []\n",
    "    for _, row in tqdm(code_df.iterrows(), total=len(code_df)):\n",
    "        file_tokens = full_tokenize(row['SR_file_content'])\n",
    "        total_tokens = len(file_tokens)\n",
    "        cur_diff = row['SR_diff']\n",
    "        cur_triplets = []\n",
    "        if cur_diff is None:\n",
    "            # NOTE: for cases where status is added probably or if diff was not able to be stored (encoding issue, etc)\n",
    "            # THIS WILL LEAD TO A FEW POSITIVES MISSING - don't freak out, it's normal, I checked ;)\n",
    "            continue\n",
    "\n",
    "        # Process the diffs to removee @@ stuff \n",
    "        cur_diff_lines = parse_diff(cur_diff) # split into lines and remove deletions, only additions remaining\n",
    "\n",
    "        # get the diff tokens\n",
    "        total_tokens = len(file_tokens)\n",
    "\n",
    "        for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "            cur_passage = []\n",
    "\n",
    "            # get tokens for current passage\n",
    "            cur_passage.extend(file_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "\n",
    "            # now convert current passage tokens back into a string\n",
    "            cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "            # for ranking acc. to number of common lines with diff, split on \\n\n",
    "            cur_passage_lines = cur_passage_decoded.split('\\n')\n",
    "\n",
    "            # remove lines with less than 2 characters since we do the same for diff preprocessing\n",
    "            cur_passage_lines = [line for line in cur_passage_lines if len(prep_line(line)) > 2] # otherwise empty characters and brackets match\n",
    "\n",
    "            # get number of common lines b/w diff\n",
    "            common_line_count = count_matching_lines(cur_passage_lines, cur_diff_lines)\n",
    "\n",
    "            # add the cur_passage to cur_result_passages\n",
    "            cur_triplets.append((common_line_count, (row['train_query'], row['SR_file_path'], cur_passage_decoded, row['label'])))\n",
    "\n",
    "        # # sort the cur_triplets by the number of common lines\n",
    "        cur_triplets.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # now add the top code_reranker.psg_cnt to triplets\n",
    "        for triplet in cur_triplets[:code_reranker.psg_cnt]:\n",
    "            # print(f\"Found {triplet[0]} matching lines for diff in cur_passage at index\")\n",
    "            triplets.append(triplet[1])\n",
    "    print(len(triplets))\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code triplets with mode sliding_window for 126 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:26<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = prepare_code_triplets(code_df, code_reranker, mode='sliding_window', cache_file=None, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_diff_content_triplets(code_df, code_reranker):\n",
    "\n",
    "    #### Helper functions #### \n",
    "\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "    def prep_line(line):\n",
    "        return line.rstrip().lstrip()\n",
    "        \n",
    "    def full_parse_diffs(diff):\n",
    "       # keep both insertions and deletions to be passed to the model\n",
    "        return [\n",
    "            line[1:] if (line.startswith('+') or line.startswith('-')) else line\n",
    "            for line in diff.split('\\n')\n",
    "            if not (len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "        ]\n",
    "    #### end of helper functions ####\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(code_df.iterrows(), total=len(code_df)):\n",
    "        cur_diff = row['SR_diff']\n",
    "        if cur_diff is None:\n",
    "            # NOTE: for cases where status is added probably or if diff was not able to be stored (encoding issue, etc)\n",
    "            # THIS WILL LEAD TO A FEW POSITIVES MISSING - don't freak out, it's normal, I checked ;)\n",
    "            continue\n",
    "        cur_diff_lines = full_parse_diffs(cur_diff) # keep both insertions and deletions\n",
    "        diff_tokens = full_tokenize(''.join(cur_diff_lines))\n",
    "        total_tokens = len(diff_tokens)\n",
    "        for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "            cur_passage = []\n",
    "    \n",
    "            cur_passage.extend(diff_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "    \n",
    "            # now convert cur_passage into a string\n",
    "            cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "            # add the cur_passage to cur_result_passages\n",
    "            triplets.append((row['train_query'], row['SR_file_path'], cur_passage_decoded, row['label']))\n",
    "\n",
    "        # now add the top code_reranker.psg_cnt to triplets\n",
    "        return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code triplets with mode diff_content for 126 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp2 = prepare_code_triplets(code_df, code_reranker, mode='diff_content', cache_file=None, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ -569,8 +569,6 @@ function resolveLocksOnRoot(root: FiberRoot, expirationTime: ExpirationTime) {\n",
      "     firstBatch._defer &&\n",
      "     firstBatch._expirationTime >= expirationTime\n",
      "   ) {\n",
      "-    root.finishedWork = root.current.alternate;\n",
      "-    root.pendingCommitExpirationTime = expirationTime;\n",
      "     scheduleCallback(NormalPriority, () => {\n",
      "       firstBatch._onComplete();\n",
      "       return null;\n",
      "@@ -689,7 +687,8 @@ export function flushControlled(fn: () => mixed): void {\n",
      " }\n",
      " \n",
      " function prepareFreshStack(root, expirationTime) {\n",
      "-  root.pendingCommitExpirationTime = NoWork;\n",
      "+  root.finishedWork = null;\n",
      "+  root.finishedExpirationTime = NoWork;\n",
      " \n",
      "   const timeoutHandle = root.timeoutHandle;\n",
      "   if (timeoutHandle !== noTimeout) {\n",
      "@@ -741,10 +740,9 @@ function renderRoot(\n",
      "     return null;\n",
      "   }\n",
      " \n",
      "-  if (root.pendingCommitExpirationTime === expirationTime) {\n",
      "+  if (root.finishedExpirationTime === expirationTime) {\n",
      "     // There's already a pending commit at this expiration time.\n",
      "-    root.pendingCommitExpirationTime = NoWork;\n",
      "-    return commitRoot.bind(null, root, expirationTime);\n",
      "+    return commitRoot.bind(null, root);\n",
      "   }\n",
      " \n",
      "   flushPassiveEffects();\n",
      "@@ -867,6 +865,9 @@ function renderRoot(\n",
      "   // something suspended, wait to commit it after a timeout.\n",
      "   stopFinishedWorkLoopTimer();\n",
      " \n",
      "+  root.finishedWork = root.current.alternate;\n",
      "+  root.finishedExpirationTime = expirationTime;\n",
      "+\n",
      "   const isLocked = resolveLocksOnRoot(root, expirationTime);\n",
      "   if (isLocked) {\n",
      "     // This root has a lock that prevents it from committing. Exit. If we begin\n",
      "@@ -905,7 +906,7 @@ function renderRoot(\n",
      "       }\n",
      "       // If we're already rendering synchronously, commit the root in its\n",
      "       // errored state.\n",
      "-      return commitRoot.bind(null, root, expirationTime);\n",
      "+      return commitRoot.bind(null, root);\n",
      "     }\n",
      "     case RootSuspended: {\n",
      "       if (!isSync) {\n",
      "@@ -929,7 +930,7 @@ function renderRoot(\n",
      "             // priority work to do. Instead of committing the fallback\n",
      "             // immediately, wait for more data to arrive.\n",
      "             root.timeoutHandle = scheduleTimeout(\n",
      "-              commitRoot.bind(null, root, expirationTime),\n",
      "+              commitRoot.bind(null, root),\n",
      "               msUntilTimeout,\n",
      "             );\n",
      "             return null;\n",
      "@@ -937,11 +938,11 @@ function renderRoot(\n",
      "         }\n",
      "       }\n",
      "       // The work expired. Commit immediately.\n",
      "-      return commitRoot.bind(null, root, expirationTime);\n",
      "+      return commitRoot.bind(null, root);\n",
      "     }\n",
      "     case RootCompleted: {\n",
      "       // The work completed. Ready to commit.\n",
      "-      return commitRoot.bind(null, root, expirationTime);\n",
      "+      return commitRoot.bind(null, root);\n",
      "     }\n",
      "     default: {\n",
      "       invariant(false, 'Unknown root exit status.');\n",
      "@@ -1223,11 +1224,8 @@ function resetChildExpirationTime(completedWork: Fiber) {\n",
      "   completedWork.childExpirationTime = newChildExpirationTime;\n",
      " }\n",
      " \n",
      "-function commitRoot(root, expirationTime) {\n",
      "-  runWithPriority(\n",
      "-    ImmediatePriority,\n",
      "-    commitRootImpl.bind(null, root, expirationTime),\n",
      "-  );\n",
      "+function commitRoot(root) {\n",
      "+  runWithPriority(ImmediatePriority, commitRootImpl.bind(null, root));\n",
      "   // If there are passive effects, schedule a callback to flush them. This goes\n",
      "   // outside commitRootImpl so that it inherits the priority of the render.\n",
      "   if (rootWithPendingPassiveEffects !== null) {\n",
      "@@ -1240,7 +1238,7 @@ function commitRoot(root, expirationTime) {\n",
      "   return null;\n",
      " }\n",
      " \n",
      "-function commitRootImpl(root, expirationTime) {\n",
      "+function commitRootImpl(root) {\n",
      "   flushPassiveEffects();\n",
      "   flushRenderPhaseStrictModeWarningsInDEV();\n",
      "   flushSuspensePriorityWarningInDEV();\n",
      "@@ -1249,8 +1247,20 @@ function commitRootImpl(root, expirationTime) {\n",
      "     workPhase !== RenderPhase && workPhase !== CommitPhase,\n",
      "     'Should not already be working.',\n",
      "   );\n",
      "-  const finishedWork = root.current.alternate;\n",
      "-  invariant(finishedWork !== null, 'Should have a work-in-progress root.');\n",
      "+\n",
      "+  const finishedWork = root.finishedWork;\n",
      "+  const expirationTime = root.finishedExpirationTime;\n",
      "+  if (finishedWork === null) {\n",
      "+    return null;\n",
      "+  }\n",
      "+  root.finishedWork = null;\n",
      "+  root.finishedExpirationTime = NoWork;\n",
      "+\n",
      "+  invariant(\n",
      "+    finishedWork !== root.current,\n",
      "+    'Cannot commit the same tree as before. This error is likely caused by ' +\n",
      "+      'a bug in React. Please file an issue.',\n",
      "+  );\n",
      " \n",
      "   // commitRoot never returns a continuation; it always finishes synchronously.\n",
      "   // So we can clear these now to allow a new callback to be scheduled.\n",
      "@@ -1794,6 +1804,12 @@ export function pingSuspendedRoot(\n",
      "   // Mark the time at which this ping was scheduled.\n",
      "   root.pingTime = suspendedTime;\n",
      " \n",
      "+  if (root.finishedExpirationTime === suspendedTime) {\n",
      "+    // If there's a pending fallback waiting to commit, throw it away.\n",
      "+    root.finishedExpirationTime = NoWork;\n",
      "+    root.finishedWork = null;\n",
      "+  }\n",
      "+\n",
      "   const currentTime = requestCurrentTime();\n",
      "   const priorityLevel = inferPriorityFromExpirationTime(\n",
      "     currentTime,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code_df.query('train_commit_id == \"76e569992b259b9e636ee68dcc7719539f4b9bb8\" & SR_file_path == \"packages/react-reconciler/src/ReactFiberScheduler.js\"').SR_diff.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s>     firstBatch._defer &&     firstBatch._expirationTime >= expirationTime   ) {    root.finishedWork = root.current.alternate;    root.pendingCommitExpirationTime = expirationTime;     scheduleCallback(NormalPriority, () => {       firstBatch._onComplete();       return null; }  function prepareFreshStack(root, expirationTime) {  root.pendingCommitExpirationTime = NoWork;  root.finishedWork = null;  root.finishedExpirationTime = NoWork;    const timeoutHandle = root.timeoutHandle;   if (timeoutHandle!== noTimeout) {     return null;   }   if (root.pendingCommitExpirationTime === expirationTime) {  if (root.finishedExpirationTime === expirationTime) {     // There's already a pending commit at this expiration time.    root.pendingCommitExpirationTime = NoWork;    return commitRoot.bind(null, root, expirationTime);    return commitRoot.bind(null, root);   }    flushPassiveEffects();   // something suspended, wait to commit it after a timeout.   stopFinishedWorkLoopTimer();   root.finishedWork = root.current.alternate;  root.finishedExpirationTime = expirationTime;   const isLocked = resolveLocksOnRoot(root, expirationTime);   if\",\n",
       " \"null, root, expirationTime);    return commitRoot.bind(null, root);   }    flushPassiveEffects();   // something suspended, wait to commit it after a timeout.   stopFinishedWorkLoopTimer();   root.finishedWork = root.current.alternate;  root.finishedExpirationTime = expirationTime;   const isLocked = resolveLocksOnRoot(root, expirationTime);   if (isLocked) {     // This root has a lock that prevents it from committing. Exit. If we begin       }       // If we're already rendering synchronously, commit the root in its       // errored state.      return commitRoot.bind(null, root, expirationTime);      return commitRoot.bind(null, root);     }     case RootSuspended: {       if (!isSync) {             // priority work to do. Instead of committing the fallback             // immediately, wait for more data to arrive.             root.timeoutHandle = scheduleTimeout(              commitRoot.bind(null, root, expirationTime),              commitRoot.bind(null, root),           \",\n",
       " \"             // immediately, wait for more data to arrive.             root.timeoutHandle = scheduleTimeout(              commitRoot.bind(null, root, expirationTime),              commitRoot.bind(null, root),               msUntilTimeout,             );             return null;         }       }       // The work expired. Commit immediately.      return commitRoot.bind(null, root, expirationTime);      return commitRoot.bind(null, root);     }     case RootCompleted: {       // The work completed. Ready to commit.      return commitRoot.bind(null, root, expirationTime);      return commitRoot.bind(null, root);     }     default: {       invariant(false, 'Unknown root exit status.');   completedWork.childExpirationTime = newChildExpirationTime; } function commitRoot(root, expirationTime) {  runWithPriority(    ImmediatePriority,    commitRootImpl.bind(null, root, expirationTime),  );function\",\n",
       " \" return commitRoot.bind(null, root);     }     default: {       invariant(false, 'Unknown root exit status.');   completedWork.childExpirationTime = newChildExpirationTime; } function commitRoot(root, expirationTime) {  runWithPriority(    ImmediatePriority,    commitRootImpl.bind(null, root, expirationTime),  );function commitRoot(root) {  runWithPriority(ImmediatePriority, commitRootImpl.bind(null, root));   // If there are passive effects, schedule a callback to flush them. This goes   // outside commitRootImpl so that it inherits the priority of the render.   if (rootWithPendingPassiveEffects!== null) {   return null; } function commitRootImpl(root, expirationTime) {function commitRootImpl(root) {   flushPassiveEffects();   flushRenderPhaseStrictModeWarningsInDEV();   flushSuspensePriorityWarningInDEV();     workPhase!== RenderPhase && workPhase!== CommitPhase,     'Should not already be working.',   );  const finishedWork = root.current.alternate;  invariant(finishedWork!== null, 'Should have a work-in-progress root.');  const finishedWork = root.finishedWork;  const expirationTime = root.finishedExpirationTime;  if (finishedWork === null) {    return null;  }  root.finishedWork = null\",\n",
       " \"!== CommitPhase,     'Should not already be working.',   );  const finishedWork = root.current.alternate;  invariant(finishedWork!== null, 'Should have a work-in-progress root.');  const finishedWork = root.finishedWork;  const expirationTime = root.finishedExpirationTime;  if (finishedWork === null) {    return null;  }  root.finishedWork = null;  root.finishedExpirationTime = NoWork;  invariant(    finishedWork!== root.current,    'Cannot commit the same tree as before. This error is likely caused by'+      'a bug in React. Please file an issue.',  );    // commitRoot never returns a continuation; it always finishes synchronously.   // So we can clear these now to allow a new callback to be scheduled.   // Mark the time at which this ping was scheduled.   root.pingTime = suspendedTime;   if (root.finishedExpirationTime === suspendedTime) {    // If there's a pending fallback waiting to commit, throw it away.    root.finishedExpirationTime = NoWork;    root.finishedWork = null;  }   const currentTime = requestCurrentTime();   const priorityLevel = inferPriorityFromExpirationTime(     currentTime,</s>\",\n",
       " 'back waiting to commit, throw it away.    root.finishedExpirationTime = NoWork;    root.finishedWork = null;  }   const currentTime = requestCurrentTime();   const priorityLevel = inferPriorityFromExpirationTime(     currentTime,</s>']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2.passage.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n",
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 25, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 350, 'psg_stride': 250}\n"
     ]
    }
   ],
   "source": [
    "def prepare_code_triplets(code_df, code_reranker, cache_file, combined_df, overwrite=False):\n",
    "    print(f'Preparing code triplets from scratch for {len(code_df)} diffs with psg_len: {code_reranker.psg_len}, psg_stride: {code_reranker.psg_stride}, psg_cnt: {code_reranker.psg_cnt}')\n",
    "\n",
    "    if cache_file and os.path.exists(cache_file) and not overwrite:\n",
    "        print(f\"Loading data from cache file: {cache_file}\")\n",
    "        return pd.read_parquet(cache_file)\n",
    "\n",
    "    # JS_LANGUAGE = Language('src/parser/my-languages.so', 'javascript')\n",
    "    # parser = Parser()\n",
    "    # parser.set_language(JS_LANGUAGE)\n",
    "\n",
    "\n",
    "\n",
    "    def prep_line(line):\n",
    "        return line.rstrip().lstrip()\n",
    "\n",
    "    def parse_diff(diff):\n",
    "        return [\n",
    "            line[1:] if line.startswith('+') else line\n",
    "            for line in diff.split('\\n')\n",
    "            if not (line.startswith('-') or len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "            and len(prep_line(line)) > 2\n",
    "        ]\n",
    "\n",
    "    # def parse_diff2(diff):\n",
    "    #    # for just having diffs with both insertions and deletions passed to the model\n",
    "    #     return [\n",
    "    #         line[1:] if (line.startswith('+') or line.startswith('-')) else line\n",
    "    #         for line in diff.split('\\n')\n",
    "    #         if not (len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "    #     ]\n",
    "\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "    def extract_function_texts(node, source_code):\n",
    "        function_texts = []\n",
    "        # Check if the node represents a function declaration\n",
    "        if node.type == 'function_declaration':\n",
    "            start_byte = node.start_byte\n",
    "            end_byte = node.end_byte\n",
    "            function_texts.append(source_code[start_byte:end_byte].decode('utf8'))\n",
    "        # Check for variable declarations that might include function expressions or arrow functions\n",
    "        elif node.type == 'variable_declaration':\n",
    "            for child in node.children:\n",
    "                if child.type == 'variable_declarator':\n",
    "                    init_node = child.child_by_field_name('init')\n",
    "                    if init_node and (init_node.type in ['function', 'arrow_function', 'function_expression']):\n",
    "                        start_byte = node.start_byte\n",
    "                        end_byte = node.end_byte\n",
    "                        function_texts.append(source_code[start_byte:end_byte].decode('utf8'))\n",
    "                        break  # Assuming one function per variable declaration for simplicity\n",
    "        # Recursively process all child nodes\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                function_texts.extend(extract_function_texts(child, source_code))\n",
    "        return function_texts\n",
    "\n",
    "    def count_matching_lines(passage_lines, diff_lines):\n",
    "        # Create a 2D array to store the lengths of the longest common subsequences\n",
    "        dp = [[0] * (len(diff_lines) + 1) for _ in range(len(passage_lines) + 1)]\n",
    "\n",
    "        # Fill the dp array\n",
    "        for i in range(1, len(passage_lines) + 1):\n",
    "            for j in range(1, len(diff_lines) + 1):\n",
    "                if prep_line(passage_lines[i - 1]) == prep_line(diff_lines[j - 1]):\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "\n",
    "        return dp[-1][-1]\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(code_df.iterrows(), total=len(code_df)):\n",
    "        # file_tokens = full_tokenize(row['SR_file_content'])\n",
    "        # total_tokens = len(file_tokens)\n",
    "        # cur_diff = combined_df[(combined_df['commit_id'] == row['SR_commit_id']) & (combined_df['file_path'] == row['SR_file_path'])]['diff'].values[0]\n",
    "        cur_diff = row['SR_diff']\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the source code to bytes for tree-sitter\n",
    "        source_code_bytes = bytes(row['SR_file_content'], \"utf8\")\n",
    "\n",
    "        # Parse the code\n",
    "        tree = parser.parse(source_code_bytes)\n",
    "\n",
    "        # Extract function texts\n",
    "        root_node = tree.root_node\n",
    "        function_texts = extract_function_texts(root_node, source_code_bytes)\n",
    "\n",
    "        # Print or return the list of full function texts\n",
    "\n",
    "        if pd.isna(cur_diff):\n",
    "            # if diff is NA/NaN, then skip this row\n",
    "            # possible when commit removes or renames this file or maybe god decided to remove the diff\n",
    "            continue\n",
    "\n",
    "        # cur_diff_lines = parse_diff2(cur_diff)\n",
    "        cur_diff_lines = parse_diff(cur_diff)\n",
    "\n",
    "        # diff_tokens = full_tokenize(''.join(cur_diff_lines))\n",
    "        # total_tokens = len(diff_tokens)\n",
    "\n",
    "\n",
    "\n",
    "        cur_triplets = []\n",
    "\n",
    "        for func in function_texts:\n",
    "\n",
    "            cur_func_lines = func.split('\\n')\n",
    "\n",
    "            # remove lines with less than 2 characters\n",
    "            cur_func_lines = [line for line in cur_func_lines if len(prep_line(line)) > 2]\n",
    "            # common_lines = set(cur_func_lines).intersection(set(cur_diff_lines))\n",
    "            common_line_count = count_matching_lines(cur_func_lines, cur_diff_lines)\n",
    "            cur_triplets.append((common_line_count, (row['train_query'], row['SR_file_path'], func, row['label'])))\n",
    "\n",
    "\n",
    "        # for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "        #     cur_passage = []\n",
    "\n",
    "        #     cur_passage.extend(diff_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "\n",
    "        #     # now convert cur_passage into a string\n",
    "        #     cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "        #     # cur_passage_lines = cur_passage_decoded.split('\\n')\n",
    "\n",
    "        #     # remove lines with less than 2 characters\n",
    "        #     # cur_passage_lines = [line for line in cur_passage_lines if len(prep_line(line)) > 2]\n",
    "\n",
    "        #     # check if there are lines matching the diff lines\n",
    "        #     # if there are, then we can add this directly to the triplets\n",
    "        #     # common_lines = set(cur_passage_lines).intersection(set(cur_diff_lines))\n",
    "        #     # common_line_count = count_matching_lines(cur_passage_lines, cur_diff_lines)\n",
    "\n",
    "        #     # add the cur_passage to cur_result_passages\n",
    "        #     # cur_triplets.append((common_line_count, (row['train_query'], row['SR_file_path'], cur_passage_decoded, row['label'])))\n",
    "        #     triplets.append((row['train_query'], row['SR_file_path'], cur_passage_decoded, row['label']))\n",
    "\n",
    "        # # sort the cur_triplets by the number of common lines\n",
    "        cur_triplets.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # # now we want to filter cur_triplets to have all tuplets with x[0] > 3 to be in order and shuffle the rest\n",
    "\n",
    "        # # now add the top code_reranker.psg_cnt to triplets\n",
    "        for triplet in cur_triplets[:code_reranker.psg_cnt]:\n",
    "            # print(f\"Found {triplet[0]} matching lines for diff in cur_passage at index\")\n",
    "            triplets.append(triplet[1])\n",
    "\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    triplets = pd.DataFrame(triplets, columns=['query', 'file_path', 'passage', 'label'])\n",
    "    if cache_file:\n",
    "        # with open(cache_file, 'wb') as file:\n",
    "        #     pickle.dump(triplets, file)\n",
    "        #     print(f\"Saved data to cache file: {cache_file}\")\n",
    "        print(f\"Saving data to cache file: {cache_file}\")\n",
    "        triplets.to_parquet(cache_file)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': 5,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': 250,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': 'sump',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 476.73 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 5, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 250, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_path = os.path.join('data', 'combined_commit_train', 'best_model')\n",
    "bert_reranker = BERTReranker(bert_params)\n",
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained(args.bert_best_model, num_labels=1, problem_type='regression')\n",
    "bert_reranker.model.to(bert_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerankers = [bert_reranker, code_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n",
      "Found gold_df, evaluating on 1 commits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 1 to 1\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   commit_id                 1 non-null      string\n",
      " 1   commit_date               1 non-null      int64 \n",
      " 2   commit_message            1 non-null      string\n",
      " 3   actual_files_modified     1 non-null      object\n",
      " 4   transformed_message_gpt4  1 non-null      object\n",
      "dtypes: int64(1), object(2), string(2)\n",
      "memory usage: 172.0+ bytes\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "bert_gold_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], rerankers=rerankers, gold_df=gold_df.iloc[1:2])\n",
    "\n",
    "print(\"BERT Gold Evaluation\")\n",
    "print(bert_gold_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = combined_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>commit_date</th>\n",
       "      <th>commit_id</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>file_path</th>\n",
       "      <th>previous_commit_id</th>\n",
       "      <th>previous_file_content</th>\n",
       "      <th>cur_file_content</th>\n",
       "      <th>diff</th>\n",
       "      <th>status</th>\n",
       "      <th>is_merge_request</th>\n",
       "      <th>file_extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook</td>\n",
       "      <td>react</td>\n",
       "      <td>1696522497</td>\n",
       "      <td>dddfe688206dafa5646550d351eb9a8e9c53654a</td>\n",
       "      <td>pull implementations from the right react-dom ...</td>\n",
       "      <td>packages/react-dom/server-rendering-stub.js</td>\n",
       "      <td>546178f9109424f6a0176ea8702a7620c4417569</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>@@ -30,7 +30,10 @@ export {\n",
       " } from './src/ser...</td>\n",
       "      <td>modified</td>\n",
       "      <td>False</td>\n",
       "      <td>js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook</td>\n",
       "      <td>react</td>\n",
       "      <td>1696521194</td>\n",
       "      <td>546178f9109424f6a0176ea8702a7620c4417569</td>\n",
       "      <td>`react-dom/server-rendering-stub`: restore exp...</td>\n",
       "      <td>packages/react-dom/server-rendering-stub.js</td>\n",
       "      <td>16619f106ab5ba8e6aca19d55be46cce22e4a7ff</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>@@ -28,3 +28,30 @@ export {\n",
       "   useFormState,\n",
       " ...</td>\n",
       "      <td>modified</td>\n",
       "      <td>False</td>\n",
       "      <td>js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook</td>\n",
       "      <td>react</td>\n",
       "      <td>1696452492</td>\n",
       "      <td>0fba3ecf73900a1b54ed6d3b0617462ac92d2fef</td>\n",
       "      <td>[Fizz] Reset error component stack and fix err...</td>\n",
       "      <td>packages/react-dom/src/__tests__/ReactDOMFizzS...</td>\n",
       "      <td>6f132439578ee11e04b41a278df51c52b0dc8563</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>@@ -981,4 +981,149 @@ describe('ReactDOMFizzSt...</td>\n",
       "      <td>modified</td>\n",
       "      <td>False</td>\n",
       "      <td>js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook</td>\n",
       "      <td>react</td>\n",
       "      <td>1696452492</td>\n",
       "      <td>0fba3ecf73900a1b54ed6d3b0617462ac92d2fef</td>\n",
       "      <td>[Fizz] Reset error component stack and fix err...</td>\n",
       "      <td>packages/react-server/src/ReactFizzServer.js</td>\n",
       "      <td>6f132439578ee11e04b41a278df51c52b0dc8563</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>@@ -1110,7 +1110,6 @@ function replaySuspenseB...</td>\n",
       "      <td>modified</td>\n",
       "      <td>False</td>\n",
       "      <td>js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook</td>\n",
       "      <td>react</td>\n",
       "      <td>1696450581</td>\n",
       "      <td>6f132439578ee11e04b41a278df51c52b0dc8563</td>\n",
       "      <td>Move ReactCurrentDispatcher back to shared int...</td>\n",
       "      <td>packages/react-server/src/ReactFlightServer.js</td>\n",
       "      <td>ca237d6f0ab986e799f192224d3066f76d66b73b</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>/**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...</td>\n",
       "      <td>@@ -108,6 +108,7 @@ import {\n",
       " } from 'shared/R...</td>\n",
       "      <td>modified</td>\n",
       "      <td>False</td>\n",
       "      <td>js</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      owner repo_name  commit_date                                 commit_id  \\\n",
       "0  facebook     react   1696522497  dddfe688206dafa5646550d351eb9a8e9c53654a   \n",
       "1  facebook     react   1696521194  546178f9109424f6a0176ea8702a7620c4417569   \n",
       "2  facebook     react   1696452492  0fba3ecf73900a1b54ed6d3b0617462ac92d2fef   \n",
       "3  facebook     react   1696452492  0fba3ecf73900a1b54ed6d3b0617462ac92d2fef   \n",
       "4  facebook     react   1696450581  6f132439578ee11e04b41a278df51c52b0dc8563   \n",
       "\n",
       "                                      commit_message  \\\n",
       "0  pull implementations from the right react-dom ...   \n",
       "1  `react-dom/server-rendering-stub`: restore exp...   \n",
       "2  [Fizz] Reset error component stack and fix err...   \n",
       "3  [Fizz] Reset error component stack and fix err...   \n",
       "4  Move ReactCurrentDispatcher back to shared int...   \n",
       "\n",
       "                                           file_path  \\\n",
       "0        packages/react-dom/server-rendering-stub.js   \n",
       "1        packages/react-dom/server-rendering-stub.js   \n",
       "2  packages/react-dom/src/__tests__/ReactDOMFizzS...   \n",
       "3       packages/react-server/src/ReactFizzServer.js   \n",
       "4     packages/react-server/src/ReactFlightServer.js   \n",
       "\n",
       "                          previous_commit_id  \\\n",
       "0  546178f9109424f6a0176ea8702a7620c4417569\n",
       "   \n",
       "1  16619f106ab5ba8e6aca19d55be46cce22e4a7ff\n",
       "   \n",
       "2  6f132439578ee11e04b41a278df51c52b0dc8563\n",
       "   \n",
       "3  6f132439578ee11e04b41a278df51c52b0dc8563\n",
       "   \n",
       "4  ca237d6f0ab986e799f192224d3066f76d66b73b\n",
       "   \n",
       "\n",
       "                               previous_file_content  \\\n",
       "0  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "1  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "2  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "3  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "4  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "\n",
       "                                    cur_file_content  \\\n",
       "0  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "1  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "2  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "3  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "4  /**\n",
       " * Copyright (c) Meta Platforms, Inc. and ...   \n",
       "\n",
       "                                                diff    status  \\\n",
       "0  @@ -30,7 +30,10 @@ export {\n",
       " } from './src/ser...  modified   \n",
       "1  @@ -28,3 +28,30 @@ export {\n",
       "   useFormState,\n",
       " ...  modified   \n",
       "2  @@ -981,4 +981,149 @@ describe('ReactDOMFizzSt...  modified   \n",
       "3  @@ -1110,7 +1110,6 @@ function replaySuspenseB...  modified   \n",
       "4  @@ -108,6 +108,7 @@ import {\n",
       " } from 'shared/R...  modified   \n",
       "\n",
       "   is_merge_request file_extension  \n",
       "0             False             js  \n",
       "1             False             js  \n",
       "2             False             js  \n",
       "3             False             js  \n",
       "4             False             js  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first row\n",
    "code = subdf.iloc[0].cur_file_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
