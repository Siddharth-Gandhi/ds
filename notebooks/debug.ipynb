{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utils import get_combined_df\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../data/2_7/facebook_react/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='../data/2_7/facebook_react/index_commit_tokenized', repo_path='../data/2_7/facebook_react', k=1000, n=100, model_path='microsoft/codebert-base', overwrite_cache=False, batch_size=32, num_epochs=10, learning_rate=5e-05, run_name='repr_0.1663', notes='reproducing current best 0.1663 MAP result for CodeReranker', num_positives=10, num_negatives=10, train_depth=1000, num_workers=8, train_commits=1000, psg_cnt=25, aggregation_strategy='sump', use_gpu=True, rerank_depth=100, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4', overwrite_eval=False, sanity_check=True, debug=False, best_model_path=None, bert_best_model='data/combined_commit_train/best_model', psg_len=350, psg_stride=250, ignore_gold_in_training=False, eval_folder='repr_0.1663', use_gpt_train=True\n",
    ")\n",
    "\n",
    "metrics =['MAP', 'P@1', 'P@10', 'P@20', 'P@30', 'MRR', 'R@1', 'R@10', 'R@100', 'R@1000']\n",
    "repo_path = args.repo_path\n",
    "repo_name = repo_path.split('/')[-1]\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'\n",
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)\n",
    "\n",
    "test_path = os.path.join('..', 'gold', 'facebook_react', 'v2_facebook_react_gpt4_gold.parquet')\n",
    "gold_df = pd.read_parquet(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = os.path.join(args.repo_path, 'cache', '4X_function_split')\n",
    "code_df = pd.read_parquet(os.path.join(cache_path, 'code_df.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1890"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.train_commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    18114\n",
       "1     5862\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_df = pd.read_parquet(os.path.join(cache_path, 'diff_code_triplets.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    305107\n",
       "1     79627\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file_path</th>\n",
       "      <th>passage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>&lt;s&gt;function resolveLocksOnRoot(root: FiberRoot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>&lt;s&gt;function prepareFreshStack(root, expiration...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>pirationTime = expirationTime;\\n  workInProgre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>&lt;s&gt;function computeUniqueAsyncExpiration(): Ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>&lt;s&gt;function scheduleUpdateOnFiber(\\n  fiber: F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>// should be deferred until the end of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>\\n  } else {\\n    // TODO: computeExpirationFo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>\\n          rootsWithPendingDiscreteUpdates.se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>&lt;s&gt;function markUpdateTimeFromFiberToRoot(fibe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>alternate!== null &amp;&amp;\\n          alternate.c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Malformed data types (`commitDetails`, `intera...   \n",
       "1  Malformed data types (`commitDetails`, `intera...   \n",
       "2  Malformed data types (`commitDetails`, `intera...   \n",
       "3  Malformed data types (`commitDetails`, `intera...   \n",
       "4  Malformed data types (`commitDetails`, `intera...   \n",
       "5  Malformed data types (`commitDetails`, `intera...   \n",
       "6  Malformed data types (`commitDetails`, `intera...   \n",
       "7  Malformed data types (`commitDetails`, `intera...   \n",
       "8  Malformed data types (`commitDetails`, `intera...   \n",
       "9  Malformed data types (`commitDetails`, `intera...   \n",
       "\n",
       "                                           file_path  \\\n",
       "0  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "1  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "2  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "3  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "4  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "5  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "6  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "7  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "8  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "9  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "\n",
       "                                             passage  label  \n",
       "0  <s>function resolveLocksOnRoot(root: FiberRoot...      0  \n",
       "1  <s>function prepareFreshStack(root, expiration...      0  \n",
       "2  pirationTime = expirationTime;\\n  workInProgre...      0  \n",
       "3  <s>function computeUniqueAsyncExpiration(): Ex...      0  \n",
       "4  <s>function scheduleUpdateOnFiber(\\n  fiber: F...      0  \n",
       "5        // should be deferred until the end of t...      0  \n",
       "6  \\n  } else {\\n    // TODO: computeExpirationFo...      0  \n",
       "7  \\n          rootsWithPendingDiscreteUpdates.se...      0  \n",
       "8  <s>function markUpdateTimeFromFiberToRoot(fibe...      0  \n",
       "9     alternate!== null &&\\n          alternate.c...      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BERTCodeReranker:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        print(f\"Initialized Code File BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        query_passage_pairs, per_result_contribution = self.split_into_query_passage_pairs(query, aggregated_results)\n",
    "\n",
    "\n",
    "        # for agg_result in aggregated_results:\n",
    "        #     query_passage_pairs.extend(\n",
    "        #         (query, result.commit_message)\n",
    "        #         for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "        #     )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for i, agg_result in enumerate(aggregated_results):\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + per_result_contribution[i] # only use psg_cnt contributing_results\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "\n",
    "    def split_into_query_passage_pairs(self, query, aggregated_results):\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        def full_tokenize(s):\n",
    "            return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        query_passage_pairs = []\n",
    "        per_result_contribution = []\n",
    "        for agg_result in aggregated_results:\n",
    "            agg_result.contributing_results.sort(key=lambda res: res.commit_date, reverse=True)\n",
    "            # get most recent file version\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            # get the file_path and commit_id\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            commit_id = most_recent_search_result.commit_id\n",
    "            # get the file content from combined_df\n",
    "            file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['cur_file_content'].values[0]\n",
    "\n",
    "            # now need to split this file content into psg_cnt passages\n",
    "            # first tokenize the file content\n",
    "\n",
    "            # warning these asserts are useless since we are using NaNs\n",
    "            assert file_content is not None, f'file_content is None for commit_id: {commit_id}, file_path: {file_path}'\n",
    "            assert file_path is not None, f'file_path is None for commit_id: {commit_id}'\n",
    "            assert query is not None, f'query is None'\n",
    "\n",
    "            query_tokens = full_tokenize(query)\n",
    "            path_tokens = full_tokenize(file_path)\n",
    "\n",
    "            if pd.isna(file_content):\n",
    "                # if file_content is NaN, then we can just set file_content to empty string\n",
    "                print(f'WARNING: file_content is NaN for commit_id: {commit_id}, file_path: {file_path}, setting file_content to empty string')\n",
    "                file_content = ''\n",
    "\n",
    "            file_tokens = full_tokenize(file_content)\n",
    "\n",
    "\n",
    "            # now split the file content into psg_cnt passages\n",
    "            cur_result_passages = []\n",
    "            # get the input ids\n",
    "            # input_ids = file_content['input_ids'].squeeze()\n",
    "            # get the number of tokens in the file content\n",
    "            total_tokens = len(file_tokens)\n",
    "\n",
    "            for cur_start in range(0, total_tokens, self.psg_stride):\n",
    "                cur_passage = []\n",
    "                # add query tokens and path tokens\n",
    "                # cur_passage.extend(query_tokens) # ??????????????\n",
    "                cur_passage.extend(path_tokens)\n",
    "\n",
    "                # add the file tokens\n",
    "                cur_passage.extend(file_tokens[cur_start:cur_start+self.psg_len])\n",
    "\n",
    "                # now convert cur_passage into a string\n",
    "                cur_passage_decoded = self.tokenizer.decode(cur_passage)\n",
    "\n",
    "                # add the cur_passage to cur_result_passages\n",
    "                cur_result_passages.append(cur_passage_decoded)\n",
    "\n",
    "                if len(cur_result_passages) == self.psg_cnt:\n",
    "                    break\n",
    "\n",
    "            # now add the query, passage pairs to query_passage_pairs\n",
    "            per_result_contribution.append(len(cur_result_passages))\n",
    "            query_passage_pairs.extend((query, passage) for passage in cur_result_passages)\n",
    "        return query_passage_pairs, per_result_contribution\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.1542, 'P@1': 0.11, 'P@10': 0.087, 'P@20': 0.063, 'P@30': 0.0517, 'MRR': 0.2133, 'R@1': 0.0509, 'R@10': 0.2285, 'R@100': 0.5077, 'R@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=BM25_AGGR_STRAT)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing/Verifying code triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple modes:\n",
    "1. Random splits\n",
    "2. Diff only\n",
    "3. Function split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with 500 gold, 500 gold + 500 normal commits, 500 + 1500 normal commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126 entries, 0 to 125\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   train_commit_id         126 non-null    object\n",
      " 1   train_query             126 non-null    object\n",
      " 2   train_original_message  126 non-null    object\n",
      " 3   SR_file_path            126 non-null    object\n",
      " 4   SR_commit_id            126 non-null    object\n",
      " 5   SR_file_content         126 non-null    object\n",
      " 6   SR_diff                 123 non-null    object\n",
      " 7   label                   126 non-null    int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 8.0+ KB\n"
     ]
    }
   ],
   "source": [
    "code_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_commit_id</th>\n",
       "      <th>train_query</th>\n",
       "      <th>train_original_message</th>\n",
       "      <th>SR_file_path</th>\n",
       "      <th>SR_commit_id</th>\n",
       "      <th>SR_file_content</th>\n",
       "      <th>SR_diff</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76e569992b259b9e636ee68dcc7719539f4b9bb8</td>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>Cleanup profile export/import data types, add ...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>cc24d0ea56b0538d1ac61dc09faedd70ced5bb47</td>\n",
       "      <td>/**\\n * Copyright (c) Facebook, Inc. and its a...</td>\n",
       "      <td>@@ -569,8 +569,6 @@ function resolveLocksOnRoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76e569992b259b9e636ee68dcc7719539f4b9bb8</td>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>Cleanup profile export/import data types, add ...</td>\n",
       "      <td>src/renderers/shared/fiber/ReactChildFiber.js</td>\n",
       "      <td>c22b94f14a809abb376f07a53f36860a7c6a342e</td>\n",
       "      <td>/**\\n * Copyright 2013-present, Facebook, Inc....</td>\n",
       "      <td>@@ -13,8 +13,7 @@\\n 'use strict';\\n \\n import ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            train_commit_id  \\\n",
       "0  76e569992b259b9e636ee68dcc7719539f4b9bb8   \n",
       "1  76e569992b259b9e636ee68dcc7719539f4b9bb8   \n",
       "\n",
       "                                         train_query  \\\n",
       "0  Malformed data types (`commitDetails`, `intera...   \n",
       "1  Malformed data types (`commitDetails`, `intera...   \n",
       "\n",
       "                              train_original_message  \\\n",
       "0  Cleanup profile export/import data types, add ...   \n",
       "1  Cleanup profile export/import data types, add ...   \n",
       "\n",
       "                                        SR_file_path  \\\n",
       "0  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "1      src/renderers/shared/fiber/ReactChildFiber.js   \n",
       "\n",
       "                               SR_commit_id  \\\n",
       "0  cc24d0ea56b0538d1ac61dc09faedd70ced5bb47   \n",
       "1  c22b94f14a809abb376f07a53f36860a7c6a342e   \n",
       "\n",
       "                                     SR_file_content  \\\n",
       "0  /**\\n * Copyright (c) Facebook, Inc. and its a...   \n",
       "1  /**\\n * Copyright 2013-present, Facebook, Inc....   \n",
       "\n",
       "                                             SR_diff  label  \n",
       "0  @@ -569,8 +569,6 @@ function resolveLocksOnRoo...      0  \n",
       "1  @@ -13,8 +13,7 @@\\n 'use strict';\\n \\n import ...      0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n",
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 25, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 350, 'psg_stride': 250}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': args.psg_cnt,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': args.rerank_depth,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': 'sump',\n",
    "        'psg_len': args.psg_len,\n",
    "        'psg_stride': args.psg_stride\n",
    "    }\n",
    "code_reranker = BERTCodeReranker(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_code_triplets(code_df, code_reranker, mode, cache_file, overwrite=False):\n",
    "    print(f\"Preparing code triplets with mode {mode} for {len(code_df)} rows.\")\n",
    "    if cache_file and os.path.exists(cache_file) and not overwrite:\n",
    "        print(f\"Loading data from cache file: {cache_file}\")\n",
    "        return pd.read_parquet(cache_file)\n",
    "\n",
    "    if mode == 'sliding_window':\n",
    "        triplets = prepare_sliding_window_triplets(code_df, code_reranker)\n",
    "    elif mode == 'parse_functions':\n",
    "        triplets = prepare_function_triplets(code_df, code_reranker)\n",
    "    elif mode == 'diff_content':\n",
    "        triplets = prepare_diff_content_triplets(code_df, code_reranker)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "    triplets_df = pd.DataFrame(triplets, columns=['query', 'file_path', 'passage', 'label'])\n",
    "    if cache_file:\n",
    "        print(f\"Saving data to cache file: {cache_file}\")\n",
    "        triplets_df.to_parquet(cache_file)\n",
    "\n",
    "    return triplets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sliding_window_triplets(code_df, code_reranker):\n",
    "\n",
    "    #### Helper functions ####\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        \n",
    "    def prep_line(line):\n",
    "        return line.rstrip().lstrip()\n",
    "\n",
    "    def parse_diff(diff):\n",
    "        return [\n",
    "            line[1:] if line.startswith('+') else line\n",
    "            for line in diff.split('\\n')\n",
    "            if not (line.startswith('-') or len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "            and len(prep_line(line)) > 2\n",
    "        ]\n",
    "        \n",
    "    def count_matching_lines(passage_lines, diff_lines):\n",
    "        # Create a 2D array to store the lengths of the longest common subsequences\n",
    "        dp = [[0] * (len(diff_lines) + 1) for _ in range(len(passage_lines) + 1)]\n",
    "\n",
    "        # Fill the dp array\n",
    "        for i in range(1, len(passage_lines) + 1):\n",
    "            for j in range(1, len(diff_lines) + 1):\n",
    "                if prep_line(passage_lines[i - 1]) == prep_line(diff_lines[j - 1]):\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "\n",
    "        return dp[-1][-1]\n",
    "\n",
    "    #### End of helper functions ####\n",
    "\n",
    "    triplets = []\n",
    "    for _, row in tqdm(code_df.iterrows(), total=len(code_df)):\n",
    "        file_tokens = full_tokenize(row['SR_file_content'])\n",
    "        total_tokens = len(file_tokens)\n",
    "        cur_diff = row['SR_diff']\n",
    "        cur_triplets = []\n",
    "        if cur_diff is None:\n",
    "            # NOTE: for cases where status is added probably or if diff was not able to be stored (encoding issue, etc)\n",
    "            # THIS WILL LEAD TO A FEW POSITIVES MISSING - don't freak out, it's normal, I checked ;)\n",
    "            continue\n",
    "\n",
    "        # Process the diffs to removee @@ stuff \n",
    "        cur_diff_lines = parse_diff(cur_diff) # split into lines and remove deletions, only additions remaining\n",
    "\n",
    "        # get the diff tokens\n",
    "        total_tokens = len(file_tokens)\n",
    "\n",
    "        for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "            cur_passage = []\n",
    "\n",
    "            # get tokens for current passage\n",
    "            cur_passage.extend(file_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "\n",
    "            # now convert current passage tokens back into a string\n",
    "            cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "            # for ranking acc. to number of common lines with diff, split on \\n\n",
    "            cur_passage_lines = cur_passage_decoded.split('\\n')\n",
    "\n",
    "            # remove lines with less than 2 characters since we do the same for diff preprocessing\n",
    "            cur_passage_lines = [line for line in cur_passage_lines if len(prep_line(line)) > 2] # otherwise empty characters and brackets match\n",
    "\n",
    "            # get number of common lines b/w diff\n",
    "            common_line_count = count_matching_lines(cur_passage_lines, cur_diff_lines)\n",
    "\n",
    "            # add the cur_passage to cur_result_passages\n",
    "            cur_triplets.append((common_line_count, (row['train_query'], row['SR_file_path'], cur_passage_decoded, row['label'])))\n",
    "\n",
    "        # # sort the cur_triplets by the number of common lines\n",
    "        cur_triplets.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # now add the top code_reranker.psg_cnt to triplets\n",
    "        for triplet in cur_triplets[:code_reranker.psg_cnt]:\n",
    "            # print(f\"Found {triplet[0]} matching lines for diff in cur_passage at index\")\n",
    "            triplets.append(triplet[1])\n",
    "    print(len(triplets))\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code triplets with mode sliding_window for 126 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:26<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = prepare_code_triplets(code_df, code_reranker, mode='sliding_window', cache_file=None, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_diff_content_triplets(code_df, code_reranker):\n",
    "\n",
    "    #### Helper functions #### \n",
    "\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "    def prep_line(line):\n",
    "        return line.rstrip().lstrip()\n",
    "        \n",
    "    def full_parse_diffs(diff):\n",
    "       # keep both insertions and deletions to be passed to the model\n",
    "        return [\n",
    "            line[1:] if (line.startswith('+') or line.startswith('-')) else line\n",
    "            for line in diff.split('\\n')\n",
    "            if not (len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "        ]\n",
    "    #### end of helper functions ####\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(code_df.iterrows(), total=len(code_df)):\n",
    "        cur_diff = row['SR_diff']\n",
    "        if cur_diff is None:\n",
    "            # NOTE: for cases where status is added probably or if diff was not able to be stored (encoding issue, etc)\n",
    "            # THIS WILL LEAD TO A FEW POSITIVES MISSING - don't freak out, it's normal, I checked ;)\n",
    "            continue\n",
    "        cur_diff_lines = full_parse_diffs(cur_diff) # keep both insertions and deletions\n",
    "        diff_tokens = full_tokenize(''.join(cur_diff_lines))\n",
    "        total_tokens = len(diff_tokens)\n",
    "        for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "            cur_passage = []\n",
    "    \n",
    "            cur_passage.extend(diff_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "    \n",
    "            # now convert cur_passage into a string\n",
    "            cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "            # add the cur_passage to cur_result_passages\n",
    "            triplets.append((row['train_query'], row['SR_file_path'], cur_passage_decoded, row['label']))\n",
    "\n",
    "        # now add the top code_reranker.psg_cnt to triplets\n",
    "        return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code triplets with mode diff_content for 126 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp2 = prepare_code_triplets(code_df, code_reranker, mode='diff_content', cache_file=None, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file_path</th>\n",
       "      <th>passage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>&lt;s&gt;     firstBatch._defer &amp;&amp;     firstBatch._e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>null, root, expirationTime);    return commitR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>// immediately, wait for more dat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>return commitRoot.bind(null, root);     }    ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>!== CommitPhase,     'Should not already be wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Malformed data types (`commitDetails`, `intera...   \n",
       "1  Malformed data types (`commitDetails`, `intera...   \n",
       "2  Malformed data types (`commitDetails`, `intera...   \n",
       "3  Malformed data types (`commitDetails`, `intera...   \n",
       "4  Malformed data types (`commitDetails`, `intera...   \n",
       "\n",
       "                                           file_path  \\\n",
       "0  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "1  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "2  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "3  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "4  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "\n",
       "                                             passage  label  \n",
       "0  <s>     firstBatch._defer &&     firstBatch._e...      0  \n",
       "1  null, root, expirationTime);    return commitR...      0  \n",
       "2               // immediately, wait for more dat...      0  \n",
       "3   return commitRoot.bind(null, root);     }    ...      0  \n",
       "4  !== CommitPhase,     'Should not already be wo...      0  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(code_df.query('train_commit_id == \"76e569992b259b9e636ee68dcc7719539f4b9bb8\" & SR_file_path == \"packages/react-reconciler/src/ReactFiberScheduler.js\"').SR_diff.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recent_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecent_df\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recent_df' is not defined"
     ]
    }
   ],
   "source": [
    "recent_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73765 entries, 0 to 73764\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   owner                  73765 non-null  string\n",
      " 1   repo_name              73765 non-null  string\n",
      " 2   commit_date            73765 non-null  int64 \n",
      " 3   commit_id              73765 non-null  string\n",
      " 4   commit_message         73765 non-null  string\n",
      " 5   file_path              73765 non-null  string\n",
      " 6   previous_commit_id     73765 non-null  string\n",
      " 7   previous_file_content  73765 non-null  string\n",
      " 8   cur_file_content       73765 non-null  string\n",
      " 9   diff                   58037 non-null  string\n",
      " 10  status                 73765 non-null  object\n",
      " 11  is_merge_request       73765 non-null  bool  \n",
      " 12  file_extension         73765 non-null  object\n",
      "dtypes: bool(1), int64(1), object(2), string(9)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_function_triplets(code_df, code_reranker):    \n",
    "    #### Helper Functions ####\n",
    "    def prep_line(line):\n",
    "        return line.rstrip().lstrip()\n",
    "\n",
    "    def parse_diff(diff):\n",
    "        return [\n",
    "            line[1:] if line.startswith('+') else line\n",
    "            for line in diff.split('\\n')\n",
    "            if not (line.startswith('-') or len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "            and len(prep_line(line)) > 2\n",
    "        ]\n",
    "\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "    def extract_function_texts(node, source_code):\n",
    "        function_texts = []\n",
    "        # Check if the node represents a function declaration\n",
    "        if node.type == 'function_declaration':\n",
    "            start_byte = node.start_byte\n",
    "            end_byte = node.end_byte\n",
    "            function_texts.append(source_code[start_byte:end_byte].decode('utf8'))\n",
    "        # Check for variable declarations that might include function expressions or arrow functions\n",
    "        elif node.type == 'variable_declaration':\n",
    "            for child in node.children:\n",
    "                if child.type == 'variable_declarator':\n",
    "                    init_node = child.child_by_field_name('init')\n",
    "                    if init_node and (init_node.type in ['function', 'arrow_function', 'function_expression']):\n",
    "                        start_byte = node.start_byte\n",
    "                        end_byte = node.end_byte\n",
    "                        function_texts.append(source_code[start_byte:end_byte].decode('utf8'))\n",
    "                        break  # Assuming one function per variable declaration for simplicity\n",
    "        # Recursively process all child nodes\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                function_texts.extend(extract_function_texts(child, source_code))\n",
    "        return function_texts\n",
    "\n",
    "    def count_matching_lines(passage_lines, diff_lines):\n",
    "        # Create a 2D array to store the lengths of the longest common subsequences\n",
    "        dp = [[0] * (len(diff_lines) + 1) for _ in range(len(passage_lines) + 1)]\n",
    "\n",
    "        # Fill the dp array\n",
    "        for i in range(1, len(passage_lines) + 1):\n",
    "            for j in range(1, len(diff_lines) + 1):\n",
    "                if prep_line(passage_lines[i - 1]) == prep_line(diff_lines[j - 1]):\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "\n",
    "        return dp[-1][-1]\n",
    "\n",
    "    #### end of helper functions #### \n",
    "\n",
    "    JS_LANGUAGE = Language('../src/parser/my-languages.so', 'javascript')\n",
    "    parser = Parser()\n",
    "    parser.set_language(JS_LANGUAGE)\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(code_df.iterrows(), total=len(code_df)):\n",
    "        file_content = row['SR_file_content']\n",
    "        cur_diff = row['SR_diff']\n",
    "\n",
    "        if cur_diff is None:\n",
    "            continue\n",
    "\n",
    "        # Convert the source code to bytes for tree-sitter\n",
    "        source_code_bytes = bytes(file_content, \"utf8\")\n",
    "\n",
    "        # Parse the code\n",
    "        tree = parser.parse(source_code_bytes)\n",
    "\n",
    "        # Extract function texts\n",
    "        root_node = tree.root_node\n",
    "        function_texts = extract_function_texts(root_node, source_code_bytes)\n",
    "\n",
    "        cur_diff_lines = parse_diff(cur_diff)\n",
    "        cur_triplets = []\n",
    "\n",
    "        for func in function_texts:\n",
    "            cur_func_lines = func.split('\\n')\n",
    "\n",
    "            # remove lines with less than 2 characters\n",
    "            cur_func_lines = [line for line in cur_func_lines if len(prep_line(line)) > 2]\n",
    "            # common_lines = set(cur_func_lines).intersection(set(cur_diff_lines))\n",
    "            common_line_count = count_matching_lines(cur_func_lines, cur_diff_lines)\n",
    "            cur_triplets.append((common_line_count, (row['train_query'], row['SR_file_path'], func, row['label'])))\n",
    "\n",
    "        # # sort the cur_triplets by the number of common lines\n",
    "        cur_triplets.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # # now we want to filter cur_triplets to have all tuplets with x[0] > 3 to be in order and shuffle the rest\n",
    "\n",
    "        # # now add the top code_reranker.psg_cnt to triplets\n",
    "        for triplet in cur_triplets[:code_reranker.psg_cnt]:\n",
    "            query, file_path, function, label = triplet[1]\n",
    "            function_tokenized = full_tokenize(function)\n",
    "            total_tokens = len(function_tokenized)\n",
    "\n",
    "            for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "                cur_passage = []\n",
    "                cur_passage.extend(function_tokenized[cur_start:cur_start+code_reranker.psg_len])\n",
    "                cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "                triplets.append((query, file_path, cur_passage_decoded, label))\n",
    "            \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the diff tokens\n",
    "# total_tokens = len(file_tokens)\n",
    "\n",
    "# for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "#     cur_passage = []\n",
    "\n",
    "#     # get tokens for current passage\n",
    "#     cur_passage.extend(file_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "\n",
    "#     # now convert current passage tokens back into a string\n",
    "#     cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "#     # for ranking acc. to number of common lines with diff, split on \\n\n",
    "#     cur_passage_lines = cur_passage_decoded.split('\\n')\n",
    "\n",
    "#     # remove lines with less than 2 characters since we do the same for diff preprocessing\n",
    "#     cur_passage_lines = [line for line in cur_passage_lines if len(prep_line(line)) > 2] # otherwise empty characters and brackets match\n",
    "\n",
    "#     # get number of common lines b/w diff\n",
    "#     common_line_count = count_matching_lines(cur_passage_lines, cur_diff_lines)\n",
    "\n",
    "#     # add the cur_passage to cur_result_passages\n",
    "#     cur_triplets.append((common_line_count, (row['train_query'], row['SR_file_path'], cur_passage_decoded, row['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code triplets with mode parse_functions for 6631 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6631 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp3 = prepare_code_triplets(code_df, code_reranker, mode='parse_functions', cache_file=None, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12 entries, 0 to 11\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      12 non-null     object\n",
      " 1   file_path  12 non-null     object\n",
      " 2   passage    12 non-null     object\n",
      " 3   label      12 non-null     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 512.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "tmp3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code triplets with mode parse_functions for 6631 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 324/6631 [00:26<08:27, 12.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tmp4 \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_code_triplets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_reranker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparse_functions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 10\u001b[0m, in \u001b[0;36mprepare_code_triplets\u001b[0;34m(code_df, code_reranker, mode, cache_file, overwrite)\u001b[0m\n\u001b[1;32m      8\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m prepare_sliding_window_triplets(code_df, code_reranker)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_functions\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_function_triplets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_reranker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiff_content\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m prepare_diff_content_triplets(code_df, code_reranker)\n",
      "Cell \u001b[0;32mIn[169], line 105\u001b[0m, in \u001b[0;36mprepare_function_triplets\u001b[0;34m(code_df, code_reranker)\u001b[0m\n\u001b[1;32m    103\u001b[0m             cur_passage \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    104\u001b[0m             cur_passage\u001b[38;5;241m.\u001b[39mextend(function_tokenized[cur_start:cur_start\u001b[38;5;241m+\u001b[39mcode_reranker\u001b[38;5;241m.\u001b[39mpsg_len])\n\u001b[0;32m--> 105\u001b[0m             cur_passage_decoded \u001b[38;5;241m=\u001b[39m \u001b[43mcode_reranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_passage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m             triplets\u001b[38;5;241m.\u001b[39mappend((query, file_path, cur_passage_decoded, label))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m triplets\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3736\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3715\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3716\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m-> 3736\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3739\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3740\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3741\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3743\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/utils/generic.py:247\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [to_py_obj(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/utils/generic.py:247\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/utils/generic.py:252\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    250\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtest_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m framework_to_py_obj[framework](obj)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tmp4 = prepare_code_triplets(code_df, code_reranker, mode='parse_functions', cache_file=None, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      22 non-null     object\n",
      " 1   file_path  22 non-null     object\n",
      " 2   passage    22 non-null     object\n",
      " 3   label      22 non-null     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 832.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "tmp4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_commit_id</th>\n",
       "      <th>train_query</th>\n",
       "      <th>train_original_message</th>\n",
       "      <th>SR_file_path</th>\n",
       "      <th>SR_commit_id</th>\n",
       "      <th>SR_file_content</th>\n",
       "      <th>SR_diff</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76e569992b259b9e636ee68dcc7719539f4b9bb8</td>\n",
       "      <td>Malformed data types (`commitDetails`, `intera...</td>\n",
       "      <td>Cleanup profile export/import data types, add ...</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberSchedu...</td>\n",
       "      <td>cc24d0ea56b0538d1ac61dc09faedd70ced5bb47</td>\n",
       "      <td>/**\\n * Copyright (c) Facebook, Inc. and its a...</td>\n",
       "      <td>@@ -569,8 +569,6 @@ function resolveLocksOnRoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            train_commit_id  \\\n",
       "0  76e569992b259b9e636ee68dcc7719539f4b9bb8   \n",
       "\n",
       "                                         train_query  \\\n",
       "0  Malformed data types (`commitDetails`, `intera...   \n",
       "\n",
       "                              train_original_message  \\\n",
       "0  Cleanup profile export/import data types, add ...   \n",
       "\n",
       "                                        SR_file_path  \\\n",
       "0  packages/react-reconciler/src/ReactFiberSchedu...   \n",
       "\n",
       "                               SR_commit_id  \\\n",
       "0  cc24d0ea56b0538d1ac61dc09faedd70ced5bb47   \n",
       "\n",
       "                                     SR_file_content  \\\n",
       "0  /**\\n * Copyright (c) Facebook, Inc. and its a...   \n",
       "\n",
       "                                             SR_diff  label  \n",
       "0  @@ -569,8 +569,6 @@ function resolveLocksOnRoo...      0  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11609"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6631 entries, 0 to 6630\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   train_commit_id         6631 non-null   object\n",
      " 1   train_query             6631 non-null   object\n",
      " 2   train_original_message  6631 non-null   object\n",
      " 3   SR_file_path            6631 non-null   object\n",
      " 4   SR_commit_id            6631 non-null   object\n",
      " 5   SR_file_content         6631 non-null   object\n",
      " 6   SR_diff                 6446 non-null   object\n",
      " 7   label                   6631 non-null   int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 414.6+ KB\n"
     ]
    }
   ],
   "source": [
    "code_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
