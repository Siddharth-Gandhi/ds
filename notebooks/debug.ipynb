{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utils import get_combined_df, prepare_code_triplets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../data/2_7/facebook_react/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='../data/2_7/facebook_react/index_commit_tokenized', repo_path='../data/2_7/facebook_react', k=1000, n=100, model_path='microsoft/codebert-base', overwrite_cache=False, batch_size=32, num_epochs=10, learning_rate=5e-05, run_name='repr_0.1663', notes='reproducing current best 0.1663 MAP result for CodeReranker', num_positives=10, num_negatives=10, train_depth=1000, num_workers=8, train_commits=1000, psg_cnt=25, aggregation_strategy='sump', use_gpu=True, rerank_depth=100, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4', overwrite_eval=False, sanity_check=True, debug=False, best_model_path=None, bert_best_model='data/combined_commit_train/best_model', psg_len=350, psg_stride=250, ignore_gold_in_training=False, eval_folder='repr_0.1663', use_gpt_train=True\n",
    ")\n",
    "\n",
    "metrics =['MAP', 'P@1', 'P@10', 'P@20', 'P@30', 'MRR', 'R@1', 'R@10', 'R@100', 'R@1000']\n",
    "repo_path = args.repo_path\n",
    "repo_name = repo_path.split('/')[-1]\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'\n",
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)\n",
    "\n",
    "test_path = os.path.join('..', 'gold', 'facebook_react', 'v2_facebook_react_gpt4_gold.parquet')\n",
    "gold_df = pd.read_parquet(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = os.path.join(args.repo_path, 'cache', 'X_diff_split')\n",
    "code_df = pd.read_parquet('../data/2_7/facebook_react/cache/repr_0.1663/code_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_line(line):\n",
    "    return line.rstrip().lstrip()\n",
    "\n",
    "def parse_diff_remove_minus(diff):\n",
    "    return [\n",
    "        line[1:] if line.startswith('+') else line\n",
    "        for line in diff.split('\\n')\n",
    "        if not (line.startswith('-') or len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "        and len(prep_line(line)) > 2\n",
    "    ]\n",
    "\n",
    "def full_parse_diffs(diff):\n",
    "   # keep both insertions and deletions to be passed to the model\n",
    "    return [\n",
    "        line[1:] if (line.startswith('+') or line.startswith('-')) else line\n",
    "        for line in diff.split('\\n')\n",
    "        if not (len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "    ]\n",
    "\n",
    "def full_parse_diffs_split(diff):\n",
    "   # keep both insertions and deletions to be passed to the model\n",
    "    res = []\n",
    "    cur = []\n",
    "    for line in diff.split('\\n'):\n",
    "        if not len(line) == 0:\n",
    "            if (line.startswith('@@') and line.count('@@') > 1):\n",
    "                if cur:\n",
    "                    res.append(cur)\n",
    "                cur = []\n",
    "            else:\n",
    "                cur.append(line[1:] if (line.startswith('+') or line.startswith('-')) else line)\n",
    "    if cur:\n",
    "        res.append(cur)\n",
    "    return res\n",
    "\n",
    "def full_tokenize(s, tokenizer):\n",
    "        return tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff_tokens(diff):\n",
    "    ntokens = len(full_tokenize(diff, tokenizer))\n",
    "    return ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_df = pd.read_parquet('../data/merged_code_df/multi_code_df.parquet')\n",
    "# code_df.train_commit_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61173/61173 [02:48<00:00, 362.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95828835, 1598.1594176311664)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average token in each diff (only insertions)\n",
    "total_rows = 0\n",
    "total_diff_tokens = 0\n",
    "for i, row in tqdm(code_df.iterrows(), total=code_df.shape[0]):\n",
    "    diff = row.SR_diff\n",
    "    if diff or not pd.isna(diff):\n",
    "        total_diff_tokens += find_diff_tokens(diff)\n",
    "        total_rows += 1\n",
    "\n",
    "total_diff_tokens, total_diff_tokens / total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61173/61173 [02:46<00:00, 367.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(85946036, 326.5960472265606)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average number of tokens in each diff split\n",
    "total_diff_splits = 0\n",
    "total_diff_split_tokens = 0\n",
    "for i, row in tqdm(code_df.iterrows(), total=code_df.shape[0]):\n",
    "    diff = row.SR_diff\n",
    "    if diff or not pd.isna(diff):\n",
    "        diff_split_list = full_parse_diffs_split(diff)\n",
    "        total_diff_splits += len(diff_split_list)\n",
    "        for diff_split in diff_split_list:\n",
    "            total_diff_split_tokens += find_diff_tokens('\\n'.join(diff_split))\n",
    "\n",
    "total_diff_split_tokens, total_diff_split_tokens/total_diff_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.388729528701511"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average splits per diff (number of @@ -- @@ changes)\n",
    "# so this will be number of distinct places where the file is edited\n",
    "total_diff_splits / total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_code_triplets(code_df, args, mode, cache_file, overwrite=False):\n",
    "    if not mode:\n",
    "        raise ValueError(f\"Mode: {mode} must be specified for preparing code triplets\")\n",
    "\n",
    "    print(f\"Preparing code triplets with mode {mode} for {len(code_df)} rows.\")\n",
    "    if cache_file and os.path.exists(cache_file) and not overwrite:\n",
    "        print(f\"Loading data from cache file: {cache_file}\")\n",
    "        return pd.read_parquet(cache_file)\n",
    "\n",
    "    if mode == 'sliding_window':\n",
    "        triplets = prepare_sliding_window_triplets(code_df, args)\n",
    "    elif mode == 'parse_functions':\n",
    "        triplets = prepare_function_triplets(code_df, args)\n",
    "    elif mode == 'diff_content':\n",
    "        triplets = prepare_diff_content_triplets(code_df, args)\n",
    "    elif mode == 'diff_subsplit':\n",
    "      triplets = prepare_split_diff_triplets(code_df, args)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "    triplets_df = pd.DataFrame(triplets, columns=['query', 'file_path', 'passage', 'label'])\n",
    "    if cache_file:\n",
    "        print(f\"Saving data to cache file: {cache_file}\")\n",
    "        triplets_df.to_parquet(cache_file)\n",
    "\n",
    "    print(triplets_df.head(5))\n",
    "\n",
    "    return triplets_df\n",
    "\n",
    "def prepare_split_diff_triplets(code_df, args):\n",
    "    print('Preparing triplets split by diff content (further subplit at @@)')\n",
    "    #### Helper functions ####\n",
    "    #### end of helper functions ####\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(code_df.iterrows(), total=len(code_df)):\n",
    "        cur_diff = row['SR_diff']\n",
    "        if cur_diff is None or pd.isna(cur_diff):\n",
    "            # NOTE: for cases where status is added probably or if diff was not able to be stored (encoding issue, etc)\n",
    "            # THIS WILL LEAD TO A FEW POSITIVES MISSING - don't freak out, it's normal, I checked ;)\n",
    "            continue\n",
    "        diff_split_list = full_parse_diffs_split(cur_diff) # keep both insertions and deletions\n",
    "        for diff_split in diff_split_list:\n",
    "          triplets.append((row['train_query'], row['SR_file_path'], '\\n'.join(diff_split), row['label']))\n",
    "\n",
    "    # now add the top code_reranker.psg_cnt to triplets\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_df = pd.read_parquet(os.path.join(cache_path, 'diff_code_triplets.parquet'))\n",
    "# triplets_df = prepare_code_triplets(code_df, args, mode='diff_subsplit', cache_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31734 entries, 0 to 31733\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      31734 non-null  object\n",
      " 1   file_path  31734 non-null  object\n",
      " 2   passage    31734 non-null  object\n",
      " 3   label      31734 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 991.8+ KB\n"
     ]
    }
   ],
   "source": [
    "triplets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    26940\n",
       "1    12169\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/2_7/facebook_react/cache/4X_random_split'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39109 entries, 0 to 39108\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      39109 non-null  object\n",
      " 1   file_path  39109 non-null  object\n",
      " 2   passage    39109 non-null  object\n",
      " 3   label      39109 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "triplets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@@ -569,8 +569,6 @@ function resolveLocksOnRoot(root: FiberRoot, expirationTime: ExpirationTime) {\\n     firstBatch._defer &&\\n     firstBatch._expirationTime >= expirationTime\\n   ) {\\n-    root.finishedWork = root.current.alternate;\\n-    root.pendingCommitExpirationTime = expirationTime;\\n     scheduleCallback(NormalPriority, () => {\\n       firstBatch._onComplete();\\n       return null;\\n@@ -689,7 +687,8 @@ export function flushControlled(fn: () => mixed): void {\\n }\\n \\n function prepareFreshStack(root, expirationTime) {\\n-  root.pendingCommitExpirationTime = NoWork;\\n+  root.finishedWork = null;\\n+  root.finishedExpirationTime = NoWork;\\n \\n   const timeoutHandle = root.timeoutHandle;\\n   if (timeoutHandle !== noTimeout) {\\n@@ -741,10 +740,9 @@ function renderRoot(\\n     return null;\\n   }\\n \\n-  if (root.pendingCommitExpirationTime === expirationTime) {\\n+  if (root.finishedExpirationTime === expirationTime) {\\n     // There's already a pending commit at this expiration time.\\n-    root.pendingCommitExpirationTime = NoWork;\\n-    return commitRoot.bind(null, root, expirationTime);\\n+    return commitRoot.bind(null, root);\\n   }\\n \\n   flushPassiveEffects();\\n@@ -867,6 +865,9 @@ function renderRoot(\\n   // something suspended, wait to commit it after a timeout.\\n   stopFinishedWorkLoopTimer();\\n \\n+  root.finishedWork = root.current.alternate;\\n+  root.finishedExpirationTime = expirationTime;\\n+\\n   const isLocked = resolveLocksOnRoot(root, expirationTime);\\n   if (isLocked) {\\n     // This root has a lock that prevents it from committing. Exit. If we begin\\n@@ -905,7 +906,7 @@ function renderRoot(\\n       }\\n       // If we're already rendering synchronously, commit the root in its\\n       // errored state.\\n-      return commitRoot.bind(null, root, expirationTime);\\n+      return commitRoot.bind(null, root);\\n     }\\n     case RootSuspended: {\\n       if (!isSync) {\\n@@ -929,7 +930,7 @@ function renderRoot(\\n             // priority work to do. Instead of committing the fallback\\n             // immediately, wait for more data to arrive.\\n             root.timeoutHandle = scheduleTimeout(\\n-              commitRoot.bind(null, root, expirationTime),\\n+              commitRoot.bind(null, root),\\n               msUntilTimeout,\\n             );\\n             return null;\\n@@ -937,11 +938,11 @@ function renderRoot(\\n         }\\n       }\\n       // The work expired. Commit immediately.\\n-      return commitRoot.bind(null, root, expirationTime);\\n+      return commitRoot.bind(null, root);\\n     }\\n     case RootCompleted: {\\n       // The work completed. Ready to commit.\\n-      return commitRoot.bind(null, root, expirationTime);\\n+      return commitRoot.bind(null, root);\\n     }\\n     default: {\\n       invariant(false, 'Unknown root exit status.');\\n@@ -1223,11 +1224,8 @@ function resetChildExpirationTime(completedWork: Fiber) {\\n   completedWork.childExpirationTime = newChildExpirationTime;\\n }\\n \\n-function commitRoot(root, expirationTime) {\\n-  runWithPriority(\\n-    ImmediatePriority,\\n-    commitRootImpl.bind(null, root, expirationTime),\\n-  );\\n+function commitRoot(root) {\\n+  runWithPriority(ImmediatePriority, commitRootImpl.bind(null, root));\\n   // If there are passive effects, schedule a callback to flush them. This goes\\n   // outside commitRootImpl so that it inherits the priority of the render.\\n   if (rootWithPendingPassiveEffects !== null) {\\n@@ -1240,7 +1238,7 @@ function commitRoot(root, expirationTime) {\\n   return null;\\n }\\n \\n-function commitRootImpl(root, expirationTime) {\\n+function commitRootImpl(root) {\\n   flushPassiveEffects();\\n   flushRenderPhaseStrictModeWarningsInDEV();\\n   flushSuspensePriorityWarningInDEV();\\n@@ -1249,8 +1247,20 @@ function commitRootImpl(root, expirationTime) {\\n     workPhase !== RenderPhase && workPhase !== CommitPhase,\\n     'Should not already be working.',\\n   );\\n-  const finishedWork = root.current.alternate;\\n-  invariant(finishedWork !== null, 'Should have a work-in-progress root.');\\n+\\n+  const finishedWork = root.finishedWork;\\n+  const expirationTime = root.finishedExpirationTime;\\n+  if (finishedWork === null) {\\n+    return null;\\n+  }\\n+  root.finishedWork = null;\\n+  root.finishedExpirationTime = NoWork;\\n+\\n+  invariant(\\n+    finishedWork !== root.current,\\n+    'Cannot commit the same tree as before. This error is likely caused by ' +\\n+      'a bug in React. Please file an issue.',\\n+  );\\n \\n   // commitRoot never returns a continuation; it always finishes synchronously.\\n   // So we can clear these now to allow a new callback to be scheduled.\\n@@ -1794,6 +1804,12 @@ export function pingSuspendedRoot(\\n   // Mark the time at which this ping was scheduled.\\n   root.pingTime = suspendedTime;\\n \\n+  if (root.finishedExpirationTime === suspendedTime) {\\n+    // If there's a pending fallback waiting to commit, throw it away.\\n+    root.finishedExpirationTime = NoWork;\\n+    root.finishedWork = null;\\n+  }\\n+\\n   const currentTime = requestCurrentTime();\\n   const priorityLevel = inferPriorityFromExpirationTime(\\n     currentTime,\\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df.iloc[0].SR_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" return commitRoot.bind(null, root);     }     default: {       invariant(false, 'Unknown root exit status.');   completedWork.childExpirationTime = newChildExpirationTime; } function commitRoot(root, expirationTime) {  runWithPriority(    ImmediatePriority,    commitRootImpl.bind(null, root, expirationTime),  );function commitRoot(root) {  runWithPriority(ImmediatePriority, commitRootImpl.bind(null, root));   // If there are passive effects, schedule a callback to flush them. This goes   // outside commitRootImpl so that it inherits the priority of the render.   if (rootWithPendingPassiveEffects!== null) {   return null; } function commitRootImpl(root, expirationTime) {function commitRootImpl(root) {   flushPassiveEffects();   flushRenderPhaseStrictModeWarningsInDEV();   flushSuspensePriorityWarningInDEV();     workPhase!== RenderPhase && workPhase!== CommitPhase,     'Should not already be working.',   );  const finishedWork = root.current.alternate;  invariant(finishedWork!== null, 'Should have a work-in-progress root.');  const finishedWork = root.finishedWork;  const expirationTime = root.finishedExpirationTime;  if (finishedWork === null) {    return null;  }  root.finishedWork = null\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_df.iloc[3].passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    328390\n",
       "1     93438\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_paths = [\n",
    "    \"../data/2_7/apache_spark\",\n",
    "    \"../data/2_7/apache_kafka\",\n",
    "    \"../data/2_8/angular_angular\",\n",
    "    \"../data/2_8/django_django\",\n",
    "    \"../data/2_8/pytorch_pytorch\",\n",
    "    \"../data/2_7/julialang_julia\",\n",
    "    \"../data/2_7/ruby_ruby\",\n",
    "    \"../data/2_9/huggingface_transformers\",\n",
    "    \"../data/2_9/redis_redis\",\n",
    "    \"../data/2_7/facebook_react\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for repo in repo_paths:\n",
    "    gold_df_path = os.path.join('..', 'gold', repo, f'v2_{repo}_gpt4_train.parquet')\n",
    "    gold_df = pd.read_parquet(gold_df_path)\n",
    "    dfs.append(gold_df)\n",
    "\n",
    "print(len(dfs))\n",
    "big_gold_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   commit_id                 5000 non-null   string\n",
      " 1   commit_date               5000 non-null   int64 \n",
      " 2   commit_message            5000 non-null   string\n",
      " 3   actual_files_modified     5000 non-null   object\n",
      " 4   transformed_message_gpt4  5000 non-null   object\n",
      "dtypes: int64(1), object(2), string(2)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "big_gold_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_gold_df.to_parquet('merged_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_code_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# code_df_list = []\n",
    "# print(repo_paths)\n",
    "# for repo_path in repo_paths:\n",
    "#     repo_name = repo_path.split('/')[-1]\n",
    "#     print(f'processing {repo_name}')\n",
    "#     index_path = os.path.join(repo_path, 'index_commit_tokenized')\n",
    "#     K = args.k\n",
    "#     n = args.n\n",
    "#     combined_df = get_combined_df(repo_path)\n",
    "#     BM25_AGGR_STRAT = 'sump'\n",
    "#     eval_path = os.path.join(repo_path, 'eval')\n",
    "#     if not os.path.exists(eval_path):\n",
    "#         os.makedirs(eval_path)\n",
    "    \n",
    "#     bm25_searcher = BM25Searcher(index_path)\n",
    "#     evaluator = SearchEvaluator(metrics)\n",
    "#     model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)\n",
    "    \n",
    "#     gold_df_path = os.path.join('..', 'gold', repo_name, f'v2_{repo_name}_gpt4_train.parquet')\n",
    "\n",
    "#     recent_df = pd.read_parquet(gold_df_path)\n",
    "#     recent_df = recent_df.rename(columns={'commit_message': 'original_message', f'transformed_message_{args.openai_model}': 'commit_message'})\n",
    "#     cache_path = f'{repo_name}_code_df.parquet'\n",
    "#     code_df = get_code_df(recent_df, bm25_searcher, args.train_depth, args.num_positives, args.num_negatives, combined_df, cache_path, False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
