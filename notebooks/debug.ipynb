{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bm25_v2 import BM25Searcher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import get_combined_df, Args\n",
    "from eval import SearchEvaluator, ModelEvaluator\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"data/2_7/facebook_react\"\n",
    "repo_name = data_path.split(\"/\")[-1]\n",
    "combined_df = get_combined_df(data_path)\n",
    "index_path=f\"{data_path}/index_commit_tokenized\"\n",
    "# REPO_DIR = '/home/ssg2/ssg2/ds/repos/facebook_react'\n",
    "k=10000 # initial ranker depth\n",
    "n=100 # number of samples to evaluate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fid_to_path and path_to_fid json files to dicts\n",
    "with open(f\"facebook_react_FID_to_paths.json\") as f:\n",
    "    fid_to_path = json.load(f)\n",
    "\n",
    "# make all fids ints\n",
    "fid_to_path = {int(k): v for k, v in fid_to_path.items()}\n",
    "\n",
    "with open(f\"facebook_react_path_to_FID.json\") as f:\n",
    "    path_to_fid = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at data/2_7/facebook_react/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "metrics = ['MAP', 'P@1', 'P@10', 'P@20', 'P@30', 'MRR', 'R@1', 'R@10', 'R@100', 'R@1000']\n",
    "bm25_searcher = BM25Searcher(index_path, fid_to_path, path_to_fid)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df, fid_to_path, path_to_fid)\n",
    "gold_df = pd.read_parquet(f\"gold/{repo_name}/v2_{repo_name}_gpt4_gold.parquet\")\n",
    "#! rename commit_message to original_message\n",
    "gold_df = gold_df.rename(columns={'commit_message': 'original_message'})\n",
    "#!rename transformed_message to commit_message\n",
    "gold_df = gold_df.rename(columns={f'transformed_message_gpt4': 'commit_message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Default setting for \"collapse new nodes\" is enabled causing inconvenience. Settings popup UI does not adapt properly to smaller sizes, and \"Inspect the matching DOM element\" button is visible in standalone mode unnecessarily. Also, there is an issue with undefined window.addEventListener/window.removeEventListener in Hermes and incorrect viewing size for settings icon.',\n",
       " array(['packages/react-devtools-core/src/standalone.js',\n",
       "        'src/backend/renderer.js',\n",
       "        'src/backend/views/Highlighter/index.js', 'src/devtools/store.js',\n",
       "        'src/devtools/views/ButtonIcon.js',\n",
       "        'src/devtools/views/Components/Components.js',\n",
       "        'src/devtools/views/Components/SelectedElement.js',\n",
       "        'src/devtools/views/DevTools.js', 'src/devtools/views/Icon.js',\n",
       "        'src/devtools/views/Profiler/Profiler.js',\n",
       "        'src/devtools/views/Settings/ComponentsSettings.js',\n",
       "        'src/devtools/views/Settings/SettingsContext.js',\n",
       "        'src/devtools/views/Settings/SettingsModal.js',\n",
       "        'src/devtools/views/TabBar.js'], dtype=object),\n",
       " [2234,\n",
       "  1382,\n",
       "  1387,\n",
       "  1394,\n",
       "  1398,\n",
       "  1402,\n",
       "  1433,\n",
       "  1442,\n",
       "  1445,\n",
       "  1467,\n",
       "  1492,\n",
       "  1495,\n",
       "  1497,\n",
       "  1503])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id, test_date, test_orig, test_files, test_query = gold_df.iloc[0]\n",
    "test_files_ids = [path_to_fid[f] for f in test_files]\n",
    "test_query, test_files, test_files_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results = bm25_searcher.search(test_query, test_date, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = bm25_searcher.aggregate_file_scores(bm25_results, 'maxp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52.900299072265625,\n",
       " 52.900299072265625,\n",
       " 52.90029525756836,\n",
       " 52.90029525756836,\n",
       " 52.90029525756836,\n",
       " 52.90029525756836,\n",
       " 52.900291442871094,\n",
       " 52.900291442871094,\n",
       " 52.900291442871094,\n",
       " 52.90028762817383]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.score for o in aggregated_results][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_fid = [o.fid for o in aggregated_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0066,\n",
       " 'P@1': 0.0,\n",
       " 'P@10': 0.0,\n",
       " 'P@20': 0.0,\n",
       " 'P@30': 0.0,\n",
       " 'MRR': 0.0014,\n",
       " 'R@1': 0.0,\n",
       " 'R@10': 0.0,\n",
       " 'R@100': 0.0,\n",
       " 'R@1000': 0.6429}"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(aggregated_results, test_files_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[721, 738, 742, 744, 745, 754, 757, 759, 762]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get indices of all test_file_ids in bm25_fid\n",
    "[bm25_fid.index(fid) for fid in test_files_ids if fid in bm25_fid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why this file doesn't exist\n",
    "# path_to_fid['packages/react/src/ReactServerSharedInternals.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Default setting for \"collapse new nodes\" is enabled causing inconvenience. Settings popup UI does not adapt properly to smaller sizes, and \"Inspect the matching DOM element\" button is visible in standalone mode unnecessarily. Also, there is an issue with undefined window.addEventListener/window.removeEventListener in Hermes and incorrect viewing size for settings icon.'"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/codebert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = test_query\n",
    "text2 = aggregated_results[0].contributing_results[0].commit_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text1, text2, return_tensors=\"pt\", padding=True, truncation='only_second', max_length=tokenizer.model_max_length, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5136816501617432"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "inputs.to('cuda')\n",
    "# Predict with model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Assuming the model has been fine-tuned for regression task\n",
    "# Convert logits to similarity score (sigmoid function to get value between 0 and 1)\n",
    "\n",
    "# get the similarity score\n",
    "similarity_score = torch.softmax(outputs.logits.squeeze(), dim=0)[1].item()\n",
    "model.train()\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe of (txt1, tx2, 1) repeating for 100 rows\n",
    "# train_df = pd.DataFrame({'txt1': [text1]*100, 'txt2': [text2]*100, 'label': [1]*100})\n",
    "train_df = pd.read_pickle('data/2_7/facebook_react/cache/triplet_data_cache.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>passage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fix getSnapshot warning when a selector return...</td>\n",
       "      <td>[useSES/extra] Reuse old selection if possible...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fix getSnapshot warning when a selector return...</td>\n",
       "      <td>Add warning and test for useSyncExternalStore ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fix getSnapshot warning when a selector return...</td>\n",
       "      <td>fix: useSyncExternalStoreExtra (#22500)\\n\\n* m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fix getSnapshot warning when a selector return...</td>\n",
       "      <td>Implement getServerSnapshot in userspace shim ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fix getSnapshot warning when a selector return...</td>\n",
       "      <td>Implement useSyncExternalStore on server (#223...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  fix getSnapshot warning when a selector return...   \n",
       "1  fix getSnapshot warning when a selector return...   \n",
       "2  fix getSnapshot warning when a selector return...   \n",
       "3  fix getSnapshot warning when a selector return...   \n",
       "4  fix getSnapshot warning when a selector return...   \n",
       "\n",
       "                                             passage  label  \n",
       "0  [useSES/extra] Reuse old selection if possible...      1  \n",
       "1  Add warning and test for useSyncExternalStore ...      1  \n",
       "2  fix: useSyncExternalStoreExtra (#22500)\\n\\n* m...      1  \n",
       "3  Implement getServerSnapshot in userspace shim ...      1  \n",
       "4  Implement useSyncExternalStore on server (#223...      1  "
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# repo_paths = [\n",
    "#     \"data/2_7/apache_spark\",\n",
    "#     \"data/2_7/apache_kafka\",\n",
    "#     \"data/2_7/facebook_react\",\n",
    "#     \"data/2_8/angular_angular\",\n",
    "#     \"data/2_8/django_django\",\n",
    "#     \"data/2_8/pytorch_pytorch\",\n",
    "#     \"data/2_7/julialang_julia\",\n",
    "#     \"data/2_7/ruby_ruby\",\n",
    "#     \"data/2_9/huggingface_transformers\",\n",
    "#     \"data/2_9/redis_redis\",\n",
    "# ]\n",
    "# combined_triplet_data = pd.DataFrame()\n",
    "# for data_path in tqdm(repo_paths, total=len(repo_paths)):\n",
    "#     triplet_cache = os.path.join(data_path, 'cache', 'gpt_triplet_data_cache.pkl')\n",
    "#     if os.path.exists(triplet_cache):\n",
    "#         repo_triplet_data = pd.read_pickle(triplet_cache)\n",
    "#         combined_triplet_data = pd.concat([combined_triplet_data, repo_triplet_data], ignore_index=True)\n",
    "#     else:\n",
    "#         print(f\"Warning: Triplet cache not found for {data_path}, skipping this repository.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = combined_triplet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3cdc47594b420b881fc6bb477bf74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d729fa5d682b4403a1adb0121e5e7c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['query'], example['passage'], return_tensors=\"pt\", padding=True, truncation='only_second', max_length=tokenizer.model_max_length, add_special_tokens=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.map(lambda examples: {'labels': examples['label']}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./tmp',          # output directory for model checkpoints\n",
    "    evaluation_strategy=\"no\",     # evaluation is done at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    weight_decay=0.01,               # strength of weight decay,\n",
    "    report_to='none',\n",
    "    fp16=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=bert_reranker.model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='data/2_7/facebook_react/index_commit_tokenized',\n",
    "    repo_path='data/2_7/facebook_react', k=10000, n=100,\n",
    "    model_path='microsoft/codebert-base', overwrite_cache=False,\n",
    "    batch_size=32, num_epochs=3, learning_rate=2e-05,\n",
    "    run_name='debug',\n",
    "    notes='debug (ignore)',\n",
    "    num_positives=10, num_negatives=10, train_depth=10000, num_workers=8,\n",
    "    train_commits=1000, psg_cnt=25, use_gpu=True,\n",
    "    rerank_depth=250, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4',\n",
    "    overwrite_eval=False, sanity_check=True, debug=False,\n",
    "    psg_len=350, psg_stride=250, ignore_gold_in_training=False,\n",
    "    eval_folder='debug', use_gpt_train=True,\n",
    "    aggregation_strategy='sump',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'microsoft/codebert-base',\n",
       " 'psg_cnt': 5,\n",
       " 'aggregation_strategy': 'maxp',\n",
       " 'batch_size': 32,\n",
       " 'use_gpu': True,\n",
       " 'rerank_depth': 1000,\n",
       " 'num_epochs': 3,\n",
       " 'lr': 2e-05,\n",
       " 'num_positives': 10,\n",
       " 'num_negatives': 10,\n",
       " 'train_depth': 10000,\n",
       " 'num_workers': 8,\n",
       " 'train_commits': 1000,\n",
       " 'bm25_aggr_strategy': 'maxp',\n",
       " 'output_length': 1000}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': 5,\n",
    "        'aggregation_strategy': 'maxp',\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': 1000,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': 'maxp',\n",
    "        'output_length': 1000\n",
    "    }\n",
    "bert_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BERTReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 6230.80 MB\n",
      "Using classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 5, 'aggregation_strategy': 'maxp', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 1000, 'num_epochs': 3, 'lr': 2e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 10000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'maxp', 'output_length': 1000}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(bert_params, 'classification')\n",
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained('/home/ssg2/ssg2/ds/models/facebook_react/bert_reranker/bm25_fix_combined_bert_classification/best_model', num_labels=2)\n",
    "bert_reranker.model.to(bert_reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_agg_results = bert_reranker.rerank_pipeline(test_query, aggregated_results)\n",
    "bert_fids = [o.fid for o in bert_agg_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[721, 738, 742, 744, -1, 745, 754, 757, -1, 759, -1, 762, -1, -1]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bm25_fid.index(fid) if fid in bm25_fid else -1 for fid in test_files_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 13, 497, 499, -1, 500, 509, 512, -1, 514, -1, 15, -1, -1]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get indices of all test_file_ids in bm25_fid\n",
    "[bert_fids.index(fid) if fid in bert_fids else -1 for fid in test_files_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0553,\n",
       " 'P@1': 0.0,\n",
       " 'P@10': 0.0,\n",
       " 'P@20': 0.15,\n",
       " 'P@30': 0.1,\n",
       " 'MRR': 0.0909,\n",
       " 'R@1': 0.0,\n",
       " 'R@10': 0.0,\n",
       " 'R@100': 0.2143,\n",
       " 'R@1000': 0.6429}"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(bert_agg_results, test_files_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'MAP': 0.0066,\n",
    " 'P@1': 0.0,\n",
    " 'P@10': 0.0,\n",
    " 'P@20': 0.0,\n",
    " 'P@30': 0.0,\n",
    " 'MRR': 0.0014,\n",
    " 'R@1': 0.0,\n",
    " 'R@10': 0.0,\n",
    " 'R@100': 0.0,\n",
    " 'R@1000': 0.6429}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[721, 738, 742, 744, 745, 754, 757, 759, 762]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get indices of all test_file_ids in bm25_fid\n",
    "[bm25_fid.index(fid) for fid in test_files_ids if fid in bm25_fid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Default setting for \"collapse new nodes\" is enabled causing inconvenience. Settings popup UI does not adapt properly to smaller sizes, and \"Inspect the matching DOM element\" button is visible in standalone mode unnecessarily. Also, there is an issue with undefined window.addEventListener/window.removeEventListener in Hermes and incorrect viewing size for settings icon.'"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[scheduler] 5/n Error handling in scheduler (#12920)\\n\\n* Initial failing unit test for error handling in schedule\\n\\n**what is the change?:**\\nsee title\\n\\n**why make this change?:**\\nAdding tests for the error handling behavior we are about to add. This\\ntest is failing, which gives us the chance to make it pass.\\n\\nWrote skeletons of some other tests to add.\\n\\nUnit testing this way is really hacky, and I'm also adding to the\\nfixture to test this in the real browser environment.\\n\\n**test plan:**\\nRan new test, saw it fail!\\n\\n* Add fixture for testing error handling in scheduler\\n\\n**what is the change?:**\\nAdded a fixture which does the following -\\nlogs in the console to show what happens when you use\\n`requestAnimationFrame` to schedule a series of callbacks and some of\\nthem throw errors.\\n\\nThen does the same actions with the `scheduler` and verifies that it\\nbehaves in a similar way.\\n\\nHard to really verify the errors get thrown at the proper time without\\nlooking at the console.\\n\\n**why make this change?:**\\nWe want the most authentic, accurate test of how errors are handled in\\nthe scheduler. That's what this fixture should be.\\n\\n**test plan:**\\nManually verified that this test does what I expect - right now it's\\nfailing but follow up commits will fix that.\\n\\n* Handle errors in scheduler\\n\\n**what is the change?:**\\nWe set a flag before calling any callback, and then use a 'try/finally'\\nblock to wrap it. Note that we *do not* catch the error, if one is\\nthrown. But, we only unset the flag after the callback successfully\\nfinishes.\\n\\nIf we reach the 'finally' block and the flag was not unset, then it\\nmeans an error was thrown.\\n\\nIn that case we start a new postMessage callback, to finish calling any\\nother pending callbacks if there is time.\\n\\n**why make this change?:**\\nWe need to make sure that an error thrown from one callback doesn't stop\\nother callbacks from firing, but we also don't want to catch or swallow\\nthe error because we want engineers to still be able to log and debug\\nerrors.\\n\\n**test plan:**\\nNew tests added are passing, and we verified that they fail without this\\nchange.\\n\\n* Add more tests for error handling in scheduler\\n\\n**what is the change?:**\\nAdded tests for more situations where error handling may come up.\\n\\n**why make this change?:**\\nTo get additional protection against this being broken in the future.\\n\\n**test plan:**\\nRan new tests and verified that they fail when error handling fails.\\n\\n* callSafely -> callUnsafely\\n\\n* Fix bugs with error handling in schedule\\n\\n**what is the change?:**\\n- ensure that we properly remove the callback from the linked list, even\\nif it throws an error and is timed out.\\n- ensure that you can call 'cancelScheduledWork' more than once and it\\nis idempotent.\\n\\n**why make this change?:**\\nTo fix bugs :)\\n\\n**test plan:**\\nExisting tests pass, and we'll add more tests in a follow up commit.\\n\\n* Unit tests for error handling with timed out callbacks\\n\\n**what is the change?:**\\nMore unit tests, to cover behavior which we missed; error handling of\\ntimed out callbacks.\\n\\n**why make this change?:**\\nTo protect the future!~\\n\\n**test plan:**\\nRun the new tests.\\n\\n* Adds fixture to test timed out callbacks with scheduler\\n\\n**what is the change?:**\\nSee title\\n\\nIn the other error handling fixture we compare 'scheduleWork' error\\nhandling to 'requestAnimationFrame' and try to get as close as possible.\\nThere is no 'timing out' feature with 'requestAnimationFrame' but\\neffectively the 'timing out' feature changes the order in which things\\nare called. So we just changed the order in the 'requestAnimationFrame'\\nversion and that works well for illustrating the behavior we expect in\\nthe 'scheduleWork' test.\\n\\n**why make this change?:**\\nWe need more test coverage of timed out callbacks.\\n\\n**test plan:**\\nExecuted the fixture manually in Firefox and Chrome. Results looked\\ngood.\\n\\n* fix rebase problems\\n\\n* make fixture compensate for chrome JS speed\\n\\n* ran prettier\\n\\n* Remove 'cancelled' flag on callbackConfig in scheduler, add test\\n\\n**what is the change?:**\\n- Instead of using a 'cancelled' flag on the callbackConfig, it's easier\\nto just check the state of the callbackConfig inside\\n'cancelScheduledWork' to determine if it's already been cancelled. That\\nway we don't have to remember to set the 'cancelled' flag every time we\\ncall a callback or cancel it. One less thing to remember.\\n- We added a test for calling 'cancelScheduledWork' more than once,\\nwhich would have failed before.\\n\\nThanks @acdlite for suggesting this in code review. :)\\n\\n**why make this change?:**\\nTo increase stability of the schedule module, increase test coverage.\\n\\n**test plan:**\\nExisting tests pass and we added a new test to cover this behavior.\\n\\n* fix typo\\n\\n\""
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_agg_results[0].contributing_results[0].commit_message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
