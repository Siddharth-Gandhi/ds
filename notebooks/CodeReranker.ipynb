{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:01.723338335Z",
     "start_time": "2023-11-27T04:01:01.497195817Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:04.805496544Z",
     "start_time": "2023-11-27T04:01:04.581595287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "from utils import (\n",
    "    AggregatedSearchResult,\n",
    "    get_combined_df,\n",
    "    prepare_triplet_data_from_df,\n",
    "    sanity_check_triplets,\n",
    "    set_seed,\n",
    "    tokenize,\n",
    "    get_recent_df\n",
    ")\n",
    "from BERTReranker_v4 import BERTReranker\n",
    "# set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:21.300426891Z",
     "start_time": "2023-11-27T04:01:21.229196368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  1\n",
      "Current cuda device:  0\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "# print torch devices available\n",
    "print('Available devices: ', torch.cuda.device_count())\n",
    "print('Current cuda device: ', torch.cuda.current_device())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='2_7/apache_kafka/index_commit_tokenized', repo_path='2_7/apache_kafka', k=1000, n=100, no_bm25=True, model_path='microsoft/codebert-base', overwrite_cache=False, batch_size=32, num_epochs=10, learning_rate=5e-05, num_positives=10, num_negatives=10, train_depth=1000, num_workers=8, train_commits=1500, psg_cnt=5, aggregation_strategy='sump', use_gpu=True, rerank_depth=250, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4', overwrite_eval=False, sanity_check_triplets=False, debug=False, eval_before_training=False, do_combined_train=False, repo_paths=None, best_model_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']\n",
    "repo_path = args.repo_path\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at 2_7/apache_kafka/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 10796945, 'documents': 75655, 'non_empty_documents': 75655, 'unique_terms': 15591}\n"
     ]
    }
   ],
   "source": [
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 output path: 2_7/apache_kafka/eval/bm25_baseline_N100_K1000_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to 2_7/apache_kafka/eval/bm25_baseline_N100_K1000_metrics.txt\n",
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.2137, 'P@10': 0.114, 'P@100': 0.034, 'P@1000': 0.0053, 'MRR': 0.305, 'Recall@100': 0.5517, 'Recall@1000': 0.7426}\n"
     ]
    }
   ],
   "source": [
    "bm25_output_path = os.path.join(eval_path, f'bm25_baseline_N{n}_K{K}_metrics.txt')\n",
    "print(f'BM25 output path: {bm25_output_path}')\n",
    "\n",
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=bm25_output_path, aggregation_strategy=BM25_AGGR_STRAT)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': args.psg_cnt,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': args.rerank_depth,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': BM25_AGGR_STRAT,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': 25,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': 100,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': BM25_AGGR_STRAT,\n",
    "        'psg_len': 250,\n",
    "        'psg_stride': 200,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Number of commits after midpoint date: 5222\n",
      "Number of commits after filtering by commit message length: 1736\n",
      "Number of commits after sampling: 1500\n"
     ]
    }
   ],
   "source": [
    "recent_df = get_recent_df(combined_df, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data first\n",
    "if not os.path.exists(os.path.join(repo_path, 'cache')):\n",
    "    os.makedirs(os.path.join(repo_path, 'cache'))\n",
    "triplet_cache = os.path.join(repo_path, 'cache', 'triplet_data_cache.pkl')\n",
    "diff_cache = os.path.join(repo_path, 'cache', 'diff_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73765 entries, 0 to 73764\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   owner                  73765 non-null  string\n",
      " 1   repo_name              73765 non-null  string\n",
      " 2   commit_date            73765 non-null  int64 \n",
      " 3   commit_id              73765 non-null  string\n",
      " 4   commit_message         73765 non-null  string\n",
      " 5   file_path              73765 non-null  string\n",
      " 6   previous_commit_id     73765 non-null  string\n",
      " 7   previous_file_content  73765 non-null  string\n",
      " 8   cur_file_content       73765 non-null  string\n",
      " 9   diff                   58037 non-null  string\n",
      " 10  status                 73765 non-null  object\n",
      " 11  is_merge_request       73765 non-null  bool  \n",
      " 12  file_extension         73765 non-null  object\n",
      "dtypes: bool(1), int64(1), object(2), string(9)\n",
      "memory usage: 6.8+ MB\n",
      "Average number of words in commit message (whitespace): 93.9145\n",
      "Average number of words in commit message (AutoTokenizer): 156.9581\n",
      "Approx number of tokens passed to bert: 470.8743\n",
      "Approx number of tokens remaining for code: 41.125699999999995\n",
      "Average number of code tokens in diff column: 776.6853514342018\n",
      "Average number of code tokens in cur_file_content column: 10067.9974\n"
     ]
    }
   ],
   "source": [
    "def aside():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "    def tokenize(x):\n",
    "        # tokenize with no max length\n",
    "        return tokenizer.encode(x, add_special_tokens=True, truncation=False, max_length=None)\n",
    "    combined_df.info()\n",
    "    # print the average number of words in commit_message column\n",
    "\n",
    "    # sample 100 rows from combined_df\n",
    "    # sample_df = combined_df.sample(100, random_state=52)\n",
    "    # sample_df = combined_df[:10000]\n",
    "\n",
    "    avg_words = sample_df['commit_message'].str.split().str.len().mean()\n",
    "    print(f'Average number of words in commit message (whitespace): {avg_words}')\n",
    "    avg_words = sample_df['commit_message'].apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of words in commit message (AutoTokenizer): {avg_words}')\n",
    "\n",
    "    # print approx number of tokens in passed to bert which is 2 * avg_words * 1.5\n",
    "    approx_tokens = 2 * avg_words * 1.5\n",
    "    print(f'Approx number of tokens passed to bert: {approx_tokens}')\n",
    "\n",
    "    # print remaining number of tokens in bert (max is 512)\n",
    "    print(f'Approx number of tokens remaining for code: {512 - approx_tokens}')\n",
    "\n",
    "    # print average number of code tokens in diff column by using tokenize function but only on the non-null diff values\n",
    "    avg_code_tokens = sample_df['diff'].dropna().apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of code tokens in diff column: {avg_code_tokens}')\n",
    "\n",
    "    avg_file_tokens = sample_df['cur_file_content'].dropna().apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of code tokens in cur_file_content column: {avg_file_tokens}')\n",
    "\n",
    "aside()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
    "Average number of words in commit message (whitespace): 60.66661695926252\n",
    "Average number of words in commit message (AutoTokenizer): 105.2017759099844\n",
    "Approx number of tokens passed to bert: 315.6053277299532\n",
    "Approx number of tokens remaining for code: 196.39467227004678\n",
    "Average number of code tokens in diff column: 775.7933559625756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_code_prep(df, searcher, search_depth, num_positives, num_negatives):\n",
    "    code_data = []\n",
    "    print(f'Preparing code data from dataframe of size: {len(df)} with search_depth: {search_depth}')\n",
    "    # for _, row in df.iterrows():\n",
    "    total_positives, total_negatives = 0, 0\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        cur_positives = 0\n",
    "        cur_negatives = 0\n",
    "        commit_message = row['commit_message']\n",
    "        actual_files_modified = row['actual_files_modified']\n",
    "\n",
    "        agg_search_results = searcher.pipeline(commit_message, row['commit_date'], search_depth, 'sump', sort_contributing_result_by_date=True)\n",
    "\n",
    "        # for each agg_result, find out how many files it has edited are in actual_files_modified and sort by score\n",
    "\n",
    "        # for agg_result in agg_search_results:\n",
    "        #     agg_result_files = set([result.file_path for result in agg_result.contributing_results])\n",
    "        #     intersection = agg_result_files.intersection(actual_files_modified)\n",
    "        #     # TODO maybe try this for training\n",
    "        #     # agg_result.score = len(intersection) / len(agg_result_files) # how focused the commit is\n",
    "        #     agg_result.score = len(intersection) / len(agg_result_files) # how focused the commit is\n",
    "        #     # agg_result.score = math.log(cur_score+1)\n",
    "        #     # agg_result.score = len(intersection)\n",
    "\n",
    "        # agg_search_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        # if use_diff:\n",
    "        #     for agg_result in agg_search_results:\n",
    "        #         # now we want to get diffs for each file in agg_result which has multiple contributing results (files)\n",
    "        #         # agg_result.contributing_results is a list of SearchResult objects\n",
    "        #         # each SearchResult object has a file_path attribute\n",
    "        #         # just get the first contributing result for now\n",
    "        #         # TODO: use diff_cnt instead of just the first contributing result\n",
    "        #         for contributing_result in agg_result.contributing_results[:5]:\n",
    "        #             # contributing_result = agg_result.contributing_results[0]\n",
    "        #             # get the just the file path and commit id\n",
    "        #             file_path = contributing_result.file_path\n",
    "        #             commit_id = contributing_result.commit_id\n",
    "        #             if file_path in actual_files_modified and cur_code_positives < num_positives:\n",
    "        #                 # this is a positive sample\n",
    "        #                 diff_data.append((commit_message, file_path, commit_id, 1))\n",
    "        #                 cur_code_positives += 1\n",
    "        #             elif file_path not in actual_files_modified and cur_code_negatives < num_negatives:\n",
    "        #                 # this is a negative sample\n",
    "        #                 diff_data.append((commit_message, file_path, commit_id, 0))\n",
    "        #                 cur_code_negatives += 1\n",
    "\n",
    "        #         if cur_code_positives == num_positives and cur_code_negatives == num_negatives:\n",
    "        #             break\n",
    "\n",
    "\n",
    "        for agg_result in agg_search_results:\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            commit_id = most_recent_search_result.commit_id\n",
    "\n",
    "            if file_path in actual_files_modified and cur_positives < num_positives:\n",
    "                # this is a positive sample\n",
    "                code_data.append((commit_message, file_path, commit_id, 1))\n",
    "                cur_positives += 1\n",
    "                total_positives += 1\n",
    "            elif file_path not in actual_files_modified and cur_negatives < num_negatives:\n",
    "                # this is a negative sample\n",
    "                code_data.append((commit_message, file_path, commit_id, 0))\n",
    "                cur_negatives += 1\n",
    "                total_negatives += 1\n",
    "\n",
    "            if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        # go from top to bottom, first num_positives non-0 scores are positive samples and the next num_negatives are negative samples\n",
    "        # for agg_result in agg_search_results:\n",
    "        #     cur_commit_msg = agg_result.contributing_results[0].commit_message\n",
    "        #     if cur_positives < num_positives and agg_result.score > 0:\n",
    "        #         # meaning there is at least one file in the agg_result that is in actual_files_modified\n",
    "        #         # pos_commits.append(agg_result)\n",
    "        #         data.append((commit_message, cur_commit_msg, 1))\n",
    "        #         cur_positives += 1\n",
    "        #         pos_commit_ids.add(agg_result.commit_id)\n",
    "        #     elif cur_negatives < num_negatives:\n",
    "        #         # neg_commits.append(agg_result)\n",
    "        #         data.append((commit_message, cur_commit_msg, 0))\n",
    "        #         cur_negatives += 1\n",
    "        #         neg_commit_ids.add(agg_result.commit_id)\n",
    "        #     if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "        #         break\n",
    "\n",
    "        # assert len(pos_commit_ids.intersection(neg_commit_ids)) == 0, 'Positive and negative commit ids should not intersect'\n",
    "        # print(f\"Total positives: {cur_positives}, Total negatives: {cur_negatives}\")\n",
    "        # total_positives += cur_positives\n",
    "        # total_negatives += cur_negatives\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    # data = pd.DataFrame(data, columns=['query', 'passage', 'label'])\n",
    "    code_df = pd.DataFrame(code_data, columns=['query', 'file_path', 'commit_id', 'label'])\n",
    "    # print distribution of labels\n",
    "    print(f\"Total positives: {total_positives}, Total negatives: {total_negatives}\")\n",
    "    # print percentage of positives and negatives\n",
    "    denom = total_positives + total_negatives\n",
    "    print(f\"Percentage of positives: {total_positives / denom}, Percentage of negatives: {total_negatives / denom}\")\n",
    "    return code_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code data from dataframe of size: 1500 with search_depth: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [06:53<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positives: 5753, Total negatives: 15000\n",
      "Percentage of positives: 0.2772129330699176, Percentage of negatives: 0.7227870669300824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "code_df = test_code_prep(recent_df, bm25_searcher, params['train_depth'], params['num_positives'], params['num_negatives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20753 entries, 0 to 20752\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      20753 non-null  object\n",
      " 1   file_path  20753 non-null  object\n",
      " 2   commit_id  20753 non-null  object\n",
      " 3   label      20753 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 648.7+ KB\n"
     ]
    }
   ],
   "source": [
    "code_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20753/20753 [04:16<00:00, 80.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_diff_data(diff_data, df):\n",
    "    # given diff_data, we want to use commit_id and file_path to get the diff from the df\n",
    "\n",
    "    # first we need to get the diff from the df\n",
    "    # we can use the commit_id and file_path to get the diff\n",
    "    res_df = []\n",
    "    null_rows = 0\n",
    "    # for _, row in diff_data.iterrows():\n",
    "    for _, row in tqdm(diff_data.iterrows(), total=len(diff_data)):\n",
    "        commit_id = row['commit_id']\n",
    "        file_path = row['file_path']\n",
    "        # get the diff from the df\n",
    "        diff = df[(df['commit_id'] == commit_id) & (df['file_path'] == file_path)]['cur_file_content']\n",
    "        # check if diff is NA/NaN\n",
    "        if diff.isnull().values.any():\n",
    "            # if it is, then we can just skip this row\n",
    "            null_rows += 1\n",
    "            continue\n",
    "        diff = diff.values[0]\n",
    "\n",
    "        res_df.append((commit_id, file_path, row['query'], diff, row['label']))\n",
    "\n",
    "    res_df = pd.DataFrame(res_df, columns=['commit_id', 'file_path', 'query', 'passage', 'label'])\n",
    "    # make query and passage into strings and label into int\n",
    "    res_df['query'] = res_df['query'].astype(str)\n",
    "    res_df['passage'] = res_df['passage'].astype(str)\n",
    "    res_df['label'] = res_df['label'].astype(int)\n",
    "    print(f\"Number of null rows: {null_rows}\")\n",
    "    return res_df\n",
    "\n",
    "processed_diff_data = process_diff_data(code_df, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20753 entries, 0 to 20752\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   commit_id  20753 non-null  object\n",
      " 1   file_path  20753 non-null  object\n",
      " 2   query      20753 non-null  object\n",
      " 3   passage    20753 non-null  object\n",
      " 4   label      20753 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 810.8+ KB\n"
     ]
    }
   ],
   "source": [
    "processed_diff_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n *\\n * @flow\\n */\\n\\nimport {REACT_STRICT_MODE_TYPE} from \\'shared/ReactSymbols\\';\\n\\nimport type {Wakeable} from \\'shared/ReactTypes\\';\\nimport type {Fiber, FiberRoot} from \\'./ReactInternalTypes\\';\\nimport type {Lanes, Lane} from \\'./ReactFiberLane.new\\';\\nimport type {SuspenseState} from \\'./ReactFiberSuspenseComponent.new\\';\\nimport type {FunctionComponentUpdateQueue} from \\'./ReactFiberHooks.new\\';\\nimport type {EventPriority} from \\'./ReactEventPriorities.new\\';\\nimport type {\\n  PendingTransitionCallbacks,\\n  PendingBoundaries,\\n  Transition,\\n  TransitionAbort,\\n} from \\'./ReactFiberTracingMarkerComponent.new\\';\\nimport type {OffscreenInstance} from \\'./ReactFiberOffscreenComponent\\';\\n\\nimport {\\n  warnAboutDeprecatedLifecycles,\\n  replayFailedUnitOfWorkWithInvokeGuardedCallback,\\n  enableCreateEventHandleAPI,\\n  enableProfilerTimer,\\n  enableProfilerCommitHooks,\\n  enableProfilerNestedUpdatePhase,\\n  enableProfilerNestedUpdateScheduledHook,\\n  deferRenderPhaseUpdateToNextBatch,\\n  enableDebugTracing,\\n  enableSchedulingProfiler,\\n  disableSchedulerTimeoutInWorkLoop,\\n  enableStrictEffects,\\n  skipUnmountedBoundaries,\\n  enableUpdaterTracking,\\n  enableCache,\\n  enableTransitionTracing,\\n} from \\'shared/ReactFeatureFlags\\';\\nimport ReactSharedInternals from \\'shared/ReactSharedInternals\\';\\nimport is from \\'shared/objectIs\\';\\n\\nimport {\\n  // Aliased because `act` will override and push to an internal queue\\n  scheduleCallback as Scheduler_scheduleCallback,\\n  cancelCallback as Scheduler_cancelCallback,\\n  shouldYield,\\n  requestPaint,\\n  now,\\n  ImmediatePriority as ImmediateSchedulerPriority,\\n  UserBlockingPriority as UserBlockingSchedulerPriority,\\n  NormalPriority as NormalSchedulerPriority,\\n  IdlePriority as IdleSchedulerPriority,\\n} from \\'./Scheduler\\';\\nimport {\\n  flushSyncCallbacks,\\n  flushSyncCallbacksOnlyInLegacyMode,\\n  scheduleSyncCallback,\\n  scheduleLegacySyncCallback,\\n} from \\'./ReactFiberSyncTaskQueue.new\\';\\nimport {\\n  logCommitStarted,\\n  logCommitStopped,\\n  logLayoutEffectsStarted,\\n  logLayoutEffectsStopped,\\n  logPassiveEffectsStarted,\\n  logPassiveEffectsStopped,\\n  logRenderStarted,\\n  logRenderStopped,\\n} from \\'./DebugTracing\\';\\n\\nimport {\\n  resetAfterCommit,\\n  scheduleTimeout,\\n  cancelTimeout,\\n  noTimeout,\\n  afterActiveInstanceBlur,\\n  getCurrentEventPriority,\\n  supportsMicrotasks,\\n  errorHydratingContainer,\\n  scheduleMicrotask,\\n} from \\'./ReactFiberHostConfig\\';\\n\\nimport {\\n  createWorkInProgress,\\n  assignFiberPropertiesInDEV,\\n  resetWorkInProgress,\\n} from \\'./ReactFiber.new\\';\\nimport {isRootDehydrated} from \\'./ReactFiberShellHydration\\';\\nimport {didSuspendOrErrorWhileHydratingDEV} from \\'./ReactFiberHydrationContext.new\\';\\nimport {\\n  NoMode,\\n  ProfileMode,\\n  ConcurrentMode,\\n  StrictLegacyMode,\\n  StrictEffectsMode,\\n} from \\'./ReactTypeOfMode\\';\\nimport {\\n  HostRoot,\\n  IndeterminateComponent,\\n  ClassComponent,\\n  SuspenseComponent,\\n  SuspenseListComponent,\\n  OffscreenComponent,\\n  FunctionComponent,\\n  ForwardRef,\\n  MemoComponent,\\n  SimpleMemoComponent,\\n  Profiler,\\n} from \\'./ReactWorkTags\\';\\nimport {ConcurrentRoot, LegacyRoot} from \\'./ReactRootTags\\';\\nimport {\\n  NoFlags,\\n  Incomplete,\\n  StoreConsistency,\\n  HostEffectMask,\\n  ForceClientRender,\\n  BeforeMutationMask,\\n  MutationMask,\\n  LayoutMask,\\n  PassiveMask,\\n  PlacementDEV,\\n} from \\'./ReactFiberFlags\\';\\nimport {\\n  NoLanes,\\n  NoLane,\\n  SyncLane,\\n  NoTimestamp,\\n  claimNextTransitionLane,\\n  claimNextRetryLane,\\n  includesSomeLane,\\n  isSubsetOfLanes,\\n  mergeLanes,\\n  removeLanes,\\n  pickArbitraryLane,\\n  includesNonIdleWork,\\n  includesOnlyRetries,\\n  includesOnlyTransitions,\\n  includesBlockingLane,\\n  includesExpiredLane,\\n  getNextLanes,\\n  markStarvedLanesAsExpired,\\n  getLanesToRetrySynchronouslyOnError,\\n  getMostRecentEventTime,\\n  markRootUpdated,\\n  markRootSuspended as markRootSuspended_dontCallThisOneDirectly,\\n  markRootPinged,\\n  markRootEntangled,\\n  markRootFinished,\\n  getHighestPriorityLane,\\n  addFiberToLanesMap,\\n  movePendingFibersToMemoized,\\n  addTransitionToLanesMap,\\n  getTransitionsForLanes,\\n} from \\'./ReactFiberLane.new\\';\\nimport {\\n  DiscreteEventPriority,\\n  ContinuousEventPriority,\\n  DefaultEventPriority,\\n  IdleEventPriority,\\n  getCurrentUpdatePriority,\\n  setCurrentUpdatePriority,\\n  lowerEventPriority,\\n  lanesToEventPriority,\\n} from \\'./ReactEventPriorities.new\\';\\nimport {requestCurrentTransition, NoTransition} from \\'./ReactFiberTransition\\';\\nimport {beginWork as originalBeginWork} from \\'./ReactFiberBeginWork.new\\';\\nimport {completeWork} from \\'./ReactFiberCompleteWork.new\\';\\nimport {unwindWork, unwindInterruptedWork} from \\'./ReactFiberUnwindWork.new\\';\\nimport {\\n  throwException,\\n  createRootErrorUpdate,\\n  createClassErrorUpdate,\\n} from \\'./ReactFiberThrow.new\\';\\nimport {\\n  commitBeforeMutationEffects,\\n  commitLayoutEffects,\\n  commitMutationEffects,\\n  commitPassiveEffectDurations,\\n  commitPassiveMountEffects,\\n  commitPassiveUnmountEffects,\\n  disappearLayoutEffects,\\n  reconnectPassiveEffects,\\n  reappearLayoutEffects,\\n  disconnectPassiveEffect,\\n  reportUncaughtErrorInDEV,\\n} from \\'./ReactFiberCommitWork.new\\';\\nimport {enqueueUpdate} from \\'./ReactFiberClassUpdateQueue.new\\';\\nimport {resetContextDependencies} from \\'./ReactFiberNewContext.new\\';\\nimport {\\n  resetHooksAfterThrow,\\n  ContextOnlyDispatcher,\\n  getIsUpdatingOpaqueValueInRenderPhaseInDEV,\\n} from \\'./ReactFiberHooks.new\\';\\nimport {\\n  createCapturedValueAtFiber,\\n  type CapturedValue,\\n} from \\'./ReactCapturedValue\\';\\nimport {\\n  enqueueConcurrentRenderForLane,\\n  finishQueueingConcurrentUpdates,\\n  getConcurrentlyUpdatedLanes,\\n} from \\'./ReactFiberConcurrentUpdates.new\\';\\n\\nimport {\\n  markNestedUpdateScheduled,\\n  recordCommitTime,\\n  resetNestedUpdateFlag,\\n  startProfilerTimer,\\n  stopProfilerTimerIfRunningAndRecordDelta,\\n  syncNestedUpdateFlag,\\n} from \\'./ReactProfilerTimer.new\\';\\n\\n// DEV stuff\\nimport getComponentNameFromFiber from \\'react-reconciler/src/getComponentNameFromFiber\\';\\nimport ReactStrictModeWarnings from \\'./ReactStrictModeWarnings.new\\';\\nimport {\\n  isRendering as ReactCurrentDebugFiberIsRenderingInDEV,\\n  current as ReactCurrentFiberCurrent,\\n  resetCurrentFiber as resetCurrentDebugFiberInDEV,\\n  setCurrentFiber as setCurrentDebugFiberInDEV,\\n} from \\'./ReactCurrentFiber\\';\\nimport {\\n  invokeGuardedCallback,\\n  hasCaughtError,\\n  clearCaughtError,\\n} from \\'shared/ReactErrorUtils\\';\\nimport {\\n  isDevToolsPresent,\\n  markCommitStarted,\\n  markCommitStopped,\\n  markComponentRenderStopped,\\n  markComponentSuspended,\\n  markComponentErrored,\\n  markLayoutEffectsStarted,\\n  markLayoutEffectsStopped,\\n  markPassiveEffectsStarted,\\n  markPassiveEffectsStopped,\\n  markRenderStarted,\\n  markRenderYielded,\\n  markRenderStopped,\\n  onCommitRoot as onCommitRootDevTools,\\n  onPostCommitRoot as onPostCommitRootDevTools,\\n} from \\'./ReactFiberDevToolsHook.new\\';\\nimport {onCommitRoot as onCommitRootTestSelector} from \\'./ReactTestSelectors\\';\\nimport {releaseCache} from \\'./ReactFiberCacheComponent.new\\';\\nimport {\\n  isLegacyActEnvironment,\\n  isConcurrentActEnvironment,\\n} from \\'./ReactFiberAct.new\\';\\nimport {processTransitionCallbacks} from \\'./ReactFiberTracingMarkerComponent.new\\';\\nimport {\\n  resetWakeableStateAfterEachAttempt,\\n  resetThenableStateOnCompletion,\\n  trackSuspendedWakeable,\\n  suspendedThenableDidResolve,\\n  isTrackingSuspendedThenable,\\n} from \\'./ReactFiberWakeable.new\\';\\n\\nconst ceil = Math.ceil;\\n\\nconst PossiblyWeakMap = typeof WeakMap === \\'function\\' ? WeakMap : Map;\\n\\nconst {\\n  ReactCurrentDispatcher,\\n  ReactCurrentOwner,\\n  ReactCurrentBatchConfig,\\n  ReactCurrentActQueue,\\n} = ReactSharedInternals;\\n\\ntype ExecutionContext = number;\\n\\nexport const NoContext = /*             */ 0b000;\\nconst BatchedContext = /*               */ 0b001;\\nconst RenderContext = /*                */ 0b010;\\nexport const CommitContext = /*         */ 0b100;\\n\\ntype RootExitStatus = 0 | 1 | 2 | 3 | 4 | 5 | 6;\\nconst RootInProgress = 0;\\nconst RootFatalErrored = 1;\\nconst RootErrored = 2;\\nconst RootSuspended = 3;\\nconst RootSuspendedWithDelay = 4;\\nconst RootCompleted = 5;\\nconst RootDidNotComplete = 6;\\n\\n// Describes where we are in the React execution stack\\nlet executionContext: ExecutionContext = NoContext;\\n// The root we\\'re working on\\nlet workInProgressRoot: FiberRoot | null = null;\\n// The fiber we\\'re working on\\nlet workInProgress: Fiber | null = null;\\n// The lanes we\\'re rendering\\nlet workInProgressRootRenderLanes: Lanes = NoLanes;\\n\\n// When this is true, the work-in-progress fiber just suspended (or errored) and\\n// we\\'ve yet to unwind the stack. In some cases, we may yield to the main thread\\n// after this happens. If the fiber is pinged before we resume, we can retry\\n// immediately instead of unwinding the stack.\\nlet workInProgressIsSuspended: boolean = false;\\nlet workInProgressThrownValue: mixed = null;\\n\\n// Whether a ping listener was attached during this render. This is slightly\\n// different that whether something suspended, because we don\\'t add multiple\\n// listeners to a promise we\\'ve already seen (per root and lane).\\nlet workInProgressRootDidAttachPingListener: boolean = false;\\n\\n// A contextual version of workInProgressRootRenderLanes. It is a superset of\\n// the lanes that we started working on at the root. When we enter a subtree\\n// that is currently hidden, we add the lanes that would have committed if\\n// the hidden tree hadn\\'t been deferred. This is modified by the\\n// HiddenContext module.\\n//\\n// Most things in the work loop should deal with workInProgressRootRenderLanes.\\n// Most things in begin/complete phases should deal with renderLanes.\\nexport let renderLanes: Lanes = NoLanes;\\n\\n// Whether to root completed, errored, suspended, etc.\\nlet workInProgressRootExitStatus: RootExitStatus = RootInProgress;\\n// A fatal error, if one is thrown\\nlet workInProgressRootFatalError: mixed = null;\\n// The work left over by components that were visited during this render. Only\\n// includes unprocessed updates, not work in bailed out children.\\nlet workInProgressRootSkippedLanes: Lanes = NoLanes;\\n// Lanes that were updated (in an interleaved event) during this render.\\nlet workInProgressRootInterleavedUpdatedLanes: Lanes = NoLanes;\\n// Lanes that were updated during the render phase (*not* an interleaved event).\\nlet workInProgressRootRenderPhaseUpdatedLanes: Lanes = NoLanes;\\n// Lanes that were pinged (in an interleaved event) during this render.\\nlet workInProgressRootPingedLanes: Lanes = NoLanes;\\n// Errors that are thrown during the render phase.\\nlet workInProgressRootConcurrentErrors: Array<\\n  CapturedValue<mixed>,\\n> | null = null;\\n// These are errors that we recovered from without surfacing them to the UI.\\n// We will log them once the tree commits.\\nlet workInProgressRootRecoverableErrors: Array<\\n  CapturedValue<mixed>,\\n> | null = null;\\n\\n// The most recent time we committed a fallback. This lets us ensure a train\\n// model where we don\\'t commit new loading states in too quick succession.\\nlet globalMostRecentFallbackTime: number = 0;\\nconst FALLBACK_THROTTLE_MS: number = 500;\\n\\n// The absolute time for when we should start giving up on rendering\\n// more and prefer CPU suspense heuristics instead.\\nlet workInProgressRootRenderTargetTime: number = Infinity;\\n// How long a render is supposed to take before we start following CPU\\n// suspense heuristics and opt out of rendering more content.\\nconst RENDER_TIMEOUT_MS = 500;\\n\\nlet workInProgressTransitions: Array<Transition> | null = null;\\nexport function getWorkInProgressTransitions() {\\n  return workInProgressTransitions;\\n}\\n\\nlet currentPendingTransitionCallbacks: PendingTransitionCallbacks | null = null;\\n\\nexport function addTransitionStartCallbackToPendingTransition(\\n  transition: Transition,\\n) {\\n  if (enableTransitionTracing) {\\n    if (currentPendingTransitionCallbacks === null) {\\n      currentPendingTransitionCallbacks = {\\n        transitionStart: [],\\n        transitionProgress: null,\\n        transitionComplete: null,\\n        markerProgress: null,\\n        markerIncomplete: null,\\n        markerComplete: null,\\n      };\\n    }\\n\\n    if (currentPendingTransitionCallbacks.transitionStart === null) {\\n      currentPendingTransitionCallbacks.transitionStart = [];\\n    }\\n\\n    currentPendingTransitionCallbacks.transitionStart.push(transition);\\n  }\\n}\\n\\nexport function addMarkerProgressCallbackToPendingTransition(\\n  markerName: string,\\n  transitions: Set<Transition>,\\n  pendingBoundaries: PendingBoundaries,\\n) {\\n  if (enableTransitionTracing) {\\n    if (currentPendingTransitionCallbacks === null) {\\n      currentPendingTransitionCallbacks = {\\n        transitionStart: null,\\n        transitionProgress: null,\\n        transitionComplete: null,\\n        markerProgress: new Map(),\\n        markerIncomplete: null,\\n        markerComplete: null,\\n      };\\n    }\\n\\n    if (currentPendingTransitionCallbacks.markerProgress === null) {\\n      currentPendingTransitionCallbacks.markerProgress = new Map();\\n    }\\n\\n    currentPendingTransitionCallbacks.markerProgress.set(markerName, {\\n      pendingBoundaries,\\n      transitions,\\n    });\\n  }\\n}\\n\\nexport function addMarkerIncompleteCallbackToPendingTransition(\\n  markerName: string,\\n  transitions: Set<Transition>,\\n  aborts: Array<TransitionAbort>,\\n) {\\n  if (enableTransitionTracing) {\\n    if (currentPendingTransitionCallbacks === null) {\\n      currentPendingTransitionCallbacks = {\\n        transitionStart: null,\\n        transitionProgress: null,\\n        transitionComplete: null,\\n        markerProgress: null,\\n        markerIncomplete: new Map(),\\n        markerComplete: null,\\n      };\\n    }\\n\\n    if (currentPendingTransitionCallbacks.markerIncomplete === null) {\\n      currentPendingTransitionCallbacks.markerIncomplete = new Map();\\n    }\\n\\n    currentPendingTransitionCallbacks.markerIncomplete.set(markerName, {\\n      transitions,\\n      aborts,\\n    });\\n  }\\n}\\n\\nexport function addMarkerCompleteCallbackToPendingTransition(\\n  markerName: string,\\n  transitions: Set<Transition>,\\n) {\\n  if (enableTransitionTracing) {\\n    if (currentPendingTransitionCallbacks === null) {\\n      currentPendingTransitionCallbacks = {\\n        transitionStart: null,\\n        transitionProgress: null,\\n        transitionComplete: null,\\n        markerProgress: null,\\n        markerIncomplete: null,\\n        markerComplete: new Map(),\\n      };\\n    }\\n\\n    if (currentPendingTransitionCallbacks.markerComplete === null) {\\n      currentPendingTransitionCallbacks.markerComplete = new Map();\\n    }\\n\\n    currentPendingTransitionCallbacks.markerComplete.set(\\n      markerName,\\n      transitions,\\n    );\\n  }\\n}\\n\\nexport function addTransitionProgressCallbackToPendingTransition(\\n  transition: Transition,\\n  boundaries: PendingBoundaries,\\n) {\\n  if (enableTransitionTracing) {\\n    if (currentPendingTransitionCallbacks === null) {\\n      currentPendingTransitionCallbacks = {\\n        transitionStart: null,\\n        transitionProgress: new Map(),\\n        transitionComplete: null,\\n        markerProgress: null,\\n        markerIncomplete: null,\\n        markerComplete: null,\\n      };\\n    }\\n\\n    if (currentPendingTransitionCallbacks.transitionProgress === null) {\\n      currentPendingTransitionCallbacks.transitionProgress = new Map();\\n    }\\n\\n    currentPendingTransitionCallbacks.transitionProgress.set(\\n      transition,\\n      boundaries,\\n    );\\n  }\\n}\\n\\nexport function addTransitionCompleteCallbackToPendingTransition(\\n  transition: Transition,\\n) {\\n  if (enableTransitionTracing) {\\n    if (currentPendingTransitionCallbacks === null) {\\n      currentPendingTransitionCallbacks = {\\n        transitionStart: null,\\n        transitionProgress: null,\\n        transitionComplete: [],\\n        markerProgress: null,\\n        markerIncomplete: null,\\n        markerComplete: null,\\n      };\\n    }\\n\\n    if (currentPendingTransitionCallbacks.transitionComplete === null) {\\n      currentPendingTransitionCallbacks.transitionComplete = [];\\n    }\\n\\n    currentPendingTransitionCallbacks.transitionComplete.push(transition);\\n  }\\n}\\n\\nfunction resetRenderTimer() {\\n  workInProgressRootRenderTargetTime = now() + RENDER_TIMEOUT_MS;\\n}\\n\\nexport function getRenderTargetTime(): number {\\n  return workInProgressRootRenderTargetTime;\\n}\\n\\nlet hasUncaughtError = false;\\nlet firstUncaughtError = null;\\nlet legacyErrorBoundariesThatAlreadyFailed: Set<mixed> | null = null;\\n\\n// Only used when enableProfilerNestedUpdateScheduledHook is true;\\n// to track which root is currently committing layout effects.\\nlet rootCommittingMutationOrLayoutEffects: FiberRoot | null = null;\\n\\nlet rootDoesHavePassiveEffects: boolean = false;\\nlet rootWithPendingPassiveEffects: FiberRoot | null = null;\\nlet pendingPassiveEffectsLanes: Lanes = NoLanes;\\nlet pendingPassiveProfilerEffects: Array<Fiber> = [];\\nlet pendingPassiveEffectsRemainingLanes: Lanes = NoLanes;\\nlet pendingPassiveTransitions: Array<Transition> | null = null;\\n\\n// Use these to prevent an infinite loop of nested updates\\nconst NESTED_UPDATE_LIMIT = 50;\\nlet nestedUpdateCount: number = 0;\\nlet rootWithNestedUpdates: FiberRoot | null = null;\\nlet isFlushingPassiveEffects = false;\\nlet didScheduleUpdateDuringPassiveEffects = false;\\n\\nconst NESTED_PASSIVE_UPDATE_LIMIT = 50;\\nlet nestedPassiveUpdateCount: number = 0;\\nlet rootWithPassiveNestedUpdates: FiberRoot | null = null;\\n\\n// If two updates are scheduled within the same event, we should treat their\\n// event times as simultaneous, even if the actual clock time has advanced\\n// between the first and second call.\\nlet currentEventTime: number = NoTimestamp;\\nlet currentEventTransitionLane: Lanes = NoLanes;\\n\\nlet isRunningInsertionEffect = false;\\n\\nexport function getWorkInProgressRoot(): FiberRoot | null {\\n  return workInProgressRoot;\\n}\\n\\nexport function getWorkInProgressRootRenderLanes(): Lanes {\\n  return workInProgressRootRenderLanes;\\n}\\n\\nexport function requestEventTime() {\\n  if ((executionContext & (RenderContext | CommitContext)) !== NoContext) {\\n    // We\\'re inside React, so it\\'s fine to read the actual time.\\n    return now();\\n  }\\n  // We\\'re not inside React, so we may be in the middle of a browser event.\\n  if (currentEventTime !== NoTimestamp) {\\n    // Use the same start time for all updates until we enter React again.\\n    return currentEventTime;\\n  }\\n  // This is the first update since React yielded. Compute a new start time.\\n  currentEventTime = now();\\n  return currentEventTime;\\n}\\n\\nexport function getCurrentTime() {\\n  return now();\\n}\\n\\nexport function requestUpdateLane(fiber: Fiber): Lane {\\n  // Special cases\\n  const mode = fiber.mode;\\n  if ((mode & ConcurrentMode) === NoMode) {\\n    return (SyncLane: Lane);\\n  } else if (\\n    !deferRenderPhaseUpdateToNextBatch &&\\n    (executionContext & RenderContext) !== NoContext &&\\n    workInProgressRootRenderLanes !== NoLanes\\n  ) {\\n    // This is a render phase update. These are not officially supported. The\\n    // old behavior is to give this the same \"thread\" (lanes) as\\n    // whatever is currently rendering. So if you call `setState` on a component\\n    // that happens later in the same render, it will flush. Ideally, we want to\\n    // remove the special case and treat them as if they came from an\\n    // interleaved event. Regardless, this pattern is not officially supported.\\n    // This behavior is only a fallback. The flag only exists until we can roll\\n    // out the setState warning, since existing code might accidentally rely on\\n    // the current behavior.\\n    return pickArbitraryLane(workInProgressRootRenderLanes);\\n  }\\n\\n  const isTransition = requestCurrentTransition() !== NoTransition;\\n  if (isTransition) {\\n    if (__DEV__ && ReactCurrentBatchConfig.transition !== null) {\\n      const transition = ReactCurrentBatchConfig.transition;\\n      if (!transition._updatedFibers) {\\n        transition._updatedFibers = new Set();\\n      }\\n\\n      transition._updatedFibers.add(fiber);\\n    }\\n    // The algorithm for assigning an update to a lane should be stable for all\\n    // updates at the same priority within the same event. To do this, the\\n    // inputs to the algorithm must be the same.\\n    //\\n    // The trick we use is to cache the first of each of these inputs within an\\n    // event. Then reset the cached values once we can be sure the event is\\n    // over. Our heuristic for that is whenever we enter a concurrent work loop.\\n    if (currentEventTransitionLane === NoLane) {\\n      // All transitions within the same event are assigned the same lane.\\n      currentEventTransitionLane = claimNextTransitionLane();\\n    }\\n    return currentEventTransitionLane;\\n  }\\n\\n  // Updates originating inside certain React methods, like flushSync, have\\n  // their priority set by tracking it with a context variable.\\n  //\\n  // The opaque type returned by the host config is internally a lane, so we can\\n  // use that directly.\\n  // TODO: Move this type conversion to the event priority module.\\n  const updateLane: Lane = (getCurrentUpdatePriority(): any);\\n  if (updateLane !== NoLane) {\\n    return updateLane;\\n  }\\n\\n  // This update originated outside React. Ask the host environment for an\\n  // appropriate priority, based on the type of event.\\n  //\\n  // The opaque type returned by the host config is internally a lane, so we can\\n  // use that directly.\\n  // TODO: Move this type conversion to the event priority module.\\n  const eventLane: Lane = (getCurrentEventPriority(): any);\\n  return eventLane;\\n}\\n\\nfunction requestRetryLane(fiber: Fiber) {\\n  // This is a fork of `requestUpdateLane` designed specifically for Suspense\\n  // \"retries\" — a special update that attempts to flip a Suspense boundary\\n  // from its placeholder state to its primary/resolved state.\\n\\n  // Special cases\\n  const mode = fiber.mode;\\n  if ((mode & ConcurrentMode) === NoMode) {\\n    return (SyncLane: Lane);\\n  }\\n\\n  return claimNextRetryLane();\\n}\\n\\nexport function scheduleUpdateOnFiber(\\n  root: FiberRoot,\\n  fiber: Fiber,\\n  lane: Lane,\\n  eventTime: number,\\n) {\\n  if (__DEV__) {\\n    if (isRunningInsertionEffect) {\\n      console.error(\\'useInsertionEffect must not schedule updates.\\');\\n    }\\n  }\\n\\n  if (__DEV__) {\\n    if (isFlushingPassiveEffects) {\\n      didScheduleUpdateDuringPassiveEffects = true;\\n    }\\n  }\\n\\n  // Mark that the root has a pending update.\\n  markRootUpdated(root, lane, eventTime);\\n\\n  if (\\n    (executionContext & RenderContext) !== NoLanes &&\\n    root === workInProgressRoot\\n  ) {\\n    // This update was dispatched during the render phase. This is a mistake\\n    // if the update originates from user space (with the exception of local\\n    // hook updates, which are handled differently and don\\'t reach this\\n    // function), but there are some internal React features that use this as\\n    // an implementation detail, like selective hydration.\\n    warnAboutRenderPhaseUpdatesInDEV(fiber);\\n\\n    // Track lanes that were updated during the render phase\\n    workInProgressRootRenderPhaseUpdatedLanes = mergeLanes(\\n      workInProgressRootRenderPhaseUpdatedLanes,\\n      lane,\\n    );\\n  } else {\\n    // This is a normal update, scheduled from outside the render phase. For\\n    // example, during an input event.\\n    if (enableUpdaterTracking) {\\n      if (isDevToolsPresent) {\\n        addFiberToLanesMap(root, fiber, lane);\\n      }\\n    }\\n\\n    warnIfUpdatesNotWrappedWithActDEV(fiber);\\n\\n    if (enableProfilerTimer && enableProfilerNestedUpdateScheduledHook) {\\n      if (\\n        (executionContext & CommitContext) !== NoContext &&\\n        root === rootCommittingMutationOrLayoutEffects\\n      ) {\\n        if (fiber.mode & ProfileMode) {\\n          let current = fiber;\\n          while (current !== null) {\\n            if (current.tag === Profiler) {\\n              const {id, onNestedUpdateScheduled} = current.memoizedProps;\\n              if (typeof onNestedUpdateScheduled === \\'function\\') {\\n                onNestedUpdateScheduled(id);\\n              }\\n            }\\n            current = current.return;\\n          }\\n        }\\n      }\\n    }\\n\\n    if (enableTransitionTracing) {\\n      const transition = ReactCurrentBatchConfig.transition;\\n      if (transition !== null && transition.name != null) {\\n        if (transition.startTime === -1) {\\n          transition.startTime = now();\\n        }\\n\\n        addTransitionToLanesMap(root, transition, lane);\\n      }\\n    }\\n\\n    if (root === workInProgressRoot) {\\n      // Received an update to a tree that\\'s in the middle of rendering. Mark\\n      // that there was an interleaved update work on this root. Unless the\\n      // `deferRenderPhaseUpdateToNextBatch` flag is off and this is a render\\n      // phase update. In that case, we don\\'t treat render phase updates as if\\n      // they were interleaved, for backwards compat reasons.\\n      if (\\n        deferRenderPhaseUpdateToNextBatch ||\\n        (executionContext & RenderContext) === NoContext\\n      ) {\\n        workInProgressRootInterleavedUpdatedLanes = mergeLanes(\\n          workInProgressRootInterleavedUpdatedLanes,\\n          lane,\\n        );\\n      }\\n      if (workInProgressRootExitStatus === RootSuspendedWithDelay) {\\n        // The root already suspended with a delay, which means this render\\n        // definitely won\\'t finish. Since we have a new update, let\\'s mark it as\\n        // suspended now, right before marking the incoming update. This has the\\n        // effect of interrupting the current render and switching to the update.\\n        // TODO: Make sure this doesn\\'t override pings that happen while we\\'ve\\n        // already started rendering.\\n        markRootSuspended(root, workInProgressRootRenderLanes);\\n      }\\n    }\\n\\n    ensureRootIsScheduled(root, eventTime);\\n    if (\\n      lane === SyncLane &&\\n      executionContext === NoContext &&\\n      (fiber.mode & ConcurrentMode) === NoMode &&\\n      // Treat `act` as if it\\'s inside `batchedUpdates`, even in legacy mode.\\n      !(__DEV__ && ReactCurrentActQueue.isBatchingLegacy)\\n    ) {\\n      // Flush the synchronous work now, unless we\\'re already working or inside\\n      // a batch. This is intentionally inside scheduleUpdateOnFiber instead of\\n      // scheduleCallbackForFiber to preserve the ability to schedule a callback\\n      // without immediately flushing it. We only do this for user-initiated\\n      // updates, to preserve historical behavior of legacy mode.\\n      resetRenderTimer();\\n      flushSyncCallbacksOnlyInLegacyMode();\\n    }\\n  }\\n}\\n\\nexport function scheduleInitialHydrationOnRoot(\\n  root: FiberRoot,\\n  lane: Lane,\\n  eventTime: number,\\n) {\\n  // This is a special fork of scheduleUpdateOnFiber that is only used to\\n  // schedule the initial hydration of a root that has just been created. Most\\n  // of the stuff in scheduleUpdateOnFiber can be skipped.\\n  //\\n  // The main reason for this separate path, though, is to distinguish the\\n  // initial children from subsequent updates. In fully client-rendered roots\\n  // (createRoot instead of hydrateRoot), all top-level renders are modeled as\\n  // updates, but hydration roots are special because the initial render must\\n  // match what was rendered on the server.\\n  const current = root.current;\\n  current.lanes = lane;\\n  markRootUpdated(root, lane, eventTime);\\n  ensureRootIsScheduled(root, eventTime);\\n}\\n\\nexport function isUnsafeClassRenderPhaseUpdate(fiber: Fiber) {\\n  // Check if this is a render phase update. Only called by class components,\\n  // which special (deprecated) behavior for UNSAFE_componentWillReceive props.\\n  return (\\n    // TODO: Remove outdated deferRenderPhaseUpdateToNextBatch experiment. We\\n    // decided not to enable it.\\n    (!deferRenderPhaseUpdateToNextBatch ||\\n      (fiber.mode & ConcurrentMode) === NoMode) &&\\n    (executionContext & RenderContext) !== NoContext\\n  );\\n}\\n\\n// Use this function to schedule a task for a root. There\\'s only one task per\\n// root; if a task was already scheduled, we\\'ll check to make sure the priority\\n// of the existing task is the same as the priority of the next level that the\\n// root has work on. This function is called on every update, and right before\\n// exiting a task.\\nfunction ensureRootIsScheduled(root: FiberRoot, currentTime: number) {\\n  const existingCallbackNode = root.callbackNode;\\n\\n  // Check if any lanes are being starved by other work. If so, mark them as\\n  // expired so we know to work on those next.\\n  markStarvedLanesAsExpired(root, currentTime);\\n\\n  // Determine the next lanes to work on, and their priority.\\n  const nextLanes = getNextLanes(\\n    root,\\n    root === workInProgressRoot ? workInProgressRootRenderLanes : NoLanes,\\n  );\\n\\n  if (nextLanes === NoLanes) {\\n    // Special case: There\\'s nothing to work on.\\n    if (existingCallbackNode !== null) {\\n      cancelCallback(existingCallbackNode);\\n    }\\n    root.callbackNode = null;\\n    root.callbackPriority = NoLane;\\n    return;\\n  }\\n\\n  // We use the highest priority lane to represent the priority of the callback.\\n  const newCallbackPriority = getHighestPriorityLane(nextLanes);\\n\\n  // Check if there\\'s an existing task. We may be able to reuse it.\\n  const existingCallbackPriority = root.callbackPriority;\\n  if (\\n    existingCallbackPriority === newCallbackPriority &&\\n    // Special case related to `act`. If the currently scheduled task is a\\n    // Scheduler task, rather than an `act` task, cancel it and re-scheduled\\n    // on the `act` queue.\\n    !(\\n      __DEV__ &&\\n      ReactCurrentActQueue.current !== null &&\\n      existingCallbackNode !== fakeActCallbackNode\\n    )\\n  ) {\\n    if (__DEV__) {\\n      // If we\\'re going to re-use an existing task, it needs to exist.\\n      // Assume that discrete update microtasks are non-cancellable and null.\\n      // TODO: Temporary until we confirm this warning is not fired.\\n      if (\\n        existingCallbackNode == null &&\\n        existingCallbackPriority !== SyncLane\\n      ) {\\n        console.error(\\n          \\'Expected scheduled callback to exist. This error is likely caused by a bug in React. Please file an issue.\\',\\n        );\\n      }\\n    }\\n    // The priority hasn\\'t changed. We can reuse the existing task. Exit.\\n    return;\\n  }\\n\\n  if (existingCallbackNode != null) {\\n    // Cancel the existing callback. We\\'ll schedule a new one below.\\n    cancelCallback(existingCallbackNode);\\n  }\\n\\n  // Schedule a new callback.\\n  let newCallbackNode;\\n  if (newCallbackPriority === SyncLane) {\\n    // Special case: Sync React callbacks are scheduled on a special\\n    // internal queue\\n    if (root.tag === LegacyRoot) {\\n      if (__DEV__ && ReactCurrentActQueue.isBatchingLegacy !== null) {\\n        ReactCurrentActQueue.didScheduleLegacyUpdate = true;\\n      }\\n      scheduleLegacySyncCallback(performSyncWorkOnRoot.bind(null, root));\\n    } else {\\n      scheduleSyncCallback(performSyncWorkOnRoot.bind(null, root));\\n    }\\n    if (supportsMicrotasks) {\\n      // Flush the queue in a microtask.\\n      if (__DEV__ && ReactCurrentActQueue.current !== null) {\\n        // Inside `act`, use our internal `act` queue so that these get flushed\\n        // at the end of the current scope even when using the sync version\\n        // of `act`.\\n        ReactCurrentActQueue.current.push(flushSyncCallbacks);\\n      } else {\\n        scheduleMicrotask(() => {\\n          // In Safari, appending an iframe forces microtasks to run.\\n          // https://github.com/facebook/react/issues/22459\\n          // We don\\'t support running callbacks in the middle of render\\n          // or commit so we need to check against that.\\n          if (\\n            (executionContext & (RenderContext | CommitContext)) ===\\n            NoContext\\n          ) {\\n            // Note that this would still prematurely flush the callbacks\\n            // if this happens outside render or commit phase (e.g. in an event).\\n            flushSyncCallbacks();\\n          }\\n        });\\n      }\\n    } else {\\n      // Flush the queue in an Immediate task.\\n      scheduleCallback(ImmediateSchedulerPriority, flushSyncCallbacks);\\n    }\\n    newCallbackNode = null;\\n  } else {\\n    let schedulerPriorityLevel;\\n    switch (lanesToEventPriority(nextLanes)) {\\n      case DiscreteEventPriority:\\n        schedulerPriorityLevel = ImmediateSchedulerPriority;\\n        break;\\n      case ContinuousEventPriority:\\n        schedulerPriorityLevel = UserBlockingSchedulerPriority;\\n        break;\\n      case DefaultEventPriority:\\n        schedulerPriorityLevel = NormalSchedulerPriority;\\n        break;\\n      case IdleEventPriority:\\n        schedulerPriorityLevel = IdleSchedulerPriority;\\n        break;\\n      default:\\n        schedulerPriorityLevel = NormalSchedulerPriority;\\n        break;\\n    }\\n    newCallbackNode = scheduleCallback(\\n      schedulerPriorityLevel,\\n      performConcurrentWorkOnRoot.bind(null, root),\\n    );\\n  }\\n\\n  root.callbackPriority = newCallbackPriority;\\n  root.callbackNode = newCallbackNode;\\n}\\n\\n// This is the entry point for every concurrent task, i.e. anything that\\n// goes through Scheduler.\\nfunction performConcurrentWorkOnRoot(root, didTimeout) {\\n  if (enableProfilerTimer && enableProfilerNestedUpdatePhase) {\\n    resetNestedUpdateFlag();\\n  }\\n\\n  // Since we know we\\'re in a React event, we can clear the current\\n  // event time. The next update will compute a new event time.\\n  currentEventTime = NoTimestamp;\\n  currentEventTransitionLane = NoLanes;\\n\\n  if ((executionContext & (RenderContext | CommitContext)) !== NoContext) {\\n    throw new Error(\\'Should not already be working.\\');\\n  }\\n\\n  // Flush any pending passive effects before deciding which lanes to work on,\\n  // in case they schedule additional work.\\n  const originalCallbackNode = root.callbackNode;\\n  const didFlushPassiveEffects = flushPassiveEffects();\\n  if (didFlushPassiveEffects) {\\n    // Something in the passive effect phase may have canceled the current task.\\n    // Check if the task node for this root was changed.\\n    if (root.callbackNode !== originalCallbackNode) {\\n      // The current task was canceled. Exit. We don\\'t need to call\\n      // `ensureRootIsScheduled` because the check above implies either that\\n      // there\\'s a new task, or that there\\'s no remaining work on this root.\\n      return null;\\n    } else {\\n      // Current task was not canceled. Continue.\\n    }\\n  }\\n\\n  // Determine the next lanes to work on, using the fields stored\\n  // on the root.\\n  let lanes = getNextLanes(\\n    root,\\n    root === workInProgressRoot ? workInProgressRootRenderLanes : NoLanes,\\n  );\\n  if (lanes === NoLanes) {\\n    // Defensive coding. This is never expected to happen.\\n    return null;\\n  }\\n\\n  // We disable time-slicing in some cases: if the work has been CPU-bound\\n  // for too long (\"expired\" work, to prevent starvation), or we\\'re in\\n  // sync-updates-by-default mode.\\n  // TODO: We only check `didTimeout` defensively, to account for a Scheduler\\n  // bug we\\'re still investigating. Once the bug in Scheduler is fixed,\\n  // we can remove this, since we track expiration ourselves.\\n  const shouldTimeSlice =\\n    !includesBlockingLane(root, lanes) &&\\n    !includesExpiredLane(root, lanes) &&\\n    (disableSchedulerTimeoutInWorkLoop || !didTimeout);\\n  let exitStatus = shouldTimeSlice\\n    ? renderRootConcurrent(root, lanes)\\n    : renderRootSync(root, lanes);\\n  if (exitStatus !== RootInProgress) {\\n    if (exitStatus === RootErrored) {\\n      // If something threw an error, try rendering one more time. We\\'ll\\n      // render synchronously to block concurrent data mutations, and we\\'ll\\n      // includes all pending updates are included. If it still fails after\\n      // the second attempt, we\\'ll give up and commit the resulting tree.\\n      const originallyAttemptedLanes = lanes;\\n      const errorRetryLanes = getLanesToRetrySynchronouslyOnError(\\n        root,\\n        originallyAttemptedLanes,\\n      );\\n      if (errorRetryLanes !== NoLanes) {\\n        lanes = errorRetryLanes;\\n        exitStatus = recoverFromConcurrentError(\\n          root,\\n          originallyAttemptedLanes,\\n          errorRetryLanes,\\n        );\\n      }\\n    }\\n    if (exitStatus === RootFatalErrored) {\\n      const fatalError = workInProgressRootFatalError;\\n      prepareFreshStack(root, NoLanes);\\n      markRootSuspended(root, lanes);\\n      ensureRootIsScheduled(root, now());\\n      throw fatalError;\\n    }\\n\\n    if (exitStatus === RootDidNotComplete) {\\n      // The render unwound without completing the tree. This happens in special\\n      // cases where need to exit the current render without producing a\\n      // consistent tree or committing.\\n      //\\n      // This should only happen during a concurrent render, not a discrete or\\n      // synchronous update. We should have already checked for this when we\\n      // unwound the stack.\\n      markRootSuspended(root, lanes);\\n    } else {\\n      // The render completed.\\n\\n      // Check if this render may have yielded to a concurrent event, and if so,\\n      // confirm that any newly rendered stores are consistent.\\n      // TODO: It\\'s possible that even a concurrent render may never have yielded\\n      // to the main thread, if it was fast enough, or if it expired. We could\\n      // skip the consistency check in that case, too.\\n      const renderWasConcurrent = !includesBlockingLane(root, lanes);\\n      const finishedWork: Fiber = (root.current.alternate: any);\\n      if (\\n        renderWasConcurrent &&\\n        !isRenderConsistentWithExternalStores(finishedWork)\\n      ) {\\n        // A store was mutated in an interleaved event. Render again,\\n        // synchronously, to block further mutations.\\n        exitStatus = renderRootSync(root, lanes);\\n\\n        // We need to check again if something threw\\n        if (exitStatus === RootErrored) {\\n          const originallyAttemptedLanes = lanes;\\n          const errorRetryLanes = getLanesToRetrySynchronouslyOnError(\\n            root,\\n            originallyAttemptedLanes,\\n          );\\n          if (errorRetryLanes !== NoLanes) {\\n            lanes = errorRetryLanes;\\n            exitStatus = recoverFromConcurrentError(\\n              root,\\n              originallyAttemptedLanes,\\n              errorRetryLanes,\\n            );\\n            // We assume the tree is now consistent because we didn\\'t yield to any\\n            // concurrent events.\\n          }\\n        }\\n        if (exitStatus === RootFatalErrored) {\\n          const fatalError = workInProgressRootFatalError;\\n          prepareFreshStack(root, NoLanes);\\n          markRootSuspended(root, lanes);\\n          ensureRootIsScheduled(root, now());\\n          throw fatalError;\\n        }\\n      }\\n\\n      // We now have a consistent tree. The next step is either to commit it,\\n      // or, if something suspended, wait to commit it after a timeout.\\n      root.finishedWork = finishedWork;\\n      root.finishedLanes = lanes;\\n      finishConcurrentRender(root, exitStatus, lanes);\\n    }\\n  }\\n\\n  ensureRootIsScheduled(root, now());\\n  if (root.callbackNode === originalCallbackNode) {\\n    // The task node scheduled for this root is the same one that\\'s\\n    // currently executed. Need to return a continuation.\\n    return performConcurrentWorkOnRoot.bind(null, root);\\n  }\\n  return null;\\n}\\n\\nfunction recoverFromConcurrentError(\\n  root,\\n  originallyAttemptedLanes,\\n  errorRetryLanes,\\n) {\\n  // If an error occurred during hydration, discard server response and fall\\n  // back to client side render.\\n\\n  // Before rendering again, save the errors from the previous attempt.\\n  const errorsFromFirstAttempt = workInProgressRootConcurrentErrors;\\n\\n  const wasRootDehydrated = isRootDehydrated(root);\\n  if (wasRootDehydrated) {\\n    // The shell failed to hydrate. Set a flag to force a client rendering\\n    // during the next attempt. To do this, we call prepareFreshStack now\\n    // to create the root work-in-progress fiber. This is a bit weird in terms\\n    // of factoring, because it relies on renderRootSync not calling\\n    // prepareFreshStack again in the call below, which happens because the\\n    // root and lanes haven\\'t changed.\\n    //\\n    // TODO: I think what we should do is set ForceClientRender inside\\n    // throwException, like we do for nested Suspense boundaries. The reason\\n    // it\\'s here instead is so we can switch to the synchronous work loop, too.\\n    // Something to consider for a future refactor.\\n    const rootWorkInProgress = prepareFreshStack(root, errorRetryLanes);\\n    rootWorkInProgress.flags |= ForceClientRender;\\n    if (__DEV__) {\\n      errorHydratingContainer(root.containerInfo);\\n    }\\n  }\\n\\n  const exitStatus = renderRootSync(root, errorRetryLanes);\\n  if (exitStatus !== RootErrored) {\\n    // Successfully finished rendering on retry\\n\\n    if (workInProgressRootDidAttachPingListener && !wasRootDehydrated) {\\n      // During the synchronous render, we attached additional ping listeners.\\n      // This is highly suggestive of an uncached promise (though it\\'s not the\\n      // only reason this would happen). If it was an uncached promise, then\\n      // it may have masked a downstream error from ocurring without actually\\n      // fixing it. Example:\\n      //\\n      //    use(Promise.resolve(\\'uncached\\'))\\n      //    throw new Error(\\'Oops!\\')\\n      //\\n      // When this happens, there\\'s a conflict between blocking potential\\n      // concurrent data races and unwrapping uncached promise values. We\\n      // have to choose one or the other. Because the data race recovery is\\n      // a last ditch effort, we\\'ll disable it.\\n      root.errorRecoveryDisabledLanes = mergeLanes(\\n        root.errorRecoveryDisabledLanes,\\n        originallyAttemptedLanes,\\n      );\\n\\n      // Mark the current render as suspended and force it to restart. Once\\n      // these lanes finish successfully, we\\'ll re-enable the error recovery\\n      // mechanism for subsequent updates.\\n      workInProgressRootInterleavedUpdatedLanes |= originallyAttemptedLanes;\\n      return RootSuspendedWithDelay;\\n    }\\n\\n    // The errors from the failed first attempt have been recovered. Add\\n    // them to the collection of recoverable errors. We\\'ll log them in the\\n    // commit phase.\\n    const errorsFromSecondAttempt = workInProgressRootRecoverableErrors;\\n    workInProgressRootRecoverableErrors = errorsFromFirstAttempt;\\n    // The errors from the second attempt should be queued after the errors\\n    // from the first attempt, to preserve the causal sequence.\\n    if (errorsFromSecondAttempt !== null) {\\n      queueRecoverableErrors(errorsFromSecondAttempt);\\n    }\\n  } else {\\n    // The UI failed to recover.\\n  }\\n  return exitStatus;\\n}\\n\\nexport function queueRecoverableErrors(errors: Array<CapturedValue<mixed>>) {\\n  if (workInProgressRootRecoverableErrors === null) {\\n    workInProgressRootRecoverableErrors = errors;\\n  } else {\\n    workInProgressRootRecoverableErrors.push.apply(\\n      workInProgressRootRecoverableErrors,\\n      errors,\\n    );\\n  }\\n}\\n\\nfunction finishConcurrentRender(root, exitStatus, lanes) {\\n  switch (exitStatus) {\\n    case RootInProgress:\\n    case RootFatalErrored: {\\n      throw new Error(\\'Root did not complete. This is a bug in React.\\');\\n    }\\n    // Flow knows about invariant, so it complains if I add a break\\n    // statement, but eslint doesn\\'t know about invariant, so it complains\\n    // if I do. eslint-disable-next-line no-fallthrough\\n    case RootErrored: {\\n      // We should have already attempted to retry this tree. If we reached\\n      // this point, it errored again. Commit it.\\n      commitRoot(\\n        root,\\n        workInProgressRootRecoverableErrors,\\n        workInProgressTransitions,\\n      );\\n      break;\\n    }\\n    case RootSuspended: {\\n      markRootSuspended(root, lanes);\\n\\n      // We have an acceptable loading state. We need to figure out if we\\n      // should immediately commit it or wait a bit.\\n\\n      if (\\n        includesOnlyRetries(lanes) &&\\n        // do not delay if we\\'re inside an act() scope\\n        !shouldForceFlushFallbacksInDEV()\\n      ) {\\n        // This render only included retries, no updates. Throttle committing\\n        // retries so that we don\\'t show too many loading states too quickly.\\n        const msUntilTimeout =\\n          globalMostRecentFallbackTime + FALLBACK_THROTTLE_MS - now();\\n        // Don\\'t bother with a very short suspense time.\\n        if (msUntilTimeout > 10) {\\n          const nextLanes = getNextLanes(root, NoLanes);\\n          if (nextLanes !== NoLanes) {\\n            // There\\'s additional work on this root.\\n            break;\\n          }\\n          const suspendedLanes = root.suspendedLanes;\\n          if (!isSubsetOfLanes(suspendedLanes, lanes)) {\\n            // We should prefer to render the fallback of at the last\\n            // suspended level. Ping the last suspended level to try\\n            // rendering it again.\\n            // FIXME: What if the suspended lanes are Idle? Should not restart.\\n            const eventTime = requestEventTime();\\n            markRootPinged(root, suspendedLanes, eventTime);\\n            break;\\n          }\\n\\n          // The render is suspended, it hasn\\'t timed out, and there\\'s no\\n          // lower priority work to do. Instead of committing the fallback\\n          // immediately, wait for more data to arrive.\\n          root.timeoutHandle = scheduleTimeout(\\n            commitRoot.bind(\\n              null,\\n              root,\\n              workInProgressRootRecoverableErrors,\\n              workInProgressTransitions,\\n            ),\\n            msUntilTimeout,\\n          );\\n          break;\\n        }\\n      }\\n      // The work expired. Commit immediately.\\n      commitRoot(\\n        root,\\n        workInProgressRootRecoverableErrors,\\n        workInProgressTransitions,\\n      );\\n      break;\\n    }\\n    case RootSuspendedWithDelay: {\\n      markRootSuspended(root, lanes);\\n\\n      if (includesOnlyTransitions(lanes)) {\\n        // This is a transition, so we should exit without committing a\\n        // placeholder and without scheduling a timeout. Delay indefinitely\\n        // until we receive more data.\\n        break;\\n      }\\n\\n      if (!shouldForceFlushFallbacksInDEV()) {\\n        // This is not a transition, but we did trigger an avoided state.\\n        // Schedule a placeholder to display after a short delay, using the Just\\n        // Noticeable Difference.\\n        // TODO: Is the JND optimization worth the added complexity? If this is\\n        // the only reason we track the event time, then probably not.\\n        // Consider removing.\\n\\n        const mostRecentEventTime = getMostRecentEventTime(root, lanes);\\n        const eventTimeMs = mostRecentEventTime;\\n        const timeElapsedMs = now() - eventTimeMs;\\n        const msUntilTimeout = jnd(timeElapsedMs) - timeElapsedMs;\\n\\n        // Don\\'t bother with a very short suspense time.\\n        if (msUntilTimeout > 10) {\\n          // Instead of committing the fallback immediately, wait for more data\\n          // to arrive.\\n          root.timeoutHandle = scheduleTimeout(\\n            commitRoot.bind(\\n              null,\\n              root,\\n              workInProgressRootRecoverableErrors,\\n              workInProgressTransitions,\\n            ),\\n            msUntilTimeout,\\n          );\\n          break;\\n        }\\n      }\\n\\n      // Commit the placeholder.\\n      commitRoot(\\n        root,\\n        workInProgressRootRecoverableErrors,\\n        workInProgressTransitions,\\n      );\\n      break;\\n    }\\n    case RootCompleted: {\\n      // The work completed. Ready to commit.\\n      commitRoot(\\n        root,\\n        workInProgressRootRecoverableErrors,\\n        workInProgressTransitions,\\n      );\\n      break;\\n    }\\n    default: {\\n      throw new Error(\\'Unknown root exit status.\\');\\n    }\\n  }\\n}\\n\\nfunction isRenderConsistentWithExternalStores(finishedWork: Fiber): boolean {\\n  // Search the rendered tree for external store reads, and check whether the\\n  // stores were mutated in a concurrent event. Intentionally using an iterative\\n  // loop instead of recursion so we can exit early.\\n  let node: Fiber = finishedWork;\\n  while (true) {\\n    if (node.flags & StoreConsistency) {\\n      const updateQueue: FunctionComponentUpdateQueue | null = (node.updateQueue: any);\\n      if (updateQueue !== null) {\\n        const checks = updateQueue.stores;\\n        if (checks !== null) {\\n          for (let i = 0; i < checks.length; i++) {\\n            const check = checks[i];\\n            const getSnapshot = check.getSnapshot;\\n            const renderedValue = check.value;\\n            try {\\n              if (!is(getSnapshot(), renderedValue)) {\\n                // Found an inconsistent store.\\n                return false;\\n              }\\n            } catch (error) {\\n              // If `getSnapshot` throws, return `false`. This will schedule\\n              // a re-render, and the error will be rethrown during render.\\n              return false;\\n            }\\n          }\\n        }\\n      }\\n    }\\n    const child = node.child;\\n    if (node.subtreeFlags & StoreConsistency && child !== null) {\\n      child.return = node;\\n      node = child;\\n      continue;\\n    }\\n    if (node === finishedWork) {\\n      return true;\\n    }\\n    while (node.sibling === null) {\\n      if (node.return === null || node.return === finishedWork) {\\n        return true;\\n      }\\n      node = node.return;\\n    }\\n    node.sibling.return = node.return;\\n    node = node.sibling;\\n  }\\n  // Flow doesn\\'t know this is unreachable, but eslint does\\n  // eslint-disable-next-line no-unreachable\\n  return true;\\n}\\n\\nfunction markRootSuspended(root, suspendedLanes) {\\n  // When suspending, we should always exclude lanes that were pinged or (more\\n  // rarely, since we try to avoid it) updated during the render phase.\\n  // TODO: Lol maybe there\\'s a better way to factor this besides this\\n  // obnoxiously named function :)\\n  suspendedLanes = removeLanes(suspendedLanes, workInProgressRootPingedLanes);\\n  suspendedLanes = removeLanes(\\n    suspendedLanes,\\n    workInProgressRootInterleavedUpdatedLanes,\\n  );\\n  markRootSuspended_dontCallThisOneDirectly(root, suspendedLanes);\\n}\\n\\n// This is the entry point for synchronous tasks that don\\'t go\\n// through Scheduler\\nfunction performSyncWorkOnRoot(root) {\\n  if (enableProfilerTimer && enableProfilerNestedUpdatePhase) {\\n    syncNestedUpdateFlag();\\n  }\\n\\n  if ((executionContext & (RenderContext | CommitContext)) !== NoContext) {\\n    throw new Error(\\'Should not already be working.\\');\\n  }\\n\\n  flushPassiveEffects();\\n\\n  let lanes = getNextLanes(root, NoLanes);\\n  if (!includesSomeLane(lanes, SyncLane)) {\\n    // There\\'s no remaining sync work left.\\n    ensureRootIsScheduled(root, now());\\n    return null;\\n  }\\n\\n  let exitStatus = renderRootSync(root, lanes);\\n  if (root.tag !== LegacyRoot && exitStatus === RootErrored) {\\n    // If something threw an error, try rendering one more time. We\\'ll render\\n    // synchronously to block concurrent data mutations, and we\\'ll includes\\n    // all pending updates are included. If it still fails after the second\\n    // attempt, we\\'ll give up and commit the resulting tree.\\n    const originallyAttemptedLanes = lanes;\\n    const errorRetryLanes = getLanesToRetrySynchronouslyOnError(\\n      root,\\n      originallyAttemptedLanes,\\n    );\\n    if (errorRetryLanes !== NoLanes) {\\n      lanes = errorRetryLanes;\\n      exitStatus = recoverFromConcurrentError(\\n        root,\\n        originallyAttemptedLanes,\\n        errorRetryLanes,\\n      );\\n    }\\n  }\\n\\n  if (exitStatus === RootFatalErrored) {\\n    const fatalError = workInProgressRootFatalError;\\n    prepareFreshStack(root, NoLanes);\\n    markRootSuspended(root, lanes);\\n    ensureRootIsScheduled(root, now());\\n    throw fatalError;\\n  }\\n\\n  if (exitStatus === RootDidNotComplete) {\\n    throw new Error(\\'Root did not complete. This is a bug in React.\\');\\n  }\\n\\n  // We now have a consistent tree. Because this is a sync render, we\\n  // will commit it even if something suspended.\\n  const finishedWork: Fiber = (root.current.alternate: any);\\n  root.finishedWork = finishedWork;\\n  root.finishedLanes = lanes;\\n  commitRoot(\\n    root,\\n    workInProgressRootRecoverableErrors,\\n    workInProgressTransitions,\\n  );\\n\\n  // Before exiting, make sure there\\'s a callback scheduled for the next\\n  // pending level.\\n  ensureRootIsScheduled(root, now());\\n\\n  return null;\\n}\\n\\nexport function flushRoot(root: FiberRoot, lanes: Lanes) {\\n  if (lanes !== NoLanes) {\\n    markRootEntangled(root, mergeLanes(lanes, SyncLane));\\n    ensureRootIsScheduled(root, now());\\n    if ((executionContext & (RenderContext | CommitContext)) === NoContext) {\\n      resetRenderTimer();\\n      flushSyncCallbacks();\\n    }\\n  }\\n}\\n\\nexport function getExecutionContext(): ExecutionContext {\\n  return executionContext;\\n}\\n\\nexport function deferredUpdates<A>(fn: () => A): A {\\n  const previousPriority = getCurrentUpdatePriority();\\n  const prevTransition = ReactCurrentBatchConfig.transition;\\n\\n  try {\\n    ReactCurrentBatchConfig.transition = null;\\n    setCurrentUpdatePriority(DefaultEventPriority);\\n    return fn();\\n  } finally {\\n    setCurrentUpdatePriority(previousPriority);\\n    ReactCurrentBatchConfig.transition = prevTransition;\\n  }\\n}\\n\\nexport function batchedUpdates<A, R>(fn: A => R, a: A): R {\\n  const prevExecutionContext = executionContext;\\n  executionContext |= BatchedContext;\\n  try {\\n    return fn(a);\\n  } finally {\\n    executionContext = prevExecutionContext;\\n    // If there were legacy sync updates, flush them at the end of the outer\\n    // most batchedUpdates-like method.\\n    if (\\n      executionContext === NoContext &&\\n      // Treat `act` as if it\\'s inside `batchedUpdates`, even in legacy mode.\\n      !(__DEV__ && ReactCurrentActQueue.isBatchingLegacy)\\n    ) {\\n      resetRenderTimer();\\n      flushSyncCallbacksOnlyInLegacyMode();\\n    }\\n  }\\n}\\n\\nexport function discreteUpdates<A, B, C, D, R>(\\n  fn: (A, B, C, D) => R,\\n  a: A,\\n  b: B,\\n  c: C,\\n  d: D,\\n): R {\\n  const previousPriority = getCurrentUpdatePriority();\\n  const prevTransition = ReactCurrentBatchConfig.transition;\\n  try {\\n    ReactCurrentBatchConfig.transition = null;\\n    setCurrentUpdatePriority(DiscreteEventPriority);\\n    return fn(a, b, c, d);\\n  } finally {\\n    setCurrentUpdatePriority(previousPriority);\\n    ReactCurrentBatchConfig.transition = prevTransition;\\n    if (executionContext === NoContext) {\\n      resetRenderTimer();\\n    }\\n  }\\n}\\n\\n// Overload the definition to the two valid signatures.\\n// Warning, this opts-out of checking the function body.\\ndeclare function flushSync<R>(fn: () => R): R;\\n// eslint-disable-next-line no-redeclare\\ndeclare function flushSync(): void;\\n// eslint-disable-next-line no-redeclare\\nexport function flushSync(fn) {\\n  // In legacy mode, we flush pending passive effects at the beginning of the\\n  // next event, not at the end of the previous one.\\n  if (\\n    rootWithPendingPassiveEffects !== null &&\\n    rootWithPendingPassiveEffects.tag === LegacyRoot &&\\n    (executionContext & (RenderContext | CommitContext)) === NoContext\\n  ) {\\n    flushPassiveEffects();\\n  }\\n\\n  const prevExecutionContext = executionContext;\\n  executionContext |= BatchedContext;\\n\\n  const prevTransition = ReactCurrentBatchConfig.transition;\\n  const previousPriority = getCurrentUpdatePriority();\\n\\n  try {\\n    ReactCurrentBatchConfig.transition = null;\\n    setCurrentUpdatePriority(DiscreteEventPriority);\\n    if (fn) {\\n      return fn();\\n    } else {\\n      return undefined;\\n    }\\n  } finally {\\n    setCurrentUpdatePriority(previousPriority);\\n    ReactCurrentBatchConfig.transition = prevTransition;\\n\\n    executionContext = prevExecutionContext;\\n    // Flush the immediate callbacks that were scheduled during this batch.\\n    // Note that this will happen even if batchedUpdates is higher up\\n    // the stack.\\n    if ((executionContext & (RenderContext | CommitContext)) === NoContext) {\\n      flushSyncCallbacks();\\n    }\\n  }\\n}\\n\\nexport function isAlreadyRendering() {\\n  // Used by the renderer to print a warning if certain APIs are called from\\n  // the wrong context.\\n  return (\\n    __DEV__ &&\\n    (executionContext & (RenderContext | CommitContext)) !== NoContext\\n  );\\n}\\n\\nexport function flushControlled(fn: () => mixed): void {\\n  const prevExecutionContext = executionContext;\\n  executionContext |= BatchedContext;\\n  const prevTransition = ReactCurrentBatchConfig.transition;\\n  const previousPriority = getCurrentUpdatePriority();\\n  try {\\n    ReactCurrentBatchConfig.transition = null;\\n    setCurrentUpdatePriority(DiscreteEventPriority);\\n    fn();\\n  } finally {\\n    setCurrentUpdatePriority(previousPriority);\\n    ReactCurrentBatchConfig.transition = prevTransition;\\n\\n    executionContext = prevExecutionContext;\\n    if (executionContext === NoContext) {\\n      // Flush the immediate callbacks that were scheduled during this batch\\n      resetRenderTimer();\\n      flushSyncCallbacks();\\n    }\\n  }\\n}\\n\\n// This is called by the HiddenContext module when we enter or leave a\\n// hidden subtree. The stack logic is managed there because that\\'s the only\\n// place that ever modifies it. Which module it lives in doesn\\'t matter for\\n// performance because this function will get inlined regardless\\nexport function setRenderLanes(subtreeRenderLanes: Lanes) {\\n  renderLanes = subtreeRenderLanes;\\n}\\n\\nexport function getRenderLanes(): Lanes {\\n  return renderLanes;\\n}\\n\\nfunction prepareFreshStack(root: FiberRoot, lanes: Lanes): Fiber {\\n  root.finishedWork = null;\\n  root.finishedLanes = NoLanes;\\n\\n  const timeoutHandle = root.timeoutHandle;\\n  if (timeoutHandle !== noTimeout) {\\n    // The root previous suspended and scheduled a timeout to commit a fallback\\n    // state. Now that we have additional work, cancel the timeout.\\n    root.timeoutHandle = noTimeout;\\n    // $FlowFixMe Complains noTimeout is not a TimeoutID, despite the check above\\n    cancelTimeout(timeoutHandle);\\n  }\\n\\n  if (workInProgress !== null) {\\n    let interruptedWork = workInProgress.return;\\n    while (interruptedWork !== null) {\\n      const current = interruptedWork.alternate;\\n      unwindInterruptedWork(\\n        current,\\n        interruptedWork,\\n        workInProgressRootRenderLanes,\\n      );\\n      interruptedWork = interruptedWork.return;\\n    }\\n    resetWakeableStateAfterEachAttempt();\\n    resetThenableStateOnCompletion();\\n  }\\n  workInProgressRoot = root;\\n  const rootWorkInProgress = createWorkInProgress(root.current, null);\\n  workInProgress = rootWorkInProgress;\\n  workInProgressRootRenderLanes = renderLanes = lanes;\\n  workInProgressIsSuspended = false;\\n  workInProgressThrownValue = null;\\n  workInProgressRootDidAttachPingListener = false;\\n  workInProgressRootExitStatus = RootInProgress;\\n  workInProgressRootFatalError = null;\\n  workInProgressRootSkippedLanes = NoLanes;\\n  workInProgressRootInterleavedUpdatedLanes = NoLanes;\\n  workInProgressRootRenderPhaseUpdatedLanes = NoLanes;\\n  workInProgressRootPingedLanes = NoLanes;\\n  workInProgressRootConcurrentErrors = null;\\n  workInProgressRootRecoverableErrors = null;\\n\\n  finishQueueingConcurrentUpdates();\\n\\n  if (__DEV__) {\\n    ReactStrictModeWarnings.discardPendingWarnings();\\n  }\\n\\n  return rootWorkInProgress;\\n}\\n\\nfunction handleThrow(root, thrownValue): void {\\n  // Reset module-level state that was set during the render phase.\\n  resetContextDependencies();\\n  resetHooksAfterThrow();\\n  resetCurrentDebugFiberInDEV();\\n  // TODO: I found and added this missing line while investigating a\\n  // separate issue. Write a regression test using string refs.\\n  ReactCurrentOwner.current = null;\\n\\n  // Setting this to `true` tells the work loop to unwind the stack instead\\n  // of entering the begin phase. It\\'s called \"suspended\" because it usually\\n  // happens because of Suspense, but it also applies to errors. Think of it\\n  // as suspending the execution of the work loop.\\n  workInProgressIsSuspended = true;\\n  workInProgressThrownValue = thrownValue;\\n\\n  const erroredWork = workInProgress;\\n  if (erroredWork === null) {\\n    // This is a fatal error\\n    workInProgressRootExitStatus = RootFatalErrored;\\n    workInProgressRootFatalError = thrownValue;\\n    return;\\n  }\\n\\n  const isWakeable =\\n    thrownValue !== null &&\\n    typeof thrownValue === \\'object\\' &&\\n    typeof thrownValue.then === \\'function\\';\\n\\n  if (enableProfilerTimer && erroredWork.mode & ProfileMode) {\\n    // Record the time spent rendering before an error was thrown. This\\n    // avoids inaccurate Profiler durations in the case of a\\n    // suspended render.\\n    stopProfilerTimerIfRunningAndRecordDelta(erroredWork, true);\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markComponentRenderStopped();\\n    if (isWakeable) {\\n      const wakeable: Wakeable = (thrownValue: any);\\n      markComponentSuspended(\\n        erroredWork,\\n        wakeable,\\n        workInProgressRootRenderLanes,\\n      );\\n    } else {\\n      markComponentErrored(\\n        erroredWork,\\n        thrownValue,\\n        workInProgressRootRenderLanes,\\n      );\\n    }\\n  }\\n\\n  if (isWakeable) {\\n    const wakeable: Wakeable = (thrownValue: any);\\n\\n    trackSuspendedWakeable(wakeable);\\n  }\\n}\\n\\nfunction pushDispatcher() {\\n  const prevDispatcher = ReactCurrentDispatcher.current;\\n  ReactCurrentDispatcher.current = ContextOnlyDispatcher;\\n  if (prevDispatcher === null) {\\n    // The React isomorphic package does not include a default dispatcher.\\n    // Instead the first renderer will lazily attach one, in order to give\\n    // nicer error messages.\\n    return ContextOnlyDispatcher;\\n  } else {\\n    return prevDispatcher;\\n  }\\n}\\n\\nfunction popDispatcher(prevDispatcher) {\\n  ReactCurrentDispatcher.current = prevDispatcher;\\n}\\n\\nexport function markCommitTimeOfFallback() {\\n  globalMostRecentFallbackTime = now();\\n}\\n\\nexport function markSkippedUpdateLanes(lane: Lane | Lanes): void {\\n  workInProgressRootSkippedLanes = mergeLanes(\\n    lane,\\n    workInProgressRootSkippedLanes,\\n  );\\n}\\n\\nexport function renderDidSuspend(): void {\\n  if (workInProgressRootExitStatus === RootInProgress) {\\n    workInProgressRootExitStatus = RootSuspended;\\n  }\\n}\\n\\nexport function renderDidSuspendDelayIfPossible(): void {\\n  if (\\n    workInProgressRootExitStatus === RootInProgress ||\\n    workInProgressRootExitStatus === RootSuspended ||\\n    workInProgressRootExitStatus === RootErrored\\n  ) {\\n    workInProgressRootExitStatus = RootSuspendedWithDelay;\\n  }\\n\\n  // Check if there are updates that we skipped tree that might have unblocked\\n  // this render.\\n  if (\\n    workInProgressRoot !== null &&\\n    (includesNonIdleWork(workInProgressRootSkippedLanes) ||\\n      includesNonIdleWork(workInProgressRootInterleavedUpdatedLanes))\\n  ) {\\n    // Mark the current render as suspended so that we switch to working on\\n    // the updates that were skipped. Usually we only suspend at the end of\\n    // the render phase.\\n    // TODO: We should probably always mark the root as suspended immediately\\n    // (inside this function), since by suspending at the end of the render\\n    // phase introduces a potential mistake where we suspend lanes that were\\n    // pinged or updated while we were rendering.\\n    markRootSuspended(workInProgressRoot, workInProgressRootRenderLanes);\\n  }\\n}\\n\\nexport function renderDidError(error: CapturedValue<mixed>) {\\n  if (workInProgressRootExitStatus !== RootSuspendedWithDelay) {\\n    workInProgressRootExitStatus = RootErrored;\\n  }\\n  if (workInProgressRootConcurrentErrors === null) {\\n    workInProgressRootConcurrentErrors = [error];\\n  } else {\\n    workInProgressRootConcurrentErrors.push(error);\\n  }\\n}\\n\\n// Called during render to determine if anything has suspended.\\n// Returns false if we\\'re not sure.\\nexport function renderHasNotSuspendedYet(): boolean {\\n  // If something errored or completed, we can\\'t really be sure,\\n  // so those are false.\\n  return workInProgressRootExitStatus === RootInProgress;\\n}\\n\\nfunction renderRootSync(root: FiberRoot, lanes: Lanes) {\\n  const prevExecutionContext = executionContext;\\n  executionContext |= RenderContext;\\n  const prevDispatcher = pushDispatcher();\\n\\n  // If the root or lanes have changed, throw out the existing stack\\n  // and prepare a fresh one. Otherwise we\\'ll continue where we left off.\\n  if (workInProgressRoot !== root || workInProgressRootRenderLanes !== lanes) {\\n    if (enableUpdaterTracking) {\\n      if (isDevToolsPresent) {\\n        const memoizedUpdaters = root.memoizedUpdaters;\\n        if (memoizedUpdaters.size > 0) {\\n          restorePendingUpdaters(root, workInProgressRootRenderLanes);\\n          memoizedUpdaters.clear();\\n        }\\n\\n        // At this point, move Fibers that scheduled the upcoming work from the Map to the Set.\\n        // If we bailout on this work, we\\'ll move them back (like above).\\n        // It\\'s important to move them now in case the work spawns more work at the same priority with different updaters.\\n        // That way we can keep the current update and future updates separate.\\n        movePendingFibersToMemoized(root, lanes);\\n      }\\n    }\\n\\n    workInProgressTransitions = getTransitionsForLanes(root, lanes);\\n    prepareFreshStack(root, lanes);\\n  }\\n\\n  if (__DEV__) {\\n    if (enableDebugTracing) {\\n      logRenderStarted(lanes);\\n    }\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markRenderStarted(lanes);\\n  }\\n\\n  do {\\n    try {\\n      workLoopSync();\\n      break;\\n    } catch (thrownValue) {\\n      handleThrow(root, thrownValue);\\n    }\\n  } while (true);\\n  resetContextDependencies();\\n\\n  executionContext = prevExecutionContext;\\n  popDispatcher(prevDispatcher);\\n\\n  if (workInProgress !== null) {\\n    // This is a sync render, so we should have finished the whole tree.\\n    throw new Error(\\n      \\'Cannot commit an incomplete root. This error is likely caused by a \\' +\\n        \\'bug in React. Please file an issue.\\',\\n    );\\n  }\\n\\n  if (__DEV__) {\\n    if (enableDebugTracing) {\\n      logRenderStopped();\\n    }\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markRenderStopped();\\n  }\\n\\n  // Set this to null to indicate there\\'s no in-progress render.\\n  workInProgressRoot = null;\\n  workInProgressRootRenderLanes = NoLanes;\\n\\n  return workInProgressRootExitStatus;\\n}\\n\\n// The work loop is an extremely hot path. Tell Closure not to inline it.\\n/** @noinline */\\nfunction workLoopSync() {\\n  // Perform work without checking if we need to yield between fiber.\\n\\n  if (workInProgressIsSuspended) {\\n    // The current work-in-progress was already attempted. We need to unwind\\n    // it before we continue the normal work loop.\\n    const thrownValue = workInProgressThrownValue;\\n    workInProgressIsSuspended = false;\\n    workInProgressThrownValue = null;\\n    if (workInProgress !== null) {\\n      resumeSuspendedUnitOfWork(workInProgress, thrownValue);\\n    }\\n  }\\n\\n  while (workInProgress !== null) {\\n    performUnitOfWork(workInProgress);\\n  }\\n}\\n\\nfunction renderRootConcurrent(root: FiberRoot, lanes: Lanes) {\\n  const prevExecutionContext = executionContext;\\n  executionContext |= RenderContext;\\n  const prevDispatcher = pushDispatcher();\\n\\n  // If the root or lanes have changed, throw out the existing stack\\n  // and prepare a fresh one. Otherwise we\\'ll continue where we left off.\\n  if (workInProgressRoot !== root || workInProgressRootRenderLanes !== lanes) {\\n    if (enableUpdaterTracking) {\\n      if (isDevToolsPresent) {\\n        const memoizedUpdaters = root.memoizedUpdaters;\\n        if (memoizedUpdaters.size > 0) {\\n          restorePendingUpdaters(root, workInProgressRootRenderLanes);\\n          memoizedUpdaters.clear();\\n        }\\n\\n        // At this point, move Fibers that scheduled the upcoming work from the Map to the Set.\\n        // If we bailout on this work, we\\'ll move them back (like above).\\n        // It\\'s important to move them now in case the work spawns more work at the same priority with different updaters.\\n        // That way we can keep the current update and future updates separate.\\n        movePendingFibersToMemoized(root, lanes);\\n      }\\n    }\\n\\n    workInProgressTransitions = getTransitionsForLanes(root, lanes);\\n    resetRenderTimer();\\n    prepareFreshStack(root, lanes);\\n  }\\n\\n  if (__DEV__) {\\n    if (enableDebugTracing) {\\n      logRenderStarted(lanes);\\n    }\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markRenderStarted(lanes);\\n  }\\n\\n  do {\\n    try {\\n      workLoopConcurrent();\\n      break;\\n    } catch (thrownValue) {\\n      handleThrow(root, thrownValue);\\n      if (isTrackingSuspendedThenable()) {\\n        // If this fiber just suspended, it\\'s possible the data is already\\n        // cached. Yield to the the main thread to give it a chance to ping. If\\n        // it does, we can retry immediately without unwinding the stack.\\n        break;\\n      }\\n    }\\n  } while (true);\\n  resetContextDependencies();\\n\\n  popDispatcher(prevDispatcher);\\n  executionContext = prevExecutionContext;\\n\\n  if (__DEV__) {\\n    if (enableDebugTracing) {\\n      logRenderStopped();\\n    }\\n  }\\n\\n  // Check if the tree has completed.\\n  if (workInProgress !== null) {\\n    // Still work remaining.\\n    if (enableSchedulingProfiler) {\\n      markRenderYielded();\\n    }\\n    return RootInProgress;\\n  } else {\\n    // Completed the tree.\\n    if (enableSchedulingProfiler) {\\n      markRenderStopped();\\n    }\\n\\n    // Set this to null to indicate there\\'s no in-progress render.\\n    workInProgressRoot = null;\\n    workInProgressRootRenderLanes = NoLanes;\\n\\n    // Return the final exit status.\\n    return workInProgressRootExitStatus;\\n  }\\n}\\n\\n/** @noinline */\\nfunction workLoopConcurrent() {\\n  // Perform work until Scheduler asks us to yield\\n\\n  if (workInProgressIsSuspended) {\\n    // The current work-in-progress was already attempted. We need to unwind\\n    // it before we continue the normal work loop.\\n    const thrownValue = workInProgressThrownValue;\\n    workInProgressIsSuspended = false;\\n    workInProgressThrownValue = null;\\n    if (workInProgress !== null) {\\n      resumeSuspendedUnitOfWork(workInProgress, thrownValue);\\n    }\\n  }\\n\\n  while (workInProgress !== null && !shouldYield()) {\\n    performUnitOfWork(workInProgress);\\n  }\\n}\\n\\nfunction performUnitOfWork(unitOfWork: Fiber): void {\\n  // The current, flushed, state of this fiber is the alternate. Ideally\\n  // nothing should rely on this, but relying on it here means that we don\\'t\\n  // need an additional field on the work in progress.\\n  const current = unitOfWork.alternate;\\n  setCurrentDebugFiberInDEV(unitOfWork);\\n\\n  let next;\\n  if (enableProfilerTimer && (unitOfWork.mode & ProfileMode) !== NoMode) {\\n    startProfilerTimer(unitOfWork);\\n    next = beginWork(current, unitOfWork, renderLanes);\\n    stopProfilerTimerIfRunningAndRecordDelta(unitOfWork, true);\\n  } else {\\n    next = beginWork(current, unitOfWork, renderLanes);\\n  }\\n\\n  resetCurrentDebugFiberInDEV();\\n  unitOfWork.memoizedProps = unitOfWork.pendingProps;\\n  if (next === null) {\\n    // If this doesn\\'t spawn new work, complete the current work.\\n    completeUnitOfWork(unitOfWork);\\n  } else {\\n    workInProgress = next;\\n  }\\n\\n  ReactCurrentOwner.current = null;\\n}\\n\\nfunction resumeSuspendedUnitOfWork(\\n  unitOfWork: Fiber,\\n  thrownValue: mixed,\\n): void {\\n  // This is a fork of performUnitOfWork specifcally for resuming a fiber that\\n  // just suspended. In some cases, we may choose to retry the fiber immediately\\n  // instead of unwinding the stack. It\\'s a separate function to keep the\\n  // additional logic out of the work loop\\'s hot path.\\n\\n  const wasPinged = suspendedThenableDidResolve();\\n  resetWakeableStateAfterEachAttempt();\\n\\n  if (!wasPinged) {\\n    // The thenable wasn\\'t pinged. Return to the normal work loop. This will\\n    // unwind the stack, and potentially result in showing a fallback.\\n    resetThenableStateOnCompletion();\\n\\n    const returnFiber = unitOfWork.return;\\n    if (returnFiber === null || workInProgressRoot === null) {\\n      // Expected to be working on a non-root fiber. This is a fatal error\\n      // because there\\'s no ancestor that can handle it; the root is\\n      // supposed to capture all errors that weren\\'t caught by an error\\n      // boundary.\\n      workInProgressRootExitStatus = RootFatalErrored;\\n      workInProgressRootFatalError = thrownValue;\\n      // Set `workInProgress` to null. This represents advancing to the next\\n      // sibling, or the parent if there are no siblings. But since the root\\n      // has no siblings nor a parent, we set it to null. Usually this is\\n      // handled by `completeUnitOfWork` or `unwindWork`, but since we\\'re\\n      // intentionally not calling those, we need set it here.\\n      // TODO: Consider calling `unwindWork` to pop the contexts.\\n      workInProgress = null;\\n      return;\\n    }\\n\\n    try {\\n      // Find and mark the nearest Suspense or error boundary that can handle\\n      // this \"exception\".\\n      throwException(\\n        workInProgressRoot,\\n        returnFiber,\\n        unitOfWork,\\n        thrownValue,\\n        workInProgressRootRenderLanes,\\n      );\\n    } catch (error) {\\n      // We had trouble processing the error. An example of this happening is\\n      // when accessing the `componentDidCatch` property of an error boundary\\n      // throws an error. A weird edge case. There\\'s a regression test for this.\\n      // To prevent an infinite loop, bubble the error up to the next parent.\\n      workInProgress = returnFiber;\\n      throw error;\\n    }\\n\\n    // Return to the normal work loop.\\n    completeUnitOfWork(unitOfWork);\\n    return;\\n  }\\n\\n  // The work-in-progress was immediately pinged. Instead of unwinding the\\n  // stack and potentially showing a fallback, unwind only the last stack frame,\\n  // reset the fiber, and try rendering it again.\\n  const current = unitOfWork.alternate;\\n  unwindInterruptedWork(current, unitOfWork, workInProgressRootRenderLanes);\\n  unitOfWork = workInProgress = resetWorkInProgress(unitOfWork, renderLanes);\\n\\n  setCurrentDebugFiberInDEV(unitOfWork);\\n\\n  let next;\\n  if (enableProfilerTimer && (unitOfWork.mode & ProfileMode) !== NoMode) {\\n    startProfilerTimer(unitOfWork);\\n    next = beginWork(current, unitOfWork, renderLanes);\\n    stopProfilerTimerIfRunningAndRecordDelta(unitOfWork, true);\\n  } else {\\n    next = beginWork(current, unitOfWork, renderLanes);\\n  }\\n\\n  // The begin phase finished successfully without suspending. Reset the state\\n  // used to track the fiber while it was suspended. Then return to the normal\\n  // work loop.\\n  resetThenableStateOnCompletion();\\n\\n  resetCurrentDebugFiberInDEV();\\n  unitOfWork.memoizedProps = unitOfWork.pendingProps;\\n  if (next === null) {\\n    // If this doesn\\'t spawn new work, complete the current work.\\n    completeUnitOfWork(unitOfWork);\\n  } else {\\n    workInProgress = next;\\n  }\\n\\n  ReactCurrentOwner.current = null;\\n}\\n\\nfunction completeUnitOfWork(unitOfWork: Fiber): void {\\n  // Attempt to complete the current unit of work, then move to the next\\n  // sibling. If there are no more siblings, return to the parent fiber.\\n  let completedWork = unitOfWork;\\n  do {\\n    // The current, flushed, state of this fiber is the alternate. Ideally\\n    // nothing should rely on this, but relying on it here means that we don\\'t\\n    // need an additional field on the work in progress.\\n    const current = completedWork.alternate;\\n    const returnFiber = completedWork.return;\\n\\n    // Check if the work completed or if something threw.\\n    if ((completedWork.flags & Incomplete) === NoFlags) {\\n      setCurrentDebugFiberInDEV(completedWork);\\n      let next;\\n      if (\\n        !enableProfilerTimer ||\\n        (completedWork.mode & ProfileMode) === NoMode\\n      ) {\\n        next = completeWork(current, completedWork, renderLanes);\\n      } else {\\n        startProfilerTimer(completedWork);\\n        next = completeWork(current, completedWork, renderLanes);\\n        // Update render duration assuming we didn\\'t error.\\n        stopProfilerTimerIfRunningAndRecordDelta(completedWork, false);\\n      }\\n      resetCurrentDebugFiberInDEV();\\n\\n      if (next !== null) {\\n        // Completing this fiber spawned new work. Work on that next.\\n        workInProgress = next;\\n        return;\\n      }\\n    } else {\\n      // This fiber did not complete because something threw. Pop values off\\n      // the stack without entering the complete phase. If this is a boundary,\\n      // capture values if possible.\\n      const next = unwindWork(current, completedWork, renderLanes);\\n\\n      // Because this fiber did not complete, don\\'t reset its lanes.\\n\\n      if (next !== null) {\\n        // If completing this work spawned new work, do that next. We\\'ll come\\n        // back here again.\\n        // Since we\\'re restarting, remove anything that is not a host effect\\n        // from the effect tag.\\n        next.flags &= HostEffectMask;\\n        workInProgress = next;\\n        return;\\n      }\\n\\n      if (\\n        enableProfilerTimer &&\\n        (completedWork.mode & ProfileMode) !== NoMode\\n      ) {\\n        // Record the render duration for the fiber that errored.\\n        stopProfilerTimerIfRunningAndRecordDelta(completedWork, false);\\n\\n        // Include the time spent working on failed children before continuing.\\n        let actualDuration = completedWork.actualDuration;\\n        let child = completedWork.child;\\n        while (child !== null) {\\n          actualDuration += child.actualDuration;\\n          child = child.sibling;\\n        }\\n        completedWork.actualDuration = actualDuration;\\n      }\\n\\n      if (returnFiber !== null) {\\n        // Mark the parent fiber as incomplete and clear its subtree flags.\\n        returnFiber.flags |= Incomplete;\\n        returnFiber.subtreeFlags = NoFlags;\\n        returnFiber.deletions = null;\\n      } else {\\n        // We\\'ve unwound all the way to the root.\\n        workInProgressRootExitStatus = RootDidNotComplete;\\n        workInProgress = null;\\n        return;\\n      }\\n    }\\n\\n    const siblingFiber = completedWork.sibling;\\n    if (siblingFiber !== null) {\\n      // If there is more work to do in this returnFiber, do that next.\\n      workInProgress = siblingFiber;\\n      return;\\n    }\\n    // Otherwise, return to the parent\\n    completedWork = returnFiber;\\n    // Update the next thing we\\'re working on in case something throws.\\n    workInProgress = completedWork;\\n  } while (completedWork !== null);\\n\\n  // We\\'ve reached the root.\\n  if (workInProgressRootExitStatus === RootInProgress) {\\n    workInProgressRootExitStatus = RootCompleted;\\n  }\\n}\\n\\nfunction commitRoot(\\n  root: FiberRoot,\\n  recoverableErrors: null | Array<CapturedValue<mixed>>,\\n  transitions: Array<Transition> | null,\\n) {\\n  // TODO: This no longer makes any sense. We already wrap the mutation and\\n  // layout phases. Should be able to remove.\\n  const previousUpdateLanePriority = getCurrentUpdatePriority();\\n  const prevTransition = ReactCurrentBatchConfig.transition;\\n\\n  try {\\n    ReactCurrentBatchConfig.transition = null;\\n    setCurrentUpdatePriority(DiscreteEventPriority);\\n    commitRootImpl(\\n      root,\\n      recoverableErrors,\\n      transitions,\\n      previousUpdateLanePriority,\\n    );\\n  } finally {\\n    ReactCurrentBatchConfig.transition = prevTransition;\\n    setCurrentUpdatePriority(previousUpdateLanePriority);\\n  }\\n\\n  return null;\\n}\\n\\nfunction commitRootImpl(\\n  root: FiberRoot,\\n  recoverableErrors: null | Array<CapturedValue<mixed>>,\\n  transitions: Array<Transition> | null,\\n  renderPriorityLevel: EventPriority,\\n) {\\n  do {\\n    // `flushPassiveEffects` will call `flushSyncUpdateQueue` at the end, which\\n    // means `flushPassiveEffects` will sometimes result in additional\\n    // passive effects. So we need to keep flushing in a loop until there are\\n    // no more pending effects.\\n    // TODO: Might be better if `flushPassiveEffects` did not automatically\\n    // flush synchronous work at the end, to avoid factoring hazards like this.\\n    flushPassiveEffects();\\n  } while (rootWithPendingPassiveEffects !== null);\\n  flushRenderPhaseStrictModeWarningsInDEV();\\n\\n  if ((executionContext & (RenderContext | CommitContext)) !== NoContext) {\\n    throw new Error(\\'Should not already be working.\\');\\n  }\\n\\n  const finishedWork = root.finishedWork;\\n  const lanes = root.finishedLanes;\\n\\n  if (__DEV__) {\\n    if (enableDebugTracing) {\\n      logCommitStarted(lanes);\\n    }\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markCommitStarted(lanes);\\n  }\\n\\n  if (finishedWork === null) {\\n    if (__DEV__) {\\n      if (enableDebugTracing) {\\n        logCommitStopped();\\n      }\\n    }\\n\\n    if (enableSchedulingProfiler) {\\n      markCommitStopped();\\n    }\\n\\n    return null;\\n  } else {\\n    if (__DEV__) {\\n      if (lanes === NoLanes) {\\n        console.error(\\n          \\'root.finishedLanes should not be empty during a commit. This is a \\' +\\n            \\'bug in React.\\',\\n        );\\n      }\\n    }\\n  }\\n  root.finishedWork = null;\\n  root.finishedLanes = NoLanes;\\n\\n  if (finishedWork === root.current) {\\n    throw new Error(\\n      \\'Cannot commit the same tree as before. This error is likely caused by \\' +\\n        \\'a bug in React. Please file an issue.\\',\\n    );\\n  }\\n\\n  // commitRoot never returns a continuation; it always finishes synchronously.\\n  // So we can clear these now to allow a new callback to be scheduled.\\n  root.callbackNode = null;\\n  root.callbackPriority = NoLane;\\n\\n  // Check which lanes no longer have any work scheduled on them, and mark\\n  // those as finished.\\n  let remainingLanes = mergeLanes(finishedWork.lanes, finishedWork.childLanes);\\n\\n  // Make sure to account for lanes that were updated by a concurrent event\\n  // during the render phase; don\\'t mark them as finished.\\n  const concurrentlyUpdatedLanes = getConcurrentlyUpdatedLanes();\\n  remainingLanes = mergeLanes(remainingLanes, concurrentlyUpdatedLanes);\\n\\n  markRootFinished(root, remainingLanes);\\n\\n  if (root === workInProgressRoot) {\\n    // We can reset these now that they are finished.\\n    workInProgressRoot = null;\\n    workInProgress = null;\\n    workInProgressRootRenderLanes = NoLanes;\\n  } else {\\n    // This indicates that the last root we worked on is not the same one that\\n    // we\\'re committing now. This most commonly happens when a suspended root\\n    // times out.\\n  }\\n\\n  // If there are pending passive effects, schedule a callback to process them.\\n  // Do this as early as possible, so it is queued before anything else that\\n  // might get scheduled in the commit phase. (See #16714.)\\n  // TODO: Delete all other places that schedule the passive effect callback\\n  // They\\'re redundant.\\n  if (\\n    (finishedWork.subtreeFlags & PassiveMask) !== NoFlags ||\\n    (finishedWork.flags & PassiveMask) !== NoFlags\\n  ) {\\n    if (!rootDoesHavePassiveEffects) {\\n      rootDoesHavePassiveEffects = true;\\n      pendingPassiveEffectsRemainingLanes = remainingLanes;\\n      // workInProgressTransitions might be overwritten, so we want\\n      // to store it in pendingPassiveTransitions until they get processed\\n      // We need to pass this through as an argument to commitRoot\\n      // because workInProgressTransitions might have changed between\\n      // the previous render and commit if we throttle the commit\\n      // with setTimeout\\n      pendingPassiveTransitions = transitions;\\n      scheduleCallback(NormalSchedulerPriority, () => {\\n        flushPassiveEffects();\\n        // This render triggered passive effects: release the root cache pool\\n        // *after* passive effects fire to avoid freeing a cache pool that may\\n        // be referenced by a node in the tree (HostRoot, Cache boundary etc)\\n        return null;\\n      });\\n    }\\n  }\\n\\n  // Check if there are any effects in the whole tree.\\n  // TODO: This is left over from the effect list implementation, where we had\\n  // to check for the existence of `firstEffect` to satisfy Flow. I think the\\n  // only other reason this optimization exists is because it affects profiling.\\n  // Reconsider whether this is necessary.\\n  const subtreeHasEffects =\\n    (finishedWork.subtreeFlags &\\n      (BeforeMutationMask | MutationMask | LayoutMask | PassiveMask)) !==\\n    NoFlags;\\n  const rootHasEffect =\\n    (finishedWork.flags &\\n      (BeforeMutationMask | MutationMask | LayoutMask | PassiveMask)) !==\\n    NoFlags;\\n\\n  if (subtreeHasEffects || rootHasEffect) {\\n    const prevTransition = ReactCurrentBatchConfig.transition;\\n    ReactCurrentBatchConfig.transition = null;\\n    const previousPriority = getCurrentUpdatePriority();\\n    setCurrentUpdatePriority(DiscreteEventPriority);\\n\\n    const prevExecutionContext = executionContext;\\n    executionContext |= CommitContext;\\n\\n    // Reset this to null before calling lifecycles\\n    ReactCurrentOwner.current = null;\\n\\n    // The commit phase is broken into several sub-phases. We do a separate pass\\n    // of the effect list for each phase: all mutation effects come before all\\n    // layout effects, and so on.\\n\\n    // The first phase a \"before mutation\" phase. We use this phase to read the\\n    // state of the host tree right before we mutate it. This is where\\n    // getSnapshotBeforeUpdate is called.\\n    const shouldFireAfterActiveInstanceBlur = commitBeforeMutationEffects(\\n      root,\\n      finishedWork,\\n    );\\n\\n    if (enableProfilerTimer) {\\n      // Mark the current commit time to be shared by all Profilers in this\\n      // batch. This enables them to be grouped later.\\n      recordCommitTime();\\n    }\\n\\n    if (enableProfilerTimer && enableProfilerNestedUpdateScheduledHook) {\\n      // Track the root here, rather than in commitLayoutEffects(), because of ref setters.\\n      // Updates scheduled during ref detachment should also be flagged.\\n      rootCommittingMutationOrLayoutEffects = root;\\n    }\\n\\n    // The next phase is the mutation phase, where we mutate the host tree.\\n    commitMutationEffects(root, finishedWork, lanes);\\n\\n    if (enableCreateEventHandleAPI) {\\n      if (shouldFireAfterActiveInstanceBlur) {\\n        afterActiveInstanceBlur();\\n      }\\n    }\\n    resetAfterCommit(root.containerInfo);\\n\\n    // The work-in-progress tree is now the current tree. This must come after\\n    // the mutation phase, so that the previous tree is still current during\\n    // componentWillUnmount, but before the layout phase, so that the finished\\n    // work is current during componentDidMount/Update.\\n    root.current = finishedWork;\\n\\n    // The next phase is the layout phase, where we call effects that read\\n    // the host tree after it\\'s been mutated. The idiomatic use case for this is\\n    // layout, but class component lifecycles also fire here for legacy reasons.\\n    if (__DEV__) {\\n      if (enableDebugTracing) {\\n        logLayoutEffectsStarted(lanes);\\n      }\\n    }\\n    if (enableSchedulingProfiler) {\\n      markLayoutEffectsStarted(lanes);\\n    }\\n    commitLayoutEffects(finishedWork, root, lanes);\\n    if (__DEV__) {\\n      if (enableDebugTracing) {\\n        logLayoutEffectsStopped();\\n      }\\n    }\\n\\n    if (enableSchedulingProfiler) {\\n      markLayoutEffectsStopped();\\n    }\\n\\n    if (enableProfilerTimer && enableProfilerNestedUpdateScheduledHook) {\\n      rootCommittingMutationOrLayoutEffects = null;\\n    }\\n\\n    // Tell Scheduler to yield at the end of the frame, so the browser has an\\n    // opportunity to paint.\\n    requestPaint();\\n\\n    executionContext = prevExecutionContext;\\n\\n    // Reset the priority to the previous non-sync value.\\n    setCurrentUpdatePriority(previousPriority);\\n    ReactCurrentBatchConfig.transition = prevTransition;\\n  } else {\\n    // No effects.\\n    root.current = finishedWork;\\n    // Measure these anyway so the flamegraph explicitly shows that there were\\n    // no effects.\\n    // TODO: Maybe there\\'s a better way to report this.\\n    if (enableProfilerTimer) {\\n      recordCommitTime();\\n    }\\n  }\\n\\n  const rootDidHavePassiveEffects = rootDoesHavePassiveEffects;\\n\\n  if (rootDoesHavePassiveEffects) {\\n    // This commit has passive effects. Stash a reference to them. But don\\'t\\n    // schedule a callback until after flushing layout work.\\n    rootDoesHavePassiveEffects = false;\\n    rootWithPendingPassiveEffects = root;\\n    pendingPassiveEffectsLanes = lanes;\\n  } else {\\n    // There were no passive effects, so we can immediately release the cache\\n    // pool for this render.\\n    releaseRootPooledCache(root, remainingLanes);\\n    if (__DEV__) {\\n      nestedPassiveUpdateCount = 0;\\n      rootWithPassiveNestedUpdates = null;\\n    }\\n  }\\n\\n  // Read this again, since an effect might have updated it\\n  remainingLanes = root.pendingLanes;\\n\\n  // Check if there\\'s remaining work on this root\\n  // TODO: This is part of the `componentDidCatch` implementation. Its purpose\\n  // is to detect whether something might have called setState inside\\n  // `componentDidCatch`. The mechanism is known to be flawed because `setState`\\n  // inside `componentDidCatch` is itself flawed — that\\'s why we recommend\\n  // `getDerivedStateFromError` instead. However, it could be improved by\\n  // checking if remainingLanes includes Sync work, instead of whether there\\'s\\n  // any work remaining at all (which would also include stuff like Suspense\\n  // retries or transitions). It\\'s been like this for a while, though, so fixing\\n  // it probably isn\\'t that urgent.\\n  if (remainingLanes === NoLanes) {\\n    // If there\\'s no remaining work, we can clear the set of already failed\\n    // error boundaries.\\n    legacyErrorBoundariesThatAlreadyFailed = null;\\n  }\\n\\n  if (__DEV__ && enableStrictEffects) {\\n    if (!rootDidHavePassiveEffects) {\\n      commitDoubleInvokeEffectsInDEV(root);\\n    }\\n  }\\n\\n  onCommitRootDevTools(finishedWork.stateNode, renderPriorityLevel);\\n\\n  if (enableUpdaterTracking) {\\n    if (isDevToolsPresent) {\\n      root.memoizedUpdaters.clear();\\n    }\\n  }\\n\\n  if (__DEV__) {\\n    onCommitRootTestSelector();\\n  }\\n\\n  // Always call this before exiting `commitRoot`, to ensure that any\\n  // additional work on this root is scheduled.\\n  ensureRootIsScheduled(root, now());\\n\\n  if (recoverableErrors !== null) {\\n    // There were errors during this render, but recovered from them without\\n    // needing to surface it to the UI. We log them here.\\n    const onRecoverableError = root.onRecoverableError;\\n    for (let i = 0; i < recoverableErrors.length; i++) {\\n      const recoverableError = recoverableErrors[i];\\n      const componentStack = recoverableError.stack;\\n      const digest = recoverableError.digest;\\n      onRecoverableError(recoverableError.value, {componentStack, digest});\\n    }\\n  }\\n\\n  if (hasUncaughtError) {\\n    hasUncaughtError = false;\\n    const error = firstUncaughtError;\\n    firstUncaughtError = null;\\n    throw error;\\n  }\\n\\n  // If the passive effects are the result of a discrete render, flush them\\n  // synchronously at the end of the current task so that the result is\\n  // immediately observable. Otherwise, we assume that they are not\\n  // order-dependent and do not need to be observed by external systems, so we\\n  // can wait until after paint.\\n  // TODO: We can optimize this by not scheduling the callback earlier. Since we\\n  // currently schedule the callback in multiple places, will wait until those\\n  // are consolidated.\\n  if (\\n    includesSomeLane(pendingPassiveEffectsLanes, SyncLane) &&\\n    root.tag !== LegacyRoot\\n  ) {\\n    flushPassiveEffects();\\n  }\\n\\n  // Read this again, since a passive effect might have updated it\\n  remainingLanes = root.pendingLanes;\\n  if (includesSomeLane(remainingLanes, (SyncLane: Lane))) {\\n    if (enableProfilerTimer && enableProfilerNestedUpdatePhase) {\\n      markNestedUpdateScheduled();\\n    }\\n\\n    // Count the number of times the root synchronously re-renders without\\n    // finishing. If there are too many, it indicates an infinite update loop.\\n    if (root === rootWithNestedUpdates) {\\n      nestedUpdateCount++;\\n    } else {\\n      nestedUpdateCount = 0;\\n      rootWithNestedUpdates = root;\\n    }\\n  } else {\\n    nestedUpdateCount = 0;\\n  }\\n\\n  // If layout work was scheduled, flush it now.\\n  flushSyncCallbacks();\\n\\n  if (__DEV__) {\\n    if (enableDebugTracing) {\\n      logCommitStopped();\\n    }\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markCommitStopped();\\n  }\\n\\n  return null;\\n}\\n\\nfunction releaseRootPooledCache(root: FiberRoot, remainingLanes: Lanes) {\\n  if (enableCache) {\\n    const pooledCacheLanes = (root.pooledCacheLanes &= remainingLanes);\\n    if (pooledCacheLanes === NoLanes) {\\n      // None of the remaining work relies on the cache pool. Clear it so\\n      // subsequent requests get a new cache\\n      const pooledCache = root.pooledCache;\\n      if (pooledCache != null) {\\n        root.pooledCache = null;\\n        releaseCache(pooledCache);\\n      }\\n    }\\n  }\\n}\\n\\nexport function flushPassiveEffects(): boolean {\\n  // Returns whether passive effects were flushed.\\n  // TODO: Combine this check with the one in flushPassiveEFfectsImpl. We should\\n  // probably just combine the two functions. I believe they were only separate\\n  // in the first place because we used to wrap it with\\n  // `Scheduler.runWithPriority`, which accepts a function. But now we track the\\n  // priority within React itself, so we can mutate the variable directly.\\n  if (rootWithPendingPassiveEffects !== null) {\\n    // Cache the root since rootWithPendingPassiveEffects is cleared in\\n    // flushPassiveEffectsImpl\\n    const root = rootWithPendingPassiveEffects;\\n    // Cache and clear the remaining lanes flag; it must be reset since this\\n    // method can be called from various places, not always from commitRoot\\n    // where the remaining lanes are known\\n    const remainingLanes = pendingPassiveEffectsRemainingLanes;\\n    pendingPassiveEffectsRemainingLanes = NoLanes;\\n\\n    const renderPriority = lanesToEventPriority(pendingPassiveEffectsLanes);\\n    const priority = lowerEventPriority(DefaultEventPriority, renderPriority);\\n    const prevTransition = ReactCurrentBatchConfig.transition;\\n    const previousPriority = getCurrentUpdatePriority();\\n\\n    try {\\n      ReactCurrentBatchConfig.transition = null;\\n      setCurrentUpdatePriority(priority);\\n      return flushPassiveEffectsImpl();\\n    } finally {\\n      setCurrentUpdatePriority(previousPriority);\\n      ReactCurrentBatchConfig.transition = prevTransition;\\n\\n      // Once passive effects have run for the tree - giving components a\\n      // chance to retain cache instances they use - release the pooled\\n      // cache at the root (if there is one)\\n      releaseRootPooledCache(root, remainingLanes);\\n    }\\n  }\\n  return false;\\n}\\n\\nexport function enqueuePendingPassiveProfilerEffect(fiber: Fiber): void {\\n  if (enableProfilerTimer && enableProfilerCommitHooks) {\\n    pendingPassiveProfilerEffects.push(fiber);\\n    if (!rootDoesHavePassiveEffects) {\\n      rootDoesHavePassiveEffects = true;\\n      scheduleCallback(NormalSchedulerPriority, () => {\\n        flushPassiveEffects();\\n        return null;\\n      });\\n    }\\n  }\\n}\\n\\nfunction flushPassiveEffectsImpl() {\\n  if (rootWithPendingPassiveEffects === null) {\\n    return false;\\n  }\\n\\n  // Cache and clear the transitions flag\\n  const transitions = pendingPassiveTransitions;\\n  pendingPassiveTransitions = null;\\n\\n  const root = rootWithPendingPassiveEffects;\\n  const lanes = pendingPassiveEffectsLanes;\\n  rootWithPendingPassiveEffects = null;\\n  // TODO: This is sometimes out of sync with rootWithPendingPassiveEffects.\\n  // Figure out why and fix it. It\\'s not causing any known issues (probably\\n  // because it\\'s only used for profiling), but it\\'s a refactor hazard.\\n  pendingPassiveEffectsLanes = NoLanes;\\n\\n  if ((executionContext & (RenderContext | CommitContext)) !== NoContext) {\\n    throw new Error(\\'Cannot flush passive effects while already rendering.\\');\\n  }\\n\\n  if (__DEV__) {\\n    isFlushingPassiveEffects = true;\\n    didScheduleUpdateDuringPassiveEffects = false;\\n\\n    if (enableDebugTracing) {\\n      logPassiveEffectsStarted(lanes);\\n    }\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markPassiveEffectsStarted(lanes);\\n  }\\n\\n  const prevExecutionContext = executionContext;\\n  executionContext |= CommitContext;\\n\\n  commitPassiveUnmountEffects(root.current);\\n  commitPassiveMountEffects(root, root.current, lanes, transitions);\\n\\n  // TODO: Move to commitPassiveMountEffects\\n  if (enableProfilerTimer && enableProfilerCommitHooks) {\\n    const profilerEffects = pendingPassiveProfilerEffects;\\n    pendingPassiveProfilerEffects = [];\\n    for (let i = 0; i < profilerEffects.length; i++) {\\n      const fiber = ((profilerEffects[i]: any): Fiber);\\n      commitPassiveEffectDurations(root, fiber);\\n    }\\n  }\\n\\n  if (__DEV__) {\\n    if (enableDebugTracing) {\\n      logPassiveEffectsStopped();\\n    }\\n  }\\n\\n  if (enableSchedulingProfiler) {\\n    markPassiveEffectsStopped();\\n  }\\n\\n  if (__DEV__ && enableStrictEffects) {\\n    commitDoubleInvokeEffectsInDEV(root);\\n  }\\n\\n  executionContext = prevExecutionContext;\\n\\n  flushSyncCallbacks();\\n\\n  if (enableTransitionTracing) {\\n    const prevPendingTransitionCallbacks = currentPendingTransitionCallbacks;\\n    const prevRootTransitionCallbacks = root.transitionCallbacks;\\n    if (\\n      prevPendingTransitionCallbacks !== null &&\\n      prevRootTransitionCallbacks !== null\\n    ) {\\n      // TODO(luna) Refactor this code into the Host Config\\n      // TODO(luna) The end time here is not necessarily accurate\\n      // because passive effects could be called before paint\\n      // (synchronously) or after paint (normally). We need\\n      // to come up with a way to get the correct end time for both cases.\\n      // One solution is in the host config, if the passive effects\\n      // have not yet been run, make a call to flush the passive effects\\n      // right after paint.\\n      const endTime = now();\\n      currentPendingTransitionCallbacks = null;\\n\\n      scheduleCallback(IdleSchedulerPriority, () =>\\n        processTransitionCallbacks(\\n          prevPendingTransitionCallbacks,\\n          endTime,\\n          prevRootTransitionCallbacks,\\n        ),\\n      );\\n    }\\n  }\\n\\n  if (__DEV__) {\\n    // If additional passive effects were scheduled, increment a counter. If this\\n    // exceeds the limit, we\\'ll fire a warning.\\n    if (didScheduleUpdateDuringPassiveEffects) {\\n      if (root === rootWithPassiveNestedUpdates) {\\n        nestedPassiveUpdateCount++;\\n      } else {\\n        nestedPassiveUpdateCount = 0;\\n        rootWithPassiveNestedUpdates = root;\\n      }\\n    } else {\\n      nestedPassiveUpdateCount = 0;\\n    }\\n    isFlushingPassiveEffects = false;\\n    didScheduleUpdateDuringPassiveEffects = false;\\n  }\\n\\n  // TODO: Move to commitPassiveMountEffects\\n  onPostCommitRootDevTools(root);\\n  if (enableProfilerTimer && enableProfilerCommitHooks) {\\n    const stateNode = root.current.stateNode;\\n    stateNode.effectDuration = 0;\\n    stateNode.passiveEffectDuration = 0;\\n  }\\n\\n  return true;\\n}\\n\\nexport function isAlreadyFailedLegacyErrorBoundary(instance: mixed): boolean {\\n  return (\\n    legacyErrorBoundariesThatAlreadyFailed !== null &&\\n    legacyErrorBoundariesThatAlreadyFailed.has(instance)\\n  );\\n}\\n\\nexport function markLegacyErrorBoundaryAsFailed(instance: mixed) {\\n  if (legacyErrorBoundariesThatAlreadyFailed === null) {\\n    legacyErrorBoundariesThatAlreadyFailed = new Set([instance]);\\n  } else {\\n    legacyErrorBoundariesThatAlreadyFailed.add(instance);\\n  }\\n}\\n\\nfunction prepareToThrowUncaughtError(error: mixed) {\\n  if (!hasUncaughtError) {\\n    hasUncaughtError = true;\\n    firstUncaughtError = error;\\n  }\\n}\\nexport const onUncaughtError = prepareToThrowUncaughtError;\\n\\nfunction captureCommitPhaseErrorOnRoot(\\n  rootFiber: Fiber,\\n  sourceFiber: Fiber,\\n  error: mixed,\\n) {\\n  const errorInfo = createCapturedValueAtFiber(error, sourceFiber);\\n  const update = createRootErrorUpdate(rootFiber, errorInfo, (SyncLane: Lane));\\n  const root = enqueueUpdate(rootFiber, update, (SyncLane: Lane));\\n  const eventTime = requestEventTime();\\n  if (root !== null) {\\n    markRootUpdated(root, SyncLane, eventTime);\\n    ensureRootIsScheduled(root, eventTime);\\n  }\\n}\\n\\nexport function captureCommitPhaseError(\\n  sourceFiber: Fiber,\\n  nearestMountedAncestor: Fiber | null,\\n  error: mixed,\\n) {\\n  if (__DEV__) {\\n    reportUncaughtErrorInDEV(error);\\n    setIsRunningInsertionEffect(false);\\n  }\\n  if (sourceFiber.tag === HostRoot) {\\n    // Error was thrown at the root. There is no parent, so the root\\n    // itself should capture it.\\n    captureCommitPhaseErrorOnRoot(sourceFiber, sourceFiber, error);\\n    return;\\n  }\\n\\n  let fiber = null;\\n  if (skipUnmountedBoundaries) {\\n    fiber = nearestMountedAncestor;\\n  } else {\\n    fiber = sourceFiber.return;\\n  }\\n\\n  while (fiber !== null) {\\n    if (fiber.tag === HostRoot) {\\n      captureCommitPhaseErrorOnRoot(fiber, sourceFiber, error);\\n      return;\\n    } else if (fiber.tag === ClassComponent) {\\n      const ctor = fiber.type;\\n      const instance = fiber.stateNode;\\n      if (\\n        typeof ctor.getDerivedStateFromError === \\'function\\' ||\\n        (typeof instance.componentDidCatch === \\'function\\' &&\\n          !isAlreadyFailedLegacyErrorBoundary(instance))\\n      ) {\\n        const errorInfo = createCapturedValueAtFiber(error, sourceFiber);\\n        const update = createClassErrorUpdate(\\n          fiber,\\n          errorInfo,\\n          (SyncLane: Lane),\\n        );\\n        const root = enqueueUpdate(fiber, update, (SyncLane: Lane));\\n        const eventTime = requestEventTime();\\n        if (root !== null) {\\n          markRootUpdated(root, SyncLane, eventTime);\\n          ensureRootIsScheduled(root, eventTime);\\n        }\\n        return;\\n      }\\n    }\\n    fiber = fiber.return;\\n  }\\n\\n  if (__DEV__) {\\n    // TODO: Until we re-land skipUnmountedBoundaries (see #20147), this warning\\n    // will fire for errors that are thrown by destroy functions inside deleted\\n    // trees. What it should instead do is propagate the error to the parent of\\n    // the deleted tree. In the meantime, do not add this warning to the\\n    // allowlist; this is only for our internal use.\\n    console.error(\\n      \\'Internal React error: Attempted to capture a commit phase error \\' +\\n        \\'inside a detached tree. This indicates a bug in React. Likely \\' +\\n        \\'causes include deleting the same fiber more than once, committing an \\' +\\n        \\'already-finished tree, or an inconsistent return pointer.\\\\n\\\\n\\' +\\n        \\'Error message:\\\\n\\\\n%s\\',\\n      error,\\n    );\\n  }\\n}\\n\\nexport function attachPingListener(\\n  root: FiberRoot,\\n  wakeable: Wakeable,\\n  lanes: Lanes,\\n) {\\n  // Attach a ping listener\\n  //\\n  // The data might resolve before we have a chance to commit the fallback. Or,\\n  // in the case of a refresh, we\\'ll never commit a fallback. So we need to\\n  // attach a listener now. When it resolves (\"pings\"), we can decide whether to\\n  // try rendering the tree again.\\n  //\\n  // Only attach a listener if one does not already exist for the lanes\\n  // we\\'re currently rendering (which acts like a \"thread ID\" here).\\n  //\\n  // We only need to do this in concurrent mode. Legacy Suspense always\\n  // commits fallbacks synchronously, so there are no pings.\\n  let pingCache = root.pingCache;\\n  let threadIDs;\\n  if (pingCache === null) {\\n    pingCache = root.pingCache = new PossiblyWeakMap();\\n    threadIDs = new Set();\\n    pingCache.set(wakeable, threadIDs);\\n  } else {\\n    threadIDs = pingCache.get(wakeable);\\n    if (threadIDs === undefined) {\\n      threadIDs = new Set();\\n      pingCache.set(wakeable, threadIDs);\\n    }\\n  }\\n  if (!threadIDs.has(lanes)) {\\n    workInProgressRootDidAttachPingListener = true;\\n\\n    // Memoize using the thread ID to prevent redundant listeners.\\n    threadIDs.add(lanes);\\n    const ping = pingSuspendedRoot.bind(null, root, wakeable, lanes);\\n    if (enableUpdaterTracking) {\\n      if (isDevToolsPresent) {\\n        // If we have pending work still, restore the original updaters\\n        restorePendingUpdaters(root, lanes);\\n      }\\n    }\\n    wakeable.then(ping, ping);\\n  }\\n}\\n\\nfunction pingSuspendedRoot(\\n  root: FiberRoot,\\n  wakeable: Wakeable,\\n  pingedLanes: Lanes,\\n) {\\n  const pingCache = root.pingCache;\\n  if (pingCache !== null) {\\n    // The wakeable resolved, so we no longer need to memoize, because it will\\n    // never be thrown again.\\n    pingCache.delete(wakeable);\\n  }\\n\\n  const eventTime = requestEventTime();\\n  markRootPinged(root, pingedLanes, eventTime);\\n\\n  warnIfSuspenseResolutionNotWrappedWithActDEV(root);\\n\\n  if (\\n    workInProgressRoot === root &&\\n    isSubsetOfLanes(workInProgressRootRenderLanes, pingedLanes)\\n  ) {\\n    // Received a ping at the same priority level at which we\\'re currently\\n    // rendering. We might want to restart this render. This should mirror\\n    // the logic of whether or not a root suspends once it completes.\\n    // TODO: If we\\'re rendering sync either due to Sync, Batched or expired,\\n    // we should probably never restart.\\n\\n    // If we\\'re suspended with delay, or if it\\'s a retry, we\\'ll always suspend\\n    // so we can always restart.\\n    if (\\n      workInProgressRootExitStatus === RootSuspendedWithDelay ||\\n      (workInProgressRootExitStatus === RootSuspended &&\\n        includesOnlyRetries(workInProgressRootRenderLanes) &&\\n        now() - globalMostRecentFallbackTime < FALLBACK_THROTTLE_MS)\\n    ) {\\n      // Restart from the root.\\n      prepareFreshStack(root, NoLanes);\\n    } else {\\n      // Even though we can\\'t restart right now, we might get an\\n      // opportunity later. So we mark this render as having a ping.\\n      workInProgressRootPingedLanes = mergeLanes(\\n        workInProgressRootPingedLanes,\\n        pingedLanes,\\n      );\\n    }\\n  }\\n\\n  ensureRootIsScheduled(root, eventTime);\\n}\\n\\nfunction retryTimedOutBoundary(boundaryFiber: Fiber, retryLane: Lane) {\\n  // The boundary fiber (a Suspense component or SuspenseList component)\\n  // previously was rendered in its fallback state. One of the promises that\\n  // suspended it has resolved, which means at least part of the tree was\\n  // likely unblocked. Try rendering again, at a new lanes.\\n  if (retryLane === NoLane) {\\n    // TODO: Assign this to `suspenseState.retryLane`? to avoid\\n    // unnecessary entanglement?\\n    retryLane = requestRetryLane(boundaryFiber);\\n  }\\n  // TODO: Special case idle priority?\\n  const eventTime = requestEventTime();\\n  const root = enqueueConcurrentRenderForLane(boundaryFiber, retryLane);\\n  if (root !== null) {\\n    markRootUpdated(root, retryLane, eventTime);\\n    ensureRootIsScheduled(root, eventTime);\\n  }\\n}\\n\\nexport function retryDehydratedSuspenseBoundary(boundaryFiber: Fiber) {\\n  const suspenseState: null | SuspenseState = boundaryFiber.memoizedState;\\n  let retryLane = NoLane;\\n  if (suspenseState !== null) {\\n    retryLane = suspenseState.retryLane;\\n  }\\n  retryTimedOutBoundary(boundaryFiber, retryLane);\\n}\\n\\nexport function resolveRetryWakeable(boundaryFiber: Fiber, wakeable: Wakeable) {\\n  let retryLane = NoLane; // Default\\n  let retryCache: WeakSet<Wakeable> | Set<Wakeable> | null;\\n  switch (boundaryFiber.tag) {\\n    case SuspenseComponent:\\n      retryCache = boundaryFiber.stateNode;\\n      const suspenseState: null | SuspenseState = boundaryFiber.memoizedState;\\n      if (suspenseState !== null) {\\n        retryLane = suspenseState.retryLane;\\n      }\\n      break;\\n    case SuspenseListComponent:\\n      retryCache = boundaryFiber.stateNode;\\n      break;\\n    case OffscreenComponent: {\\n      const instance: OffscreenInstance = boundaryFiber.stateNode;\\n      retryCache = instance.retryCache;\\n      break;\\n    }\\n    default:\\n      throw new Error(\\n        \\'Pinged unknown suspense boundary type. \\' +\\n          \\'This is probably a bug in React.\\',\\n      );\\n  }\\n\\n  if (retryCache !== null) {\\n    // The wakeable resolved, so we no longer need to memoize, because it will\\n    // never be thrown again.\\n    retryCache.delete(wakeable);\\n  }\\n\\n  retryTimedOutBoundary(boundaryFiber, retryLane);\\n}\\n\\n// Computes the next Just Noticeable Difference (JND) boundary.\\n// The theory is that a person can\\'t tell the difference between small differences in time.\\n// Therefore, if we wait a bit longer than necessary that won\\'t translate to a noticeable\\n// difference in the experience. However, waiting for longer might mean that we can avoid\\n// showing an intermediate loading state. The longer we have already waited, the harder it\\n// is to tell small differences in time. Therefore, the longer we\\'ve already waited,\\n// the longer we can wait additionally. At some point we have to give up though.\\n// We pick a train model where the next boundary commits at a consistent schedule.\\n// These particular numbers are vague estimates. We expect to adjust them based on research.\\nfunction jnd(timeElapsed: number) {\\n  return timeElapsed < 120\\n    ? 120\\n    : timeElapsed < 480\\n    ? 480\\n    : timeElapsed < 1080\\n    ? 1080\\n    : timeElapsed < 1920\\n    ? 1920\\n    : timeElapsed < 3000\\n    ? 3000\\n    : timeElapsed < 4320\\n    ? 4320\\n    : ceil(timeElapsed / 1960) * 1960;\\n}\\n\\nexport function throwIfInfiniteUpdateLoopDetected() {\\n  if (nestedUpdateCount > NESTED_UPDATE_LIMIT) {\\n    nestedUpdateCount = 0;\\n    nestedPassiveUpdateCount = 0;\\n    rootWithNestedUpdates = null;\\n    rootWithPassiveNestedUpdates = null;\\n\\n    throw new Error(\\n      \\'Maximum update depth exceeded. This can happen when a component \\' +\\n        \\'repeatedly calls setState inside componentWillUpdate or \\' +\\n        \\'componentDidUpdate. React limits the number of nested updates to \\' +\\n        \\'prevent infinite loops.\\',\\n    );\\n  }\\n\\n  if (__DEV__) {\\n    if (nestedPassiveUpdateCount > NESTED_PASSIVE_UPDATE_LIMIT) {\\n      nestedPassiveUpdateCount = 0;\\n      rootWithPassiveNestedUpdates = null;\\n\\n      console.error(\\n        \\'Maximum update depth exceeded. This can happen when a component \\' +\\n          \"calls setState inside useEffect, but useEffect either doesn\\'t \" +\\n          \\'have a dependency array, or one of the dependencies changes on \\' +\\n          \\'every render.\\',\\n      );\\n    }\\n  }\\n}\\n\\nfunction flushRenderPhaseStrictModeWarningsInDEV() {\\n  if (__DEV__) {\\n    ReactStrictModeWarnings.flushLegacyContextWarning();\\n\\n    if (warnAboutDeprecatedLifecycles) {\\n      ReactStrictModeWarnings.flushPendingUnsafeLifecycleWarnings();\\n    }\\n  }\\n}\\n\\nfunction recursivelyTraverseAndDoubleInvokeEffectsInDEV(\\n  root: FiberRoot,\\n  parentFiber: Fiber,\\n  isInStrictMode: boolean,\\n) {\\n  let child = parentFiber.child;\\n  while (child !== null) {\\n    doubleInvokeEffectsInDEV(root, child, isInStrictMode);\\n    child = child.sibling;\\n  }\\n}\\n\\nfunction doubleInvokeEffectsInDEV(\\n  root: FiberRoot,\\n  fiber: Fiber,\\n  parentIsInStrictMode: boolean,\\n) {\\n  const isStrictModeFiber = fiber.type === REACT_STRICT_MODE_TYPE;\\n  const isInStrictMode = parentIsInStrictMode || isStrictModeFiber;\\n  if (fiber.flags & PlacementDEV || fiber.tag === OffscreenComponent) {\\n    setCurrentDebugFiberInDEV(fiber);\\n    if (isInStrictMode) {\\n      disappearLayoutEffects(fiber);\\n      disconnectPassiveEffect(fiber);\\n      reappearLayoutEffects(root, fiber.alternate, fiber, false);\\n      reconnectPassiveEffects(root, fiber, NoLanes, null, false);\\n    }\\n    resetCurrentDebugFiberInDEV();\\n  } else {\\n    recursivelyTraverseAndDoubleInvokeEffectsInDEV(root, fiber, isInStrictMode);\\n  }\\n}\\n\\nfunction commitDoubleInvokeEffectsInDEV(root: FiberRoot) {\\n  if (__DEV__ && enableStrictEffects) {\\n    let doubleInvokeEffects = true;\\n\\n    if (root.tag === LegacyRoot && !(root.current.mode & StrictLegacyMode)) {\\n      doubleInvokeEffects = false;\\n    }\\n    if (\\n      root.tag === ConcurrentRoot &&\\n      !(root.current.mode & (StrictLegacyMode | StrictEffectsMode))\\n    ) {\\n      doubleInvokeEffects = false;\\n    }\\n    recursivelyTraverseAndDoubleInvokeEffectsInDEV(\\n      root,\\n      root.current,\\n      doubleInvokeEffects,\\n    );\\n  }\\n}\\n\\nlet didWarnStateUpdateForNotYetMountedComponent: Set<string> | null = null;\\nexport function warnAboutUpdateOnNotYetMountedFiberInDEV(fiber: Fiber) {\\n  if (__DEV__) {\\n    if ((executionContext & RenderContext) !== NoContext) {\\n      // We let the other warning about render phase updates deal with this one.\\n      return;\\n    }\\n\\n    if (!(fiber.mode & ConcurrentMode)) {\\n      return;\\n    }\\n\\n    const tag = fiber.tag;\\n    if (\\n      tag !== IndeterminateComponent &&\\n      tag !== HostRoot &&\\n      tag !== ClassComponent &&\\n      tag !== FunctionComponent &&\\n      tag !== ForwardRef &&\\n      tag !== MemoComponent &&\\n      tag !== SimpleMemoComponent\\n    ) {\\n      // Only warn for user-defined components, not internal ones like Suspense.\\n      return;\\n    }\\n\\n    // We show the whole stack but dedupe on the top component\\'s name because\\n    // the problematic code almost always lies inside that component.\\n    const componentName = getComponentNameFromFiber(fiber) || \\'ReactComponent\\';\\n    if (didWarnStateUpdateForNotYetMountedComponent !== null) {\\n      if (didWarnStateUpdateForNotYetMountedComponent.has(componentName)) {\\n        return;\\n      }\\n      didWarnStateUpdateForNotYetMountedComponent.add(componentName);\\n    } else {\\n      didWarnStateUpdateForNotYetMountedComponent = new Set([componentName]);\\n    }\\n\\n    const previousFiber = ReactCurrentFiberCurrent;\\n    try {\\n      setCurrentDebugFiberInDEV(fiber);\\n      console.error(\\n        \"Can\\'t perform a React state update on a component that hasn\\'t mounted yet. \" +\\n          \\'This indicates that you have a side-effect in your render function that \\' +\\n          \\'asynchronously later calls tries to update the component. Move this work to \\' +\\n          \\'useEffect instead.\\',\\n      );\\n    } finally {\\n      if (previousFiber) {\\n        setCurrentDebugFiberInDEV(fiber);\\n      } else {\\n        resetCurrentDebugFiberInDEV();\\n      }\\n    }\\n  }\\n}\\n\\nlet beginWork;\\nif (__DEV__ && replayFailedUnitOfWorkWithInvokeGuardedCallback) {\\n  const dummyFiber = null;\\n  beginWork = (current, unitOfWork, lanes) => {\\n    // If a component throws an error, we replay it again in a synchronously\\n    // dispatched event, so that the debugger will treat it as an uncaught\\n    // error See ReactErrorUtils for more information.\\n\\n    // Before entering the begin phase, copy the work-in-progress onto a dummy\\n    // fiber. If beginWork throws, we\\'ll use this to reset the state.\\n    const originalWorkInProgressCopy = assignFiberPropertiesInDEV(\\n      dummyFiber,\\n      unitOfWork,\\n    );\\n    try {\\n      return originalBeginWork(current, unitOfWork, lanes);\\n    } catch (originalError) {\\n      if (\\n        didSuspendOrErrorWhileHydratingDEV() ||\\n        (originalError !== null &&\\n          typeof originalError === \\'object\\' &&\\n          typeof originalError.then === \\'function\\')\\n      ) {\\n        // Don\\'t replay promises.\\n        // Don\\'t replay errors if we are hydrating and have already suspended or handled an error\\n        throw originalError;\\n      }\\n\\n      // Keep this code in sync with handleThrow; any changes here must have\\n      // corresponding changes there.\\n      resetContextDependencies();\\n      resetHooksAfterThrow();\\n      // Don\\'t reset current debug fiber, since we\\'re about to work on the\\n      // same fiber again.\\n\\n      // Unwind the failed stack frame\\n      unwindInterruptedWork(current, unitOfWork, workInProgressRootRenderLanes);\\n\\n      // Restore the original properties of the fiber.\\n      assignFiberPropertiesInDEV(unitOfWork, originalWorkInProgressCopy);\\n\\n      if (enableProfilerTimer && unitOfWork.mode & ProfileMode) {\\n        // Reset the profiler timer.\\n        startProfilerTimer(unitOfWork);\\n      }\\n\\n      // Run beginWork again.\\n      invokeGuardedCallback(\\n        null,\\n        originalBeginWork,\\n        null,\\n        current,\\n        unitOfWork,\\n        lanes,\\n      );\\n\\n      if (hasCaughtError()) {\\n        const replayError = clearCaughtError();\\n        if (\\n          typeof replayError === \\'object\\' &&\\n          replayError !== null &&\\n          replayError._suppressLogging &&\\n          typeof originalError === \\'object\\' &&\\n          originalError !== null &&\\n          !originalError._suppressLogging\\n        ) {\\n          // If suppressed, let the flag carry over to the original error which is the one we\\'ll rethrow.\\n          originalError._suppressLogging = true;\\n        }\\n      }\\n      // We always throw the original error in case the second render pass is not idempotent.\\n      // This can happen if a memoized function or CommonJS module doesn\\'t throw after first invocation.\\n      throw originalError;\\n    }\\n  };\\n} else {\\n  beginWork = originalBeginWork;\\n}\\n\\nlet didWarnAboutUpdateInRender = false;\\nlet didWarnAboutUpdateInRenderForAnotherComponent;\\nif (__DEV__) {\\n  didWarnAboutUpdateInRenderForAnotherComponent = new Set();\\n}\\n\\nfunction warnAboutRenderPhaseUpdatesInDEV(fiber) {\\n  if (__DEV__) {\\n    if (\\n      ReactCurrentDebugFiberIsRenderingInDEV &&\\n      !getIsUpdatingOpaqueValueInRenderPhaseInDEV()\\n    ) {\\n      switch (fiber.tag) {\\n        case FunctionComponent:\\n        case ForwardRef:\\n        case SimpleMemoComponent: {\\n          const renderingComponentName =\\n            (workInProgress && getComponentNameFromFiber(workInProgress)) ||\\n            \\'Unknown\\';\\n          // Dedupe by the rendering component because it\\'s the one that needs to be fixed.\\n          const dedupeKey = renderingComponentName;\\n          if (!didWarnAboutUpdateInRenderForAnotherComponent.has(dedupeKey)) {\\n            didWarnAboutUpdateInRenderForAnotherComponent.add(dedupeKey);\\n            const setStateComponentName =\\n              getComponentNameFromFiber(fiber) || \\'Unknown\\';\\n            console.error(\\n              \\'Cannot update a component (`%s`) while rendering a \\' +\\n                \\'different component (`%s`). To locate the bad setState() call inside `%s`, \\' +\\n                \\'follow the stack trace as described in https://reactjs.org/link/setstate-in-render\\',\\n              setStateComponentName,\\n              renderingComponentName,\\n              renderingComponentName,\\n            );\\n          }\\n          break;\\n        }\\n        case ClassComponent: {\\n          if (!didWarnAboutUpdateInRender) {\\n            console.error(\\n              \\'Cannot update during an existing state transition (such as \\' +\\n                \\'within `render`). Render methods should be a pure \\' +\\n                \\'function of props and state.\\',\\n            );\\n            didWarnAboutUpdateInRender = true;\\n          }\\n          break;\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nexport function restorePendingUpdaters(root: FiberRoot, lanes: Lanes): void {\\n  if (enableUpdaterTracking) {\\n    if (isDevToolsPresent) {\\n      const memoizedUpdaters = root.memoizedUpdaters;\\n      memoizedUpdaters.forEach(schedulingFiber => {\\n        addFiberToLanesMap(root, schedulingFiber, lanes);\\n      });\\n\\n      // This function intentionally does not clear memoized updaters.\\n      // Those may still be relevant to the current commit\\n      // and a future one (e.g. Suspense).\\n    }\\n  }\\n}\\n\\nconst fakeActCallbackNode = {};\\nfunction scheduleCallback(priorityLevel, callback) {\\n  if (__DEV__) {\\n    // If we\\'re currently inside an `act` scope, bypass Scheduler and push to\\n    // the `act` queue instead.\\n    const actQueue = ReactCurrentActQueue.current;\\n    if (actQueue !== null) {\\n      actQueue.push(callback);\\n      return fakeActCallbackNode;\\n    } else {\\n      return Scheduler_scheduleCallback(priorityLevel, callback);\\n    }\\n  } else {\\n    // In production, always call Scheduler. This function will be stripped out.\\n    return Scheduler_scheduleCallback(priorityLevel, callback);\\n  }\\n}\\n\\nfunction cancelCallback(callbackNode) {\\n  if (__DEV__ && callbackNode === fakeActCallbackNode) {\\n    return;\\n  }\\n  // In production, always call Scheduler. This function will be stripped out.\\n  return Scheduler_cancelCallback(callbackNode);\\n}\\n\\nfunction shouldForceFlushFallbacksInDEV() {\\n  // Never force flush in production. This function should get stripped out.\\n  return __DEV__ && ReactCurrentActQueue.current !== null;\\n}\\n\\nfunction warnIfUpdatesNotWrappedWithActDEV(fiber: Fiber): void {\\n  if (__DEV__) {\\n    if (fiber.mode & ConcurrentMode) {\\n      if (!isConcurrentActEnvironment()) {\\n        // Not in an act environment. No need to warn.\\n        return;\\n      }\\n    } else {\\n      // Legacy mode has additional cases where we suppress a warning.\\n      if (!isLegacyActEnvironment(fiber)) {\\n        // Not in an act environment. No need to warn.\\n        return;\\n      }\\n      if (executionContext !== NoContext) {\\n        // Legacy mode doesn\\'t warn if the update is batched, i.e.\\n        // batchedUpdates or flushSync.\\n        return;\\n      }\\n      if (\\n        fiber.tag !== FunctionComponent &&\\n        fiber.tag !== ForwardRef &&\\n        fiber.tag !== SimpleMemoComponent\\n      ) {\\n        // For backwards compatibility with pre-hooks code, legacy mode only\\n        // warns for updates that originate from a hook.\\n        return;\\n      }\\n    }\\n\\n    if (ReactCurrentActQueue.current === null) {\\n      const previousFiber = ReactCurrentFiberCurrent;\\n      try {\\n        setCurrentDebugFiberInDEV(fiber);\\n        console.error(\\n          \\'An update to %s inside a test was not wrapped in act(...).\\\\n\\\\n\\' +\\n            \\'When testing, code that causes React state updates should be \\' +\\n            \\'wrapped into act(...):\\\\n\\\\n\\' +\\n            \\'act(() => {\\\\n\\' +\\n            \\'  /* fire events that update state */\\\\n\\' +\\n            \\'});\\\\n\\' +\\n            \\'/* assert on the output */\\\\n\\\\n\\' +\\n            \"This ensures that you\\'re testing the behavior the user would see \" +\\n            \\'in the browser.\\' +\\n            \\' Learn more at https://reactjs.org/link/wrap-tests-with-act\\',\\n          getComponentNameFromFiber(fiber),\\n        );\\n      } finally {\\n        if (previousFiber) {\\n          setCurrentDebugFiberInDEV(fiber);\\n        } else {\\n          resetCurrentDebugFiberInDEV();\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nfunction warnIfSuspenseResolutionNotWrappedWithActDEV(root: FiberRoot): void {\\n  if (__DEV__) {\\n    if (\\n      root.tag !== LegacyRoot &&\\n      isConcurrentActEnvironment() &&\\n      ReactCurrentActQueue.current === null\\n    ) {\\n      console.error(\\n        \\'A suspended resource finished loading inside a test, but the event \\' +\\n          \\'was not wrapped in act(...).\\\\n\\\\n\\' +\\n          \\'When testing, code that resolves suspended data should be wrapped \\' +\\n          \\'into act(...):\\\\n\\\\n\\' +\\n          \\'act(() => {\\\\n\\' +\\n          \\'  /* finish loading suspended data */\\\\n\\' +\\n          \\'});\\\\n\\' +\\n          \\'/* assert on the output */\\\\n\\\\n\\' +\\n          \"This ensures that you\\'re testing the behavior the user would see \" +\\n          \\'in the browser.\\' +\\n          \\' Learn more at https://reactjs.org/link/wrap-tests-with-act\\',\\n      );\\n    }\\n  }\\n}\\n\\nexport function setIsRunningInsertionEffect(isRunning: boolean): void {\\n  if (__DEV__) {\\n    isRunningInsertionEffect = isRunning;\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_diff_data.iloc[0]['passage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>query</th>\n",
       "      <th>passage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83a536e62264b7baf0bb5d83835821e6f4c45006</td>\n",
       "      <td>src/renderers/shared/fiber/ReactFiberBeginWork.js</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d289d4b634749861199556e42174a3f4a3ce2b16</td>\n",
       "      <td>packages/react-dom/src/__tests__/ReactDOMCompo...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83a536e62264b7baf0bb5d83835821e6f4c45006</td>\n",
       "      <td>src/renderers/shared/fiber/ReactFiberReconcile...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>566259567773a0af56e6c19da615e6802d73f834</td>\n",
       "      <td>packages/react-dom/src/client/ReactDOMFiberCom...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1dbc8e4708d1de49d8e6a22e9682987d08a15c7</td>\n",
       "      <td>src/renderers/dom/fiber/ReactDOMFiberComponent.js</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright 2013-present, Facebook, Inc....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  commit_id  \\\n",
       "0  83a536e62264b7baf0bb5d83835821e6f4c45006   \n",
       "1  d289d4b634749861199556e42174a3f4a3ce2b16   \n",
       "2  83a536e62264b7baf0bb5d83835821e6f4c45006   \n",
       "3  566259567773a0af56e6c19da615e6802d73f834   \n",
       "4  c1dbc8e4708d1de49d8e6a22e9682987d08a15c7   \n",
       "\n",
       "                                           file_path  \\\n",
       "0  src/renderers/shared/fiber/ReactFiberBeginWork.js   \n",
       "1  packages/react-dom/src/__tests__/ReactDOMCompo...   \n",
       "2  src/renderers/shared/fiber/ReactFiberReconcile...   \n",
       "3  packages/react-dom/src/client/ReactDOMFiberCom...   \n",
       "4  src/renderers/dom/fiber/ReactDOMFiberComponent.js   \n",
       "\n",
       "                                               query  \\\n",
       "0  Change warning() to automatically inject the s...   \n",
       "1  Change warning() to automatically inject the s...   \n",
       "2  Change warning() to automatically inject the s...   \n",
       "3  Change warning() to automatically inject the s...   \n",
       "4  Change warning() to automatically inject the s...   \n",
       "\n",
       "                                             passage  label  \n",
       "0  /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "1  /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "2  /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "3  /**\\n * Copyright (c) 2013-present, Facebook, ...      1  \n",
       "4  /**\\n * Copyright 2013-present, Facebook, Inc....      0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_diff_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_code(data):\n",
    "    problems = 0\n",
    "    for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        try:\n",
    "            if row['label'] == 0:\n",
    "                assert data[(data['query'] == row['query']) & (data['commit_id'] == row['commit_id']) & (data['file_path'] == row['file_path'])]['label'].values[0] == 0\n",
    "            else:\n",
    "                assert data[(data['query'] == row['query']) & (data['commit_id'] == row['commit_id']) & (data['file_path'] == row['file_path'])]['label'].values[0] == 1\n",
    "        except AssertionError:\n",
    "            print(f\"Assertion failed at index {i}: {row}\")\n",
    "            # break  # Optional: break after the first failure, remove if you want to see all failures\n",
    "            # remove the row with label 0\n",
    "\n",
    "            if row['label'] == 0:\n",
    "                problems += 1\n",
    "                # data.drop(i, inplace=True)\n",
    "                data = data.drop(i)\n",
    "                # print(f\"Dropped row at index {i}\")\n",
    "\n",
    "    print(f\"Total number of problems in sanity check of training data: {problems}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20753/20753 [01:47<00:00, 192.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of problems in sanity check of training data: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>query</th>\n",
       "      <th>passage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83a536e62264b7baf0bb5d83835821e6f4c45006</td>\n",
       "      <td>src/renderers/shared/fiber/ReactFiberBeginWork.js</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d289d4b634749861199556e42174a3f4a3ce2b16</td>\n",
       "      <td>packages/react-dom/src/__tests__/ReactDOMCompo...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83a536e62264b7baf0bb5d83835821e6f4c45006</td>\n",
       "      <td>src/renderers/shared/fiber/ReactFiberReconcile...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>566259567773a0af56e6c19da615e6802d73f834</td>\n",
       "      <td>packages/react-dom/src/client/ReactDOMFiberCom...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1dbc8e4708d1de49d8e6a22e9682987d08a15c7</td>\n",
       "      <td>src/renderers/dom/fiber/ReactDOMFiberComponent.js</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright 2013-present, Facebook, Inc....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20748</th>\n",
       "      <td>1f74eca9937ad6f19b6291d21edfb8747bae88ca</td>\n",
       "      <td>src/renderers/shared/fiber/ReactFiberReconcile...</td>\n",
       "      <td>Remove _ctor field from Lazy components (#1821...</td>\n",
       "      <td>/**\\n * Copyright 2013-present, Facebook, Inc....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20749</th>\n",
       "      <td>961eb65b4ba5de6bbfb6b8a075a924c284541177</td>\n",
       "      <td>packages/react-dom/src/server/ReactPartialRend...</td>\n",
       "      <td>Remove _ctor field from Lazy components (#1821...</td>\n",
       "      <td>/**\\n * Copyright (c) Facebook, Inc. and its a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20750</th>\n",
       "      <td>5031ebf6beddf88cac15f4d2c9e91f8dbb91d59d</td>\n",
       "      <td>packages/react-reconciler/src/ReactFiberLazyCo...</td>\n",
       "      <td>Remove _ctor field from Lazy components (#1821...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20751</th>\n",
       "      <td>5031ebf6beddf88cac15f4d2c9e91f8dbb91d59d</td>\n",
       "      <td>packages/shared/ReactLazyComponent.js</td>\n",
       "      <td>Remove _ctor field from Lazy components (#1821...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20752</th>\n",
       "      <td>5031ebf6beddf88cac15f4d2c9e91f8dbb91d59d</td>\n",
       "      <td>packages/shared/getComponentName.js</td>\n",
       "      <td>Remove _ctor field from Lazy components (#1821...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20753 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      commit_id  \\\n",
       "0      83a536e62264b7baf0bb5d83835821e6f4c45006   \n",
       "1      d289d4b634749861199556e42174a3f4a3ce2b16   \n",
       "2      83a536e62264b7baf0bb5d83835821e6f4c45006   \n",
       "3      566259567773a0af56e6c19da615e6802d73f834   \n",
       "4      c1dbc8e4708d1de49d8e6a22e9682987d08a15c7   \n",
       "...                                         ...   \n",
       "20748  1f74eca9937ad6f19b6291d21edfb8747bae88ca   \n",
       "20749  961eb65b4ba5de6bbfb6b8a075a924c284541177   \n",
       "20750  5031ebf6beddf88cac15f4d2c9e91f8dbb91d59d   \n",
       "20751  5031ebf6beddf88cac15f4d2c9e91f8dbb91d59d   \n",
       "20752  5031ebf6beddf88cac15f4d2c9e91f8dbb91d59d   \n",
       "\n",
       "                                               file_path  \\\n",
       "0      src/renderers/shared/fiber/ReactFiberBeginWork.js   \n",
       "1      packages/react-dom/src/__tests__/ReactDOMCompo...   \n",
       "2      src/renderers/shared/fiber/ReactFiberReconcile...   \n",
       "3      packages/react-dom/src/client/ReactDOMFiberCom...   \n",
       "4      src/renderers/dom/fiber/ReactDOMFiberComponent.js   \n",
       "...                                                  ...   \n",
       "20748  src/renderers/shared/fiber/ReactFiberReconcile...   \n",
       "20749  packages/react-dom/src/server/ReactPartialRend...   \n",
       "20750  packages/react-reconciler/src/ReactFiberLazyCo...   \n",
       "20751              packages/shared/ReactLazyComponent.js   \n",
       "20752                packages/shared/getComponentName.js   \n",
       "\n",
       "                                                   query  \\\n",
       "0      Change warning() to automatically inject the s...   \n",
       "1      Change warning() to automatically inject the s...   \n",
       "2      Change warning() to automatically inject the s...   \n",
       "3      Change warning() to automatically inject the s...   \n",
       "4      Change warning() to automatically inject the s...   \n",
       "...                                                  ...   \n",
       "20748  Remove _ctor field from Lazy components (#1821...   \n",
       "20749  Remove _ctor field from Lazy components (#1821...   \n",
       "20750  Remove _ctor field from Lazy components (#1821...   \n",
       "20751  Remove _ctor field from Lazy components (#1821...   \n",
       "20752  Remove _ctor field from Lazy components (#1821...   \n",
       "\n",
       "                                                 passage  label  \n",
       "0      /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "1      /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "2      /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "3      /**\\n * Copyright (c) 2013-present, Facebook, ...      1  \n",
       "4      /**\\n * Copyright 2013-present, Facebook, Inc....      0  \n",
       "...                                                  ...    ...  \n",
       "20748  /**\\n * Copyright 2013-present, Facebook, Inc....      0  \n",
       "20749  /**\\n * Copyright (c) Facebook, Inc. and its a...      1  \n",
       "20750  /**\\n * Copyright (c) 2013-present, Facebook, ...      1  \n",
       "20751  /**\\n * Copyright (c) 2013-present, Facebook, ...      1  \n",
       "20752  /**\\n * Copyright (c) 2013-present, Facebook, ...      1  \n",
       "\n",
       "[20753 rows x 5 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_code(processed_diff_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_diff_data.head(1000)\n",
    "processed_diff_data.to_parquet(os.path.join(repo_path, 'cache', 'code_data.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique number of query in processed_diff_data\n",
    "processed_diff_data['query'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20753 entries, 0 to 20752\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   commit_id  20753 non-null  object\n",
      " 1   file_path  20753 non-null  object\n",
      " 2   query      20753 non-null  object\n",
      " 3   passage    20753 non-null  object\n",
      " 4   label      20753 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 810.8+ KB\n"
     ]
    }
   ],
   "source": [
    "diff_data = pd.read_parquet(os.path.join(repo_path, 'cache', 'code_data.parquet'))\n",
    "# drop columns that we don't need aka commit_id and file_path\n",
    "# diff_data.drop(columns=['commit_id', 'file_path'], inplace=True)\n",
    "diff_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    15000\n",
       "1     5753\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see distribution of labels\n",
    "diff_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/**\\n * Copyright (c) 2013-present, Facebook, Inc.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n *\\n * @providesModule ReactFiberBeginWork\\n * @flow\\n */\\n\\n\\'use strict\\';\\n\\nimport type {ReactCoroutine} from \\'ReactTypes\\';\\nimport type {Fiber} from \\'ReactFiber\\';\\nimport type {HostContext} from \\'ReactFiberHostContext\\';\\nimport type {HydrationContext} from \\'ReactFiberHydrationContext\\';\\nimport type {FiberRoot} from \\'ReactFiberRoot\\';\\nimport type {HostConfig} from \\'ReactFiberReconciler\\';\\nimport type {PriorityLevel} from \\'ReactPriorityLevel\\';\\n\\nvar {\\n  mountChildFibersInPlace,\\n  reconcileChildFibers,\\n  reconcileChildFibersInPlace,\\n  cloneChildFibers,\\n} = require(\\'ReactChildFiber\\');\\nvar {beginUpdateQueue} = require(\\'ReactFiberUpdateQueue\\');\\nvar ReactTypeOfWork = require(\\'ReactTypeOfWork\\');\\nvar {\\n  getMaskedContext,\\n  getUnmaskedContext,\\n  hasContextChanged,\\n  pushContextProvider,\\n  pushTopLevelContextObject,\\n  invalidateContextProvider,\\n} = require(\\'ReactFiberContext\\');\\nvar {\\n  IndeterminateComponent,\\n  FunctionalComponent,\\n  ClassComponent,\\n  HostRoot,\\n  HostComponent,\\n  HostText,\\n  HostPortal,\\n  CoroutineComponent,\\n  CoroutineHandlerPhase,\\n  YieldComponent,\\n  Fragment,\\n} = ReactTypeOfWork;\\nvar {NoWork, OffscreenPriority} = require(\\'ReactPriorityLevel\\');\\nvar {\\n  PerformedWork,\\n  Placement,\\n  ContentReset,\\n  Err,\\n  Ref,\\n} = require(\\'ReactTypeOfSideEffect\\');\\nvar ReactFiberClassComponent = require(\\'ReactFiberClassComponent\\');\\nvar {ReactCurrentOwner} = require(\\'ReactGlobalSharedState\\');\\nvar invariant = require(\\'fbjs/lib/invariant\\');\\n\\nif (__DEV__) {\\n  var ReactDebugCurrentFiber = require(\\'ReactDebugCurrentFiber\\');\\n  var {cancelWorkTimer} = require(\\'ReactDebugFiberPerf\\');\\n  var warning = require(\\'fbjs/lib/warning\\');\\n\\n  var warnedAboutStatelessRefs = {};\\n}\\n\\nmodule.exports = function<T, P, I, TI, PI, C, CX, PL>(\\n  config: HostConfig<T, P, I, TI, PI, C, CX, PL>,\\n  hostContext: HostContext<C, CX>,\\n  hydrationContext: HydrationContext<C, CX>,\\n  scheduleUpdate: (fiber: Fiber, priorityLevel: PriorityLevel) => void,\\n  getPriorityContext: (fiber: Fiber, forceAsync: boolean) => PriorityLevel,\\n) {\\n  const {\\n    shouldSetTextContent,\\n    useSyncScheduling,\\n    shouldDeprioritizeSubtree,\\n  } = config;\\n\\n  const {pushHostContext, pushHostContainer} = hostContext;\\n\\n  const {\\n    enterHydrationState,\\n    resetHydrationState,\\n    tryToClaimNextHydratableInstance,\\n  } = hydrationContext;\\n\\n  const {\\n    adoptClassInstance,\\n    constructClassInstance,\\n    mountClassInstance,\\n    // resumeMountClassInstance,\\n    updateClassInstance,\\n  } = ReactFiberClassComponent(\\n    scheduleUpdate,\\n    getPriorityContext,\\n    memoizeProps,\\n    memoizeState,\\n  );\\n\\n  function reconcileChildren(current, workInProgress, nextChildren) {\\n    const priorityLevel = workInProgress.pendingWorkPriority;\\n    reconcileChildrenAtPriority(\\n      current,\\n      workInProgress,\\n      nextChildren,\\n      priorityLevel,\\n    );\\n  }\\n\\n  function reconcileChildrenAtPriority(\\n    current,\\n    workInProgress,\\n    nextChildren,\\n    priorityLevel,\\n  ) {\\n    if (current === null) {\\n      // If this is a fresh new component that hasn\\'t been rendered yet, we\\n      // won\\'t update its child set by applying minimal side-effects. Instead,\\n      // we will add them all to the child before it gets rendered. That means\\n      // we can optimize this reconciliation pass by not tracking side-effects.\\n      workInProgress.child = mountChildFibersInPlace(\\n        workInProgress,\\n        workInProgress.child,\\n        nextChildren,\\n        priorityLevel,\\n      );\\n    } else if (current.child === workInProgress.child) {\\n      // If the current child is the same as the work in progress, it means that\\n      // we haven\\'t yet started any work on these children. Therefore, we use\\n      // the clone algorithm to create a copy of all the current children.\\n\\n      // If we had any progressed work already, that is invalid at this point so\\n      // let\\'s throw it out.\\n      workInProgress.child = reconcileChildFibers(\\n        workInProgress,\\n        workInProgress.child,\\n        nextChildren,\\n        priorityLevel,\\n      );\\n    } else {\\n      // If, on the other hand, it is already using a clone, that means we\\'ve\\n      // already begun some work on this tree and we can continue where we left\\n      // off by reconciling against the existing children.\\n      workInProgress.child = reconcileChildFibersInPlace(\\n        workInProgress,\\n        workInProgress.child,\\n        nextChildren,\\n        priorityLevel,\\n      );\\n    }\\n  }\\n\\n  function updateFragment(current, workInProgress) {\\n    var nextChildren = workInProgress.pendingProps;\\n    if (hasContextChanged()) {\\n      // Normally we can bail out on props equality but if context has changed\\n      // we don\\'t do the bailout and we have to reuse existing props instead.\\n      if (nextChildren === null) {\\n        nextChildren = workInProgress.memoizedProps;\\n      }\\n    } else if (\\n      nextChildren === null ||\\n      workInProgress.memoizedProps === nextChildren\\n    ) {\\n      return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n    }\\n    reconcileChildren(current, workInProgress, nextChildren);\\n    memoizeProps(workInProgress, nextChildren);\\n    return workInProgress.child;\\n  }\\n\\n  function markRef(current: Fiber | null, workInProgress: Fiber) {\\n    const ref = workInProgress.ref;\\n    if (ref !== null && (!current || current.ref !== ref)) {\\n      // Schedule a Ref effect\\n      workInProgress.effectTag |= Ref;\\n    }\\n  }\\n\\n  function updateFunctionalComponent(current, workInProgress) {\\n    var fn = workInProgress.type;\\n    var nextProps = workInProgress.pendingProps;\\n\\n    const memoizedProps = workInProgress.memoizedProps;\\n    if (hasContextChanged()) {\\n      // Normally we can bail out on props equality but if context has changed\\n      // we don\\'t do the bailout and we have to reuse existing props instead.\\n      if (nextProps === null) {\\n        nextProps = memoizedProps;\\n      }\\n    } else {\\n      if (nextProps === null || memoizedProps === nextProps) {\\n        return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n      }\\n      // TODO: consider bringing fn.shouldComponentUpdate() back.\\n      // It used to be here.\\n    }\\n\\n    var unmaskedContext = getUnmaskedContext(workInProgress);\\n    var context = getMaskedContext(workInProgress, unmaskedContext);\\n\\n    var nextChildren;\\n\\n    if (__DEV__) {\\n      ReactCurrentOwner.current = workInProgress;\\n      ReactDebugCurrentFiber.setCurrentPhase(\\'render\\');\\n      nextChildren = fn(nextProps, context);\\n      ReactDebugCurrentFiber.setCurrentPhase(null);\\n    } else {\\n      nextChildren = fn(nextProps, context);\\n    }\\n    // React DevTools reads this flag.\\n    workInProgress.effectTag |= PerformedWork;\\n    reconcileChildren(current, workInProgress, nextChildren);\\n    memoizeProps(workInProgress, nextProps);\\n    return workInProgress.child;\\n  }\\n\\n  function updateClassComponent(\\n    current: Fiber | null,\\n    workInProgress: Fiber,\\n    priorityLevel: PriorityLevel,\\n  ) {\\n    // Push context providers early to prevent context stack mismatches.\\n    // During mounting we don\\'t know the child context yet as the instance doesn\\'t exist.\\n    // We will invalidate the child context in finishClassComponent() right after rendering.\\n    const hasContext = pushContextProvider(workInProgress);\\n\\n    let shouldUpdate;\\n    if (current === null) {\\n      if (!workInProgress.stateNode) {\\n        // In the initial pass we might need to construct the instance.\\n        constructClassInstance(workInProgress, workInProgress.pendingProps);\\n        mountClassInstance(workInProgress, priorityLevel);\\n        shouldUpdate = true;\\n      } else {\\n        invariant(false, \\'Resuming work not yet implemented.\\');\\n        // In a resume, we\\'ll already have an instance we can reuse.\\n        // shouldUpdate = resumeMountClassInstance(workInProgress, priorityLevel);\\n      }\\n    } else {\\n      shouldUpdate = updateClassInstance(\\n        current,\\n        workInProgress,\\n        priorityLevel,\\n      );\\n    }\\n    return finishClassComponent(\\n      current,\\n      workInProgress,\\n      shouldUpdate,\\n      hasContext,\\n    );\\n  }\\n\\n  function finishClassComponent(\\n    current: Fiber | null,\\n    workInProgress: Fiber,\\n    shouldUpdate: boolean,\\n    hasContext: boolean,\\n  ) {\\n    // Refs should update even if shouldComponentUpdate returns false\\n    markRef(current, workInProgress);\\n\\n    if (!shouldUpdate) {\\n      // Context providers should defer to sCU for rendering\\n      if (hasContext) {\\n        invalidateContextProvider(workInProgress, false);\\n      }\\n\\n      return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n    }\\n\\n    const instance = workInProgress.stateNode;\\n\\n    // Rerender\\n    ReactCurrentOwner.current = workInProgress;\\n    let nextChildren;\\n    if (__DEV__) {\\n      ReactDebugCurrentFiber.setCurrentPhase(\\'render\\');\\n      nextChildren = instance.render();\\n      ReactDebugCurrentFiber.setCurrentPhase(null);\\n    } else {\\n      nextChildren = instance.render();\\n    }\\n    // React DevTools reads this flag.\\n    workInProgress.effectTag |= PerformedWork;\\n    reconcileChildren(current, workInProgress, nextChildren);\\n    // Memoize props and state using the values we just used to render.\\n    // TODO: Restructure so we never read values from the instance.\\n    memoizeState(workInProgress, instance.state);\\n    memoizeProps(workInProgress, instance.props);\\n\\n    // The context might have changed so we need to recalculate it.\\n    if (hasContext) {\\n      invalidateContextProvider(workInProgress, true);\\n    }\\n\\n    return workInProgress.child;\\n  }\\n\\n  function pushHostRootContext(workInProgress) {\\n    const root = (workInProgress.stateNode: FiberRoot);\\n    if (root.pendingContext) {\\n      pushTopLevelContextObject(\\n        workInProgress,\\n        root.pendingContext,\\n        root.pendingContext !== root.context,\\n      );\\n    } else if (root.context) {\\n      // Should always be set\\n      pushTopLevelContextObject(workInProgress, root.context, false);\\n    }\\n    pushHostContainer(workInProgress, root.containerInfo);\\n  }\\n\\n  function updateHostRoot(current, workInProgress, priorityLevel) {\\n    pushHostRootContext(workInProgress);\\n    const updateQueue = workInProgress.updateQueue;\\n    if (updateQueue !== null) {\\n      const prevState = workInProgress.memoizedState;\\n      const state = beginUpdateQueue(\\n        current,\\n        workInProgress,\\n        updateQueue,\\n        null,\\n        prevState,\\n        null,\\n        priorityLevel,\\n      );\\n      if (prevState === state) {\\n        // If the state is the same as before, that\\'s a bailout because we had\\n        // no work matching this priority.\\n        resetHydrationState();\\n        return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n      }\\n      const element = state.element;\\n      if (\\n        (current === null || current.child === null) &&\\n        enterHydrationState(workInProgress)\\n      ) {\\n        // If we don\\'t have any current children this might be the first pass.\\n        // We always try to hydrate. If this isn\\'t a hydration pass there won\\'t\\n        // be any children to hydrate which is effectively the same thing as\\n        // not hydrating.\\n\\n        // This is a bit of a hack. We track the host root as a placement to\\n        // know that we\\'re currently in a mounting state. That way isMounted\\n        // works as expected. We must reset this before committing.\\n        // TODO: Delete this when we delete isMounted and findDOMNode.\\n        workInProgress.effectTag |= Placement;\\n\\n        // Ensure that children mount into this root without tracking\\n        // side-effects. This ensures that we don\\'t store Placement effects on\\n        // nodes that will be hydrated.\\n        workInProgress.child = mountChildFibersInPlace(\\n          workInProgress,\\n          workInProgress.child,\\n          element,\\n          priorityLevel,\\n        );\\n      } else {\\n        // Otherwise reset hydration state in case we aborted and resumed another\\n        // root.\\n        resetHydrationState();\\n        reconcileChildren(current, workInProgress, element);\\n      }\\n      memoizeState(workInProgress, state);\\n      return workInProgress.child;\\n    }\\n    resetHydrationState();\\n    // If there is no update queue, that\\'s a bailout because the root has no props.\\n    return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n  }\\n\\n  function updateHostComponent(current, workInProgress, renderPriority) {\\n    pushHostContext(workInProgress);\\n\\n    if (current === null) {\\n      tryToClaimNextHydratableInstance(workInProgress);\\n    }\\n\\n    const type = workInProgress.type;\\n    const memoizedProps = workInProgress.memoizedProps;\\n    let nextProps = workInProgress.pendingProps;\\n    if (nextProps === null) {\\n      nextProps = memoizedProps;\\n      invariant(\\n        nextProps !== null,\\n        \\'We should always have pending or current props. This error is \\' +\\n          \\'likely caused by a bug in React. Please file an issue.\\',\\n      );\\n    }\\n    const prevProps = current !== null ? current.memoizedProps : null;\\n\\n    if (hasContextChanged()) {\\n      // Normally we can bail out on props equality but if context has changed\\n      // we don\\'t do the bailout and we have to reuse existing props instead.\\n    } else if (nextProps === null || memoizedProps === nextProps) {\\n      return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n    }\\n\\n    let nextChildren = nextProps.children;\\n    const isDirectTextChild = shouldSetTextContent(type, nextProps);\\n\\n    if (isDirectTextChild) {\\n      // We special case a direct text child of a host node. This is a common\\n      // case. We won\\'t handle it as a reified child. We will instead handle\\n      // this in the host environment that also have access to this prop. That\\n      // avoids allocating another HostText fiber and traversing it.\\n      nextChildren = null;\\n    } else if (prevProps && shouldSetTextContent(type, prevProps)) {\\n      // If we\\'re switching from a direct text child to a normal child, or to\\n      // empty, we need to schedule the text content to be reset.\\n      workInProgress.effectTag |= ContentReset;\\n    }\\n\\n    markRef(current, workInProgress);\\n\\n    // Check the host config to see if the children are offscreen/hidden.\\n    if (\\n      renderPriority !== OffscreenPriority &&\\n      !useSyncScheduling &&\\n      shouldDeprioritizeSubtree(type, nextProps)\\n    ) {\\n      // Down-prioritize the children.\\n      workInProgress.pendingWorkPriority = OffscreenPriority;\\n      // Bailout and come back to this fiber later at OffscreenPriority.\\n      return null;\\n    }\\n\\n    reconcileChildren(current, workInProgress, nextChildren);\\n    memoizeProps(workInProgress, nextProps);\\n    return workInProgress.child;\\n  }\\n\\n  function updateHostText(current, workInProgress) {\\n    if (current === null) {\\n      tryToClaimNextHydratableInstance(workInProgress);\\n    }\\n    let nextProps = workInProgress.pendingProps;\\n    if (nextProps === null) {\\n      nextProps = workInProgress.memoizedProps;\\n    }\\n    memoizeProps(workInProgress, nextProps);\\n    // Nothing to do here. This is terminal. We\\'ll do the completion step\\n    // immediately after.\\n    return null;\\n  }\\n\\n  function mountIndeterminateComponent(current, workInProgress, priorityLevel) {\\n    invariant(\\n      current === null,\\n      \\'An indeterminate component should never have mounted. This error is \\' +\\n        \\'likely caused by a bug in React. Please file an issue.\\',\\n    );\\n    var fn = workInProgress.type;\\n    var props = workInProgress.pendingProps;\\n    var unmaskedContext = getUnmaskedContext(workInProgress);\\n    var context = getMaskedContext(workInProgress, unmaskedContext);\\n\\n    var value;\\n\\n    if (__DEV__) {\\n      ReactCurrentOwner.current = workInProgress;\\n      value = fn(props, context);\\n    } else {\\n      value = fn(props, context);\\n    }\\n    // React DevTools reads this flag.\\n    workInProgress.effectTag |= PerformedWork;\\n\\n    if (\\n      typeof value === \\'object\\' &&\\n      value !== null &&\\n      typeof value.render === \\'function\\'\\n    ) {\\n      // Proceed under the assumption that this is a class instance\\n      workInProgress.tag = ClassComponent;\\n\\n      // Push context providers early to prevent context stack mismatches.\\n      // During mounting we don\\'t know the child context yet as the instance doesn\\'t exist.\\n      // We will invalidate the child context in finishClassComponent() right after rendering.\\n      const hasContext = pushContextProvider(workInProgress);\\n      adoptClassInstance(workInProgress, value);\\n      mountClassInstance(workInProgress, priorityLevel);\\n      return finishClassComponent(current, workInProgress, true, hasContext);\\n    } else {\\n      // Proceed under the assumption that this is a functional component\\n      workInProgress.tag = FunctionalComponent;\\n      if (__DEV__) {\\n        const Component = workInProgress.type;\\n\\n        if (Component) {\\n          warning(\\n            !Component.childContextTypes,\\n            \\'%s(...): childContextTypes cannot be defined on a functional component.\\',\\n            Component.displayName || Component.name || \\'Component\\',\\n          );\\n        }\\n        if (workInProgress.ref !== null) {\\n          let info = \\'\\';\\n          const ownerName = ReactDebugCurrentFiber.getCurrentFiberOwnerName();\\n          if (ownerName) {\\n            info += \\'\\\\n\\\\nCheck the render method of `\\' + ownerName + \\'`.\\';\\n          }\\n\\n          let warningKey = ownerName || workInProgress._debugID || \\'\\';\\n          const debugSource = workInProgress._debugSource;\\n          if (debugSource) {\\n            warningKey = debugSource.fileName + \\':\\' + debugSource.lineNumber;\\n          }\\n          if (!warnedAboutStatelessRefs[warningKey]) {\\n            warnedAboutStatelessRefs[warningKey] = true;\\n            warning(\\n              false,\\n              \\'Stateless function components cannot be given refs. \\' +\\n                \\'Attempts to access this ref will fail.%s%s\\',\\n              info,\\n              ReactDebugCurrentFiber.getCurrentFiberStackAddendum(),\\n            );\\n          }\\n        }\\n      }\\n      reconcileChildren(current, workInProgress, value);\\n      memoizeProps(workInProgress, props);\\n      return workInProgress.child;\\n    }\\n  }\\n\\n  function updateCoroutineComponent(current, workInProgress) {\\n    var nextCoroutine = (workInProgress.pendingProps: null | ReactCoroutine);\\n    if (hasContextChanged()) {\\n      // Normally we can bail out on props equality but if context has changed\\n      // we don\\'t do the bailout and we have to reuse existing props instead.\\n      if (nextCoroutine === null) {\\n        nextCoroutine = current && current.memoizedProps;\\n        invariant(\\n          nextCoroutine !== null,\\n          \\'We should always have pending or current props. This error is \\' +\\n            \\'likely caused by a bug in React. Please file an issue.\\',\\n        );\\n      }\\n    } else if (\\n      nextCoroutine === null ||\\n      workInProgress.memoizedProps === nextCoroutine\\n    ) {\\n      nextCoroutine = workInProgress.memoizedProps;\\n      // TODO: When bailing out, we might need to return the stateNode instead\\n      // of the child. To check it for work.\\n      // return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n    }\\n\\n    const nextChildren = nextCoroutine.children;\\n    const priorityLevel = workInProgress.pendingWorkPriority;\\n\\n    // The following is a fork of reconcileChildrenAtPriority but using\\n    // stateNode to store the child.\\n    if (current === null) {\\n      workInProgress.stateNode = mountChildFibersInPlace(\\n        workInProgress,\\n        workInProgress.stateNode,\\n        nextChildren,\\n        priorityLevel,\\n      );\\n    } else if (current.child === workInProgress.child) {\\n      workInProgress.stateNode = reconcileChildFibers(\\n        workInProgress,\\n        workInProgress.stateNode,\\n        nextChildren,\\n        priorityLevel,\\n      );\\n    } else {\\n      workInProgress.stateNode = reconcileChildFibersInPlace(\\n        workInProgress,\\n        workInProgress.stateNode,\\n        nextChildren,\\n        priorityLevel,\\n      );\\n    }\\n\\n    memoizeProps(workInProgress, nextCoroutine);\\n    // This doesn\\'t take arbitrary time so we could synchronously just begin\\n    // eagerly do the work of workInProgress.child as an optimization.\\n    return workInProgress.stateNode;\\n  }\\n\\n  function updatePortalComponent(current, workInProgress) {\\n    pushHostContainer(workInProgress, workInProgress.stateNode.containerInfo);\\n    const priorityLevel = workInProgress.pendingWorkPriority;\\n    let nextChildren = workInProgress.pendingProps;\\n    if (hasContextChanged()) {\\n      // Normally we can bail out on props equality but if context has changed\\n      // we don\\'t do the bailout and we have to reuse existing props instead.\\n      if (nextChildren === null) {\\n        nextChildren = current && current.memoizedProps;\\n        invariant(\\n          nextChildren != null,\\n          \\'We should always have pending or current props. This error is \\' +\\n            \\'likely caused by a bug in React. Please file an issue.\\',\\n        );\\n      }\\n    } else if (\\n      nextChildren === null ||\\n      workInProgress.memoizedProps === nextChildren\\n    ) {\\n      return bailoutOnAlreadyFinishedWork(current, workInProgress);\\n    }\\n\\n    if (current === null) {\\n      // Portals are special because we don\\'t append the children during mount\\n      // but at commit. Therefore we need to track insertions which the normal\\n      // flow doesn\\'t do during mount. This doesn\\'t happen at the root because\\n      // the root always starts with a \"current\" with a null child.\\n      // TODO: Consider unifying this with how the root works.\\n      workInProgress.child = reconcileChildFibersInPlace(\\n        workInProgress,\\n        workInProgress.child,\\n        nextChildren,\\n        priorityLevel,\\n      );\\n      memoizeProps(workInProgress, nextChildren);\\n    } else {\\n      reconcileChildren(current, workInProgress, nextChildren);\\n      memoizeProps(workInProgress, nextChildren);\\n    }\\n    return workInProgress.child;\\n  }\\n\\n  /*\\n  function reuseChildrenEffects(returnFiber : Fiber, firstChild : Fiber) {\\n    let child = firstChild;\\n    do {\\n      // Ensure that the first and last effect of the parent corresponds\\n      // to the children\\'s first and last effect.\\n      if (!returnFiber.firstEffect) {\\n        returnFiber.firstEffect = child.firstEffect;\\n      }\\n      if (child.lastEffect) {\\n        if (returnFiber.lastEffect) {\\n          returnFiber.lastEffect.nextEffect = child.firstEffect;\\n        }\\n        returnFiber.lastEffect = child.lastEffect;\\n      }\\n    } while (child = child.sibling);\\n  }\\n  */\\n\\n  function bailoutOnAlreadyFinishedWork(\\n    current,\\n    workInProgress: Fiber,\\n  ): Fiber | null {\\n    if (__DEV__) {\\n      cancelWorkTimer(workInProgress);\\n    }\\n\\n    // TODO: We should ideally be able to bail out early if the children have no\\n    // more work to do. However, since we don\\'t have a separation of this\\n    // Fiber\\'s priority and its children yet - we don\\'t know without doing lots\\n    // of the same work we do anyway. Once we have that separation we can just\\n    // bail out here if the children has no more work at this priority level.\\n    // if (workInProgress.priorityOfChildren <= priorityLevel) {\\n    //   // If there are side-effects in these children that have not yet been\\n    //   // committed we need to ensure that they get properly transferred up.\\n    //   if (current && current.child !== workInProgress.child) {\\n    //     reuseChildrenEffects(workInProgress, child);\\n    //   }\\n    //   return null;\\n    // }\\n\\n    cloneChildFibers(current, workInProgress);\\n    return workInProgress.child;\\n  }\\n\\n  function bailoutOnLowPriority(current, workInProgress) {\\n    if (__DEV__) {\\n      cancelWorkTimer(workInProgress);\\n    }\\n\\n    // TODO: Handle HostComponent tags here as well and call pushHostContext()?\\n    // See PR 8590 discussion for context\\n    switch (workInProgress.tag) {\\n      case HostRoot:\\n        pushHostRootContext(workInProgress);\\n        break;\\n      case ClassComponent:\\n        pushContextProvider(workInProgress);\\n        break;\\n      case HostPortal:\\n        pushHostContainer(\\n          workInProgress,\\n          workInProgress.stateNode.containerInfo,\\n        );\\n        break;\\n    }\\n    // TODO: What if this is currently in progress?\\n    // How can that happen? How is this not being cloned?\\n    return null;\\n  }\\n\\n  // TODO: Delete memoizeProps/State and move to reconcile/bailout instead\\n  function memoizeProps(workInProgress: Fiber, nextProps: any) {\\n    workInProgress.memoizedProps = nextProps;\\n  }\\n\\n  function memoizeState(workInProgress: Fiber, nextState: any) {\\n    workInProgress.memoizedState = nextState;\\n    // Don\\'t reset the updateQueue, in case there are pending updates. Resetting\\n    // is handled by beginUpdateQueue.\\n  }\\n\\n  function beginWork(\\n    current: Fiber | null,\\n    workInProgress: Fiber,\\n    priorityLevel: PriorityLevel,\\n  ): Fiber | null {\\n    if (\\n      workInProgress.pendingWorkPriority === NoWork ||\\n      workInProgress.pendingWorkPriority > priorityLevel\\n    ) {\\n      return bailoutOnLowPriority(current, workInProgress);\\n    }\\n\\n    switch (workInProgress.tag) {\\n      case IndeterminateComponent:\\n        return mountIndeterminateComponent(\\n          current,\\n          workInProgress,\\n          priorityLevel,\\n        );\\n      case FunctionalComponent:\\n        return updateFunctionalComponent(current, workInProgress);\\n      case ClassComponent:\\n        return updateClassComponent(current, workInProgress, priorityLevel);\\n      case HostRoot:\\n        return updateHostRoot(current, workInProgress, priorityLevel);\\n      case HostComponent:\\n        return updateHostComponent(current, workInProgress, priorityLevel);\\n      case HostText:\\n        return updateHostText(current, workInProgress);\\n      case CoroutineHandlerPhase:\\n        // This is a restart. Reset the tag to the initial phase.\\n        workInProgress.tag = CoroutineComponent;\\n      // Intentionally fall through since this is now the same.\\n      case CoroutineComponent:\\n        return updateCoroutineComponent(current, workInProgress);\\n      case YieldComponent:\\n        // A yield component is just a placeholder, we can just run through the\\n        // next one immediately.\\n        return null;\\n      case HostPortal:\\n        return updatePortalComponent(current, workInProgress);\\n      case Fragment:\\n        return updateFragment(current, workInProgress);\\n      default:\\n        invariant(\\n          false,\\n          \\'Unknown unit of work tag. This error is likely caused by a bug in \\' +\\n            \\'React. Please file an issue.\\',\\n        );\\n    }\\n  }\\n\\n  function beginFailedWork(\\n    current: Fiber | null,\\n    workInProgress: Fiber,\\n    priorityLevel: PriorityLevel,\\n  ) {\\n    // Push context providers here to avoid a push/pop context mismatch.\\n    switch (workInProgress.tag) {\\n      case ClassComponent:\\n        pushContextProvider(workInProgress);\\n        break;\\n      case HostRoot:\\n        pushHostRootContext(workInProgress);\\n        break;\\n      default:\\n        invariant(\\n          false,\\n          \\'Invalid type of work. This error is likely caused by a bug in React. \\' +\\n            \\'Please file an issue.\\',\\n        );\\n    }\\n\\n    // Add an error effect so we can handle the error during the commit phase\\n    workInProgress.effectTag |= Err;\\n\\n    // This is a weird case where we do \"resume\" work — work that failed on\\n    // our first attempt. Because we no longer have a notion of \"progressed\\n    // deletions,\" reset the child to the current child to make sure we delete\\n    // it again. TODO: Find a better way to handle this, perhaps during a more\\n    // general overhaul of error handling.\\n    if (current === null) {\\n      workInProgress.child = null;\\n    } else if (workInProgress.child !== current.child) {\\n      workInProgress.child = current.child;\\n    }\\n\\n    if (\\n      workInProgress.pendingWorkPriority === NoWork ||\\n      workInProgress.pendingWorkPriority > priorityLevel\\n    ) {\\n      return bailoutOnLowPriority(current, workInProgress);\\n    }\\n\\n    // If we don\\'t bail out, we\\'re going be recomputing our children so we need\\n    // to drop our effect list.\\n    workInProgress.firstEffect = null;\\n    workInProgress.lastEffect = null;\\n\\n    // Unmount the current children as if the component rendered null\\n    const nextChildren = null;\\n    reconcileChildrenAtPriority(\\n      current,\\n      workInProgress,\\n      nextChildren,\\n      priorityLevel,\\n    );\\n\\n    if (workInProgress.tag === ClassComponent) {\\n      const instance = workInProgress.stateNode;\\n      workInProgress.memoizedProps = instance.props;\\n      workInProgress.memoizedState = instance.state;\\n    }\\n\\n    return workInProgress.child;\\n  }\\n\\n  return {\\n    beginWork,\\n    beginFailedWork,\\n  };\\n};\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_data.iloc[0]['passage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 25, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1500, 'bm25_aggr_strategy': 'sump', 'psg_len': 250, 'psg_stride': 200}\n"
     ]
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(params)\n",
    "save_model_name = params['model_name'].replace('/', '_')\n",
    "# repo_name = 'facebook_react'\n",
    "bert_best_model_path = os.path.join(args.repo_path, 'models', f\"{save_model_name}_model_output\", 'best_model')\n",
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained(bert_best_model_path)\n",
    "bert_reranker.model.to(bert_reranker.device)\n",
    "rerankers = [bert_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>query</th>\n",
       "      <th>passage</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83a536e62264b7baf0bb5d83835821e6f4c45006</td>\n",
       "      <td>src/renderers/shared/fiber/ReactFiberBeginWork.js</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d289d4b634749861199556e42174a3f4a3ce2b16</td>\n",
       "      <td>packages/react-dom/src/__tests__/ReactDOMCompo...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83a536e62264b7baf0bb5d83835821e6f4c45006</td>\n",
       "      <td>src/renderers/shared/fiber/ReactFiberReconcile...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>566259567773a0af56e6c19da615e6802d73f834</td>\n",
       "      <td>packages/react-dom/src/client/ReactDOMFiberCom...</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright (c) 2013-present, Facebook, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1dbc8e4708d1de49d8e6a22e9682987d08a15c7</td>\n",
       "      <td>src/renderers/dom/fiber/ReactDOMFiberComponent.js</td>\n",
       "      <td>Change warning() to automatically inject the s...</td>\n",
       "      <td>/**\\n * Copyright 2013-present, Facebook, Inc....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  commit_id  \\\n",
       "0  83a536e62264b7baf0bb5d83835821e6f4c45006   \n",
       "1  d289d4b634749861199556e42174a3f4a3ce2b16   \n",
       "2  83a536e62264b7baf0bb5d83835821e6f4c45006   \n",
       "3  566259567773a0af56e6c19da615e6802d73f834   \n",
       "4  c1dbc8e4708d1de49d8e6a22e9682987d08a15c7   \n",
       "\n",
       "                                           file_path  \\\n",
       "0  src/renderers/shared/fiber/ReactFiberBeginWork.js   \n",
       "1  packages/react-dom/src/__tests__/ReactDOMCompo...   \n",
       "2  src/renderers/shared/fiber/ReactFiberReconcile...   \n",
       "3  packages/react-dom/src/client/ReactDOMFiberCom...   \n",
       "4  src/renderers/dom/fiber/ReactDOMFiberComponent.js   \n",
       "\n",
       "                                               query  \\\n",
       "0  Change warning() to automatically inject the s...   \n",
       "1  Change warning() to automatically inject the s...   \n",
       "2  Change warning() to automatically inject the s...   \n",
       "3  Change warning() to automatically inject the s...   \n",
       "4  Change warning() to automatically inject the s...   \n",
       "\n",
       "                                             passage  label  \n",
       "0  /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "1  /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "2  /**\\n * Copyright (c) 2013-present, Facebook, ...      0  \n",
       "3  /**\\n * Copyright (c) 2013-present, Facebook, ...      1  \n",
       "4  /**\\n * Copyright 2013-present, Facebook, Inc....      0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCodeReranker:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.model_name = parameters['model_name']\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f'Using device: {self.device}')\n",
    "\n",
    "        # print GPU info\n",
    "        if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "            print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "        self.psg_len = parameters['psg_len']\n",
    "        self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "        self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "        self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "        self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "        self.rerank_depth = parameters['rerank_depth']\n",
    "        self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "        print(f\"Initialized Code File BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "    def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "        \"\"\"\n",
    "        Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "        query: The issue query string.\n",
    "        aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "        \"\"\"\n",
    "        # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "        # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        query_passage_pairs, per_result_contribution = self.split_into_query_passage_pairs(query, aggregated_results)\n",
    "\n",
    "\n",
    "        # for agg_result in aggregated_results:\n",
    "        #     query_passage_pairs.extend(\n",
    "        #         (query, result.commit_message)\n",
    "        #         for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "        #     )\n",
    "\n",
    "        if not query_passage_pairs:\n",
    "            print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "            print(query, aggregated_results, self.psg_cnt)\n",
    "            return aggregated_results\n",
    "\n",
    "        # tokenize the query passage pairs\n",
    "        encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "        # create tensors for the input ids, attention masks\n",
    "        input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "        attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "        # Create a dataloader for feeding the data to the model\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "        scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "        score_index = 0\n",
    "        # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "        for i, agg_result in enumerate(aggregated_results):\n",
    "            # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "            assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "            end_index = score_index + per_result_contribution[i] # only use psg_cnt contributing_results\n",
    "            cur_passage_scores = scores[score_index:end_index]\n",
    "            score_index = end_index\n",
    "\n",
    "\n",
    "            # Aggregate the scores for the current aggregated result\n",
    "            agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "            agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "        assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "        # Sort by the new aggregated score\n",
    "        aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "    def get_scores(self, dataloader, model):\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Unpack the batch and move it to GPU\n",
    "                b_input_ids, b_attention_mask = batch\n",
    "                b_input_ids = b_input_ids.to(self.device)\n",
    "                b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "                # Get scores from the model\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "                scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "        return scores\n",
    "\n",
    "    def aggregate_scores(self, passage_scores):\n",
    "        \"\"\"\n",
    "        Aggregate passage scores based on the specified strategy.\n",
    "        \"\"\"\n",
    "        if len(passage_scores) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.aggregation_strategy == 'firstp':\n",
    "            return passage_scores[0]\n",
    "        if self.aggregation_strategy == 'maxp':\n",
    "            return max(passage_scores)\n",
    "        if self.aggregation_strategy == 'avgp':\n",
    "            return sum(passage_scores) / len(passage_scores)\n",
    "        if self.aggregation_strategy == 'sump':\n",
    "            return sum(passage_scores)\n",
    "        # else:\n",
    "        raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "\n",
    "    def split_into_query_passage_pairs(self, query, aggregated_results):\n",
    "        # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "        def full_tokenize(s):\n",
    "            return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "        query_passage_pairs = []\n",
    "        per_result_contribution = []\n",
    "        for agg_result in aggregated_results:\n",
    "            agg_result.contributing_results.sort(key=lambda res: res.commit_date, reverse=True)\n",
    "            # get most recent file version\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            # get the file_path and commit_id\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            commit_id = most_recent_search_result.commit_id\n",
    "            # get the file content from combined_df\n",
    "            file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['cur_file_content'].values[0]\n",
    "\n",
    "            # assert file_content is not None, f'file_content is None for commit_id: {commit_id} and file_path: {file_path}'\n",
    "\n",
    "            # now need to split this file content into psg_cnt passages\n",
    "            # first tokenize the file content\n",
    "            # check if file_content is pd.NA\n",
    "            query_tokens = full_tokenize(query)\n",
    "            path_tokens = full_tokenize(file_path)\n",
    "            if pd.isna(file_content):\n",
    "                # query_passage_pairs.extend((query, file_path))\n",
    "                # per_result_contribution.append(1)\n",
    "                # continue\n",
    "                file_content = ''\n",
    "            file_tokens = full_tokenize(file_content)\n",
    "\n",
    "\n",
    "\n",
    "            # now split the file content into psg_cnt passages\n",
    "            cur_result_passages = []\n",
    "            # get the input ids\n",
    "            # input_ids = file_content['input_ids'].squeeze()\n",
    "            # get the number of tokens in the file content\n",
    "            total_tokens = len(file_tokens)\n",
    "\n",
    "            for cur_start in range(0, total_tokens, self.psg_stride):\n",
    "                cur_passage = []\n",
    "                # add query tokens and path tokens\n",
    "                cur_passage.extend(query_tokens)\n",
    "                cur_passage.extend(path_tokens)\n",
    "\n",
    "                # add the file tokens\n",
    "                cur_passage.extend(file_tokens[cur_start:cur_start+self.psg_len])\n",
    "\n",
    "                # now convert cur_passage into a string\n",
    "                cur_passage_decoded = self.tokenizer.decode(cur_passage)\n",
    "\n",
    "                # add the cur_passage to cur_result_passages\n",
    "                cur_result_passages.append(cur_passage_decoded)\n",
    "\n",
    "                if len(cur_result_passages) == self.psg_cnt:\n",
    "                    break\n",
    "\n",
    "            # now add the query, passage pairs to query_passage_pairs\n",
    "            per_result_contribution.append(len(cur_result_passages))\n",
    "            query_passage_pairs.extend((query, passage) for passage in cur_result_passages)\n",
    "        return query_passage_pairs, per_result_contribution\n",
    "\n",
    "    def rerank_pipeline(self, query, aggregated_results):\n",
    "        if len(aggregated_results) == 0:\n",
    "            return aggregated_results\n",
    "        top_results = aggregated_results[:self.rerank_depth]\n",
    "        bottom_results = aggregated_results[self.rerank_depth:]\n",
    "        reranked_results = self.rerank(query, top_results)\n",
    "        min_top_score = reranked_results[-1].score\n",
    "        # now adjust the scores of bottom_results\n",
    "        for i, result in enumerate(bottom_results):\n",
    "            result.score = min_top_score - i - 1\n",
    "        # combine the results\n",
    "        reranked_results.extend(bottom_results)\n",
    "        assert(len(reranked_results) == len(aggregated_results))\n",
    "        return reranked_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 1437.94 MB\n",
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 25, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1500, 'bm25_aggr_strategy': 'sump', 'psg_len': 250, 'psg_stride': 200}\n"
     ]
    }
   ],
   "source": [
    "code_reranker = BERTCodeReranker(params)\n",
    "# code_reranker.rerank_depth = 100\n",
    "# rerankers = [bert_reranker, code_reranker]\n",
    "rerankers = [code_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.1121, 'P@10': 0.2, 'P@100': 0.03, 'P@1000': 0.007, 'MRR': 0.3333, 'Recall@100': 0.2308, 'Recall@1000': 0.5385}\n",
      "Code BERT Reranker Evaluation\n",
      "{'MAP': 0.0551, 'P@10': 0.1, 'P@100': 0.03, 'P@1000': 0.007, 'MRR': 0.1667, 'Recall@100': 0.2308, 'Recall@1000': 0.5385}\n"
     ]
    }
   ],
   "source": [
    "def test1():\n",
    "    # get a random query and aggregated_results\n",
    "    # query = 'How to use React Hooks?'\n",
    "    random_row = recent_df.sample(1, random_state=52).iloc[0]\n",
    "    query = random_row['commit_message']\n",
    "    query_date = random_row['commit_date']\n",
    "    actual_files_modified = random_row['actual_files_modified']\n",
    "    aggregated_results = bm25_searcher.pipeline(query, query_date, 1000, 'sump')\n",
    "    # evaluate the results\n",
    "    print(\"BM25 Baseline Evaluation\")\n",
    "    print(evaluator.evaluate(aggregated_results, actual_files_modified))\n",
    "\n",
    "    reranked_results = code_reranker.rerank_pipeline(query, aggregated_results)\n",
    "    print(\"Code BERT Reranker Evaluation\")\n",
    "    print(evaluator.evaluate(reranked_results, actual_files_modified))\n",
    "\n",
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_triplets(diff_data):\n",
    "    # given diff_data, the passage column is way too long. We need to split it into passages of length psg_len with stride psg_stride\n",
    "    # then we can create triplets from that\n",
    "\n",
    "    # diff_data has columns: commit_id, file_path, query, passage, label\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(diff_data.iterrows(), total=len(diff_data)):\n",
    "        # get the input ids\n",
    "        # input_ids = file_content['input_ids'].squeeze()\n",
    "        # get the number of tokens in the file content\n",
    "        file_tokens = full_tokenize(row['passage'])\n",
    "        # query_tokens = full_tokenize(row['query'])\n",
    "        # path_tokens = full_tokenize(row['file_path'])\n",
    "        total_tokens = len(file_tokens)\n",
    "\n",
    "        cur_psg_cnt = 0\n",
    "        for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "            cur_passage = []\n",
    "            # add query tokens and path tokens\n",
    "            # cur_passage.extend(query_tokens)\n",
    "            # cur_passage.extend(path_tokens)\n",
    "\n",
    "            # add the file tokens\n",
    "            cur_passage.extend(file_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "\n",
    "            # now convert cur_passage into a string\n",
    "            cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "\n",
    "            # add the cur_passage to cur_result_passages\n",
    "            triplets.append((row['query'], row['file_path'], cur_passage_decoded, row['passage'], row['label']))\n",
    "\n",
    "            cur_psg_cnt += 1\n",
    "\n",
    "            if cur_psg_cnt == code_reranker.psg_cnt:\n",
    "                break\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    triplets = pd.DataFrame(triplets, columns=['query', 'file_path', 'passage', 'full_passage', 'label'])\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20753 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10203 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 20753/20753 [28:52<00:00, 11.98it/s]\n"
     ]
    }
   ],
   "source": [
    "triplets = prepare_triplets(diff_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.06831764802337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deep memory usage\n",
    "\n",
    "triplets.memory_usage(deep=True).sum()/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save triplets to cache\n",
    "triplets.to_parquet(os.path.join(repo_path, 'cache', 'code_triplets.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load triplets from cache\n",
    "triplets = pd.read_parquet(os.path.join(repo_path, 'cache', 'code_triplets.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = triplets.sample(100000, random_state=42)\n",
    "# drop column called full_passage\n",
    "triplets.drop(columns=['full_passage'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, 666907 to 359581\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   query      100000 non-null  object\n",
      " 1   file_path  100000 non-null  object\n",
      " 2   passage    100000 non-null  object\n",
      " 3   label      100000 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "triplets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in query: 237.96949838577555\n",
      "Average number of tokens in path: 18.898038837758396\n",
      "Avg number of tokens remaining for code: 255.13246277646607\n",
      "Average number of code tokens in cur_file_content column: 17368.505324531394\n"
     ]
    }
   ],
   "source": [
    "def aside2():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "    def tokenize(x):\n",
    "        # tokenize with no max length\n",
    "        return tokenizer.encode(x, add_special_tokens=False, truncation=False, max_length=None)\n",
    "    # print the average number of words in commit_message column\n",
    "\n",
    "    # sample 100 rows from combined_df\n",
    "    # sample_df = combined_df.sample(100, random_state=52)\n",
    "    # sample_df = combined_df[:10000]\n",
    "    sample_df = diff_data\n",
    "    avg_words = sample_df['query'].apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of tokens in query: {avg_words}')\n",
    "\n",
    "    avg_path = sample_df['file_path'].apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of tokens in path: {avg_path}')\n",
    "\n",
    "\n",
    "    # print remaining number of tokens in bert (max is 512)\n",
    "    print(f'Avg number of tokens remaining for code: {512 - avg_words - avg_path}')\n",
    "\n",
    "    avg_file_tokens = sample_df['passage'].dropna().apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of code tokens in cur_file_content column: {avg_file_tokens}')\n",
    "\n",
    "aside2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1859, 'P@10': 0.08, 'P@100': 0.028, 'P@1000': 0.0043, 'MRR': 0.2331, 'Recall@100': 0.4443, 'Recall@1000': 0.5752}\n",
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1676, 'P@10': 0.08, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.2541, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n",
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:36<00:00,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1038, 'P@10': 0.07, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.1732, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=None, aggregation_strategy=params['aggregation_strategy']))\n",
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=[bert_reranker], aggregation_strategy=params['aggregation_strategy']))\n",
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:49<00:00, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.2113, 'P@10': 0.12, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.3317, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:34<00:00, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1807, 'P@10': 0.117, 'P@100': 0.0295, 'P@1000': 0.0041, 'MRR': 0.2545, 'Recall@100': 0.527, 'Recall@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=100, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('microsoft_codebert-base',\n",
       " '2_7/apache_kafka/models/code_microsoft_codebert-base_model_output')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model_name = params['model_name'].replace('/', '_')\n",
    "# hf_output_dir = os.path.join('smalldata', 'ftr', f'100k_code_{save_model_name}_model_output')\n",
    "hf_output_dir = os.path.join(repo_path, 'models', f'code_{save_model_name}_model_output')\n",
    "save_model_name, hf_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(triplet_data, reranker, hf_output_dir, args):\n",
    "    def tokenize_hf(example):\n",
    "        len(example)\n",
    "        return reranker.tokenizer(example['query'], example['passage'], truncation=True, padding='max_length', max_length=reranker.max_seq_length, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "\n",
    "    # triplet_data = triplet_data.sample(1000, random_state=42)\n",
    "    print('Training the model...')\n",
    "    print('Label distribution:')\n",
    "    print(triplet_data['label'].value_counts())\n",
    "\n",
    "    # merge columns file_path and passage into one column called passage\n",
    "    triplet_data['passage'] = triplet_data['file_path'] + ' ' + triplet_data['passage']\n",
    "\n",
    "    # if args.sanity_check:\n",
    "    #     print('Running sanity check on training data...')\n",
    "    #     triplet_data = sanity_check(triplet_data)\n",
    "    # Step 7: convert triplet_data to HuggingFace Dataset\n",
    "    # convert triplet_data to HuggingFace Dataset\n",
    "    triplet_data['label'] = triplet_data['label'].astype(float)\n",
    "    train_df, val_df = train_test_split(triplet_data, test_size=0.2, random_state=42, stratify=triplet_data['label'])\n",
    "    train_hf_dataset = HFDataset.from_pandas(train_df, split='train') # type: ignore\n",
    "    val_hf_dataset = HFDataset.from_pandas(val_df, split='validation') # type: ignore\n",
    "    # Step 8: tokenize the data\n",
    "    tokenized_train_dataset = train_hf_dataset.map(tokenize_hf, batched=True)\n",
    "    tokenized_val_dataset = val_hf_dataset.map(tokenize_hf, batched=True)\n",
    "\n",
    "    # Step 9: set format for pytorch\n",
    "    tokenized_train_dataset = tokenized_train_dataset.remove_columns(['query', 'passage', 'file_path'])\n",
    "    tokenized_val_dataset = tokenized_val_dataset.remove_columns(['query', 'passage', 'file_path'])\n",
    "\n",
    "    # rename label column to labels\n",
    "    tokenized_train_dataset = tokenized_train_dataset.rename_column('label', 'labels')\n",
    "    tokenized_val_dataset = tokenized_val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "    # set format to pytorch\n",
    "    tokenized_train_dataset = tokenized_train_dataset.with_format('torch')\n",
    "    tokenized_val_dataset = tokenized_val_dataset.with_format('torch')\n",
    "    print('Training dataset features:')\n",
    "    print(tokenized_train_dataset.features)\n",
    "\n",
    "    # Step 10: set up training arguments\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=hf_output_dir,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        logging_steps=100,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=args.num_workers,\n",
    "        )\n",
    "\n",
    "    # small_train_dataset = tokenized_train_dataset.shuffle(seed=42).select(range(100))\n",
    "    # small_val_dataset = tokenized_val_dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "    # if args.debug:\n",
    "    #     print('Running in debug mode, using small datasets')\n",
    "    #     tokenized_train_dataset = small_train_dataset\n",
    "    #     tokenized_val_dataset = small_val_dataset\n",
    "\n",
    "    # Step 11: set up trainer\n",
    "    trainer = Trainer(\n",
    "        model = reranker.model,\n",
    "        args = train_args,\n",
    "        train_dataset = tokenized_train_dataset, # type: ignore\n",
    "        eval_dataset = tokenized_val_dataset, # type: ignore\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Step 12: train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Step 13: save the model\n",
    "    best_model_path = os.path.join(hf_output_dir, 'best_model')\n",
    "    trainer.save_model(best_model_path)\n",
    "    print(f'Saved model to {best_model_path}')\n",
    "    print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.sanity_check = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Label distribution:\n",
      "label\n",
      "0.0    76703\n",
      "1.0    23297\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80000/80000 [00:41<00:00, 1934.68 examples/s]\n",
      "Map: 100%|██████████| 20000/20000 [00:10<00:00, 1918.61 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset features:\n",
      "{'labels': Value(dtype='float64', id=None), '__index_level_0__': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8499' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8499/25000 1:05:45 < 2:07:41, 2.15 it/s, Epoch 3.40/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.064053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.034442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.023169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m do_training(triplets, code_reranker, hf_output_dir, args)\n",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb Cell 51\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     model \u001b[39m=\u001b[39m reranker\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     args \u001b[39m=\u001b[39m train_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Step 12: train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# Step 13: save the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m best_model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(hf_output_dir, \u001b[39m'\u001b[39m\u001b[39mbest_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:1894\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_training(triplets, code_reranker, hf_output_dir, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker.model = AutoModelForSequenceClassification.from_pretrained(os.path.join(repo_path, 'models', f\"code_{save_model_name}_model_output\", 'best_model'))\n",
    "code_reranker.psg_cnt = 25\n",
    "code_reranker.model.to(code_reranker.device)\n",
    "rerankers = [bert_reranker, code_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (98775 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100/100 [1:15:51<00:00, 45.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.2703,\n",
       " 'P@10': 0.119,\n",
       " 'P@100': 0.034,\n",
       " 'P@1000': 0.0053,\n",
       " 'MRR': 0.4074,\n",
       " 'Recall@100': 0.5517,\n",
       " 'Recall@1000': 0.7426}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], rerankers=rerankers, overwrite_eval=args.overwrite_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are several issues that need to be addressed to enhance the user experience:\\n\\n1. The modals currently do not have a shadow, affecting visibility and overall aesthetic of the UI.\\n2. The default setting of \"collapse new nodes\" option is currently enabled, which may not be the most user-friendly approach.\\n3. The label \"Collapse newly added components by default\" may confuse users, a clearer phrasing would help understanding.\\n4. The CSS media query for the settings popup is currently not optimized for smaller sizes, resulting in labels being hidden.\\n5. The \"Inspect the matching DOM element\" button is present in standalone mode, despite not serving any functional purpose.\\n6. There is a size issue with the settings icon, it\\'s currently at 20x20 viewbox instead of the intended 24x24.\\n7. There is a bug where \"window.addEventListener\" and \"window.removeEventListener\" are not defined in Hermes, causing operation failure.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.iloc[0]['transformed_message_gpt3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt4\n",
      "Found gold data for facebook_react with shape (100, 5) at gold/facebook_react/facebook_react_gpt4_gold.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    object\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    object\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gold_dir = os.path.join('gold', 'facebook_react')\n",
    "gold_data_path = os.path.join(gold_dir, f'{repo_name}_{args.openai_model}_gold.csv')\n",
    "print(f'Model: {args.openai_model}')\n",
    "gold_df = pd.read_csv(gold_data_path)\n",
    "assert gold_df[f'transformed_message_{args.openai_model}'].notnull().all()\n",
    "# rename commit_message to original_message\n",
    "gold_df = gold_df.rename(columns={'commit_message': 'original_message'})\n",
    "# rename transformed_message to commit_message\n",
    "gold_df = gold_df.rename(columns={f'transformed_message_{args.openai_model}': 'commit_message'})\n",
    "print(f'Found gold data for {repo_name} with shape {gold_df.shape} at {gold_data_path}')\n",
    "print(gold_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BM25 on gold data...\n",
      "WARNING: Output file path not provided, not writing results to file\n",
      "Found gold_df, evaluating on 100 commits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    object\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    object\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Gold Evaluation\n",
      "{'MAP': 0.1223, 'P@10': 0.055, 'P@100': 0.0186, 'P@1000': 0.0026, 'MRR': 0.1908, 'Recall@100': 0.3617, 'Recall@1000': 0.5439}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Running BM25 on gold data...')\n",
    "# bm25_gold_output_path = os.path.join(eval_path, f'bm25_v2_{args.openai_model}_gold_metrics.txt')\n",
    "bm25_gold_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['bm25_aggr_strategy'], gold_df=gold_df, overwrite_eval=args.overwrite_eval)\n",
    "print(\"BM25 Gold Evaluation\")\n",
    "print(bm25_gold_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<BERTReranker_v4.BERTReranker at 0x7ef810f3bc40>,\n",
       " <__main__.BERTCodeReranker at 0x7ef93dff1f40>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERT on gold data...\n",
      "WARNING: Output file path not provided, not writing results to file\n",
      "Found gold_df, evaluating on 100 commits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    object\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    object\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2127 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100/100 [52:50<00:00, 31.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Gold Evaluation\n",
      "{'MAP': 0.1821, 'P@10': 0.085, 'P@100': 0.0186, 'P@1000': 0.0026, 'MRR': 0.2484, 'Recall@100': 0.3617, 'Recall@1000': 0.5439}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Running BERT on gold data...')\n",
    "# bert_gold_output_path = os.path.join(eval_path, f'bert_v2_{args.openai_model}_gold.txt')\n",
    "bert_gold_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], rerankers=rerankers, gold_df=gold_df, overwrite_eval=args.overwrite_eval)\n",
    "\n",
    "print(\"BERT Gold Evaluation\")\n",
    "print(bert_gold_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.062 MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_data['commit_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commit ids in diff_data: 831\n",
      "Number of commit ids in gold_df: 100\n",
      "Number of commit ids in both diff_data and gold_df: 33\n"
     ]
    }
   ],
   "source": [
    "# find intersection of commit ids between diff_data and gold_df\n",
    "diff_commit_ids = set(diff_data['commit_id'].unique())\n",
    "gold_commit_ids = set(gold_df['commit_id'].unique())\n",
    "\n",
    "print(f\"Number of commit ids in diff_data: {len(diff_commit_ids)}\")\n",
    "print(f\"Number of commit ids in gold_df: {len(gold_commit_ids)}\")\n",
    "\n",
    "print(f\"Number of commit ids in both diff_data and gold_df: {len(diff_commit_ids.intersection(gold_commit_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   commit_id                 100 non-null    string\n",
      " 1   commit_date               100 non-null    int64 \n",
      " 2   commit_message            100 non-null    string\n",
      " 3   actual_files_modified     100 non-null    object\n",
      " 4   transformed_message_gpt4  100 non-null    object\n",
      "dtypes: int64(1), object(2), string(2)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def fix_old_parquet():\n",
    "    repo_name = 'angular_angular'\n",
    "    repo_path = os.path.join('gold', repo_name)\n",
    "    # csv_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.csv')\n",
    "    parquet_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.parquet')\n",
    "\n",
    "    # if there is a column called transformed_message_gpt3, then we need to fix the parquet file by renaming it to transformed_message_gpt4\n",
    "\n",
    "    parquet_df = pd.read_parquet(parquet_file)\n",
    "    print(parquet_df.info())\n",
    "\n",
    "    if 'transformed_message_gpt3' in parquet_df.columns:\n",
    "        print('Found transformed_message_gpt3 column in parquet file')\n",
    "        # rename it to transformed_message_gpt4\n",
    "        parquet_df = parquet_df.rename(columns={'transformed_message_gpt3': 'transformed_message_gpt4'})\n",
    "        print(parquet_df.info())\n",
    "        # now save it back to the parquet file\n",
    "        # parquet_df.to_parquet(parquet_file)\n",
    "        print('Saved parquet file')\n",
    "\n",
    "#     v2_csv_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.csv')\n",
    "#     v2_parquet_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.parquet')\n",
    "\n",
    "#     csv_df = pd.read_csv(csv_file)\n",
    "#     parquet_df = pd.read_parquet(parquet_file)\n",
    "\n",
    "#     v2_csv_df = pd.read_csv(v2_csv_file)\n",
    "#     v2_parquet_df = pd.read_parquet(v2_parquet_file)\n",
    "\n",
    "#     # ensure commit ids in both csv and parquet are the same\n",
    "#     csv_commit_ids = set(csv_df['commit_id'].unique())\n",
    "#     parquet_commit_ids = set(parquet_df['commit_id'].unique())\n",
    "\n",
    "#     print(f'Common commit ids: {len(csv_commit_ids.intersection(parquet_commit_ids))}')\n",
    "\n",
    "#     assert csv_commit_ids == parquet_commit_ids, 'Commit ids in csv and parquet are not the same'\n",
    "\n",
    "#     # ensure commit ids in both v2 csv and v2 parquet are the same\n",
    "#     v2_csv_commit_ids = set(v2_csv_df['commit_id'].unique())\n",
    "#     v2_parquet_commit_ids = set(v2_parquet_df['commit_id'].unique())\n",
    "\n",
    "#     print(f'Common commit ids: {len(v2_csv_commit_ids.intersection(v2_parquet_commit_ids))}')\n",
    "\n",
    "#     assert v2_csv_commit_ids == v2_parquet_commit_ids, 'Commit ids in v2 csv and v2 parquet are not the same'\n",
    "\n",
    "#     # ensure commit ids in both csv and v2 csv are the same\n",
    "\n",
    "#     print(f'Common commit ids: {len(csv_commit_ids.intersection(v2_csv_commit_ids))}')\n",
    "\n",
    "#     assert csv_commit_ids == v2_csv_commit_ids, 'Commit ids in csv and v2 csv are not the same'\n",
    "\n",
    "#     # only now store all commit ids in in a file called facebook_react_gpt4_gold_commit_ids.txt with each commit id on a new line\n",
    "\n",
    "#     with open(os.path.join(repo_path, f'{repo_name}_gpt4_gold_commit_ids.txt'), 'w') as f:\n",
    "#         for commit_id in csv_df['commit_id'].unique():\n",
    "#             f.write(f'{commit_id}\\n')\n",
    "\n",
    "\n",
    "# get_gold_commits()\n",
    "fix_old_parquet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
