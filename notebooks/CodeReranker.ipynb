{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:01.723338335Z",
     "start_time": "2023-11-27T04:01:01.497195817Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:04.805496544Z",
     "start_time": "2023-11-27T04:01:04.581595287Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "from utils import (\n",
    "    AggregatedSearchResult,\n",
    "    get_combined_df,\n",
    "    prepare_triplet_data_from_df,\n",
    "    sanity_check_triplets,\n",
    "    set_seed,\n",
    "    tokenize,\n",
    "    get_recent_df\n",
    ")\n",
    "from BERTReranker_v4 import BERTReranker\n",
    "from CodeReranker import BERTCodeReranker\n",
    "# set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T04:01:21.300426891Z",
     "start_time": "2023-11-27T04:01:21.229196368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  1\n",
      "Current cuda device:  0\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "# print torch devices available\n",
    "print('Available devices: ', torch.cuda.device_count())\n",
    "print('Current cuda device: ', torch.cuda.current_device())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    index_path='smalldata/ftr/index_commit_tokenized', repo_path='smalldata/ftr', k=1000, n=100, no_bm25=True, model_path='microsoft/codebert-base', overwrite_cache=False, batch_size=32, num_epochs=10, learning_rate=5e-05, num_positives=10, num_negatives=10, train_depth=1000, num_workers=8, train_commits=1000, psg_cnt=5, aggregation_strategy='sump', use_gpu=True, rerank_depth=250, do_train=True, do_eval=True, eval_gold=True, openai_model='gpt4', overwrite_eval=False, sanity_check_triplets=False, debug=False, eval_before_training=False, do_combined_train=False, repo_paths=None, best_model_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']\n",
    "repo_path = args.repo_path\n",
    "repo_name = repo_path.split('/')[-1]\n",
    "index_path = args.index_path\n",
    "K = args.k\n",
    "n = args.n\n",
    "combined_df = get_combined_df(repo_path)\n",
    "BM25_AGGR_STRAT = 'sump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at smalldata/ftr/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 7587973, 'documents': 73765, 'non_empty_documents': 73765, 'unique_terms': 14602}\n"
     ]
    }
   ],
   "source": [
    "eval_path = os.path.join(repo_path, 'eval')\n",
    "if not os.path.exists(eval_path):\n",
    "    os.makedirs(eval_path)\n",
    "\n",
    "bm25_searcher = BM25Searcher(index_path)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 output path: smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to smalldata/ftr/eval/bm25_baseline_N100_K1000_metrics.txt\n",
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.1542, 'P@10': 0.087, 'P@100': 0.0267, 'P@1000': 0.0041, 'MRR': 0.2133, 'Recall@100': 0.5077, 'Recall@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm25_output_path = os.path.join(eval_path, f'bm25_baseline_N{n}_K{K}_metrics.txt')\n",
    "print(f'BM25 output path: {bm25_output_path}')\n",
    "\n",
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=bm25_output_path, aggregation_strategy=BM25_AGGR_STRAT)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': args.psg_cnt,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': args.rerank_depth,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': BM25_AGGR_STRAT,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'model_name': args.model_path,\n",
    "        'psg_cnt': 25,\n",
    "        'aggregation_strategy': args.aggregation_strategy,\n",
    "        'batch_size': args.batch_size,\n",
    "        'use_gpu': args.use_gpu,\n",
    "        'rerank_depth': 100,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'lr': args.learning_rate,\n",
    "        'num_positives': args.num_positives,\n",
    "        'num_negatives': args.num_negatives,\n",
    "        'train_depth': args.train_depth,\n",
    "        'num_workers': args.num_workers,\n",
    "        'train_commits': args.train_commits,\n",
    "        'bm25_aggr_strategy': BM25_AGGR_STRAT,\n",
    "        'psg_len': 250,\n",
    "        'psg_stride': 200,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Number of commits after midpoint date: 5804\n",
      "Number of commits after filtering by commit message length: 1543\n",
      "Gold commit file gold/ftr/ftr_gpt4_gold_commit_ids.txt does not exist, but ignore_gold_in_training is set to True, so continuing...\n"
     ]
    }
   ],
   "source": [
    "recent_df = get_recent_df(combined_df, repo_name=repo_name, ignore_gold_in_training=True)\n",
    "\n",
    "        # Step 6: randomly sample 1500 rows from recent_df\n",
    "recent_df = recent_df.sample(params['train_commits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data first\n",
    "if not os.path.exists(os.path.join(repo_path, 'cache')):\n",
    "    os.makedirs(os.path.join(repo_path, 'cache'))\n",
    "triplet_cache = os.path.join(repo_path, 'cache', 'triplet_data_cache.pkl')\n",
    "diff_cache = os.path.join(repo_path, 'cache', 'diff_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73765 entries, 0 to 73764\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   owner                  73765 non-null  string\n",
      " 1   repo_name              73765 non-null  string\n",
      " 2   commit_date            73765 non-null  int64 \n",
      " 3   commit_id              73765 non-null  string\n",
      " 4   commit_message         73765 non-null  string\n",
      " 5   file_path              73765 non-null  string\n",
      " 6   previous_commit_id     73765 non-null  string\n",
      " 7   previous_file_content  73765 non-null  string\n",
      " 8   cur_file_content       73765 non-null  string\n",
      " 9   diff                   58037 non-null  string\n",
      " 10  status                 73765 non-null  object\n",
      " 11  is_merge_request       73765 non-null  bool  \n",
      " 12  file_extension         73765 non-null  object\n",
      "dtypes: bool(1), int64(1), object(2), string(9)\n",
      "memory usage: 6.8+ MB\n",
      "Average number of words in commit message (whitespace): 93.9145\n",
      "Average number of words in commit message (AutoTokenizer): 156.9581\n",
      "Approx number of tokens passed to bert: 470.8743\n",
      "Approx number of tokens remaining for code: 41.125699999999995\n",
      "Average number of code tokens in diff column: 776.6853514342018\n",
      "Average number of code tokens in cur_file_content column: 10067.9974\n"
     ]
    }
   ],
   "source": [
    "def aside():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "    def tokenize(x):\n",
    "        # tokenize with no max length\n",
    "        return tokenizer.encode(x, add_special_tokens=True, truncation=False, max_length=None)\n",
    "    combined_df.info()\n",
    "    # print the average number of words in commit_message column\n",
    "\n",
    "    # sample 100 rows from combined_df\n",
    "    # sample_df = combined_df.sample(100, random_state=52)\n",
    "    # sample_df = combined_df[:10000]\n",
    "\n",
    "    avg_words = sample_df['commit_message'].str.split().str.len().mean()\n",
    "    print(f'Average number of words in commit message (whitespace): {avg_words}')\n",
    "    avg_words = sample_df['commit_message'].apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of words in commit message (AutoTokenizer): {avg_words}')\n",
    "\n",
    "    # print approx number of tokens in passed to bert which is 2 * avg_words * 1.5\n",
    "    approx_tokens = 2 * avg_words * 1.5\n",
    "    print(f'Approx number of tokens passed to bert: {approx_tokens}')\n",
    "\n",
    "    # print remaining number of tokens in bert (max is 512)\n",
    "    print(f'Approx number of tokens remaining for code: {512 - approx_tokens}')\n",
    "\n",
    "    # print average number of code tokens in diff column by using tokenize function but only on the non-null diff values\n",
    "    avg_code_tokens = sample_df['diff'].dropna().apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of code tokens in diff column: {avg_code_tokens}')\n",
    "\n",
    "    avg_file_tokens = sample_df['cur_file_content'].dropna().apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of code tokens in cur_file_content column: {avg_file_tokens}')\n",
    "\n",
    "aside()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_code_prep(df, searcher, search_depth, num_positives, num_negatives):\n",
    "    code_data = []\n",
    "    print(f'Preparing code data from dataframe of size: {len(df)} with search_depth: {search_depth}')\n",
    "    # for _, row in df.iterrows():\n",
    "    total_positives, total_negatives = 0, 0\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        cur_positives = 0\n",
    "        cur_negatives = 0\n",
    "        commit_message = row['commit_message']\n",
    "        actual_files_modified = row['actual_files_modified']\n",
    "\n",
    "        agg_search_results = searcher.pipeline(commit_message, row['commit_date'], search_depth, 'sump', sort_contributing_result_by_date=True)\n",
    "\n",
    "        for agg_result in agg_search_results:\n",
    "            most_recent_search_result = agg_result.contributing_results[0]\n",
    "            file_path = most_recent_search_result.file_path\n",
    "            commit_id = most_recent_search_result.commit_id\n",
    "\n",
    "            if file_path in actual_files_modified and cur_positives < num_positives:\n",
    "                # this is a positive sample\n",
    "                code_data.append((commit_message, file_path, commit_id, 1))\n",
    "                cur_positives += 1\n",
    "                total_positives += 1\n",
    "            elif file_path not in actual_files_modified and cur_negatives < num_negatives:\n",
    "                # this is a negative sample\n",
    "                code_data.append((commit_message, file_path, commit_id, 0))\n",
    "                cur_negatives += 1\n",
    "                total_negatives += 1\n",
    "\n",
    "            if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        # go from top to bottom, first num_positives non-0 scores are positive samples and the next num_negatives are negative samples\n",
    "        # for agg_result in agg_search_results:\n",
    "        #     cur_commit_msg = agg_result.contributing_results[0].commit_message\n",
    "        #     if cur_positives < num_positives and agg_result.score > 0:\n",
    "        #         # meaning there is at least one file in the agg_result that is in actual_files_modified\n",
    "        #         # pos_commits.append(agg_result)\n",
    "        #         data.append((commit_message, cur_commit_msg, 1))\n",
    "        #         cur_positives += 1\n",
    "        #         pos_commit_ids.add(agg_result.commit_id)\n",
    "        #     elif cur_negatives < num_negatives:\n",
    "        #         # neg_commits.append(agg_result)\n",
    "        #         data.append((commit_message, cur_commit_msg, 0))\n",
    "        #         cur_negatives += 1\n",
    "        #         neg_commit_ids.add(agg_result.commit_id)\n",
    "        #     if cur_positives == num_positives and cur_negatives == num_negatives:\n",
    "        #         break\n",
    "\n",
    "        # assert len(pos_commit_ids.intersection(neg_commit_ids)) == 0, 'Positive and negative commit ids should not intersect'\n",
    "        # print(f\"Total positives: {cur_positives}, Total negatives: {cur_negatives}\")\n",
    "        # total_positives += cur_positives\n",
    "        # total_negatives += cur_negatives\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    # data = pd.DataFrame(data, columns=['query', 'passage', 'label'])\n",
    "    code_df = pd.DataFrame(code_data, columns=['query', 'file_path', 'commit_id', 'label'])\n",
    "    # print distribution of labels\n",
    "    print(f\"Total positives: {total_positives}, Total negatives: {total_negatives}\")\n",
    "    # print percentage of positives and negatives\n",
    "    denom = total_positives + total_negatives\n",
    "    print(f\"Percentage of positives: {total_positives / denom}, Percentage of negatives: {total_negatives / denom}\")\n",
    "    return code_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing code data from dataframe of size: 1500 with search_depth: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [06:53<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positives: 5753, Total negatives: 15000\n",
      "Percentage of positives: 0.2772129330699176, Percentage of negatives: 0.7227870669300824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "code_df = test_code_prep(recent_df, bm25_searcher, params['train_depth'], params['num_positives'], params['num_negatives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20753 entries, 0 to 20752\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      20753 non-null  object\n",
      " 1   file_path  20753 non-null  object\n",
      " 2   commit_id  20753 non-null  object\n",
      " 3   label      20753 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 648.7+ KB\n"
     ]
    }
   ],
   "source": [
    "code_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20753/20753 [04:16<00:00, 80.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_diff_data(diff_data, df):\n",
    "    # given diff_data, we want to use commit_id and file_path to get the diff from the df\n",
    "\n",
    "    # first we need to get the diff from the df\n",
    "    # we can use the commit_id and file_path to get the diff\n",
    "    res_df = []\n",
    "    null_rows = 0\n",
    "    # for _, row in diff_data.iterrows():\n",
    "    for _, row in tqdm(diff_data.iterrows(), total=len(diff_data)):\n",
    "        commit_id = row['commit_id']\n",
    "        file_path = row['file_path']\n",
    "        # get the diff from the df\n",
    "        diff = df[(df['commit_id'] == commit_id) & (df['file_path'] == file_path)]['cur_file_content']\n",
    "        # check if diff is NA/NaN\n",
    "        if diff.isnull().values.any():\n",
    "            # if it is, then we can just skip this row\n",
    "            null_rows += 1\n",
    "            continue\n",
    "        diff = diff.values[0]\n",
    "\n",
    "        res_df.append((commit_id, file_path, row['query'], diff, row['label']))\n",
    "\n",
    "    res_df = pd.DataFrame(res_df, columns=['commit_id', 'file_path', 'query', 'passage', 'label'])\n",
    "    # make query and passage into strings and label into int\n",
    "    res_df['query'] = res_df['query'].astype(str)\n",
    "    res_df['passage'] = res_df['passage'].astype(str)\n",
    "    res_df['label'] = res_df['label'].astype(int)\n",
    "    print(f\"Number of null rows: {null_rows}\")\n",
    "    return res_df\n",
    "\n",
    "processed_diff_data = process_diff_data(code_df, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20753 entries, 0 to 20752\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   commit_id  20753 non-null  object\n",
      " 1   file_path  20753 non-null  object\n",
      " 2   query      20753 non-null  object\n",
      " 3   passage    20753 non-null  object\n",
      " 4   label      20753 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 810.8+ KB\n"
     ]
    }
   ],
   "source": [
    "processed_diff_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_code(data):\n",
    "    problems = 0\n",
    "    for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        try:\n",
    "            if row['label'] == 0:\n",
    "                assert data[(data['query'] == row['query']) & (data['commit_id'] == row['commit_id']) & (data['file_path'] == row['file_path'])]['label'].values[0] == 0\n",
    "            else:\n",
    "                assert data[(data['query'] == row['query']) & (data['commit_id'] == row['commit_id']) & (data['file_path'] == row['file_path'])]['label'].values[0] == 1\n",
    "        except AssertionError:\n",
    "            print(f\"Assertion failed at index {i}: {row}\")\n",
    "            # break  # Optional: break after the first failure, remove if you want to see all failures\n",
    "            # remove the row with label 0\n",
    "\n",
    "            if row['label'] == 0:\n",
    "                problems += 1\n",
    "                # data.drop(i, inplace=True)\n",
    "                data = data.drop(i)\n",
    "                # print(f\"Dropped row at index {i}\")\n",
    "\n",
    "    print(f\"Total number of problems in sanity check of training data: {problems}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_code_df = pd.read_parquet(os.path.join(repo_path, 'cache', 'code_data.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n",
      "Initialized BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 25, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 250, 'psg_stride': 200}\n"
     ]
    }
   ],
   "source": [
    "bert_reranker = BERTReranker(params)\n",
    "save_model_name = params['model_name'].replace('/', '_')\n",
    "# repo_name = 'facebook_react'\n",
    "bert_best_model_path = os.path.join('2_7/facebook_react', 'models', f\"{save_model_name}_bertrr_gpt_train\", 'best_model')\n",
    "bert_reranker.model = AutoModelForSequenceClassification.from_pretrained(bert_best_model_path)\n",
    "bert_reranker.model.to(bert_reranker.device)\n",
    "rerankers = [bert_reranker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTCodeReranker:\n",
    "#     def __init__(self, parameters):\n",
    "#         self.parameters = parameters\n",
    "#         self.model_name = parameters['model_name']\n",
    "#         self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=1, problem_type='regression')\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() and parameters['use_gpu'] else \"cpu\")\n",
    "#         self.model.to(self.device)\n",
    "\n",
    "#         print(f'Using device: {self.device}')\n",
    "\n",
    "#         # print GPU info\n",
    "#         if torch.cuda.is_available() and parameters['use_gpu']:\n",
    "#             print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "#             print(f'GPU Device Count: {torch.cuda.device_count()}')\n",
    "#             print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "#         self.psg_len = parameters['psg_len']\n",
    "#         self.psg_cnt = parameters['psg_cnt'] # how many contributing_results to use per file for reranking\n",
    "#         self.psg_stride = parameters.get('psg_stride', self.psg_len)\n",
    "#         self.aggregation_strategy = parameters['aggregation_strategy'] # how to aggregate the scores of the psg_cnt contributing_results\n",
    "#         self.batch_size = parameters['batch_size'] # batch size for reranking efficiently\n",
    "#         self.rerank_depth = parameters['rerank_depth']\n",
    "#         self.max_seq_length = self.tokenizer.model_max_length # max sequence length for the model\n",
    "\n",
    "#         print(f\"Initialized Code File BERT reranker with parameters: {parameters}\")\n",
    "\n",
    "\n",
    "#     def rerank(self, query, aggregated_results: List[AggregatedSearchResult]):\n",
    "#         \"\"\"\n",
    "#         Rerank the BM25 aggregated search results using BERT model scores.\n",
    "\n",
    "#         query: The issue query string.\n",
    "#         aggregated_results: A list of AggregatedSearchResult objects from BM25 search.\n",
    "#         \"\"\"\n",
    "#         # aggregated_results = aggregated_results[:self.rerank_depth] # already done in the pipeline\n",
    "#         # print(f'Reranking {len(aggregated_results)} results')\n",
    "\n",
    "#         self.model.eval()\n",
    "\n",
    "#         query_passage_pairs, per_result_contribution = self.split_into_query_passage_pairs(query, aggregated_results)\n",
    "\n",
    "\n",
    "#         # for agg_result in aggregated_results:\n",
    "#         #     query_passage_pairs.extend(\n",
    "#         #         (query, result.commit_message)\n",
    "#         #         for result in agg_result.contributing_results[: self.psg_cnt]\n",
    "#         #     )\n",
    "\n",
    "#         if not query_passage_pairs:\n",
    "#             print('WARNING: No query passage pairs to rerank, returning original results from previous stage')\n",
    "#             print(query, aggregated_results, self.psg_cnt)\n",
    "#             return aggregated_results\n",
    "\n",
    "#         # tokenize the query passage pairs\n",
    "#         encoded_pairs = [self.tokenizer.encode_plus([query, passage], max_length=self.max_seq_length, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=True) for query, passage in query_passage_pairs]\n",
    "\n",
    "#         # create tensors for the input ids, attention masks\n",
    "#         input_ids = torch.stack([encoded_pair['input_ids'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "#         attention_masks = torch.stack([encoded_pair['attention_mask'].squeeze() for encoded_pair in encoded_pairs], dim=0) # type: ignore\n",
    "\n",
    "#         # Create a dataloader for feeding the data to the model\n",
    "#         dataset = TensorDataset(input_ids, attention_masks)\n",
    "#         dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False) # shuffle=False very important for reconstructing the results back into the original order\n",
    "\n",
    "#         scores = self.get_scores(dataloader, self.model)\n",
    "\n",
    "#         score_index = 0\n",
    "#         # Now assign the scores to the aggregated results by mapping the scores to the contributing results\n",
    "#         for i, agg_result in enumerate(aggregated_results):\n",
    "#             # Each aggregated result gets a slice of the scores equal to the number of contributing results it has which should be min(psg_cnt, len(contributing_results))\n",
    "#             assert score_index < len(scores), f'score_index {score_index} is greater than or equal to scores length {len(scores)}'\n",
    "#             end_index = score_index + per_result_contribution[i] # only use psg_cnt contributing_results\n",
    "#             cur_passage_scores = scores[score_index:end_index]\n",
    "#             score_index = end_index\n",
    "\n",
    "\n",
    "#             # Aggregate the scores for the current aggregated result\n",
    "#             agg_score = self.aggregate_scores(cur_passage_scores)\n",
    "#             agg_result.score = agg_score  # Assign the aggregated score\n",
    "\n",
    "#         assert score_index == len(scores), f'score_index {score_index} does not equal scores length {len(scores)}, indices probably not working correctly'\n",
    "\n",
    "#         # Sort by the new aggregated score\n",
    "#         aggregated_results.sort(key=lambda res: res.score, reverse=True)\n",
    "\n",
    "#         return aggregated_results\n",
    "\n",
    "#     def get_scores(self, dataloader, model):\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in dataloader:\n",
    "#                 # Unpack the batch and move it to GPU\n",
    "#                 b_input_ids, b_attention_mask = batch\n",
    "#                 b_input_ids = b_input_ids.to(self.device)\n",
    "#                 b_attention_mask = b_attention_mask.to(self.device)\n",
    "\n",
    "#                 # Get scores from the model\n",
    "#                 outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)\n",
    "#                 scores.extend(outputs.logits.detach().cpu().numpy().squeeze(-1))\n",
    "#         return scores\n",
    "\n",
    "#     def aggregate_scores(self, passage_scores):\n",
    "#         \"\"\"\n",
    "#         Aggregate passage scores based on the specified strategy.\n",
    "#         \"\"\"\n",
    "#         if len(passage_scores) == 0:\n",
    "#             return 0.0\n",
    "\n",
    "#         if self.aggregation_strategy == 'firstp':\n",
    "#             return passage_scores[0]\n",
    "#         if self.aggregation_strategy == 'maxp':\n",
    "#             return max(passage_scores)\n",
    "#         if self.aggregation_strategy == 'avgp':\n",
    "#             return sum(passage_scores) / len(passage_scores)\n",
    "#         if self.aggregation_strategy == 'sump':\n",
    "#             return sum(passage_scores)\n",
    "#         # else:\n",
    "#         raise ValueError(f\"Invalid score aggregation method: {self.aggregation_strategy}\")\n",
    "\n",
    "\n",
    "#     def split_into_query_passage_pairs(self, query, aggregated_results):\n",
    "#         # Flatten the list of results into a list of (query, passage) pairs but only keep max psg_cnt passages per file\n",
    "#         def full_tokenize(s):\n",
    "#             return self.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "#         query_passage_pairs = []\n",
    "#         per_result_contribution = []\n",
    "#         for agg_result in aggregated_results:\n",
    "#             agg_result.contributing_results.sort(key=lambda res: res.commit_date, reverse=True)\n",
    "#             # get most recent file version\n",
    "#             most_recent_search_result = agg_result.contributing_results[0]\n",
    "#             # get the file_path and commit_id\n",
    "#             file_path = most_recent_search_result.file_path\n",
    "#             commit_id = most_recent_search_result.commit_id\n",
    "#             # get the file content from combined_df\n",
    "#             file_content = combined_df[(combined_df['commit_id'] == commit_id) & (combined_df['file_path'] == file_path)]['cur_file_content'].values[0]\n",
    "\n",
    "#             # assert file_content is not None, f'file_content is None for commit_id: {commit_id} and file_path: {file_path}'\n",
    "\n",
    "#             # now need to split this file content into psg_cnt passages\n",
    "#             # first tokenize the file content\n",
    "#             # check if file_content is pd.NA\n",
    "#             query_tokens = full_tokenize(query)\n",
    "#             path_tokens = full_tokenize(file_path)\n",
    "#             if pd.isna(file_content):\n",
    "#                 # query_passage_pairs.extend((query, file_path))\n",
    "#                 # per_result_contribution.append(1)\n",
    "#                 # continue\n",
    "#                 file_content = ''\n",
    "#             file_tokens = full_tokenize(file_content)\n",
    "\n",
    "\n",
    "\n",
    "#             # now split the file content into psg_cnt passages\n",
    "#             cur_result_passages = []\n",
    "#             # get the input ids\n",
    "#             # input_ids = file_content['input_ids'].squeeze()\n",
    "#             # get the number of tokens in the file content\n",
    "#             total_tokens = len(file_tokens)\n",
    "\n",
    "#             for cur_start in range(0, total_tokens, self.psg_stride):\n",
    "#                 cur_passage = []\n",
    "#                 # add query tokens and path tokens\n",
    "#                 cur_passage.extend(query_tokens)\n",
    "#                 cur_passage.extend(path_tokens)\n",
    "\n",
    "#                 # add the file tokens\n",
    "#                 cur_passage.extend(file_tokens[cur_start:cur_start+self.psg_len])\n",
    "\n",
    "#                 # now convert cur_passage into a string\n",
    "#                 cur_passage_decoded = self.tokenizer.decode(cur_passage)\n",
    "\n",
    "#                 # add the cur_passage to cur_result_passages\n",
    "#                 cur_result_passages.append(cur_passage_decoded)\n",
    "\n",
    "#                 if len(cur_result_passages) == self.psg_cnt:\n",
    "#                     break\n",
    "\n",
    "#             # now add the query, passage pairs to query_passage_pairs\n",
    "#             per_result_contribution.append(len(cur_result_passages))\n",
    "#             query_passage_pairs.extend((query, passage) for passage in cur_result_passages)\n",
    "#         return query_passage_pairs, per_result_contribution\n",
    "\n",
    "#     def rerank_pipeline(self, query, aggregated_results):\n",
    "#         if len(aggregated_results) == 0:\n",
    "#             return aggregated_results\n",
    "#         top_results = aggregated_results[:self.rerank_depth]\n",
    "#         bottom_results = aggregated_results[self.rerank_depth:]\n",
    "#         reranked_results = self.rerank(query, top_results)\n",
    "#         min_top_score = reranked_results[-1].score\n",
    "#         # now adjust the scores of bottom_results\n",
    "#         for i, result in enumerate(bottom_results):\n",
    "#             result.score = min_top_score - i - 1\n",
    "#         # combine the results\n",
    "#         reranked_results.extend(bottom_results)\n",
    "#         assert(len(reranked_results) == len(aggregated_results))\n",
    "#         return reranked_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Quadro RTX 6000\n",
      "GPU Device Count: 1\n",
      "GPU Memory Usage: 953.46 MB\n",
      "Initialized Code File BERT reranker with parameters: {'model_name': 'microsoft/codebert-base', 'psg_cnt': 25, 'aggregation_strategy': 'sump', 'batch_size': 32, 'use_gpu': True, 'rerank_depth': 100, 'num_epochs': 10, 'lr': 5e-05, 'num_positives': 10, 'num_negatives': 10, 'train_depth': 1000, 'num_workers': 8, 'train_commits': 1000, 'bm25_aggr_strategy': 'sump', 'psg_len': 250, 'psg_stride': 200}\n"
     ]
    }
   ],
   "source": [
    "code_reranker = BERTCodeReranker(params)\n",
    "# code_reranker.rerank_depth = 100\n",
    "# rerankers = [bert_reranker, code_reranker]\n",
    "rerankers = [code_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import c\n",
    "\n",
    "\n",
    "def prep_line(line):\n",
    "        return line.rstrip().lstrip()\n",
    "\n",
    "def parse_diff(diff):\n",
    "    return [\n",
    "        line[1:] if line.startswith('+') else line\n",
    "        for line in diff.split('\\n')\n",
    "        if not (line.startswith('-') or len(line) == 0 or (line.startswith('@@') and line.count('@@') > 1))\n",
    "        and len(prep_line(line)) > 0\n",
    "    ]\n",
    "\n",
    "def prepare_code_triplets(diff_data, code_reranker, cache_file, overwrite=False):\n",
    "\n",
    "    if cache_file and os.path.exists(cache_file) and not overwrite:\n",
    "        print(f\"Loading data from cache file: {cache_file}\")\n",
    "        # with open(cache_file, 'rb') as file:\n",
    "        #     return pickle.load(file)\n",
    "        return pd.read_parquet(cache_file)\n",
    "\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "\n",
    "\n",
    "    def count_matching_lines(passage_lines, diff_lines):\n",
    "        # Create a 2D array to store the lengths of the longest common subsequences\n",
    "        dp = [[0] * (len(diff_lines) + 1) for _ in range(len(passage_lines) + 1)]\n",
    "\n",
    "        # Fill the dp array\n",
    "        for i in range(1, len(passage_lines) + 1):\n",
    "            for j in range(1, len(diff_lines) + 1):\n",
    "                if prep_line(passage_lines[i - 1]) == prep_line(diff_lines[j - 1]):\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "\n",
    "        return dp[-1][-1]\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(diff_data.iterrows(), total=len(diff_data)):\n",
    "        file_tokens = full_tokenize(row['passage'])\n",
    "        total_tokens = len(file_tokens)\n",
    "        cur_diff = combined_df[(combined_df['commit_id'] == row['commit_id']) & (combined_df['file_path'] == row['file_path'])]['diff'].values[0]\n",
    "        if pd.isna(cur_diff):\n",
    "            # if diff is NA/NaN\n",
    "            continue\n",
    "        cur_diff_lines = parse_diff(cur_diff)\n",
    "        cur_psg_cnt = 0\n",
    "        cur_triplets = []\n",
    "        for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "            cur_passage = []\n",
    "\n",
    "            cur_passage.extend(file_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "\n",
    "            # now convert cur_passage into a string\n",
    "            cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "            cur_passage_lines = cur_passage_decoded.split('\\n')\n",
    "\n",
    "            # check if there are lines matching the diff lines\n",
    "            # if there are, then we can add this directly to the triplets\n",
    "            # common_lines = set(cur_passage_lines).intersection(set(cur_diff_lines))\n",
    "            common_line_count = count_matching_lines(cur_passage_lines, cur_diff_lines)\n",
    "\n",
    "            # add the cur_passage to cur_result_passages\n",
    "            cur_triplets.append((common_line_count, (row['query'], row['file_path'], cur_passage_decoded, row['label'])))\n",
    "\n",
    "        # sort the cur_triplets by the number of common lines\n",
    "        cur_triplets.sort(key=lambda x: x[0], reverse=True)\n",
    "        # now add the top code_reranker.psg_cnt to triplets\n",
    "        for triplet in cur_triplets[:code_reranker.psg_cnt]:\n",
    "            # print(f\"Found {triplet[0]} matching lines for diff in cur_passage at index\")\n",
    "            triplets.append(triplet[1])\n",
    "\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    triplets = pd.DataFrame(triplets, columns=['query', 'file_path', 'passage', 'label'])\n",
    "    if cache_file:\n",
    "        # with open(cache_file, 'wb') as file:\n",
    "        #     pickle.dump(triplets, file)\n",
    "        #     print(f\"Saved data to cache file: {cache_file}\")\n",
    "        print(f\"Saving data to cache file: {cache_file}\")\n",
    "        triplets.to_parquet(cache_file)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_code_triplets(diff_data, code_reranker, cache_file, overwrite=False):\n",
    "    # given diff_data, the passage column is way too long. We need to split it into passages of length psg_len with stride psg_stride\n",
    "    # then we can create triplets from that\n",
    "\n",
    "    # diff_data has columns: commit_id, file_path, query, passage, label\n",
    "\n",
    "    # if cache_file and os.path.exists(cache_file) and not overwrite:\n",
    "    #     print(f\"Loading data from cache file: {cache_file}\")\n",
    "    #     # with open(cache_file, 'rb') as file:\n",
    "    #     #     return pickle.load(file)\n",
    "    #     return pd.read_parquet(cache_file)\n",
    "\n",
    "    def full_tokenize(s):\n",
    "        return code_reranker.tokenizer.encode_plus(s, max_length=None, truncation=False, return_tensors='pt', add_special_tokens=True, return_attention_mask=False, return_token_type_ids=False)['input_ids'].squeeze().tolist()\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for _, row in tqdm(diff_data.iterrows(), total=len(diff_data)):\n",
    "        file_tokens = full_tokenize(row['passage'])\n",
    "        total_tokens = len(file_tokens)\n",
    "        # cur_diff = combined_df[(combined_df['commit_id'] == row['commit_id']) & (combined_df['file_path'] == row['file_path'])]['diff'].values[0]\n",
    "        cur_psg_cnt = 0\n",
    "        for cur_start in range(0, total_tokens, code_reranker.psg_stride):\n",
    "            cur_passage = []\n",
    "\n",
    "            cur_passage.extend(file_tokens[cur_start:cur_start+code_reranker.psg_len])\n",
    "\n",
    "            # now convert cur_passage into a string\n",
    "            cur_passage_decoded = code_reranker.tokenizer.decode(cur_passage)\n",
    "\n",
    "\n",
    "            # add the cur_passage to cur_result_passages\n",
    "            triplets.append((row['query'], row['file_path'], cur_passage_decoded, row['label']))\n",
    "\n",
    "            cur_psg_cnt += 1\n",
    "\n",
    "            if cur_psg_cnt == code_reranker.psg_cnt:\n",
    "                break\n",
    "        break\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    triplets = pd.DataFrame(triplets, columns=['query', 'file_path', 'passage', 'label'])\n",
    "    # Write data to cache file\n",
    "    # if cache_file:\n",
    "    #     # with open(cache_file, 'wb') as file:\n",
    "    #     #     pickle.dump(triplets, file)\n",
    "    #     #     print(f\"Saved data to cache file: {cache_file}\")\n",
    "    #     print(f\"Saving data to cache file: {cache_file}\")\n",
    "    #     triplets.to_parquet(cache_file)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_cache = 'smalldata/ftr/cache/diff_code_triplets.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20753 entries, 0 to 20752\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   commit_id  20753 non-null  object\n",
      " 1   file_path  20753 non-null  object\n",
      " 2   query      20753 non-null  object\n",
      " 3   passage    20753 non-null  object\n",
      " 4   label      20753 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 810.8+ KB\n"
     ]
    }
   ],
   "source": [
    "processed_code_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [58:23<00:00,  1.43it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to cache file: smalldata/ftr/cache/diff_code_triplets.pkl\n"
     ]
    }
   ],
   "source": [
    "triplets = prepare_code_triplets(processed_code_df.head(5000), code_reranker, triplet_cache, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97267 entries, 0 to 97266\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query      97267 non-null  object\n",
      " 1   file_path  97267 non-null  object\n",
      " 2   passage    97267 non-null  object\n",
      " 3   label      97267 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "triplets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets.to_parquet(triplet_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    176\n",
       "1     25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of labels (number of 0s and 1s)\n",
    "triplets['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aside():\n",
    "#     df = prepare_code_triplets(processed_code_df, code_reranker, None, False)\n",
    "#     df.info()\n",
    "#     print(df.head())\n",
    "\n",
    "# aside()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, 666907 to 359581\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   query      100000 non-null  object\n",
      " 1   file_path  100000 non-null  object\n",
      " 2   passage    100000 non-null  object\n",
      " 3   label      100000 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "triplets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in query: 237.96949838577555\n",
      "Average number of tokens in path: 18.898038837758396\n",
      "Avg number of tokens remaining for code: 255.13246277646607\n",
      "Average number of code tokens in cur_file_content column: 17368.505324531394\n"
     ]
    }
   ],
   "source": [
    "def aside2():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "    def tokenize(x):\n",
    "        # tokenize with no max length\n",
    "        return tokenizer.encode(x, add_special_tokens=False, truncation=False, max_length=None)\n",
    "    # print the average number of words in commit_message column\n",
    "\n",
    "    # sample 100 rows from combined_df\n",
    "    # sample_df = combined_df.sample(100, random_state=52)\n",
    "    # sample_df = combined_df[:10000]\n",
    "    sample_df = diff_data\n",
    "    avg_words = sample_df['query'].apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of tokens in query: {avg_words}')\n",
    "\n",
    "    avg_path = sample_df['file_path'].apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of tokens in path: {avg_path}')\n",
    "\n",
    "\n",
    "    # print remaining number of tokens in bert (max is 512)\n",
    "    print(f'Avg number of tokens remaining for code: {512 - avg_words - avg_path}')\n",
    "\n",
    "    avg_file_tokens = sample_df['passage'].dropna().apply(lambda x: len(tokenize(x))).mean()\n",
    "    print(f'Average number of code tokens in cur_file_content column: {avg_file_tokens}')\n",
    "\n",
    "aside2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1859, 'P@10': 0.08, 'P@100': 0.028, 'P@1000': 0.0043, 'MRR': 0.2331, 'Recall@100': 0.4443, 'Recall@1000': 0.5752}\n",
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1676, 'P@10': 0.08, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.2541, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n",
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:36<00:00,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1038, 'P@10': 0.07, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.1732, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=None, aggregation_strategy=params['aggregation_strategy']))\n",
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=[bert_reranker], aggregation_strategy=params['aggregation_strategy']))\n",
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:49<00:00, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.2113, 'P@10': 0.12, 'P@100': 0.038, 'P@1000': 0.0043, 'MRR': 0.3317, 'Recall@100': 0.5199, 'Recall@1000': 0.5752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=10, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:34<00:00, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.1807, 'P@10': 0.117, 'P@100': 0.0295, 'P@1000': 0.0041, 'MRR': 0.2545, 'Recall@100': 0.527, 'Recall@1000': 0.6845}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_evaluator.evaluate_sampling(n=100, k=1000, output_file_path=None, rerankers=rerankers, aggregation_strategy=params['aggregation_strategy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('microsoft_codebert-base',\n",
       " '2_7/apache_kafka/models/code_microsoft_codebert-base_model_output')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model_name = params['model_name'].replace('/', '_')\n",
    "# hf_output_dir = os.path.join('smalldata', 'ftr', f'100k_code_{save_model_name}_model_output')\n",
    "hf_output_dir = os.path.join(repo_path, 'models', f'code_{save_model_name}_model_output')\n",
    "save_model_name, hf_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(triplet_data, reranker, hf_output_dir, args):\n",
    "    def tokenize_hf(example):\n",
    "        len(example)\n",
    "        return reranker.tokenizer(example['query'], example['passage'], truncation=True, padding='max_length', max_length=reranker.max_seq_length, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "\n",
    "    # triplet_data = triplet_data.sample(1000, random_state=42)\n",
    "    print('Training the model...')\n",
    "    print('Label distribution:')\n",
    "    print(triplet_data['label'].value_counts())\n",
    "\n",
    "    # merge columns file_path and passage into one column called passage\n",
    "    triplet_data['passage'] = triplet_data['file_path'] + ' ' + triplet_data['passage']\n",
    "\n",
    "    # if args.sanity_check:\n",
    "    #     print('Running sanity check on training data...')\n",
    "    #     triplet_data = sanity_check(triplet_data)\n",
    "    # Step 7: convert triplet_data to HuggingFace Dataset\n",
    "    # convert triplet_data to HuggingFace Dataset\n",
    "    triplet_data['label'] = triplet_data['label'].astype(float)\n",
    "    train_df, val_df = train_test_split(triplet_data, test_size=0.2, random_state=42, stratify=triplet_data['label'])\n",
    "    train_hf_dataset = HFDataset.from_pandas(train_df, split='train') # type: ignore\n",
    "    val_hf_dataset = HFDataset.from_pandas(val_df, split='validation') # type: ignore\n",
    "    # Step 8: tokenize the data\n",
    "    tokenized_train_dataset = train_hf_dataset.map(tokenize_hf, batched=True)\n",
    "    tokenized_val_dataset = val_hf_dataset.map(tokenize_hf, batched=True)\n",
    "\n",
    "    # Step 9: set format for pytorch\n",
    "    tokenized_train_dataset = tokenized_train_dataset.remove_columns(['query', 'passage', 'file_path'])\n",
    "    tokenized_val_dataset = tokenized_val_dataset.remove_columns(['query', 'passage', 'file_path'])\n",
    "\n",
    "    # rename label column to labels\n",
    "    tokenized_train_dataset = tokenized_train_dataset.rename_column('label', 'labels')\n",
    "    tokenized_val_dataset = tokenized_val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "    # set format to pytorch\n",
    "    tokenized_train_dataset = tokenized_train_dataset.with_format('torch')\n",
    "    tokenized_val_dataset = tokenized_val_dataset.with_format('torch')\n",
    "    print('Training dataset features:')\n",
    "    print(tokenized_train_dataset.features)\n",
    "\n",
    "    # Step 10: set up training arguments\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=hf_output_dir,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        logging_steps=100,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=args.num_workers,\n",
    "        )\n",
    "\n",
    "    # small_train_dataset = tokenized_train_dataset.shuffle(seed=42).select(range(100))\n",
    "    # small_val_dataset = tokenized_val_dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "    # if args.debug:\n",
    "    #     print('Running in debug mode, using small datasets')\n",
    "    #     tokenized_train_dataset = small_train_dataset\n",
    "    #     tokenized_val_dataset = small_val_dataset\n",
    "\n",
    "    # Step 11: set up trainer\n",
    "    trainer = Trainer(\n",
    "        model = reranker.model,\n",
    "        args = train_args,\n",
    "        train_dataset = tokenized_train_dataset, # type: ignore\n",
    "        eval_dataset = tokenized_val_dataset, # type: ignore\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Step 12: train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Step 13: save the model\n",
    "    best_model_path = os.path.join(hf_output_dir, 'best_model')\n",
    "    trainer.save_model(best_model_path)\n",
    "    print(f'Saved model to {best_model_path}')\n",
    "    print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.sanity_check = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Label distribution:\n",
      "label\n",
      "0.0    76703\n",
      "1.0    23297\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80000/80000 [00:41<00:00, 1934.68 examples/s]\n",
      "Map: 100%|██████████| 20000/20000 [00:10<00:00, 1918.61 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset features:\n",
      "{'labels': Value(dtype='float64', id=None), '__index_level_0__': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8499' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8499/25000 1:05:45 < 2:07:41, 2.15 it/s, Epoch 3.40/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.064053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.034442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.023169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m do_training(triplets, code_reranker, hf_output_dir, args)\n",
      "\u001b[1;32m/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb Cell 51\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     model \u001b[39m=\u001b[39m reranker\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     args \u001b[39m=\u001b[39m train_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Step 12: train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# Step 13: save the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboston-cluster.lti.cs.cmu.edu/home/ssg2/ssg2/ds/notebooks/BERTReranker_v5.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m best_model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(hf_output_dir, \u001b[39m'\u001b[39m\u001b[39mbest_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/transformers/trainer.py:1894\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_training(triplets, code_reranker, hf_output_dir, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_reranker.model = AutoModelForSequenceClassification.from_pretrained(os.path.join(repo_path, 'models', f\"code_{save_model_name}_model_output\", 'best_model'))\n",
    "code_reranker.psg_cnt = 25\n",
    "code_reranker.model.to(code_reranker.device)\n",
    "rerankers = [bert_reranker, code_reranker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Output file path not provided, not writing results to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (98775 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100/100 [1:15:51<00:00, 45.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.2703,\n",
       " 'P@10': 0.119,\n",
       " 'P@100': 0.034,\n",
       " 'P@1000': 0.0053,\n",
       " 'MRR': 0.4074,\n",
       " 'Recall@100': 0.5517,\n",
       " 'Recall@1000': 0.7426}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], rerankers=rerankers, overwrite_eval=args.overwrite_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are several issues that need to be addressed to enhance the user experience:\\n\\n1. The modals currently do not have a shadow, affecting visibility and overall aesthetic of the UI.\\n2. The default setting of \"collapse new nodes\" option is currently enabled, which may not be the most user-friendly approach.\\n3. The label \"Collapse newly added components by default\" may confuse users, a clearer phrasing would help understanding.\\n4. The CSS media query for the settings popup is currently not optimized for smaller sizes, resulting in labels being hidden.\\n5. The \"Inspect the matching DOM element\" button is present in standalone mode, despite not serving any functional purpose.\\n6. There is a size issue with the settings icon, it\\'s currently at 20x20 viewbox instead of the intended 24x24.\\n7. There is a bug where \"window.addEventListener\" and \"window.removeEventListener\" are not defined in Hermes, causing operation failure.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.iloc[0]['transformed_message_gpt3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt4\n",
      "Found gold data for facebook_react with shape (100, 5) at gold/facebook_react/facebook_react_gpt4_gold.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    object\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    object\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gold_dir = os.path.join('gold', 'facebook_react')\n",
    "gold_data_path = os.path.join(gold_dir, f'{repo_name}_{args.openai_model}_gold.csv')\n",
    "print(f'Model: {args.openai_model}')\n",
    "gold_df = pd.read_csv(gold_data_path)\n",
    "assert gold_df[f'transformed_message_{args.openai_model}'].notnull().all()\n",
    "# rename commit_message to original_message\n",
    "gold_df = gold_df.rename(columns={'commit_message': 'original_message'})\n",
    "# rename transformed_message to commit_message\n",
    "gold_df = gold_df.rename(columns={f'transformed_message_{args.openai_model}': 'commit_message'})\n",
    "print(f'Found gold data for {repo_name} with shape {gold_df.shape} at {gold_data_path}')\n",
    "print(gold_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BM25 on gold data...\n",
      "WARNING: Output file path not provided, not writing results to file\n",
      "Found gold_df, evaluating on 100 commits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    object\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    object\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Gold Evaluation\n",
      "{'MAP': 0.1223, 'P@10': 0.055, 'P@100': 0.0186, 'P@1000': 0.0026, 'MRR': 0.1908, 'Recall@100': 0.3617, 'Recall@1000': 0.5439}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Running BM25 on gold data...')\n",
    "# bm25_gold_output_path = os.path.join(eval_path, f'bm25_v2_{args.openai_model}_gold_metrics.txt')\n",
    "bm25_gold_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['bm25_aggr_strategy'], gold_df=gold_df, overwrite_eval=args.overwrite_eval)\n",
    "print(\"BM25 Gold Evaluation\")\n",
    "print(bm25_gold_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<BERTReranker_v4.BERTReranker at 0x7ef810f3bc40>,\n",
       " <__main__.BERTCodeReranker at 0x7ef93dff1f40>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERT on gold data...\n",
      "WARNING: Output file path not provided, not writing results to file\n",
      "Found gold_df, evaluating on 100 commits\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   commit_id              100 non-null    object\n",
      " 1   commit_date            100 non-null    int64 \n",
      " 2   original_message       100 non-null    object\n",
      " 3   actual_files_modified  100 non-null    object\n",
      " 4   commit_message         100 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2127 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100/100 [52:50<00:00, 31.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Gold Evaluation\n",
      "{'MAP': 0.1821, 'P@10': 0.085, 'P@100': 0.0186, 'P@1000': 0.0026, 'MRR': 0.2484, 'Recall@100': 0.3617, 'Recall@1000': 0.5439}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Running BERT on gold data...')\n",
    "# bert_gold_output_path = os.path.join(eval_path, f'bert_v2_{args.openai_model}_gold.txt')\n",
    "bert_gold_eval = model_evaluator.evaluate_sampling(n=n, k=K, output_file_path=None, aggregation_strategy=params['aggregation_strategy'], rerankers=rerankers, gold_df=gold_df, overwrite_eval=args.overwrite_eval)\n",
    "\n",
    "print(\"BERT Gold Evaluation\")\n",
    "print(bert_gold_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.062 MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_data['commit_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commit ids in diff_data: 831\n",
      "Number of commit ids in gold_df: 100\n",
      "Number of commit ids in both diff_data and gold_df: 33\n"
     ]
    }
   ],
   "source": [
    "# find intersection of commit ids between diff_data and gold_df\n",
    "diff_commit_ids = set(diff_data['commit_id'].unique())\n",
    "gold_commit_ids = set(gold_df['commit_id'].unique())\n",
    "\n",
    "print(f\"Number of commit ids in diff_data: {len(diff_commit_ids)}\")\n",
    "print(f\"Number of commit ids in gold_df: {len(gold_commit_ids)}\")\n",
    "\n",
    "print(f\"Number of commit ids in both diff_data and gold_df: {len(diff_commit_ids.intersection(gold_commit_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   commit_id                 100 non-null    string\n",
      " 1   commit_date               100 non-null    int64 \n",
      " 2   commit_message            100 non-null    string\n",
      " 3   actual_files_modified     100 non-null    object\n",
      " 4   transformed_message_gpt4  100 non-null    object\n",
      "dtypes: int64(1), object(2), string(2)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def fix_old_parquet():\n",
    "    repo_name = 'angular_angular'\n",
    "    repo_path = os.path.join('gold', repo_name)\n",
    "    # csv_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.csv')\n",
    "    parquet_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.parquet')\n",
    "\n",
    "    # if there is a column called transformed_message_gpt3, then we need to fix the parquet file by renaming it to transformed_message_gpt4\n",
    "\n",
    "    parquet_df = pd.read_parquet(parquet_file)\n",
    "    print(parquet_df.info())\n",
    "\n",
    "    if 'transformed_message_gpt3' in parquet_df.columns:\n",
    "        print('Found transformed_message_gpt3 column in parquet file')\n",
    "        # rename it to transformed_message_gpt4\n",
    "        parquet_df = parquet_df.rename(columns={'transformed_message_gpt3': 'transformed_message_gpt4'})\n",
    "        print(parquet_df.info())\n",
    "        # now save it back to the parquet file\n",
    "        # parquet_df.to_parquet(parquet_file)\n",
    "        print('Saved parquet file')\n",
    "\n",
    "#     v2_csv_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.csv')\n",
    "#     v2_parquet_file = os.path.join(repo_path, f'v2_{repo_name}_gpt4_gold.parquet')\n",
    "\n",
    "#     csv_df = pd.read_csv(csv_file)\n",
    "#     parquet_df = pd.read_parquet(parquet_file)\n",
    "\n",
    "#     v2_csv_df = pd.read_csv(v2_csv_file)\n",
    "#     v2_parquet_df = pd.read_parquet(v2_parquet_file)\n",
    "\n",
    "#     # ensure commit ids in both csv and parquet are the same\n",
    "#     csv_commit_ids = set(csv_df['commit_id'].unique())\n",
    "#     parquet_commit_ids = set(parquet_df['commit_id'].unique())\n",
    "\n",
    "#     print(f'Common commit ids: {len(csv_commit_ids.intersection(parquet_commit_ids))}')\n",
    "\n",
    "#     assert csv_commit_ids == parquet_commit_ids, 'Commit ids in csv and parquet are not the same'\n",
    "\n",
    "#     # ensure commit ids in both v2 csv and v2 parquet are the same\n",
    "#     v2_csv_commit_ids = set(v2_csv_df['commit_id'].unique())\n",
    "#     v2_parquet_commit_ids = set(v2_parquet_df['commit_id'].unique())\n",
    "\n",
    "#     print(f'Common commit ids: {len(v2_csv_commit_ids.intersection(v2_parquet_commit_ids))}')\n",
    "\n",
    "#     assert v2_csv_commit_ids == v2_parquet_commit_ids, 'Commit ids in v2 csv and v2 parquet are not the same'\n",
    "\n",
    "#     # ensure commit ids in both csv and v2 csv are the same\n",
    "\n",
    "#     print(f'Common commit ids: {len(csv_commit_ids.intersection(v2_csv_commit_ids))}')\n",
    "\n",
    "#     assert csv_commit_ids == v2_csv_commit_ids, 'Commit ids in csv and v2 csv are not the same'\n",
    "\n",
    "#     # only now store all commit ids in in a file called facebook_react_gpt4_gold_commit_ids.txt with each commit id on a new line\n",
    "\n",
    "#     with open(os.path.join(repo_path, f'{repo_name}_gpt4_gold_commit_ids.txt'), 'w') as f:\n",
    "#         for commit_id in csv_df['commit_id'].unique():\n",
    "#             f.write(f'{commit_id}\\n')\n",
    "\n",
    "\n",
    "# get_gold_commits()\n",
    "fix_old_parquet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
