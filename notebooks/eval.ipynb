{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import set_seed, get_combined_df\n",
    "from bm25_v2 import BM25Searcher\n",
    "from eval import ModelEvaluator, SearchEvaluator\n",
    "import os\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATH = '../2_7/apache_kafka/index_commit_tokenized'\n",
    "REPO_PATH = '../2_7/apache_kafka'\n",
    "K = 1000 # initial ranking depth\n",
    "N = 100 # number of samples\n",
    "BM25_AGGR_STRAT = 'sump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['MAP', 'P@10', 'P@100', 'P@1000', 'MRR', 'Recall@100', 'Recall@1000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = get_combined_df(REPO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = '../misc'\n",
    "bm25_output_path = os.path.join(eval_path, f'bm25_baseline_N{N}_K{K}_metrics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index at ../2_7/apache_kafka/index_commit_tokenized\n",
      "Index Stats: {'total_terms': 10796945, 'documents': 75655, 'non_empty_documents': 75655, 'unique_terms': 15591}\n"
     ]
    }
   ],
   "source": [
    "bm25_searcher = BM25Searcher(INDEX_PATH)\n",
    "evaluator = SearchEvaluator(metrics)\n",
    "model_evaluator = ModelEvaluator(bm25_searcher, evaluator, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results written to ../misc/bm25_baseline_N100_K1000_metrics.txt\n",
      "BM25 Baseline Evaluation\n",
      "{'MAP': 0.2137, 'P@10': 0.114, 'P@100': 0.034, 'P@1000': 0.0053, 'MRR': 0.305, 'Recall@100': 0.5517, 'Recall@1000': 0.7426}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm25_baseline_eval = model_evaluator.evaluate_sampling(n=N, k=K, output_file_path=bm25_output_path, aggregation_strategy=BM25_AGGR_STRAT, repo_path=REPO_PATH)\n",
    "\n",
    "print(\"BM25 Baseline Evaluation\")\n",
    "print(bm25_baseline_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_query(df, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a query from the dataframe\n",
    "    \"\"\"\n",
    "    sampled_commit = df.drop_duplicates(subset='commit_id').sample(1, random_state=seed).iloc[0]\n",
    "    return {\n",
    "        'commit_message': sampled_commit['commit_message'],\n",
    "        'commit_id': sampled_commit['commit_id'],\n",
    "        'commit_date': sampled_commit['commit_date'],\n",
    "        'actual_files_modified': df[df['commit_id'] == sampled_commit['commit_id']]['file_path'].tolist(),\n",
    "        'diff_text': df[df['commit_id'] == sampled_commit['commit_id']]['diff'].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commid Message: produce/fetch remote time metric not set correctly when num.acks = 1; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-584\n",
      "\n",
      "git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1402250 13f79535-47bb-0310-9956-ffa450edef68\n",
      "\n",
      "Acutal Files Modified: ['core/src/main/scala/kafka/network/RequestChannel.scala']\n",
      "Diff Text: ['@@ -40,9 +40,9 @@ object RequestChannel extends Logging {\\n   }\\n \\n   case class Request(processor: Int, requestKey: Any, buffer: ByteBuffer, startTimeMs: Long) {\\n-    var dequeueTimeMs = -1L\\n-    var apiLocalCompleteTimeMs = -1L\\n-    var responseCompleteTimeMs = -1L\\n+    @volatile var dequeueTimeMs = -1L\\n+    @volatile var apiLocalCompleteTimeMs = -1L\\n+    @volatile var responseCompleteTimeMs = -1L\\n     val requestId = buffer.getShort()\\n     val requestObj: RequestOrResponse = RequestKeys.deserializerForKey(requestId)(buffer)\\n     buffer.rewind()\\n@@ -50,6 +50,10 @@ object RequestChannel extends Logging {\\n \\n     def updateRequestMetrics() {\\n       val endTimeMs = SystemTime.milliseconds\\n+      // In some corner cases, apiLocalCompleteTimeMs may not be set when the request completes since the remote\\n+      // processing time is really small. In this case, use responseCompleteTimeMs as apiLocalCompleteTimeMs.\\n+      if (apiLocalCompleteTimeMs < 0)\\n+        apiLocalCompleteTimeMs = responseCompleteTimeMs\\n       val queueTime = (dequeueTimeMs - startTimeMs).max(0L)\\n       val apiLocalTime = (apiLocalCompleteTimeMs - dequeueTimeMs).max(0L)\\n       val apiRemoteTime = (responseCompleteTimeMs - apiLocalCompleteTimeMs).max(0L)\\n@@ -71,8 +75,9 @@ object RequestChannel extends Logging {\\n              m.responseSendTimeHist.update(responseSendTime)\\n              m.totalTimeHist.update(totalTime)\\n       }\\n+      trace(\"Completed request: %s totalTime:%d queueTime:%d localTime:%d remoteTime:%d sendTime:%d\"\\n+        .format(requestObj, totalTime, queueTime, apiLocalTime, apiRemoteTime, responseSendTime))\\n     }\\n-    trace(\"Completed request: %s\".format(requestObj))\\n   }\\n   \\n   case class Response(processor: Int, request: Request, responseSend: Send) {']\n"
     ]
    }
   ],
   "source": [
    "query = sample_query(combined_df)\n",
    "print(f'Commid Message: {query[\"commit_message\"]}')\n",
    "print(f'Acutal Files Modified: {query[\"actual_files_modified\"]}')\n",
    "print(f'Diff Text: {query[\"diff_text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commid Message: KAFKA-13946; Add missing parameter to kraft test kit `ControllerNode.setMetadataDirectory()` (#12225)\n",
      "\n",
      "Added parameter `metadataDirectory` to `setMetadataDirectory()` so that `this.metadataDirectory` would not be set to itself.\n",
      "\n",
      "Reviewers: Kvicii <42023367+Kvicii@users.noreply.github.com>, dengziming <dengziming1993@gmail.com>, Jason Gustafson <jason@confluent.io>\n",
      "Acutal Files Modified: ['core/src/test/java/kafka/testkit/ControllerNode.java']\n",
      "Diff Text: ['@@ -27,7 +27,7 @@ public class ControllerNode implements TestKitNode {\\n             return this;\\n         }\\n \\n-        public Builder setMetadataDirectory() {\\n+        public Builder setMetadataDirectory(String metadataDirectory) {\\n             this.metadataDirectory = metadataDirectory;\\n             return this;\\n         }']\n"
     ]
    }
   ],
   "source": [
    "query = sample_query(combined_df, seed=24)\n",
    "print(f'Commid Message: {query[\"commit_message\"]}')\n",
    "print(f'Acutal Files Modified: {query[\"actual_files_modified\"]}')\n",
    "print(f'Diff Text: {query[\"diff_text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique commits: 10445\n"
     ]
    }
   ],
   "source": [
    "# find number of unique commits\n",
    "print(f'Number of unique commits: {len(combined_df.drop_duplicates(subset=\"commit_id\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commid Message: produce/fetch remote time metric not set correctly when num.acks = 1; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-584\n",
      "\n",
      "git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1402250 13f79535-47bb-0310-9956-ffa450edef68\n",
      "\n",
      "Acutal Files Modified: ['core/src/main/scala/kafka/network/RequestChannel.scala']\n",
      "Diff Text: ['@@ -40,9 +40,9 @@ object RequestChannel extends Logging {\\n   }\\n \\n   case class Request(processor: Int, requestKey: Any, buffer: ByteBuffer, startTimeMs: Long) {\\n-    var dequeueTimeMs = -1L\\n-    var apiLocalCompleteTimeMs = -1L\\n-    var responseCompleteTimeMs = -1L\\n+    @volatile var dequeueTimeMs = -1L\\n+    @volatile var apiLocalCompleteTimeMs = -1L\\n+    @volatile var responseCompleteTimeMs = -1L\\n     val requestId = buffer.getShort()\\n     val requestObj: RequestOrResponse = RequestKeys.deserializerForKey(requestId)(buffer)\\n     buffer.rewind()\\n@@ -50,6 +50,10 @@ object RequestChannel extends Logging {\\n \\n     def updateRequestMetrics() {\\n       val endTimeMs = SystemTime.milliseconds\\n+      // In some corner cases, apiLocalCompleteTimeMs may not be set when the request completes since the remote\\n+      // processing time is really small. In this case, use responseCompleteTimeMs as apiLocalCompleteTimeMs.\\n+      if (apiLocalCompleteTimeMs < 0)\\n+        apiLocalCompleteTimeMs = responseCompleteTimeMs\\n       val queueTime = (dequeueTimeMs - startTimeMs).max(0L)\\n       val apiLocalTime = (apiLocalCompleteTimeMs - dequeueTimeMs).max(0L)\\n       val apiRemoteTime = (responseCompleteTimeMs - apiLocalCompleteTimeMs).max(0L)\\n@@ -71,8 +75,9 @@ object RequestChannel extends Logging {\\n              m.responseSendTimeHist.update(responseSendTime)\\n              m.totalTimeHist.update(totalTime)\\n       }\\n+      trace(\"Completed request: %s totalTime:%d queueTime:%d localTime:%d remoteTime:%d sendTime:%d\"\\n+        .format(requestObj, totalTime, queueTime, apiLocalTime, apiRemoteTime, responseSendTime))\\n     }\\n-    trace(\"Completed request: %s\".format(requestObj))\\n   }\\n   \\n   case class Response(processor: Int, request: Request, responseSend: Send) {']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit Message: KAFKA-5505: Incremental cooperative rebalancing in Connect (KIP-415) (#6363)\n",
      "\n",
      "Added the incremental cooperative rebalancing in Connect to avoid global rebalances on all connectors and tasks with each new/changed/removed connector. This new protocol is backward compatible and will work with heterogeneous clusters that exist during a rolling upgrade, but once the clusters consist of new workers only some affected connectors and tasks will be rebalanced: connectors and tasks on existing nodes still in the cluster and not added/changed/removed will continue running while the affected connectors and tasks are rebalanced.\n",
      "\n",
      "This commit attempted to minimize the changes to the existing V0 protocol logic, though that was not entirely possible.\n",
      "\n",
      "This commit adds extensive unit and integration tests for both the old V0 protocol and the new v1 protocol. Soak testing has been performed multiple times to verify behavior while connectors and added, changed, and removed and while workers are added and removed from the cluster.\n",
      "\n",
      "Author: Konstantine Karantasis <konstantine@confluent.io>\n",
      "Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Robert Yokota <rayokota@gmail.com>, David Arthur <mumrah@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>\n",
      "Files Changed: clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java\n",
      "Diff: @@ -30,6 +30,8 @@ import java.util.List;\n",
      " import java.util.Locale;\n",
      " import java.util.Map;\n",
      " import java.util.Set;\n",
      "+import java.util.function.BiConsumer;\n",
      "+import java.util.function.Supplier;\n",
      " import java.util.regex.Pattern;\n",
      " \n",
      " /**\n",
      "@@ -952,6 +954,32 @@ public class ConfigDef {\n",
      "         }\n",
      "     }\n",
      " \n",
      "+    public static class LambdaValidator implements Validator {\n",
      "+        BiConsumer<String, Object> ensureValid;\n",
      "+        Supplier<String> toStringFunction;\n",
      "+\n",
      "+        private LambdaValidator(BiConsumer<String, Object> ensureValid,\n",
      "+                                Supplier<String> toStringFunction) {\n",
      "+            this.ensureValid = ensureValid;\n",
      "+            this.toStringFunction = toStringFunction;\n",
      "+        }\n",
      "+\n",
      "+        public static LambdaValidator with(BiConsumer<String, Object> ensureValid,\n",
      "+                                           Supplier<String> toStringFunction) {\n",
      "+            return new LambdaValidator(ensureValid, toStringFunction);\n",
      "+        }\n",
      "+\n",
      "+        @Override\n",
      "+        public void ensureValid(String name, Object value) {\n",
      "+            ensureValid.accept(name, value);\n",
      "+        }\n",
      "+\n",
      "+        @Override\n",
      "+        public String toString() {\n",
      "+            return toStringFunction.get();\n",
      "+        }\n",
      "+    }\n",
      "+\n",
      "     public static class CompositeValidator implements Validator {\n",
      "         private final List<Validator> validators;\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Commit Message:', query['commit_message'].values[0])\n",
    "print(f'Files Changed: {query[\"file_path\"].values[0]}')\n",
    "print(f'Diff: {query[\"diff\"].values[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevant, k):\n",
    "    return sum(relevant[:k]) / k\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(relevant):\n",
    "    for idx, value in enumerate(relevant):\n",
    "        if value == 1:\n",
    "            return 1 / (idx + 1)\n",
    "    return 0\n",
    "\n",
    "def calculate_average_precision(relevant):\n",
    "    pred_rel = [1] * len(relevant)\n",
    "    relevant_documents_count = 0\n",
    "    cumulative_precision = 0.0\n",
    "\n",
    "    # We iterate through the predicted relevance scores\n",
    "    for i in range(len(pred_rel)):\n",
    "        # Check if the prediction at this rank is correct (i.e., if it is a relevant document)\n",
    "        if pred_rel[i] == 1 and relevant[i] == 1:\n",
    "            relevant_documents_count += 1\n",
    "            precision_at_i = relevant_documents_count / (i + 1)\n",
    "            cumulative_precision += precision_at_i\n",
    "\n",
    "    # The average precision is the cumulative precision divided by the number of relevant documents\n",
    "    average_precision = cumulative_precision / sum(relevant) if sum(relevant) > 0 else 0\n",
    "    return average_precision\n",
    "\n",
    "# @staticmethod\n",
    "# def calculate_recall(relevant, total_modified_files, k):\n",
    "#   # Does not work for commit based approach as it can have multiple mentions of the same file across commits leading to a higher than 1 recall\n",
    "#     print(total_modified_files)\n",
    "#     print(relevant)\n",
    "#     return sum(relevant[:k]) / total_modified_files\n",
    "\n",
    "\n",
    "def calculate_recall(retrieved_files, actual_modified_files, relevant, k):\n",
    "    # this complicated mess is required as compared to the above much simpler code to support both commit-based and file-based approaches\n",
    "    # in file-based approach, this is equivalent to the above code\n",
    "    # in code-based approach, duplicates could be present in retrieved_files, which is why we need to filter them out (the above code would not work in this case)\n",
    "\n",
    "    return len({file for idx, file in enumerate(retrieved_files[:k])\n",
    "                    if relevant[idx] == 1\n",
    "                }) / len(actual_modified_files) if len(actual_modified_files) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "All file paths are distinct (because of BM25 modification from last time)\n",
      "actual_modified_files=['a', 'b', 'c', 'd', 'e', 'f']\n",
      "retrieved_files=['q', 'b', 'd', 'x', 'a', 'z', 'c']\n",
      "relevant=[0, 1, 1, 0, 1, 0, 1]\n",
      "P@3 = 0.6666666666666666\n",
      "MRR = 0.5\n",
      "MAP = 0.5845238095238094\n",
      "Recall@3 = 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "def demo():\n",
    "    # a demo to show how P, R, MAP, MRR are calculated\n",
    "\n",
    "    # this is for a single query, meaning a eval_commit_msg and it's corresponding actual_modified_files. this is taken from the data itself\n",
    "    print('Query 1')\n",
    "    print('All file paths are distinct (because of BM25 modification from last time)')\n",
    "    k = 3\n",
    "    actual_modified_files = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "    retrieved_files = ['q', 'b', 'd', 'x', 'a', 'z', 'c']\n",
    "    relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files] # relevant=[0, 1, 1, 0, 1, 0, 1]\n",
    "    print(f'{actual_modified_files=}')\n",
    "    print(f'{retrieved_files=}')\n",
    "    print(f'{relevant=}')\n",
    "    # P@k\n",
    "    # 2/3\n",
    "    p_at_k = precision_at_k(relevant, k)\n",
    "    print(f'P@{k} = {p_at_k}')\n",
    "\n",
    "    # MRR\n",
    "    # 1/2\n",
    "    mrr = mean_reciprocal_rank(relevant)\n",
    "    print(f'MRR = {mrr}')\n",
    "\n",
    "    # MAP\n",
    "    # (1/2 + 2/3 + 3/5 + 4/7) / 4\n",
    "    _map = calculate_average_precision(relevant)\n",
    "    print(f'MAP = {_map}')\n",
    "\n",
    "    # Recall@k\n",
    "    # 2/6\n",
    "    recall_at_k = calculate_recall(retrieved_files, actual_modified_files, relevant, k)\n",
    "    print(f'Recall@{k} = {recall_at_k}')\n",
    "\n",
    "    evaluation = {\n",
    "        'P@k': p_at_k,\n",
    "        'MRR': mrr,\n",
    "        'MAP': _map,\n",
    "        'Recall@k': recall_at_k\n",
    "    }\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "r1 = demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2\n",
      "All file paths are distinct (because of BM25 modification from last time)\n",
      "actual_modified_files=['p', 'q', 'r', 'a', 'b']\n",
      "retrieved_files=['r', 'b', 'p', 'u', 'a', 'q']\n",
      "relevant=[1, 1, 1, 0, 1, 1]\n",
      "P@3 = 1.0\n",
      "MRR = 1.0\n",
      "MAP = 0.9266666666666665\n",
      "Recall@3 = 0.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def demo2():\n",
    "    print('Query 2')\n",
    "    print('All file paths are distinct (because of BM25 modification from last time)')\n",
    "    k = 3\n",
    "    actual_modified_files = ['p', 'q', 'r', 'a', 'b']\n",
    "    retrieved_files = ['r', 'b', 'p', 'u', 'a', 'q']\n",
    "\n",
    "    print(f'{actual_modified_files=}')\n",
    "    print(f'{retrieved_files=}')\n",
    "    relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "    print(f'{relevant=}')\n",
    "    # P@k\n",
    "    p_at_k = precision_at_k(relevant, k)\n",
    "    print(f'P@{k} = {p_at_k}')\n",
    "\n",
    "    # MRR\n",
    "    mrr = mean_reciprocal_rank(relevant)\n",
    "    print(f'MRR = {mrr}')\n",
    "\n",
    "    # MAP\n",
    "    _map = calculate_average_precision(relevant)\n",
    "    print(f'MAP = {_map}')\n",
    "\n",
    "    # Recall@k\n",
    "    recall_at_k = calculate_recall(retrieved_files, actual_modified_files, relevant, k)\n",
    "    print(f'Recall@{k} = {recall_at_k}')\n",
    "\n",
    "    evaluation = {\n",
    "        'P@k': p_at_k,\n",
    "        'MRR': mrr,\n",
    "        'MAP': _map,\n",
    "        'Recall@k': recall_at_k\n",
    "    }\n",
    "    return evaluation\n",
    "\n",
    "r2 = demo2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "{'P@k': 0.6666666666666666, 'MRR': 0.5, 'MAP': 0.5845238095238094, 'Recall@k': 0.3333333333333333}\n",
      "Query 2\n",
      "{'P@k': 1.0, 'MRR': 1.0, 'MAP': 0.9266666666666665, 'Recall@k': 0.6}\n",
      "Macro Average\n",
      "{'P@k': 0.8333333333333333, 'MRR': 0.75, 'MAP': 0.755595238095238, 'Recall@k': 0.4666666666666667}\n"
     ]
    }
   ],
   "source": [
    "# Macro Average\n",
    "print('Query 1')\n",
    "print(r1)\n",
    "print('Query 2')\n",
    "print(r2)\n",
    "\n",
    "print('Macro Average')\n",
    "c1=  {k: (r1[k] + r2[k]) / 2 for k in r1.keys()}\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_positions={'q': [-1, 5], 'b': [1, 1], 'd': [2], 'x': [-1], 'a': [4, 4], 'z': [-1], 'c': [6], 'r': [0], 'p': [2], 'u': [-1]}\n",
      "File: q, Metrics: {'Average Precision': 0.0, 'Average Recall': 0.0, 'MRR': 0.16666666666666666}\n",
      "File: b, Metrics: {'Average Precision': 0.5, 'Average Recall': 1.0, 'MRR': 0.5}\n",
      "File: d, Metrics: {'Average Precision': 0.3333333333333333, 'Average Recall': 1.0, 'MRR': 0.3333333333333333}\n",
      "File: x, Metrics: {'Average Precision': 0, 'Average Recall': 0, 'MRR': 0}\n",
      "File: a, Metrics: {'Average Precision': 0.0, 'Average Recall': 0.0, 'MRR': 0.2}\n",
      "File: z, Metrics: {'Average Precision': 0, 'Average Recall': 0, 'MRR': 0}\n",
      "File: c, Metrics: {'Average Precision': 0.0, 'Average Recall': 0.0, 'MRR': 0.14285714285714285}\n",
      "File: r, Metrics: {'Average Precision': 1.0, 'Average Recall': 1.0, 'MRR': 1.0}\n",
      "File: p, Metrics: {'Average Precision': 0.3333333333333333, 'Average Recall': 1.0, 'MRR': 0.3333333333333333}\n",
      "File: u, Metrics: {'Average Precision': 0, 'Average Recall': 0, 'MRR': 0}\n",
      "Average Precision: 0.21666666666666665\n",
      "Average Recall: 0.4\n",
      "Average MRR: 0.2676190476190476\n"
     ]
    }
   ],
   "source": [
    "def file_based_demo():\n",
    "\n",
    "    def file_based_evaluation(retrievals, k):\n",
    "        # retrievals is a list of tuples: (file, [list of positions in retrieved results across different queries])\n",
    "        # Example: [('a', [5, -1]), ('b', [2, 1]), ...] where -1 indicates the file was not retrieved in that query\n",
    "\n",
    "        file_evaluations = {}\n",
    "\n",
    "        for file, positions in retrievals:\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            rr_list = []  # List for Reciprocal Rank calculations\n",
    "\n",
    "            for pos in positions:\n",
    "                if pos != -1:  # File is retrieved\n",
    "                    # Calculate precision and recall at k\n",
    "                    precision = 1 / (pos + 1) if pos < k else 0\n",
    "                    recall = 1 if pos < k else 0\n",
    "\n",
    "                    precisions.append(precision)\n",
    "                    recalls.append(recall)\n",
    "\n",
    "                    # Calculate Reciprocal Rank\n",
    "                    rr_list.append(1 / (pos + 1))\n",
    "\n",
    "            # Calculate average precision, recall, and mean reciprocal rank for the file\n",
    "            avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "            avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "            mrr = sum(rr_list) / len(rr_list) if rr_list else 0\n",
    "\n",
    "            file_evaluations[file] = {\n",
    "                'Average Precision': avg_precision,\n",
    "                'Average Recall': avg_recall,\n",
    "                'MRR': mrr\n",
    "            }\n",
    "\n",
    "        return file_evaluations\n",
    "\n",
    "\n",
    "    # get file_positions\n",
    "    queries = [\n",
    "        {\n",
    "            'actual_modified_files': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "            'retrieved_files': ['q', 'b', 'd', 'x', 'a', 'z', 'c']\n",
    "        },\n",
    "        {\n",
    "            'actual_modified_files': ['p', 'q', 'r', 'a', 'b'],\n",
    "            'retrieved_files': ['r', 'b', 'p', 'u', 'a', 'q']\n",
    "        }\n",
    "    ]\n",
    "    file_positions = {}\n",
    "    for query in queries:\n",
    "        actual_modified_files = query['actual_modified_files']\n",
    "        retrieved_files = query['retrieved_files']\n",
    "        relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "        for idx, file in enumerate(retrieved_files):\n",
    "            if file not in file_positions:\n",
    "                file_positions[file] = []\n",
    "            if relevant[idx] == 1:\n",
    "                file_positions[file].append(idx)\n",
    "            else:\n",
    "                file_positions[file].append(-1)\n",
    "\n",
    "    print(f'{file_positions=}')\n",
    "    k = 3\n",
    "\n",
    "    file_based_results = file_based_evaluation(file_positions.items(), k)\n",
    "\n",
    "    # Print the evaluation results for each file\n",
    "    for file, metrics in file_based_results.items():\n",
    "        print(f\"File: {file}, Metrics: {metrics}\")\n",
    "\n",
    "    # Print the average evaluation results for all files\n",
    "    avg_precision = sum([metrics['Average Precision'] for metrics in file_based_results.values()]) / len(file_based_results)\n",
    "    avg_recall = sum([metrics['Average Recall'] for metrics in file_based_results.values()]) / len(file_based_results)\n",
    "    avg_mrr = sum([metrics['MRR'] for metrics in file_based_results.values()]) / len(file_based_results)\n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average MRR: {avg_mrr}\")\n",
    "\n",
    "    return {\n",
    "        'Average Precision': avg_precision,\n",
    "        'Average Recall': avg_recall,\n",
    "        'MRR': avg_mrr\n",
    "    }\n",
    "\n",
    "c2 = file_based_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average (Commit/Query based)\n",
      "{'P@k': 0.8333333333333333, 'MRR': 0.75, 'MAP': 0.755595238095238, 'Recall@k': 0.4666666666666667}\n",
      "\n",
      "Macro Average (File based)\n",
      "{'Average Precision': 0.21666666666666665, 'Average Recall': 0.4, 'MRR': 0.2676190476190476}\n"
     ]
    }
   ],
   "source": [
    "print('Macro Average (Commit/Query based)')\n",
    "print(c1)\n",
    "\n",
    "print()\n",
    "print('Macro Average (File based)')\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_modified_files=['a', 'b', 'c', 'd', 'e', 'f']\n",
      "retrieved_files=['q', 'b', 'd', 'x', 'a', 'z', 'c']\n",
      "relevant=[0, 1, 1, 0, 1, 0, 1]\n",
      "MAP = 0.5845238095238094\n",
      "Sklearn AP\n",
      "Actually designed for binary classification, like\n",
      "y_true=[1, 0, 0, 1]\n",
      "y_pred=[0.1, 0.5, 0.35, 0.8]\n",
      "Sklean AP = 0.75\n",
      "p=0.5\n",
      "p=1.0\n",
      "p_list=[0.5, 1.0]\n",
      "Replicated AP = sum(p_list) / len(p_list)=0.75\n",
      "OLD MAP (with sklearn) = 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "def demo3():\n",
    "    # our MAP vs sklearn's AP\n",
    "    actual_modified_files = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "    retrieved_files = ['q', 'b', 'd', 'x', 'a', 'z', 'c']\n",
    "    relevant = [1 if file in actual_modified_files else 0 for file in retrieved_files]\n",
    "    print(f'{actual_modified_files=}')\n",
    "    print(f'{retrieved_files=}')\n",
    "    print(f'{relevant=}')\n",
    "\n",
    "    # MAP\n",
    "    # (1/2 + 2/3 + 3/5 + 4/7) / 4\n",
    "    _map = calculate_average_precision(relevant)\n",
    "    print(f'MAP = {_map}')\n",
    "\n",
    "\n",
    "    print('Sklearn AP')\n",
    "    print('Actually designed for binary classification, like')\n",
    "    y_true = [1, 0, 0, 1]\n",
    "    y_pred = [0.1, 0.5, 0.35, 0.8]\n",
    "\n",
    "    # print confusion matrix\n",
    "    # P = tp / (tp + fp)\n",
    "    print(f'{y_true=}')\n",
    "    print(f'{y_pred=}')\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    print(f'Sklean AP = {ap}')\n",
    "\n",
    "    def replicate_sklearn_ap():\n",
    "        p_list = []\n",
    "        for i, threshold in enumerate(y_pred):\n",
    "            if y_true[i] == 0:\n",
    "                continue\n",
    "            y_pred_binary = [1 if score >= threshold else 0 for score in y_pred]\n",
    "            # print(f'{threshold=}')\n",
    "            # print(f'{y_pred_binary=}')\n",
    "            # print(f'{confusion_matrix(y_true, y_pred_binary)=}')\n",
    "            tp = confusion_matrix(y_true, y_pred_binary)[1][1]\n",
    "            fp = confusion_matrix(y_true, y_pred_binary)[0][1]\n",
    "            p = tp / (tp + fp)\n",
    "            print(f'{p=}')\n",
    "            p_list.append(p)\n",
    "        print(f'{p_list=}')\n",
    "        print(f'Replicated AP = {sum(p_list) / len(p_list)=}')\n",
    "\n",
    "    replicate_sklearn_ap()\n",
    "\n",
    "\n",
    "    # Old MAP with sklearn:\n",
    "    # scores = [len(retrieved_files) - i for i in range(len(retrieved_files))]\n",
    "    # scores = [i for i in range(len(retrieved_files))]\n",
    "    ap = average_precision_score(relevant, [1 for i in range(len(retrieved_files))])\n",
    "    print(f'OLD MAP (with sklearn) = {ap}')\n",
    "\n",
    "demo3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "    # Prepare data: Map each file to its positions in retrieved results for each query\n",
    "    # file_positions = {\n",
    "    #     'a': [4, -1],  # File 'a' is at position 4 in first query, not retrieved in second query\n",
    "    #     'b': [1, 1],\n",
    "    #     'c': [6, -1],\n",
    "    #     'd': [2, -1],\n",
    "    #     'e': [-1, -1],\n",
    "    #     'f': [-1, -1],\n",
    "    #     'p': [-1, 2],\n",
    "    #     'q': [0, 5],\n",
    "    #     'r': [-1, 0],\n",
    "    #     'u': [-1, 3],\n",
    "    #     'x': [3, -1],\n",
    "    #     'z': [5, -1]\n",
    "    # }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
