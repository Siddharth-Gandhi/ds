[{"commit_id": "766a30bc6e9a1c69ce007bb69caabf4c6062f0e9", "commit_message": "Merge pull request #391 from karpathy/revert-389-realtime\n\nRevert \"Minor fix: Use CLOCK_MONOTONIC instead of CLOCK_REALTIME\"", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_MONOTONIC, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "0bf29817937bea04fc4e0f0a91f2bbfbcf7986fc", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -726,7 +726,7 @@ int sample(Sampler* sampler, float* logits) {\n long time_in_ms() {\n     // return time in milliseconds, for benchmarking the model speed\n     struct timespec time;\n-    clock_gettime(CLOCK_MONOTONIC, &time);\n+    clock_gettime(CLOCK_REALTIME, &time);\n     return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n }\n ", "status": "modified"}, {"commit_id": "38c58ac336544ba08a814dea89135b0d07fd7450", "commit_message": "Revert \"Minor fix: Use CLOCK_MONOTONIC instead of CLOCK_REALTIME\"", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_MONOTONIC, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "0bf29817937bea04fc4e0f0a91f2bbfbcf7986fc", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -726,7 +726,7 @@ int sample(Sampler* sampler, float* logits) {\n long time_in_ms() {\n     // return time in milliseconds, for benchmarking the model speed\n     struct timespec time;\n-    clock_gettime(CLOCK_MONOTONIC, &time);\n+    clock_gettime(CLOCK_REALTIME, &time);\n     return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n }\n ", "status": "modified"}, {"commit_id": "d15577b541375fcf69cf808e8ec6a0e49d6fa60e", "commit_message": "Merge pull request #389 from jbochi/realtime\n\nMinor fix: Use CLOCK_MONOTONIC instead of CLOCK_REALTIME", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "35deb5e0fa55f0a257040bcf1624ed8386e63dc7", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_MONOTONIC, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -726,7 +726,7 @@ int sample(Sampler* sampler, float* logits) {\n long time_in_ms() {\n     // return time in milliseconds, for benchmarking the model speed\n     struct timespec time;\n-    clock_gettime(CLOCK_REALTIME, &time);\n+    clock_gettime(CLOCK_MONOTONIC, &time);\n     return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n }\n ", "status": "modified"}, {"commit_id": "9d73a377fb1aee70c0b8c335ce3d06eb12ae74c8", "commit_message": "Use CLOCK_MONOTONIC instead of realtime", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->wq = ptr;\n    ptr += p->n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += p->n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += p->n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(int l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f; // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;        // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;          // number of steps to run for\n    char *prompt = \"\";        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    generate(&transformer, &tokenizer, &sampler, prompt, steps);\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "d7cd98633dcc50c9e58f4b39b105fe9f9494cf85", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->wq = ptr;\n    ptr += p->n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += p->n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += p->n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(int l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_MONOTONIC, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f; // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;        // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;          // number of steps to run for\n    char *prompt = \"\";        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    generate(&transformer, &tokenizer, &sampler, prompt, steps);\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -724,7 +724,7 @@ int sample(Sampler* sampler, float* logits) {\n long time_in_ms() {\n     // return time in milliseconds, for benchmarking the model speed\n     struct timespec time;\n-    clock_gettime(CLOCK_REALTIME, &time);\n+    clock_gettime(CLOCK_MONOTONIC, &time);\n     return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n }\n ", "status": "modified"}, {"commit_id": "3b0a6d39272f11ddee1ba143a3ada7bd67a11cec", "commit_message": "Merge pull request #380 from dfurrer/master\n\nRemove duplicate word in comment. EOM", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "358fe991504d17d537274cfd76487127660fdd86", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -756,7 +756,7 @@ void generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n         // forward the transformer to get logits for the next token\n         float* logits = forward(transformer, token, pos);\n \n-        // advance the state state machine\n+        // advance the state machine\n         if (pos < num_prompt_tokens - 1) {\n             // if we are still processing the input prompt, force the next prompt token\n             next = prompt_tokens[pos + 1];", "status": "modified"}, {"commit_id": "a69ee269c5e7c4ed06c3fc8c56b66ef22438edb3", "commit_message": "Update run.c\n\nRemove duplicate word in comments.", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "b9fb86169f56bd787bb644c62a80bbab56f8dccc", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -756,7 +756,7 @@ void generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n         // forward the transformer to get logits for the next token\n         float* logits = forward(transformer, token, pos);\n \n-        // advance the state state machine\n+        // advance the state machine\n         if (pos < num_prompt_tokens - 1) {\n             // if we are still processing the input prompt, force the next prompt token\n             next = prompt_tokens[pos + 1];", "status": "modified"}, {"commit_id": "b9fb86169f56bd787bb644c62a80bbab56f8dccc", "commit_message": "Merge pull request #367 from janimo/long-multiply\n\nDo parameter count calculations in 64 bits to not overflow in case of\u2026", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->wq = ptr;\n    ptr += p->n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += p->n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += p->n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(int l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "0776f8679309111a044b16c0194e7d6cf87b0c4b", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -115,26 +115,28 @@ void free_run_state(RunState* s) {\n \n void memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n     int head_size = p->dim / p->n_heads;\n+    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n+    unsigned long long n_layers = p->n_layers;\n     w->token_embedding_table = ptr;\n     ptr += p->vocab_size * p->dim;\n     w->rms_att_weight = ptr;\n-    ptr += p->n_layers * p->dim;\n+    ptr += n_layers * p->dim;\n     w->wq = ptr;\n-    ptr += p->n_layers * p->dim * (p->n_heads * head_size);\n+    ptr += n_layers * p->dim * (p->n_heads * head_size);\n     w->wk = ptr;\n-    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n+    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n     w->wv = ptr;\n-    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n+    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n     w->wo = ptr;\n-    ptr += p->n_layers * (p->n_heads * head_size) * p->dim;\n+    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n     w->rms_ffn_weight = ptr;\n-    ptr += p->n_layers * p->dim;\n+    ptr += n_layers * p->dim;\n     w->w1 = ptr;\n-    ptr += p->n_layers * p->dim * p->hidden_dim;\n+    ptr += n_layers * p->dim * p->hidden_dim;\n     w->w2 = ptr;\n-    ptr += p->n_layers * p->hidden_dim * p->dim;\n+    ptr += n_layers * p->hidden_dim * p->dim;\n     w->w3 = ptr;\n-    ptr += p->n_layers * p->dim * p->hidden_dim;\n+    ptr += n_layers * p->dim * p->hidden_dim;\n     w->rms_final_weight = ptr;\n     ptr += p->dim;\n     ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n@@ -249,7 +251,7 @@ float* forward(Transformer* transformer, int token, int pos) {\n     memcpy(x, content_row, dim*sizeof(*x));\n \n     // forward all the layers\n-    for(int l = 0; l < p->n_layers; l++) {\n+    for(unsigned long long l = 0; l < p->n_layers; l++) {\n \n         // attention rmsnorm\n         rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);", "status": "modified"}, {"commit_id": "0776f8679309111a044b16c0194e7d6cf87b0c4b", "commit_message": "Merge pull request #374 from SpaceCowboy850/bugfix_train_vocab\n\nSetting an encoding for tiny_file tokenizer file.", "relative_path": "tinystories.py", "previous_code_file": "\"\"\"\nDownload, preprocess and serve the TinyStories dataset as a DataLoader.\n\"\"\"\n\nimport argparse\nimport glob\nimport json\nimport os\nimport random\nfrom typing import List\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nimport requests\nimport sentencepiece as spm\nimport torch\nimport torch.distributed as dist\nfrom tqdm import tqdm\n\nfrom tokenizer import Tokenizer\n\nDATA_CACHE_DIR = \"data\"\n\ndef download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"Helper function to download a file from a given url\"\"\"\n    resp = requests.get(url, stream=True)\n    total = int(resp.headers.get(\"content-length\", 0))\n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,\n        total=total,\n        unit=\"iB\",\n        unit_scale=True,\n        unit_divisor=1024,\n    ) as bar:\n        for data in resp.iter_content(chunk_size=chunk_size):\n            size = file.write(data)\n            bar.update(size)\n\n\ndef download():\n    \"\"\"Downloads the TinyStories dataset to DATA_CACHE_DIR\"\"\"\n    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n\n    # download the TinyStories dataset, unless it's already downloaded\n    data_url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\"\n    data_filename = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data.tar.gz\")\n    if not os.path.exists(data_filename):\n        print(f\"Downloading {data_url} to {data_filename}...\")\n        download_file(data_url, data_filename)\n    else:\n        print(f\"{data_filename} already exists, skipping download...\")\n\n    # unpack the tar.gz file into all the data shards (json files)\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir, exist_ok=True)\n        print(f\"Unpacking {data_filename}...\")\n        os.system(f\"tar -xzf {data_filename} -C {data_dir}\")\n    else:\n        print(f\"{data_dir} already exists, skipping unpacking...\")\n\n    # print a single example just for debugging and such\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    with open(shard_filenames[0], \"r\") as f:\n        data = json.load(f)\n    print(\"Download done.\")\n    print(f\"Number of shards: {len(shard_filenames)}\")\n    print(f\"Example story:\\n{data[0]}\")\n\ndef train_vocab(vocab_size):\n    \"\"\"\n    Trains a custom sentencepiece tokenizer on the TinyStories dataset.\n    The custom tokenizer files will be saved in DATA_CACHE_DIR/tok{N} directories,\n    where N is the vocab size. This is also where the pretok .bin files will go.\n    \"\"\"\n    assert vocab_size > 0, \"Vocab size must be positive\"\n\n    # output file prefix path for sentencepiece\n    prefix = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n\n    # how many shards we'll use for vocab training, kept low for efficiency\n    num_shards = 10\n\n    # 1) export a large chunk of text as a single text file tiny.txt\n    tiny_file = os.path.join(DATA_CACHE_DIR, \"tiny.txt\")\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n\n    print(f\"Writing temporary file {tiny_file} with {num_shards} shards...\")\n    with open(tiny_file, \"w\") as of:\n        for shard in tqdm(shard_filenames[:num_shards]):\n            with open(shard, \"r\") as f:\n                data = json.load(f)\n            for example in data:\n                text = example[\"story\"]\n                text = text.strip()\n                of.write(text + \"\\n\")\n    print(f\"Size is: {os.path.getsize(tiny_file) / 1024 / 1024:.2f} MB\")\n\n    # 2) train the sentencepiece model\n    print(\"Will now train the vocab...\")\n    spm.SentencePieceTrainer.train(input=tiny_file,\n                                   model_prefix=prefix,\n                                   model_type=\"bpe\",\n                                   vocab_size=vocab_size,\n                                   self_test_sample_size=0,\n                                   input_format=\"text\",\n                                   character_coverage=1.0,\n                                   num_threads=os.cpu_count(),\n                                   split_digits=True,\n                                   allow_whitespace_only_pieces=True,\n                                   byte_fallback=True,\n                                   unk_surface=r\" \\342\\201\\207 \",\n                                   normalization_rule_name=\"identity\")\n\n    # 3) optional cleanup, ask the user if they'd like to delete tiny.txt\n    dec = input(f\"Delete the temporary file {tiny_file}? [y/N] \")\n    if dec.lower() == \"y\":\n        os.remove(tiny_file)\n        print(f\"Deleted {tiny_file}\")\n\n    print(f\"Trained tokenizer is in {prefix}.model\")\n    print(\"Done.\")\n\n\ndef process_shard(args, vocab_size):\n    shard_id, shard = args\n    tokenizer_model = get_tokenizer_model_path(vocab_size)\n    enc = Tokenizer(tokenizer_model)\n    with open(shard, \"r\") as f:\n        data = json.load(f)\n    all_tokens = []\n    for example in tqdm(data, position=shard_id):\n        text = example[\"story\"]\n        text = text.strip()  # get rid of leading/trailing whitespace\n        tokens = enc.encode(text, bos=True, eos=False)  # encode the text, use BOS\n        all_tokens.extend(tokens)\n    # convert to uint16 nparray\n    all_tokens = np.array(all_tokens, dtype=np.uint16)\n    # calculate the output filename\n    if vocab_size == 0:\n        # if we're using Llama 2, just save the tokenized file in the same dir\n        tokenized_filename = shard.replace(\".json\", \".bin\")\n    else:\n        # save .bin files into a new tok{N} directory\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        shard_basename = os.path.basename(shard)\n        bin_basename = shard_basename.replace(\".json\", \".bin\")\n        tokenized_filename = os.path.join(bin_dir, bin_basename)\n    # write the bytes\n    with open(tokenized_filename, \"wb\") as f:\n        f.write(all_tokens.tobytes())\n    # calculate the average sequence length (they are separated by BOS=1)\n    avg_seq_len = all_tokens.size / ((all_tokens == 1).sum())\n    print(f\"Saved {tokenized_filename}, average seqlen: {avg_seq_len:.2f}\")\n\n\ndef pretokenize(vocab_size):\n    # iterate the shards and tokenize all of them one by one\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    if vocab_size > 0:\n        # .bin files will be saved into tok{N} directory, create it once here\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        os.makedirs(bin_dir, exist_ok=True)\n\n    # process all the shards in a process pool\n    fun = partial(process_shard, vocab_size=vocab_size)\n    with ProcessPoolExecutor() as executor:\n        executor.map(fun, enumerate(shard_filenames))\n    print(\"Done.\")\n\n\nclass PretokDataset(torch.utils.data.IterableDataset):\n    \"\"\"Loads pretokenized examples from disk and yields them as PyTorch tensors.\"\"\"\n\n    def __init__(self, split, max_seq_len, vocab_size, vocab_source):\n        super().__init__()\n        self.split = split\n        self.max_seq_len = max_seq_len\n        self.vocab_size = vocab_size\n        self.vocab_source = vocab_source\n\n    def __iter__(self):\n        # get worker info within a DataLoader\n        worker_info = torch.utils.data.get_worker_info()\n        worker_id = worker_info.id if worker_info else 0\n        # get DDP rank info\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        # combine the worker_id and worker_rank to create a unique seed for rng\n        seed = 42 + worker_id + 1337 * rank\n        rng = random.Random(seed)\n        print(f\"Created a PretokDataset with rng seed {seed}\")\n        if self.vocab_source == \"llama2\":\n            # the .bin files are right along the .json files\n            bin_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        elif self.vocab_source == \"custom\":\n            # the .bin files are in tok{N} directory\n            bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{self.vocab_size}\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        # train/test split. let's use only shard 0 for test split, rest train\n        shard_filenames = shard_filenames[1:] if self.split == \"train\" else shard_filenames[:1]\n        assert len(shard_filenames)>0, f\"No bin files found in {bin_dir}\"\n        while True:\n            rng.shuffle(shard_filenames)\n            for shard in shard_filenames:\n                # open the dataset for reading but keep it on disk with memmap\n                m = np.memmap(shard, dtype=np.uint16, mode=\"r\")\n                num_batches = len(m) // self.max_seq_len\n                num_batches -= 1  # drop the last partial batch\n                assert num_batches > 0, \"this shard is way too small? investigate.\"\n                ixs = list(range(num_batches))\n                rng.shuffle(ixs)\n                for ix in ixs:\n                    start = ix * self.max_seq_len\n                    end = start + self.max_seq_len + 1\n                    # calling .astype will copy the data into a new numpy array, now in RAM\n                    chunk = torch.from_numpy((m[start:end]).astype(np.int64))\n                    x = chunk[:-1]\n                    y = chunk[1:]\n                    yield x, y\n\n# -----------------------------------------------------------------------------\n# public interface functions\n\ndef get_tokenizer_model_path(vocab_size):\n    \"\"\"\n    Returns path to the sentencepiece tokenizer model for a given vocab size\n    vocab_size = 0 designates the default Llama 2 tokenizer, in that case\n    None is returned.\n    \"\"\"\n    if vocab_size == 0:\n        return None\n    else:\n        return os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}.model\")\n\nclass Task:\n\n    @staticmethod\n    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):\n        ds = PretokDataset(**dataset_kwargs)\n        dl = torch.utils.data.DataLoader(\n            ds, batch_size=batch_size, pin_memory=True, num_workers=num_workers\n        )\n        for x, y in dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            yield x, y\n\n# -----------------------------------------------------------------------------\n# CLI for constructing the dataset\n\nif __name__ == \"__main__\":\n    \"\"\"\n    These stages are designed to be run in order.\n\n    To tokenize data with the Llama 2 tokenizer:\n    python tinystories.py download\n    python tinystories.py pretokenize\n\n    To tokenize data with a custom tokenizer we train ourselves with sentencepiece, e.g.:\n    python tinystories.py download\n    python tinystories.py train_vocab --vocab_size=2048\n    python tinystories.py pretokenize --vocab_size=2048\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"stage\", type=str, choices=[\"download\", \"pretokenize\", \"train_vocab\"])\n    parser.add_argument(\"--vocab_size\", type=int, default=0, help=\"pretokenization vocab size. 0 = use Llama 2 tokenizer.\")\n    args = parser.parse_args()\n\n    # depending on the stage call the appropriate function\n    if args.stage == \"download\":\n        download()\n    elif args.stage == \"train_vocab\":\n        train_vocab(vocab_size=args.vocab_size)\n    elif args.stage == \"pretokenize\":\n        pretokenize(vocab_size=args.vocab_size)\n    else:\n        raise ValueError(f\"Unknown stage {args.stage}\")\n", "previous_id": "7325bab657406c427e7c1ca6575bace9a5982744", "new_code_file": "\"\"\"\nDownload, preprocess and serve the TinyStories dataset as a DataLoader.\n\"\"\"\n\nimport argparse\nimport glob\nimport json\nimport os\nimport random\nfrom typing import List\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nimport requests\nimport sentencepiece as spm\nimport torch\nimport torch.distributed as dist\nfrom tqdm import tqdm\n\nfrom tokenizer import Tokenizer\n\nDATA_CACHE_DIR = \"data\"\n\ndef download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"Helper function to download a file from a given url\"\"\"\n    resp = requests.get(url, stream=True)\n    total = int(resp.headers.get(\"content-length\", 0))\n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,\n        total=total,\n        unit=\"iB\",\n        unit_scale=True,\n        unit_divisor=1024,\n    ) as bar:\n        for data in resp.iter_content(chunk_size=chunk_size):\n            size = file.write(data)\n            bar.update(size)\n\n\ndef download():\n    \"\"\"Downloads the TinyStories dataset to DATA_CACHE_DIR\"\"\"\n    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n\n    # download the TinyStories dataset, unless it's already downloaded\n    data_url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\"\n    data_filename = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data.tar.gz\")\n    if not os.path.exists(data_filename):\n        print(f\"Downloading {data_url} to {data_filename}...\")\n        download_file(data_url, data_filename)\n    else:\n        print(f\"{data_filename} already exists, skipping download...\")\n\n    # unpack the tar.gz file into all the data shards (json files)\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir, exist_ok=True)\n        print(f\"Unpacking {data_filename}...\")\n        os.system(f\"tar -xzf {data_filename} -C {data_dir}\")\n    else:\n        print(f\"{data_dir} already exists, skipping unpacking...\")\n\n    # print a single example just for debugging and such\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    with open(shard_filenames[0], \"r\") as f:\n        data = json.load(f)\n    print(\"Download done.\")\n    print(f\"Number of shards: {len(shard_filenames)}\")\n    print(f\"Example story:\\n{data[0]}\")\n\ndef train_vocab(vocab_size):\n    \"\"\"\n    Trains a custom sentencepiece tokenizer on the TinyStories dataset.\n    The custom tokenizer files will be saved in DATA_CACHE_DIR/tok{N} directories,\n    where N is the vocab size. This is also where the pretok .bin files will go.\n    \"\"\"\n    assert vocab_size > 0, \"Vocab size must be positive\"\n\n    # output file prefix path for sentencepiece\n    prefix = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n\n    # how many shards we'll use for vocab training, kept low for efficiency\n    num_shards = 10\n\n    # 1) export a large chunk of text as a single text file tiny.txt\n    tiny_file = os.path.join(DATA_CACHE_DIR, \"tiny.txt\")\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n\n    print(f\"Writing temporary file {tiny_file} with {num_shards} shards...\")\n    with open(tiny_file, \"w\", encoding=\"utf-8\") as of:\n        for shard in tqdm(shard_filenames[:num_shards]):\n            with open(shard, \"r\") as f:\n                data = json.load(f)\n            for example in data:\n                text = example[\"story\"]\n                text = text.strip()\n                of.write(text + \"\\n\")\n    print(f\"Size is: {os.path.getsize(tiny_file) / 1024 / 1024:.2f} MB\")\n\n    # 2) train the sentencepiece model\n    print(\"Will now train the vocab...\")\n    spm.SentencePieceTrainer.train(input=tiny_file,\n                                   model_prefix=prefix,\n                                   model_type=\"bpe\",\n                                   vocab_size=vocab_size,\n                                   self_test_sample_size=0,\n                                   input_format=\"text\",\n                                   character_coverage=1.0,\n                                   num_threads=os.cpu_count(),\n                                   split_digits=True,\n                                   allow_whitespace_only_pieces=True,\n                                   byte_fallback=True,\n                                   unk_surface=r\" \\342\\201\\207 \",\n                                   normalization_rule_name=\"identity\")\n\n    # 3) optional cleanup, ask the user if they'd like to delete tiny.txt\n    dec = input(f\"Delete the temporary file {tiny_file}? [y/N] \")\n    if dec.lower() == \"y\":\n        os.remove(tiny_file)\n        print(f\"Deleted {tiny_file}\")\n\n    print(f\"Trained tokenizer is in {prefix}.model\")\n    print(\"Done.\")\n\n\ndef process_shard(args, vocab_size):\n    shard_id, shard = args\n    tokenizer_model = get_tokenizer_model_path(vocab_size)\n    enc = Tokenizer(tokenizer_model)\n    with open(shard, \"r\") as f:\n        data = json.load(f)\n    all_tokens = []\n    for example in tqdm(data, position=shard_id):\n        text = example[\"story\"]\n        text = text.strip()  # get rid of leading/trailing whitespace\n        tokens = enc.encode(text, bos=True, eos=False)  # encode the text, use BOS\n        all_tokens.extend(tokens)\n    # convert to uint16 nparray\n    all_tokens = np.array(all_tokens, dtype=np.uint16)\n    # calculate the output filename\n    if vocab_size == 0:\n        # if we're using Llama 2, just save the tokenized file in the same dir\n        tokenized_filename = shard.replace(\".json\", \".bin\")\n    else:\n        # save .bin files into a new tok{N} directory\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        shard_basename = os.path.basename(shard)\n        bin_basename = shard_basename.replace(\".json\", \".bin\")\n        tokenized_filename = os.path.join(bin_dir, bin_basename)\n    # write the bytes\n    with open(tokenized_filename, \"wb\") as f:\n        f.write(all_tokens.tobytes())\n    # calculate the average sequence length (they are separated by BOS=1)\n    avg_seq_len = all_tokens.size / ((all_tokens == 1).sum())\n    print(f\"Saved {tokenized_filename}, average seqlen: {avg_seq_len:.2f}\")\n\n\ndef pretokenize(vocab_size):\n    # iterate the shards and tokenize all of them one by one\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    if vocab_size > 0:\n        # .bin files will be saved into tok{N} directory, create it once here\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        os.makedirs(bin_dir, exist_ok=True)\n\n    # process all the shards in a process pool\n    fun = partial(process_shard, vocab_size=vocab_size)\n    with ProcessPoolExecutor() as executor:\n        executor.map(fun, enumerate(shard_filenames))\n    print(\"Done.\")\n\n\nclass PretokDataset(torch.utils.data.IterableDataset):\n    \"\"\"Loads pretokenized examples from disk and yields them as PyTorch tensors.\"\"\"\n\n    def __init__(self, split, max_seq_len, vocab_size, vocab_source):\n        super().__init__()\n        self.split = split\n        self.max_seq_len = max_seq_len\n        self.vocab_size = vocab_size\n        self.vocab_source = vocab_source\n\n    def __iter__(self):\n        # get worker info within a DataLoader\n        worker_info = torch.utils.data.get_worker_info()\n        worker_id = worker_info.id if worker_info else 0\n        # get DDP rank info\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        # combine the worker_id and worker_rank to create a unique seed for rng\n        seed = 42 + worker_id + 1337 * rank\n        rng = random.Random(seed)\n        print(f\"Created a PretokDataset with rng seed {seed}\")\n        if self.vocab_source == \"llama2\":\n            # the .bin files are right along the .json files\n            bin_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        elif self.vocab_source == \"custom\":\n            # the .bin files are in tok{N} directory\n            bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{self.vocab_size}\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        # train/test split. let's use only shard 0 for test split, rest train\n        shard_filenames = shard_filenames[1:] if self.split == \"train\" else shard_filenames[:1]\n        assert len(shard_filenames)>0, f\"No bin files found in {bin_dir}\"\n        while True:\n            rng.shuffle(shard_filenames)\n            for shard in shard_filenames:\n                # open the dataset for reading but keep it on disk with memmap\n                m = np.memmap(shard, dtype=np.uint16, mode=\"r\")\n                num_batches = len(m) // self.max_seq_len\n                num_batches -= 1  # drop the last partial batch\n                assert num_batches > 0, \"this shard is way too small? investigate.\"\n                ixs = list(range(num_batches))\n                rng.shuffle(ixs)\n                for ix in ixs:\n                    start = ix * self.max_seq_len\n                    end = start + self.max_seq_len + 1\n                    # calling .astype will copy the data into a new numpy array, now in RAM\n                    chunk = torch.from_numpy((m[start:end]).astype(np.int64))\n                    x = chunk[:-1]\n                    y = chunk[1:]\n                    yield x, y\n\n# -----------------------------------------------------------------------------\n# public interface functions\n\ndef get_tokenizer_model_path(vocab_size):\n    \"\"\"\n    Returns path to the sentencepiece tokenizer model for a given vocab size\n    vocab_size = 0 designates the default Llama 2 tokenizer, in that case\n    None is returned.\n    \"\"\"\n    if vocab_size == 0:\n        return None\n    else:\n        return os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}.model\")\n\nclass Task:\n\n    @staticmethod\n    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):\n        ds = PretokDataset(**dataset_kwargs)\n        dl = torch.utils.data.DataLoader(\n            ds, batch_size=batch_size, pin_memory=True, num_workers=num_workers\n        )\n        for x, y in dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            yield x, y\n\n# -----------------------------------------------------------------------------\n# CLI for constructing the dataset\n\nif __name__ == \"__main__\":\n    \"\"\"\n    These stages are designed to be run in order.\n\n    To tokenize data with the Llama 2 tokenizer:\n    python tinystories.py download\n    python tinystories.py pretokenize\n\n    To tokenize data with a custom tokenizer we train ourselves with sentencepiece, e.g.:\n    python tinystories.py download\n    python tinystories.py train_vocab --vocab_size=2048\n    python tinystories.py pretokenize --vocab_size=2048\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"stage\", type=str, choices=[\"download\", \"pretokenize\", \"train_vocab\"])\n    parser.add_argument(\"--vocab_size\", type=int, default=0, help=\"pretokenization vocab size. 0 = use Llama 2 tokenizer.\")\n    args = parser.parse_args()\n\n    # depending on the stage call the appropriate function\n    if args.stage == \"download\":\n        download()\n    elif args.stage == \"train_vocab\":\n        train_vocab(vocab_size=args.vocab_size)\n    elif args.stage == \"pretokenize\":\n        pretokenize(vocab_size=args.vocab_size)\n    else:\n        raise ValueError(f\"Unknown stage {args.stage}\")\n", "diff": "@@ -88,7 +88,7 @@ def train_vocab(vocab_size):\n     shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n \n     print(f\"Writing temporary file {tiny_file} with {num_shards} shards...\")\n-    with open(tiny_file, \"w\") as of:\n+    with open(tiny_file, \"w\", encoding=\"utf-8\") as of:\n         for shard in tqdm(shard_filenames[:num_shards]):\n             with open(shard, \"r\") as f:\n                 data = json.load(f)", "status": "modified"}, {"commit_id": "ab19aa08045f0f30db4291641ece301d7cc339f3", "commit_message": "Setting UTF encoding, otherwise windows breaks with UnicodeEncodeError: 'charmap' codec can't encode character '\\u200b' in position 971: character maps to <undefined>", "relative_path": "tinystories.py", "previous_code_file": "\"\"\"\nDownload, preprocess and serve the TinyStories dataset as a DataLoader.\n\"\"\"\n\nimport argparse\nimport glob\nimport json\nimport os\nimport random\nfrom typing import List\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nimport requests\nimport sentencepiece as spm\nimport torch\nimport torch.distributed as dist\nfrom tqdm import tqdm\n\nfrom tokenizer import Tokenizer\n\nDATA_CACHE_DIR = \"data\"\n\ndef download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"Helper function to download a file from a given url\"\"\"\n    resp = requests.get(url, stream=True)\n    total = int(resp.headers.get(\"content-length\", 0))\n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,\n        total=total,\n        unit=\"iB\",\n        unit_scale=True,\n        unit_divisor=1024,\n    ) as bar:\n        for data in resp.iter_content(chunk_size=chunk_size):\n            size = file.write(data)\n            bar.update(size)\n\n\ndef download():\n    \"\"\"Downloads the TinyStories dataset to DATA_CACHE_DIR\"\"\"\n    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n\n    # download the TinyStories dataset, unless it's already downloaded\n    data_url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\"\n    data_filename = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data.tar.gz\")\n    if not os.path.exists(data_filename):\n        print(f\"Downloading {data_url} to {data_filename}...\")\n        download_file(data_url, data_filename)\n    else:\n        print(f\"{data_filename} already exists, skipping download...\")\n\n    # unpack the tar.gz file into all the data shards (json files)\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir, exist_ok=True)\n        print(f\"Unpacking {data_filename}...\")\n        os.system(f\"tar -xzf {data_filename} -C {data_dir}\")\n    else:\n        print(f\"{data_dir} already exists, skipping unpacking...\")\n\n    # print a single example just for debugging and such\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    with open(shard_filenames[0], \"r\") as f:\n        data = json.load(f)\n    print(\"Download done.\")\n    print(f\"Number of shards: {len(shard_filenames)}\")\n    print(f\"Example story:\\n{data[0]}\")\n\ndef train_vocab(vocab_size):\n    \"\"\"\n    Trains a custom sentencepiece tokenizer on the TinyStories dataset.\n    The custom tokenizer files will be saved in DATA_CACHE_DIR/tok{N} directories,\n    where N is the vocab size. This is also where the pretok .bin files will go.\n    \"\"\"\n    assert vocab_size > 0, \"Vocab size must be positive\"\n\n    # output file prefix path for sentencepiece\n    prefix = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n\n    # how many shards we'll use for vocab training, kept low for efficiency\n    num_shards = 10\n\n    # 1) export a large chunk of text as a single text file tiny.txt\n    tiny_file = os.path.join(DATA_CACHE_DIR, \"tiny.txt\")\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n\n    print(f\"Writing temporary file {tiny_file} with {num_shards} shards...\")\n    with open(tiny_file, \"w\") as of:\n        for shard in tqdm(shard_filenames[:num_shards]):\n            with open(shard, \"r\") as f:\n                data = json.load(f)\n            for example in data:\n                text = example[\"story\"]\n                text = text.strip()\n                of.write(text + \"\\n\")\n    print(f\"Size is: {os.path.getsize(tiny_file) / 1024 / 1024:.2f} MB\")\n\n    # 2) train the sentencepiece model\n    print(\"Will now train the vocab...\")\n    spm.SentencePieceTrainer.train(input=tiny_file,\n                                   model_prefix=prefix,\n                                   model_type=\"bpe\",\n                                   vocab_size=vocab_size,\n                                   self_test_sample_size=0,\n                                   input_format=\"text\",\n                                   character_coverage=1.0,\n                                   num_threads=os.cpu_count(),\n                                   split_digits=True,\n                                   allow_whitespace_only_pieces=True,\n                                   byte_fallback=True,\n                                   unk_surface=r\" \\342\\201\\207 \",\n                                   normalization_rule_name=\"identity\")\n\n    # 3) optional cleanup, ask the user if they'd like to delete tiny.txt\n    dec = input(f\"Delete the temporary file {tiny_file}? [y/N] \")\n    if dec.lower() == \"y\":\n        os.remove(tiny_file)\n        print(f\"Deleted {tiny_file}\")\n\n    print(f\"Trained tokenizer is in {prefix}.model\")\n    print(\"Done.\")\n\n\ndef process_shard(args, vocab_size):\n    shard_id, shard = args\n    tokenizer_model = get_tokenizer_model_path(vocab_size)\n    enc = Tokenizer(tokenizer_model)\n    with open(shard, \"r\") as f:\n        data = json.load(f)\n    all_tokens = []\n    for example in tqdm(data, position=shard_id):\n        text = example[\"story\"]\n        text = text.strip()  # get rid of leading/trailing whitespace\n        tokens = enc.encode(text, bos=True, eos=False)  # encode the text, use BOS\n        all_tokens.extend(tokens)\n    # convert to uint16 nparray\n    all_tokens = np.array(all_tokens, dtype=np.uint16)\n    # calculate the output filename\n    if vocab_size == 0:\n        # if we're using Llama 2, just save the tokenized file in the same dir\n        tokenized_filename = shard.replace(\".json\", \".bin\")\n    else:\n        # save .bin files into a new tok{N} directory\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        shard_basename = os.path.basename(shard)\n        bin_basename = shard_basename.replace(\".json\", \".bin\")\n        tokenized_filename = os.path.join(bin_dir, bin_basename)\n    # write the bytes\n    with open(tokenized_filename, \"wb\") as f:\n        f.write(all_tokens.tobytes())\n    # calculate the average sequence length (they are separated by BOS=1)\n    avg_seq_len = all_tokens.size / ((all_tokens == 1).sum())\n    print(f\"Saved {tokenized_filename}, average seqlen: {avg_seq_len:.2f}\")\n\n\ndef pretokenize(vocab_size):\n    # iterate the shards and tokenize all of them one by one\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    if vocab_size > 0:\n        # .bin files will be saved into tok{N} directory, create it once here\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        os.makedirs(bin_dir, exist_ok=True)\n\n    # process all the shards in a process pool\n    fun = partial(process_shard, vocab_size=vocab_size)\n    with ProcessPoolExecutor() as executor:\n        executor.map(fun, enumerate(shard_filenames))\n    print(\"Done.\")\n\n\nclass PretokDataset(torch.utils.data.IterableDataset):\n    \"\"\"Loads pretokenized examples from disk and yields them as PyTorch tensors.\"\"\"\n\n    def __init__(self, split, max_seq_len, vocab_size, vocab_source):\n        super().__init__()\n        self.split = split\n        self.max_seq_len = max_seq_len\n        self.vocab_size = vocab_size\n        self.vocab_source = vocab_source\n\n    def __iter__(self):\n        # get worker info within a DataLoader\n        worker_info = torch.utils.data.get_worker_info()\n        worker_id = worker_info.id if worker_info else 0\n        # get DDP rank info\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        # combine the worker_id and worker_rank to create a unique seed for rng\n        seed = 42 + worker_id + 1337 * rank\n        rng = random.Random(seed)\n        print(f\"Created a PretokDataset with rng seed {seed}\")\n        if self.vocab_source == \"llama2\":\n            # the .bin files are right along the .json files\n            bin_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        elif self.vocab_source == \"custom\":\n            # the .bin files are in tok{N} directory\n            bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{self.vocab_size}\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        # train/test split. let's use only shard 0 for test split, rest train\n        shard_filenames = shard_filenames[1:] if self.split == \"train\" else shard_filenames[:1]\n        assert len(shard_filenames)>0, f\"No bin files found in {bin_dir}\"\n        while True:\n            rng.shuffle(shard_filenames)\n            for shard in shard_filenames:\n                # open the dataset for reading but keep it on disk with memmap\n                m = np.memmap(shard, dtype=np.uint16, mode=\"r\")\n                num_batches = len(m) // self.max_seq_len\n                num_batches -= 1  # drop the last partial batch\n                assert num_batches > 0, \"this shard is way too small? investigate.\"\n                ixs = list(range(num_batches))\n                rng.shuffle(ixs)\n                for ix in ixs:\n                    start = ix * self.max_seq_len\n                    end = start + self.max_seq_len + 1\n                    # calling .astype will copy the data into a new numpy array, now in RAM\n                    chunk = torch.from_numpy((m[start:end]).astype(np.int64))\n                    x = chunk[:-1]\n                    y = chunk[1:]\n                    yield x, y\n\n# -----------------------------------------------------------------------------\n# public interface functions\n\ndef get_tokenizer_model_path(vocab_size):\n    \"\"\"\n    Returns path to the sentencepiece tokenizer model for a given vocab size\n    vocab_size = 0 designates the default Llama 2 tokenizer, in that case\n    None is returned.\n    \"\"\"\n    if vocab_size == 0:\n        return None\n    else:\n        return os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}.model\")\n\nclass Task:\n\n    @staticmethod\n    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):\n        ds = PretokDataset(**dataset_kwargs)\n        dl = torch.utils.data.DataLoader(\n            ds, batch_size=batch_size, pin_memory=True, num_workers=num_workers\n        )\n        for x, y in dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            yield x, y\n\n# -----------------------------------------------------------------------------\n# CLI for constructing the dataset\n\nif __name__ == \"__main__\":\n    \"\"\"\n    These stages are designed to be run in order.\n\n    To tokenize data with the Llama 2 tokenizer:\n    python tinystories.py download\n    python tinystories.py pretokenize\n\n    To tokenize data with a custom tokenizer we train ourselves with sentencepiece, e.g.:\n    python tinystories.py download\n    python tinystories.py train_vocab --vocab_size=2048\n    python tinystories.py pretokenize --vocab_size=2048\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"stage\", type=str, choices=[\"download\", \"pretokenize\", \"train_vocab\"])\n    parser.add_argument(\"--vocab_size\", type=int, default=0, help=\"pretokenization vocab size. 0 = use Llama 2 tokenizer.\")\n    args = parser.parse_args()\n\n    # depending on the stage call the appropriate function\n    if args.stage == \"download\":\n        download()\n    elif args.stage == \"train_vocab\":\n        train_vocab(vocab_size=args.vocab_size)\n    elif args.stage == \"pretokenize\":\n        pretokenize(vocab_size=args.vocab_size)\n    else:\n        raise ValueError(f\"Unknown stage {args.stage}\")\n", "previous_id": "7325bab657406c427e7c1ca6575bace9a5982744", "new_code_file": "\"\"\"\nDownload, preprocess and serve the TinyStories dataset as a DataLoader.\n\"\"\"\n\nimport argparse\nimport glob\nimport json\nimport os\nimport random\nfrom typing import List\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nimport requests\nimport sentencepiece as spm\nimport torch\nimport torch.distributed as dist\nfrom tqdm import tqdm\n\nfrom tokenizer import Tokenizer\n\nDATA_CACHE_DIR = \"data\"\n\ndef download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"Helper function to download a file from a given url\"\"\"\n    resp = requests.get(url, stream=True)\n    total = int(resp.headers.get(\"content-length\", 0))\n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,\n        total=total,\n        unit=\"iB\",\n        unit_scale=True,\n        unit_divisor=1024,\n    ) as bar:\n        for data in resp.iter_content(chunk_size=chunk_size):\n            size = file.write(data)\n            bar.update(size)\n\n\ndef download():\n    \"\"\"Downloads the TinyStories dataset to DATA_CACHE_DIR\"\"\"\n    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n\n    # download the TinyStories dataset, unless it's already downloaded\n    data_url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\"\n    data_filename = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data.tar.gz\")\n    if not os.path.exists(data_filename):\n        print(f\"Downloading {data_url} to {data_filename}...\")\n        download_file(data_url, data_filename)\n    else:\n        print(f\"{data_filename} already exists, skipping download...\")\n\n    # unpack the tar.gz file into all the data shards (json files)\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir, exist_ok=True)\n        print(f\"Unpacking {data_filename}...\")\n        os.system(f\"tar -xzf {data_filename} -C {data_dir}\")\n    else:\n        print(f\"{data_dir} already exists, skipping unpacking...\")\n\n    # print a single example just for debugging and such\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    with open(shard_filenames[0], \"r\") as f:\n        data = json.load(f)\n    print(\"Download done.\")\n    print(f\"Number of shards: {len(shard_filenames)}\")\n    print(f\"Example story:\\n{data[0]}\")\n\ndef train_vocab(vocab_size):\n    \"\"\"\n    Trains a custom sentencepiece tokenizer on the TinyStories dataset.\n    The custom tokenizer files will be saved in DATA_CACHE_DIR/tok{N} directories,\n    where N is the vocab size. This is also where the pretok .bin files will go.\n    \"\"\"\n    assert vocab_size > 0, \"Vocab size must be positive\"\n\n    # output file prefix path for sentencepiece\n    prefix = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n\n    # how many shards we'll use for vocab training, kept low for efficiency\n    num_shards = 10\n\n    # 1) export a large chunk of text as a single text file tiny.txt\n    tiny_file = os.path.join(DATA_CACHE_DIR, \"tiny.txt\")\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n\n    print(f\"Writing temporary file {tiny_file} with {num_shards} shards...\")\n    with open(tiny_file, \"w\", encoding=\"utf-8\") as of:\n        for shard in tqdm(shard_filenames[:num_shards]):\n            with open(shard, \"r\") as f:\n                data = json.load(f)\n            for example in data:\n                text = example[\"story\"]\n                text = text.strip()\n                of.write(text + \"\\n\")\n    print(f\"Size is: {os.path.getsize(tiny_file) / 1024 / 1024:.2f} MB\")\n\n    # 2) train the sentencepiece model\n    print(\"Will now train the vocab...\")\n    spm.SentencePieceTrainer.train(input=tiny_file,\n                                   model_prefix=prefix,\n                                   model_type=\"bpe\",\n                                   vocab_size=vocab_size,\n                                   self_test_sample_size=0,\n                                   input_format=\"text\",\n                                   character_coverage=1.0,\n                                   num_threads=os.cpu_count(),\n                                   split_digits=True,\n                                   allow_whitespace_only_pieces=True,\n                                   byte_fallback=True,\n                                   unk_surface=r\" \\342\\201\\207 \",\n                                   normalization_rule_name=\"identity\")\n\n    # 3) optional cleanup, ask the user if they'd like to delete tiny.txt\n    dec = input(f\"Delete the temporary file {tiny_file}? [y/N] \")\n    if dec.lower() == \"y\":\n        os.remove(tiny_file)\n        print(f\"Deleted {tiny_file}\")\n\n    print(f\"Trained tokenizer is in {prefix}.model\")\n    print(\"Done.\")\n\n\ndef process_shard(args, vocab_size):\n    shard_id, shard = args\n    tokenizer_model = get_tokenizer_model_path(vocab_size)\n    enc = Tokenizer(tokenizer_model)\n    with open(shard, \"r\") as f:\n        data = json.load(f)\n    all_tokens = []\n    for example in tqdm(data, position=shard_id):\n        text = example[\"story\"]\n        text = text.strip()  # get rid of leading/trailing whitespace\n        tokens = enc.encode(text, bos=True, eos=False)  # encode the text, use BOS\n        all_tokens.extend(tokens)\n    # convert to uint16 nparray\n    all_tokens = np.array(all_tokens, dtype=np.uint16)\n    # calculate the output filename\n    if vocab_size == 0:\n        # if we're using Llama 2, just save the tokenized file in the same dir\n        tokenized_filename = shard.replace(\".json\", \".bin\")\n    else:\n        # save .bin files into a new tok{N} directory\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        shard_basename = os.path.basename(shard)\n        bin_basename = shard_basename.replace(\".json\", \".bin\")\n        tokenized_filename = os.path.join(bin_dir, bin_basename)\n    # write the bytes\n    with open(tokenized_filename, \"wb\") as f:\n        f.write(all_tokens.tobytes())\n    # calculate the average sequence length (they are separated by BOS=1)\n    avg_seq_len = all_tokens.size / ((all_tokens == 1).sum())\n    print(f\"Saved {tokenized_filename}, average seqlen: {avg_seq_len:.2f}\")\n\n\ndef pretokenize(vocab_size):\n    # iterate the shards and tokenize all of them one by one\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n    if vocab_size > 0:\n        # .bin files will be saved into tok{N} directory, create it once here\n        bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n        os.makedirs(bin_dir, exist_ok=True)\n\n    # process all the shards in a process pool\n    fun = partial(process_shard, vocab_size=vocab_size)\n    with ProcessPoolExecutor() as executor:\n        executor.map(fun, enumerate(shard_filenames))\n    print(\"Done.\")\n\n\nclass PretokDataset(torch.utils.data.IterableDataset):\n    \"\"\"Loads pretokenized examples from disk and yields them as PyTorch tensors.\"\"\"\n\n    def __init__(self, split, max_seq_len, vocab_size, vocab_source):\n        super().__init__()\n        self.split = split\n        self.max_seq_len = max_seq_len\n        self.vocab_size = vocab_size\n        self.vocab_source = vocab_source\n\n    def __iter__(self):\n        # get worker info within a DataLoader\n        worker_info = torch.utils.data.get_worker_info()\n        worker_id = worker_info.id if worker_info else 0\n        # get DDP rank info\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        # combine the worker_id and worker_rank to create a unique seed for rng\n        seed = 42 + worker_id + 1337 * rank\n        rng = random.Random(seed)\n        print(f\"Created a PretokDataset with rng seed {seed}\")\n        if self.vocab_source == \"llama2\":\n            # the .bin files are right along the .json files\n            bin_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        elif self.vocab_source == \"custom\":\n            # the .bin files are in tok{N} directory\n            bin_dir = os.path.join(DATA_CACHE_DIR, f\"tok{self.vocab_size}\")\n            shard_filenames = sorted(glob.glob(os.path.join(bin_dir, \"*.bin\")))\n        # train/test split. let's use only shard 0 for test split, rest train\n        shard_filenames = shard_filenames[1:] if self.split == \"train\" else shard_filenames[:1]\n        assert len(shard_filenames)>0, f\"No bin files found in {bin_dir}\"\n        while True:\n            rng.shuffle(shard_filenames)\n            for shard in shard_filenames:\n                # open the dataset for reading but keep it on disk with memmap\n                m = np.memmap(shard, dtype=np.uint16, mode=\"r\")\n                num_batches = len(m) // self.max_seq_len\n                num_batches -= 1  # drop the last partial batch\n                assert num_batches > 0, \"this shard is way too small? investigate.\"\n                ixs = list(range(num_batches))\n                rng.shuffle(ixs)\n                for ix in ixs:\n                    start = ix * self.max_seq_len\n                    end = start + self.max_seq_len + 1\n                    # calling .astype will copy the data into a new numpy array, now in RAM\n                    chunk = torch.from_numpy((m[start:end]).astype(np.int64))\n                    x = chunk[:-1]\n                    y = chunk[1:]\n                    yield x, y\n\n# -----------------------------------------------------------------------------\n# public interface functions\n\ndef get_tokenizer_model_path(vocab_size):\n    \"\"\"\n    Returns path to the sentencepiece tokenizer model for a given vocab size\n    vocab_size = 0 designates the default Llama 2 tokenizer, in that case\n    None is returned.\n    \"\"\"\n    if vocab_size == 0:\n        return None\n    else:\n        return os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}.model\")\n\nclass Task:\n\n    @staticmethod\n    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):\n        ds = PretokDataset(**dataset_kwargs)\n        dl = torch.utils.data.DataLoader(\n            ds, batch_size=batch_size, pin_memory=True, num_workers=num_workers\n        )\n        for x, y in dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            yield x, y\n\n# -----------------------------------------------------------------------------\n# CLI for constructing the dataset\n\nif __name__ == \"__main__\":\n    \"\"\"\n    These stages are designed to be run in order.\n\n    To tokenize data with the Llama 2 tokenizer:\n    python tinystories.py download\n    python tinystories.py pretokenize\n\n    To tokenize data with a custom tokenizer we train ourselves with sentencepiece, e.g.:\n    python tinystories.py download\n    python tinystories.py train_vocab --vocab_size=2048\n    python tinystories.py pretokenize --vocab_size=2048\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"stage\", type=str, choices=[\"download\", \"pretokenize\", \"train_vocab\"])\n    parser.add_argument(\"--vocab_size\", type=int, default=0, help=\"pretokenization vocab size. 0 = use Llama 2 tokenizer.\")\n    args = parser.parse_args()\n\n    # depending on the stage call the appropriate function\n    if args.stage == \"download\":\n        download()\n    elif args.stage == \"train_vocab\":\n        train_vocab(vocab_size=args.vocab_size)\n    elif args.stage == \"pretokenize\":\n        pretokenize(vocab_size=args.vocab_size)\n    else:\n        raise ValueError(f\"Unknown stage {args.stage}\")\n", "diff": "@@ -88,7 +88,7 @@ def train_vocab(vocab_size):\n     shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))\n \n     print(f\"Writing temporary file {tiny_file} with {num_shards} shards...\")\n-    with open(tiny_file, \"w\") as of:\n+    with open(tiny_file, \"w\", encoding=\"utf-8\") as of:\n         for shard in tqdm(shard_filenames[:num_shards]):\n             with open(shard, \"r\") as f:\n                 data = json.load(f)", "status": "modified"}, {"commit_id": "c5ec6e21b8659d6d3500a2af3ac1dfe7f3e19ae1", "commit_message": "Use long long so it works with MSVC", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "1ebb27f090e10117964f1fa54a0be32d10a5a6e1", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -116,7 +116,7 @@ void free_run_state(RunState* s) {\n void memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n     int head_size = p->dim / p->n_heads;\n     // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n-    unsigned long n_layers = p->n_layers;\n+    unsigned long long n_layers = p->n_layers;\n     w->token_embedding_table = ptr;\n     ptr += p->vocab_size * p->dim;\n     w->rms_att_weight = ptr;\n@@ -251,7 +251,7 @@ float* forward(Transformer* transformer, int token, int pos) {\n     memcpy(x, content_row, dim*sizeof(*x));\n \n     // forward all the layers\n-    for(unsigned long l = 0; l < p->n_layers; l++) {\n+    for(unsigned long long l = 0; l < p->n_layers; l++) {\n \n         // attention rmsnorm\n         rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);", "status": "modified"}, {"commit_id": "1ebb27f090e10117964f1fa54a0be32d10a5a6e1", "commit_message": "Do parameter count calculations in 64 bits to not overflow in case of very large models", "relative_path": "run.c", "previous_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->wq = ptr;\n    ptr += p->n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += p->n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += p->n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += p->n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += p->n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(int l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "previous_id": "7325bab657406c427e7c1ca6575bace9a5982744", "new_code_file": "/* Inference for Llama-2 Transformer model in pure C */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <time.h>\n#include <math.h>\n#include <string.h>\n#include <fcntl.h>\n#if defined _WIN32\n    #include \"win.h\"\n#else\n    #include <unistd.h>\n    #include <sys/mman.h>\n#endif\n// ----------------------------------------------------------------------------\n// Transformer model\n\ntypedef struct {\n    int dim; // transformer dimension\n    int hidden_dim; // for ffn layers\n    int n_layers; // number of layers\n    int n_heads; // number of query heads\n    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n    int vocab_size; // vocabulary size, usually 256 (byte-level)\n    int seq_len; // max sequence length\n} Config;\n\ntypedef struct {\n    // token embedding table\n    float* token_embedding_table;    // (vocab_size, dim)\n    // weights for rmsnorms\n    float* rms_att_weight; // (layer, dim) rmsnorm weights\n    float* rms_ffn_weight; // (layer, dim)\n    // weights for matmuls. note dim == n_heads * head_size\n    float* wq; // (layer, dim, n_heads * head_size)\n    float* wk; // (layer, dim, n_kv_heads * head_size)\n    float* wv; // (layer, dim, n_kv_heads * head_size)\n    float* wo; // (layer, n_heads * head_size, dim)\n    // weights for ffn\n    float* w1; // (layer, hidden_dim, dim)\n    float* w2; // (layer, dim, hidden_dim)\n    float* w3; // (layer, hidden_dim, dim)\n    // final rmsnorm\n    float* rms_final_weight; // (dim,)\n    // (optional) classifier weights for the logits, on the last layer\n    float* wcls;\n} TransformerWeights;\n\ntypedef struct {\n    // current wave of activations\n    float *x; // activation at current time stamp (dim,)\n    float *xb; // same, but inside a residual branch (dim,)\n    float *xb2; // an additional buffer just for convenience (dim,)\n    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n    float *q; // query (dim,)\n    float *k; // key (dim,)\n    float *v; // value (dim,)\n    float *att; // buffer for scores/attention values (n_heads, seq_len)\n    float *logits; // output logits\n    // kv cache\n    float* key_cache;   // (layer, seq_len, dim)\n    float* value_cache; // (layer, seq_len, dim)\n} RunState;\n\ntypedef struct {\n    Config config; // the hyperparameters of the architecture (the blueprint)\n    TransformerWeights weights; // the weights of the model\n    RunState state; // buffers for the \"wave\" of activations in the forward pass\n    // some more state needed to properly clean up the memory mapping (sigh)\n    int fd; // file descriptor for memory mapping\n    float* data; // memory mapped data pointer\n    ssize_t file_size; // size of the checkpoint file in bytes\n} Transformer;\n\nvoid malloc_run_state(RunState* s, Config* p) {\n    // we calloc instead of malloc to keep valgrind happy\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    s->x = calloc(p->dim, sizeof(float));\n    s->xb = calloc(p->dim, sizeof(float));\n    s->xb2 = calloc(p->dim, sizeof(float));\n    s->hb = calloc(p->hidden_dim, sizeof(float));\n    s->hb2 = calloc(p->hidden_dim, sizeof(float));\n    s->q = calloc(p->dim, sizeof(float));\n    s->k = calloc(kv_dim, sizeof(float));\n    s->v = calloc(kv_dim, sizeof(float));\n    s->att = calloc(p->n_heads * p->seq_len, sizeof(float));\n    s->logits = calloc(p->vocab_size, sizeof(float));\n    s->key_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n    // ensure all mallocs went fine\n    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n     || !s->k || !s->v || !s->att || !s->logits || !s->key_cache\n     || !s->value_cache) {\n        fprintf(stderr, \"malloc failed!\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid free_run_state(RunState* s) {\n    free(s->x);\n    free(s->xb);\n    free(s->xb2);\n    free(s->hb);\n    free(s->hb2);\n    free(s->q);\n    free(s->k);\n    free(s->v);\n    free(s->att);\n    free(s->logits);\n    free(s->key_cache);\n    free(s->value_cache);\n}\n\nvoid memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n    int head_size = p->dim / p->n_heads;\n    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n    unsigned long n_layers = p->n_layers;\n    w->token_embedding_table = ptr;\n    ptr += p->vocab_size * p->dim;\n    w->rms_att_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->wq = ptr;\n    ptr += n_layers * p->dim * (p->n_heads * head_size);\n    w->wk = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wv = ptr;\n    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n    w->wo = ptr;\n    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n    w->rms_ffn_weight = ptr;\n    ptr += n_layers * p->dim;\n    w->w1 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->w2 = ptr;\n    ptr += n_layers * p->hidden_dim * p->dim;\n    w->w3 = ptr;\n    ptr += n_layers * p->dim * p->hidden_dim;\n    w->rms_final_weight = ptr;\n    ptr += p->dim;\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n}\n\nvoid read_checkpoint(char* checkpoint, Config* config, TransformerWeights* weights,\n                     int* fd, float** data, ssize_t* file_size) {\n    FILE *file = fopen(checkpoint, \"rb\");\n    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n    // read in the config header\n    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n    config->vocab_size = abs(config->vocab_size);\n    // figure out the file size\n    fseek(file, 0, SEEK_END); // move file pointer to end of file\n    *file_size = ftell(file); // get the file size, in bytes\n    fclose(file);\n    // memory map the Transformer weights into the data pointer\n    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n    memory_map_weights(weights, config, weights_ptr, shared_weights);\n}\n\nvoid build_transformer(Transformer *t, char* checkpoint_path) {\n    // read in the Config and the Weights from the checkpoint\n    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n    // allocate the RunState buffers\n    malloc_run_state(&t->state, &t->config);\n}\n\nvoid free_transformer(Transformer* t) {\n    // close the memory mapping\n    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n    if (t->fd != -1) { close(t->fd); }\n    // free the RunState buffers\n    free_run_state(&t->state);\n}\n\n// ----------------------------------------------------------------------------\n// neural net blocks; the dynamics of the Transformer\n\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\n    // calculate sum of squares\n    float ss = 0.0f;\n    for (int j = 0; j < size; j++) {\n        ss += x[j] * x[j];\n    }\n    ss /= size;\n    ss += 1e-5f;\n    ss = 1.0f / sqrtf(ss);\n    // normalize and scale\n    for (int j = 0; j < size; j++) {\n        o[j] = weight[j] * (ss * x[j]);\n    }\n}\n\nvoid softmax(float* x, int size) {\n    // find max value (for numerical stability)\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) {\n            max_val = x[i];\n        }\n    }\n    // exp and sum\n    float sum = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum += x[i];\n    }\n    // normalize\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum;\n    }\n}\n\nvoid matmul(float* xout, float* x, float* w, int n, int d) {\n    // W (d,n) @ x (n,) -> xout (d,)\n    // by far the most amount of time is spent inside this little function\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < d; i++) {\n        float val = 0.0f;\n        for (int j = 0; j < n; j++) {\n            val += w[i * n + j] * x[j];\n        }\n        xout[i] = val;\n    }\n}\n\nfloat* forward(Transformer* transformer, int token, int pos) {\n\n    // a few convenience variables\n    Config* p = &transformer->config;\n    TransformerWeights* w = &transformer->weights;\n    RunState* s = &transformer->state;\n    float *x = s->x;\n    int dim = p->dim;\n    int kv_dim = (p->dim * p->n_kv_heads) / p->n_heads;\n    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n    int hidden_dim =  p->hidden_dim;\n    int head_size = dim / p->n_heads;\n\n    // copy the token embedding into x\n    float* content_row = w->token_embedding_table + token * dim;\n    memcpy(x, content_row, dim*sizeof(*x));\n\n    // forward all the layers\n    for(unsigned long l = 0; l < p->n_layers; l++) {\n\n        // attention rmsnorm\n        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n\n        // qkv matmuls for this position\n        matmul(s->q, s->xb, w->wq + l*dim*dim, dim, dim);\n        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n\n        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n        for (int i = 0; i < dim; i+=2) {\n            int head_dim = i % head_size;\n            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n            float val = pos * freq;\n            float fcr = cosf(val);\n            float fci = sinf(val);\n            int rotn = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n            for (int v = 0; v < rotn; v++) {\n                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n                float v0 = vec[i];\n                float v1 = vec[i+1];\n                vec[i]   = v0 * fcr - v1 * fci;\n                vec[i+1] = v0 * fci + v1 * fcr;\n            }\n        }\n\n        // save key,value at this time step (pos) to our kv cache\n        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n        float* key_cache_row = s->key_cache + loff + pos * kv_dim;\n        float* value_cache_row = s->value_cache + loff + pos * kv_dim;\n        memcpy(key_cache_row, s->k, kv_dim * sizeof(*key_cache_row));\n        memcpy(value_cache_row, s->v, kv_dim * sizeof(*value_cache_row));\n\n        // multihead attention. iterate over all heads\n        int h;\n        #pragma omp parallel for private(h)\n        for (h = 0; h < p->n_heads; h++) {\n            // get the query vector for this head\n            float* q = s->q + h * head_size;\n            // attention scores for this head\n            float* att = s->att + h * p->seq_len;\n            // iterate over all timesteps, including the current one\n            for (int t = 0; t <= pos; t++) {\n                // get the key vector for this head and at this timestep\n                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // calculate the attention score as the dot product of q and k\n                float score = 0.0f;\n                for (int i = 0; i < head_size; i++) {\n                    score += q[i] * k[i];\n                }\n                score /= sqrtf(head_size);\n                // save the score to the attention buffer\n                att[t] = score;\n            }\n\n            // softmax the scores to get attention weights, from 0..pos inclusively\n            softmax(att, pos + 1);\n\n            // weighted sum of the values, store back into xb\n            float* xb = s->xb + h * head_size;\n            memset(xb, 0, head_size * sizeof(float));\n            for (int t = 0; t <= pos; t++) {\n                // get the value vector for this head and at this timestep\n                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n                // get the attention weight for this timestep\n                float a = att[t];\n                // accumulate the weighted value into xb\n                for (int i = 0; i < head_size; i++) {\n                    xb[i] += a * v[i];\n                }\n            }\n        }\n\n        // final matmul to get the output of the attention\n        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n\n        // residual connection back into x\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb2[i];\n        }\n\n        // ffn rmsnorm\n        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n\n        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n        // first calculate self.w1(x) and self.w3(x)\n        matmul(s->hb, s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n\n        // SwiGLU non-linearity\n        for (int i = 0; i < hidden_dim; i++) {\n            float val = s->hb[i];\n            // silu(x)=x*\u03c3(x), where \u03c3(x) is the logistic sigmoid\n            val *= (1.0f / (1.0f + expf(-val)));\n            // elementwise multiply with w3(x)\n            val *= s->hb2[i];\n            s->hb[i] = val;\n        }\n\n        // final matmul to get the output of the ffn\n        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n\n        // residual connection\n        for (int i = 0; i < dim; i++) {\n            x[i] += s->xb[i];\n        }\n    }\n\n    // final rmsnorm\n    rmsnorm(x, x, w->rms_final_weight, dim);\n\n    // classifier into logits\n    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n    return s->logits;\n}\n\n// ----------------------------------------------------------------------------\n// The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n\ntypedef struct {\n    char *str;\n    int id;\n} TokenIndex;\n\ntypedef struct {\n    char** vocab;\n    float* vocab_scores;\n    TokenIndex *sorted_vocab;\n    int vocab_size;\n    unsigned int max_token_length;\n    unsigned char byte_pieces[512]; // stores all single-byte strings\n} Tokenizer;\n\nint compare_tokens(const void *a, const void *b) {\n    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n}\n\nvoid build_tokenizer(Tokenizer* t, char* tokenizer_path, int vocab_size) {\n    // i should have written the vocab_size into the tokenizer file... sigh\n    t->vocab_size = vocab_size;\n    // malloc space to hold the scores and the strings\n    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n    t->sorted_vocab = NULL; // initialized lazily\n    for (int i = 0; i < 256; i++) {\n        t->byte_pieces[i * 2] = (unsigned char)i;\n        t->byte_pieces[i * 2 + 1] = '\\0';\n    }\n    // read in the file\n    FILE *file = fopen(tokenizer_path, \"rb\");\n    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n    int len;\n    for (int i = 0; i < vocab_size; i++) {\n        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE);}\n        if (fread(&len, sizeof(int), 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i] = (char *)malloc(len + 1);\n        if (fread(t->vocab[i], len, 1, file) != 1) { fprintf(stderr, \"failed read\\n\"); exit(EXIT_FAILURE); }\n        t->vocab[i][len] = '\\0'; // add the string terminating token\n    }\n    fclose(file);\n}\n\nvoid free_tokenizer(Tokenizer* t) {\n    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n    free(t->vocab);\n    free(t->vocab_scores);\n    free(t->sorted_vocab);\n}\n\nchar* decode(Tokenizer* t, int prev_token, int token) {\n    char *piece = t->vocab[token];\n    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n    // parse this and convert and return the actual byte\n    unsigned char byte_val;\n    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) {\n        piece = (char*)t->byte_pieces + byte_val * 2;\n    }\n    return piece;\n}\n\nvoid safe_printf(char *piece) {\n    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n    // because some of the other bytes can be various control codes, backspace, etc.\n    if (piece == NULL) { return; }\n    if (piece[0] == '\\0') { return; }\n    if (piece[1] == '\\0') {\n        unsigned char byte_val = piece[0];\n        if (!(isprint(byte_val) || isspace(byte_val))) {\n            return; // bad byte, don't print it\n        }\n    }\n    printf(\"%s\", piece);\n}\n\nint str_lookup(char *str, TokenIndex *sorted_vocab, int vocab_size) {\n    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n    TokenIndex tok = { .str = str }; // acts as the key to search for\n    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n    return res != NULL ? res->id : -1;\n}\n\nvoid encode(Tokenizer* t, char *text, int8_t bos, int8_t eos, int *tokens, int *n_tokens) {\n    // encode the string text (input) into an upper-bound preallocated tokens[] array\n    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n\n    if (t->sorted_vocab == NULL) {\n        // lazily malloc and sort the vocabulary\n        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n        for (int i = 0; i < t->vocab_size; i++) {\n            t->sorted_vocab[i].str = t->vocab[i];\n            t->sorted_vocab[i].id = i;\n        }\n        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n    }\n\n    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n    size_t str_len = 0;\n\n    // start at 0 tokens\n    *n_tokens = 0;\n\n    // add optional BOS (=1) token, if desired\n    if (bos) tokens[(*n_tokens)++] = 1;\n\n    // add_dummy_prefix is true by default\n    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n    // TODO: pretty sure this isn't correct in the general case but I don't have the\n    // energy to read more of the sentencepiece code to figure out what it's doing\n    if (text[0] != '\\0') {\n        int dummy_prefix = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n        tokens[(*n_tokens)++] = dummy_prefix;\n    }\n\n    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n    // Code point \u2194 UTF-8 conversion\n    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n    // U+0000\tU+007F\t    0xxxxxxx\n    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n\n    // process the raw (UTF-8) byte sequence of the input string\n    for (char *c = text; *c != '\\0'; c++) {\n\n        // reset buffer if the current byte is ASCII or a leading byte\n        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n        // 0x80 is 10000000\n        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n        // so in English this is: \"if this byte is not a continuation byte\"\n        if ((*c & 0xC0) != 0x80) {\n            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n            // => reset our location, as we're starting a new UTF-8 codepoint\n            str_len = 0;\n        }\n\n        // append the current byte to the buffer\n        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n        str_buffer[str_len] = '\\0';\n\n        // while the next character is a continuation byte, continue appending\n        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n            continue;\n        }\n\n        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n\n        if (id != -1) {\n            // we found this codepoint in vocab, add it as a token\n            tokens[(*n_tokens)++] = id;\n        } else {\n            // byte_fallback encoding: just encode each byte as a token\n            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n            // so the individual bytes only start at index 3\n            for (int i=0; i < str_len; i++) {\n                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n            }\n        }\n        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n    }\n\n    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n    while (1) {\n        float best_score = -1e10;\n        int best_id = -1;\n        int best_idx = -1;\n\n        for (int i=0; i < (*n_tokens-1); i++) {\n            // check if we can merge the pair (tokens[i], tokens[i+1])\n            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n            if (id != -1 && t->vocab_scores[id] > best_score) {\n                // this merge pair exists in vocab! record its score and position\n                best_score = t->vocab_scores[id];\n                best_id = id;\n                best_idx = i;\n            }\n        }\n\n        if (best_idx == -1) {\n            break; // we couldn't find any more pairs to merge, so we're done\n        }\n\n        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n        tokens[best_idx] = best_id;\n        // delete token at position best_idx+1, shift the entire sequence back 1\n        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n            tokens[i] = tokens[i+1];\n        }\n        (*n_tokens)--; // token length decreased\n    }\n\n    // add optional EOS (=2) token, if desired\n    if (eos) tokens[(*n_tokens)++] = 2;\n\n    free(str_buffer);\n}\n\n// ----------------------------------------------------------------------------\n// The Sampler, which takes logits and returns a sampled token\n// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n\ntypedef struct {\n    float prob;\n    int index;\n} ProbIndex; // struct used when sorting probabilities during top-p sampling\n\ntypedef struct {\n    int vocab_size;\n    ProbIndex* probindex; // buffer used in top-p sampling\n    float temperature;\n    float topp;\n    unsigned long long rng_state;\n} Sampler;\n\nint sample_argmax(float* probabilities, int n) {\n    // return the index that has the highest probability\n    int max_i = 0;\n    float max_p = probabilities[0];\n    for (int i = 1; i < n; i++) {\n        if (probabilities[i] > max_p) {\n            max_i = i;\n            max_p = probabilities[i];\n        }\n    }\n    return max_i;\n}\n\nint sample_mult(float* probabilities, int n, float coin) {\n    // sample index from probabilities (they must sum to 1!)\n    // coin is a random number in [0, 1), usually from random_f32()\n    float cdf = 0.0f;\n    for (int i = 0; i < n; i++) {\n        cdf += probabilities[i];\n        if (coin < cdf) {\n            return i;\n        }\n    }\n    return n - 1; // in case of rounding errors\n}\n\nint compare(const void* a, const void* b) {\n    ProbIndex* a_ = (ProbIndex*) a;\n    ProbIndex* b_ = (ProbIndex*) b;\n    if (a_->prob > b_->prob) return -1;\n    if (a_->prob < b_->prob) return 1;\n    return 0;\n}\n\nint sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {\n    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n    // tokens that exceed probability topp. This way we never sample tokens that\n    // have very low probabilities and are less likely to go \"off the rails\".\n    // coin is a random number in [0, 1), usually from random_f32()\n\n    int n0 = 0;\n    // quicksort indices in descending order of probabilities\n    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n    // so for efficiency we crop these out as candidates before sorting\n    const float cutoff = (1.0f - topp) / (n - 1);\n    for (int i = 0; i < n; i++) {\n        if (probabilities[i] >= cutoff) {\n            probindex[n0].index = i;\n            probindex[n0].prob = probabilities[i];\n            n0++;\n        }\n    }\n    qsort(probindex, n0, sizeof(ProbIndex), compare);\n\n    // truncate the list where cumulative probability exceeds topp\n    float cumulative_prob = 0.0f;\n    int last_idx = n0 - 1; // in case of rounding errors consider all elements\n    for (int i = 0; i < n0; i++) {\n        cumulative_prob += probindex[i].prob;\n        if (cumulative_prob > topp) {\n            last_idx = i;\n            break; // we've exceeded topp by including last_idx\n        }\n    }\n\n    // sample from the truncated list\n    float r = coin * cumulative_prob;\n    float cdf = 0.0f;\n    for (int i = 0; i <= last_idx; i++) {\n        cdf += probindex[i].prob;\n        if (r < cdf) {\n            return probindex[i].index;\n        }\n    }\n    return probindex[last_idx].index; // in case of rounding errors\n}\n\nvoid build_sampler(Sampler* sampler, int vocab_size, float temperature, float topp, unsigned long long rng_seed) {\n    sampler->vocab_size = vocab_size;\n    sampler->temperature = temperature;\n    sampler->topp = topp;\n    sampler->rng_state = rng_seed;\n    // buffer only used with nucleus sampling; may not need but it's ~small\n    sampler->probindex = malloc(sampler->vocab_size * sizeof(ProbIndex));\n}\n\nvoid free_sampler(Sampler* sampler) {\n    free(sampler->probindex);\n}\n\nunsigned int random_u32(unsigned long long *state) {\n    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n    *state ^= *state >> 12;\n    *state ^= *state << 25;\n    *state ^= *state >> 27;\n    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n}\nfloat random_f32(unsigned long long *state) { // random float32 in [0,1)\n    return (random_u32(state) >> 8) / 16777216.0f;\n}\n\nint sample(Sampler* sampler, float* logits) {\n    // sample the token given the logits and some hyperparameters\n    int next;\n    if (sampler->temperature == 0.0f) {\n        // greedy argmax sampling: take the token with the highest probability\n        next = sample_argmax(logits, sampler->vocab_size);\n    } else {\n        // apply the temperature to the logits\n        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n        // apply softmax to the logits to get the probabilities for next token\n        softmax(logits, sampler->vocab_size);\n        // flip a (float) coin (this is our source of entropy for sampling)\n        float coin = random_f32(&sampler->rng_state);\n        // we sample from this distribution to get the next token\n        if (sampler->topp <= 0 || sampler->topp >= 1) {\n            // simply sample from the predicted probability distribution\n            next = sample_mult(logits, sampler->vocab_size, coin);\n        } else {\n            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n        }\n    }\n    return next;\n}\n\n// ----------------------------------------------------------------------------\n// utilities: time\n\nlong time_in_ms() {\n    // return time in milliseconds, for benchmarking the model speed\n    struct timespec time;\n    clock_gettime(CLOCK_REALTIME, &time);\n    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n}\n\n// ----------------------------------------------------------------------------\n// generation loop\n\nvoid generate(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler, char *prompt, int steps) {\n    char *empty_prompt = \"\";\n    if (prompt == NULL) { prompt = empty_prompt; }\n\n    // encode the (string) prompt into tokens sequence\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n    if (num_prompt_tokens < 1) {\n        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    // start the main loop\n    long start = 0;  // used to time our code, only initialized after first iteration\n    int next;        // will store the next token in the sequence\n    int token = prompt_tokens[0]; // kick off with the first token in the prompt\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n\n        // advance the state state machine\n        if (pos < num_prompt_tokens - 1) {\n            // if we are still processing the input prompt, force the next prompt token\n            next = prompt_tokens[pos + 1];\n        } else {\n            // otherwise sample the next token from the logits\n            next = sample(sampler, logits);\n        }\n        pos++;\n\n        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n        if (next == 1) { break; }\n\n        // print the token as string, decode it with the Tokenizer object\n        char* piece = decode(tokenizer, token, next);\n        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n        fflush(stdout);\n        token = next;\n\n        // init the timer here because the first iteration can be slower\n        if (start == 0) { start = time_in_ms(); }\n    }\n    printf(\"\\n\");\n\n    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n    if (pos > 1) {\n        long end = time_in_ms();\n        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n    }\n\n    free(prompt_tokens);\n}\n\nvoid read_stdin(const char* guide, char* buffer, size_t bufsize) {\n    // read a line from stdin, up to but not including \\n\n    printf(\"%s\", guide);\n    if (fgets(buffer, bufsize, stdin) != NULL) {\n        size_t len = strlen(buffer);\n        if (len > 0 && buffer[len - 1] == '\\n') {\n            buffer[len - 1] = '\\0'; // strip newline\n        }\n    }\n}\n\n// ----------------------------------------------------------------------------\n// chat loop\n// I manually inspected the tokens for a few chat conversations compared to\n// python reference and that seemed ok, but this was not thoroughly tested and\n// is not safely implemented, it's more a proof of concept atm.\n\nvoid chat(Transformer *transformer, Tokenizer *tokenizer, Sampler *sampler,\n          char *cli_user_prompt, char *cli_system_prompt, int steps) {\n\n    // buffers for reading the system prompt and user prompt from stdin\n    // you'll notice they are soomewhat haphazardly and unsafely set atm\n    char system_prompt[512];\n    char user_prompt[512];\n    char rendered_prompt[1152];\n    int num_prompt_tokens = 0;\n    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n    int user_idx;\n\n    // start the main loop\n    int8_t user_turn = 1; // user starts\n    int next;        // will store the next token in the sequence\n    int token;       // stores the current token to feed into the transformer\n    int prev_token;\n    int pos = 0;     // position in the sequence\n    while (pos < steps) {\n\n        // when it is the user's turn to contribute tokens to the dialog...\n        if (user_turn) {\n            // get the (optional) system prompt at position 0\n            if (pos == 0) {\n                // at position 0, the user can also contribute a system prompt\n                if (cli_system_prompt == NULL) {\n                    // system prompt was not passed in, attempt to get it from stdin\n                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n                } else {\n                    // system prompt was passed in, use it\n                    strcpy(system_prompt, cli_system_prompt);\n                }\n            }\n            // get the user prompt\n            if (pos == 0 && cli_user_prompt != NULL) {\n                // user prompt for position 0 was passed in, use it\n                strcpy(user_prompt, cli_user_prompt);\n            } else {\n                // otherwise get user prompt from stdin\n                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n            }\n            // render user/system prompts into the Llama 2 Chat schema\n            if (pos == 0 && system_prompt[0] != '\\0') {\n                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n            } else {\n                char user_template[] = \"[INST] %s [/INST]\";\n                sprintf(rendered_prompt, user_template, user_prompt);\n            }\n            // encode the rendered prompt into tokens\n            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n            user_idx = 0; // reset the user index\n            user_turn = 0;\n            printf(\"Assistant: \");\n        }\n\n        // determine the token to pass into the transformer next\n        if (user_idx < num_prompt_tokens) {\n            // if we are still processing the input prompt, force the next prompt token\n            token = prompt_tokens[user_idx++];\n        } else {\n            // otherwise use the next token sampled from previous turn\n            token = next;\n        }\n        // EOS (=2) token ends the Assistant turn\n        if (token == 2) { user_turn = 1; }\n\n        // forward the transformer to get logits for the next token\n        float* logits = forward(transformer, token, pos);\n        next = sample(sampler, logits);\n        pos++;\n\n        if (user_idx >= num_prompt_tokens && next != 2) {\n            // the Assistant is responding, so print its output\n            char* piece = decode(tokenizer, token, next);\n            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n            fflush(stdout);\n        }\n        if (next == 2) { printf(\"\\n\"); }\n    }\n    printf(\"\\n\");\n    free(prompt_tokens);\n}\n\n\n// ----------------------------------------------------------------------------\n// CLI, include only if not testing\n#ifndef TESTING\n\nvoid error_usage() {\n    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n    fprintf(stderr, \"Options:\\n\");\n    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n    fprintf(stderr, \"  -i <string> input prompt\\n\");\n    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n    exit(EXIT_FAILURE);\n}\n\nint main(int argc, char *argv[]) {\n\n    // default parameters\n    char *checkpoint_path = NULL;  // e.g. out/model.bin\n    char *tokenizer_path = \"tokenizer.bin\";\n    float temperature = 1.0f;   // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n    float topp = 0.9f;          // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n    int steps = 256;            // number of steps to run for\n    char *prompt = NULL;        // prompt string\n    unsigned long long rng_seed = 0; // seed rng with time by default\n    char *mode = \"generate\";    // generate|chat\n    char *system_prompt = NULL; // the (optional) system prompt to use in chat mode\n\n    // poor man's C argparse so we can override the defaults above from the command line\n    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n    for (int i = 2; i < argc; i+=2) {\n        // do some basic validation\n        if (i + 1 >= argc) { error_usage(); } // must have arg after flag\n        if (argv[i][0] != '-') { error_usage(); } // must start with dash\n        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n        // read in the args\n        if (argv[i][1] == 't') { temperature = atof(argv[i + 1]); }\n        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n        else { error_usage(); }\n    }\n\n    // parameter validation/overrides\n    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n    if (temperature < 0.0) temperature = 0.0;\n    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n    if (steps < 0) steps = 0;\n\n    // build the Transformer via the model .bin file\n    Transformer transformer;\n    build_transformer(&transformer, checkpoint_path);\n    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; // ovrerride to ~max length\n\n    // build the Tokenizer via the tokenizer .bin file\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n\n    // build the Sampler\n    Sampler sampler;\n    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n\n    // run!\n    if (strcmp(mode, \"generate\") == 0) {\n        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n    } else if (strcmp(mode, \"chat\") == 0) {\n        chat(&transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n    } else {\n        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n        error_usage();\n    }\n\n    // memory and file handles cleanup\n    free_sampler(&sampler);\n    free_tokenizer(&tokenizer);\n    free_transformer(&transformer);\n    return 0;\n}\n#endif\n", "diff": "@@ -115,26 +115,28 @@ void free_run_state(RunState* s) {\n \n void memory_map_weights(TransformerWeights *w, Config* p, float* ptr, int shared_weights) {\n     int head_size = p->dim / p->n_heads;\n+    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n+    unsigned long n_layers = p->n_layers;\n     w->token_embedding_table = ptr;\n     ptr += p->vocab_size * p->dim;\n     w->rms_att_weight = ptr;\n-    ptr += p->n_layers * p->dim;\n+    ptr += n_layers * p->dim;\n     w->wq = ptr;\n-    ptr += p->n_layers * p->dim * (p->n_heads * head_size);\n+    ptr += n_layers * p->dim * (p->n_heads * head_size);\n     w->wk = ptr;\n-    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n+    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n     w->wv = ptr;\n-    ptr += p->n_layers * p->dim * (p->n_kv_heads * head_size);\n+    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n     w->wo = ptr;\n-    ptr += p->n_layers * (p->n_heads * head_size) * p->dim;\n+    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n     w->rms_ffn_weight = ptr;\n-    ptr += p->n_layers * p->dim;\n+    ptr += n_layers * p->dim;\n     w->w1 = ptr;\n-    ptr += p->n_layers * p->dim * p->hidden_dim;\n+    ptr += n_layers * p->dim * p->hidden_dim;\n     w->w2 = ptr;\n-    ptr += p->n_layers * p->hidden_dim * p->dim;\n+    ptr += n_layers * p->hidden_dim * p->dim;\n     w->w3 = ptr;\n-    ptr += p->n_layers * p->dim * p->hidden_dim;\n+    ptr += n_layers * p->dim * p->hidden_dim;\n     w->rms_final_weight = ptr;\n     ptr += p->dim;\n     ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n@@ -249,7 +251,7 @@ float* forward(Transformer* transformer, int token, int pos) {\n     memcpy(x, content_row, dim*sizeof(*x));\n \n     // forward all the layers\n-    for(int l = 0; l < p->n_layers; l++) {\n+    for(unsigned long l = 0; l < p->n_layers; l++) {\n \n         // attention rmsnorm\n         rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);", "status": "modified"}, {"commit_id": "7b0017c6cdb15fede07d60e1c2e877473131be88", "commit_message": "Merge pull request #362 from byte-6174/upmaster\n\nfreeing tokenizer in test.c", "relative_path": "test.c", "previous_code_file": "#define TESTING\n#include \"run.c\"\n\nvoid assert_eq(int a, int b) {\n    if (a != b) {\n        printf(\"Assertion failed: %d != %d\\n\", a, b);\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid test_prompt_encoding(Tokenizer* tokenizer, char* prompt, int* expected_tokens, int num_expected_tokens) {\n    // encode\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int));\n    int num_prompt_tokens = 0; // the total number of prompt tokens\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n\n    #if VERBOSITY == 1\n    // print maybe\n    printf(\"expected tokens:\\n\");\n    for (int i = 0; i < num_expected_tokens; i++) printf(\"%d \", expected_tokens[i]);\n    printf(\"\\n\");\n    printf(\"actual tokens:\\n\");\n    for (int i = 0; i < num_prompt_tokens; i++) printf(\"%d \", prompt_tokens[i]);\n    printf(\"\\n\");\n    #endif\n\n    // verify\n    assert_eq(num_prompt_tokens, num_expected_tokens);\n    for (int i = 0; i < num_prompt_tokens; i++) {\n        assert_eq(prompt_tokens[i], expected_tokens[i]);\n    }\n\n    #if VERBOSITY == 1\n    printf(\"OK\\n\");\n    printf(\"---\\n\");\n    #endif\n    free(prompt_tokens);\n}\n\nvoid test_prompt_encodings() {\n    // let's verify that the Tokenizer works as expected\n\n    char *tokenizer_path = \"tokenizer.bin\";\n    int vocab_size = 32000;\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, vocab_size);\n\n    // test 0 (test the empty string) (I added this as a simple case)\n    char *prompt0 = \"\";\n    int expected_tokens0[] = {1};\n    test_prompt_encoding(&tokenizer, prompt0, expected_tokens0, sizeof(expected_tokens0) / sizeof(int));\n\n    // the tests below are taken from the Meta Llama 2 repo example code\n    // https://github.com/facebookresearch/llama/blob/main/example_text_completion.py\n    // and the expected tokens come from me breaking in the debugger in Python\n\n    // test 1\n    char *prompt = \"I believe the meaning of life is\";\n    int expected_tokens[] = {1, 306, 4658, 278, 6593, 310, 2834, 338};\n    test_prompt_encoding(&tokenizer, prompt, expected_tokens, sizeof(expected_tokens) / sizeof(int));\n\n    // test 2\n    char* prompt2 = \"Simply put, the theory of relativity states that \";\n    int expected_tokens2[] = {1, 3439, 17632, 1925, 29892, 278, 6368, 310, 14215, 537, 5922, 393, 29871};\n    test_prompt_encoding(&tokenizer, prompt2, expected_tokens2, sizeof(expected_tokens2) / sizeof(int));\n\n    // test 3\n    char* prompt3 = \"A brief message congratulating the team on the launch:\\n\\n        Hi everyone,\\n\\n        I just \";\n    int expected_tokens3[] = {1, 319, 11473, 2643, 378, 629, 271, 18099, 278, 3815, 373, 278, 6826, 29901, 13, 13, 4706, 6324, 14332, 29892, 13, 13, 4706, 306, 925, 29871};\n    test_prompt_encoding(&tokenizer, prompt3, expected_tokens3, sizeof(expected_tokens3) / sizeof(int));\n\n    // test 4\n    char* prompt4 = \"Translate English to French:\\n\\n        sea otter => loutre de mer\\n        peppermint => menthe poivr\u00e9e\\n        plush girafe => girafe peluche\\n        cheese =>\";\n    int expected_tokens4[] = {1, 4103, 9632, 4223, 304, 5176, 29901, 13, 13, 4706, 7205, 4932, 357, 1149, 301, 449, 276, 316, 2778, 13, 4706, 1236, 407, 837, 524, 1149, 6042, 354, 772, 440, 29878, 1318, 13, 4706, 715, 1878, 330, 3055, 1725, 1149, 330, 3055, 1725, 4639, 28754, 13, 4706, 923, 968, 1149};\n    test_prompt_encoding(&tokenizer, prompt4, expected_tokens4, sizeof(expected_tokens4) / sizeof(int));\n}\n\nint main(int argc, char *argv[]) {\n    test_prompt_encodings();\n    printf(\"ALL OK\\n\");\n}\n", "previous_id": "50832e3dff94ebc37059c112810e8bcc0e37b8a4", "new_code_file": "#define TESTING\n#include \"run.c\"\n\nvoid assert_eq(int a, int b) {\n    if (a != b) {\n        printf(\"Assertion failed: %d != %d\\n\", a, b);\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid test_prompt_encoding(Tokenizer* tokenizer, char* prompt, int* expected_tokens, int num_expected_tokens) {\n    // encode\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int));\n    int num_prompt_tokens = 0; // the total number of prompt tokens\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n\n    #if VERBOSITY == 1\n    // print maybe\n    printf(\"expected tokens:\\n\");\n    for (int i = 0; i < num_expected_tokens; i++) printf(\"%d \", expected_tokens[i]);\n    printf(\"\\n\");\n    printf(\"actual tokens:\\n\");\n    for (int i = 0; i < num_prompt_tokens; i++) printf(\"%d \", prompt_tokens[i]);\n    printf(\"\\n\");\n    #endif\n\n    // verify\n    assert_eq(num_prompt_tokens, num_expected_tokens);\n    for (int i = 0; i < num_prompt_tokens; i++) {\n        assert_eq(prompt_tokens[i], expected_tokens[i]);\n    }\n\n    #if VERBOSITY == 1\n    printf(\"OK\\n\");\n    printf(\"---\\n\");\n    #endif\n    free(prompt_tokens);\n}\n\nvoid test_prompt_encodings() {\n    // let's verify that the Tokenizer works as expected\n\n    char *tokenizer_path = \"tokenizer.bin\";\n    int vocab_size = 32000;\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, vocab_size);\n\n    // test 0 (test the empty string) (I added this as a simple case)\n    char *prompt0 = \"\";\n    int expected_tokens0[] = {1};\n    test_prompt_encoding(&tokenizer, prompt0, expected_tokens0, sizeof(expected_tokens0) / sizeof(int));\n\n    // the tests below are taken from the Meta Llama 2 repo example code\n    // https://github.com/facebookresearch/llama/blob/main/example_text_completion.py\n    // and the expected tokens come from me breaking in the debugger in Python\n\n    // test 1\n    char *prompt = \"I believe the meaning of life is\";\n    int expected_tokens[] = {1, 306, 4658, 278, 6593, 310, 2834, 338};\n    test_prompt_encoding(&tokenizer, prompt, expected_tokens, sizeof(expected_tokens) / sizeof(int));\n\n    // test 2\n    char* prompt2 = \"Simply put, the theory of relativity states that \";\n    int expected_tokens2[] = {1, 3439, 17632, 1925, 29892, 278, 6368, 310, 14215, 537, 5922, 393, 29871};\n    test_prompt_encoding(&tokenizer, prompt2, expected_tokens2, sizeof(expected_tokens2) / sizeof(int));\n\n    // test 3\n    char* prompt3 = \"A brief message congratulating the team on the launch:\\n\\n        Hi everyone,\\n\\n        I just \";\n    int expected_tokens3[] = {1, 319, 11473, 2643, 378, 629, 271, 18099, 278, 3815, 373, 278, 6826, 29901, 13, 13, 4706, 6324, 14332, 29892, 13, 13, 4706, 306, 925, 29871};\n    test_prompt_encoding(&tokenizer, prompt3, expected_tokens3, sizeof(expected_tokens3) / sizeof(int));\n\n    // test 4\n    char* prompt4 = \"Translate English to French:\\n\\n        sea otter => loutre de mer\\n        peppermint => menthe poivr\u00e9e\\n        plush girafe => girafe peluche\\n        cheese =>\";\n    int expected_tokens4[] = {1, 4103, 9632, 4223, 304, 5176, 29901, 13, 13, 4706, 7205, 4932, 357, 1149, 301, 449, 276, 316, 2778, 13, 4706, 1236, 407, 837, 524, 1149, 6042, 354, 772, 440, 29878, 1318, 13, 4706, 715, 1878, 330, 3055, 1725, 1149, 330, 3055, 1725, 4639, 28754, 13, 4706, 923, 968, 1149};\n    test_prompt_encoding(&tokenizer, prompt4, expected_tokens4, sizeof(expected_tokens4) / sizeof(int));\n\n    // memory and file handles cleanup\n    free_tokenizer(&tokenizer);\n}\n\nint main(int argc, char *argv[]) {\n    test_prompt_encodings();\n    printf(\"ALL OK\\n\");\n}\n", "diff": "@@ -73,6 +73,9 @@ void test_prompt_encodings() {\n     char* prompt4 = \"Translate English to French:\\n\\n        sea otter => loutre de mer\\n        peppermint => menthe poivr\u00e9e\\n        plush girafe => girafe peluche\\n        cheese =>\";\n     int expected_tokens4[] = {1, 4103, 9632, 4223, 304, 5176, 29901, 13, 13, 4706, 7205, 4932, 357, 1149, 301, 449, 276, 316, 2778, 13, 4706, 1236, 407, 837, 524, 1149, 6042, 354, 772, 440, 29878, 1318, 13, 4706, 715, 1878, 330, 3055, 1725, 1149, 330, 3055, 1725, 4639, 28754, 13, 4706, 923, 968, 1149};\n     test_prompt_encoding(&tokenizer, prompt4, expected_tokens4, sizeof(expected_tokens4) / sizeof(int));\n+\n+    // memory and file handles cleanup\n+    free_tokenizer(&tokenizer);\n }\n \n int main(int argc, char *argv[]) {", "status": "modified"}, {"commit_id": "50832e3dff94ebc37059c112810e8bcc0e37b8a4", "commit_message": "move script into the new docs folder", "relative_path": "train_vocab.sh", "previous_code_file": "#!/bin/bash\n\n# Trains a sentencepiece tokenizer model on a bunch of given data, my best\n# effort attempt to replicate how Meta trained their Llama 2 tokenizer.\n\n# usage: $ train_vocab.sh <input> <model_prefix> <vocab_size>\n# example:\n# ./train_vocab.sh tiny.txt tokenizer_tiny 1024\n# requirements:\n# install https://github.com/google/sentencepiece\n\n# check if the correct number of arguments are provided\nif [ $# -ne 3 ]; then\n    echo \"Usage: $0 <input> <model_prefix> <vocab_size>\"\n    exit 1\nfi\n\n# assign command-line arguments to variables\ninput=$1\nmodel_prefix=$2\nvocab_size=$3\n\n# check if input file exists\nif [ ! -f \"$input\" ]; then\n    echo \"Usage: $0 <input> <model_prefix> <vocab_size>\"\n    echo \"input '$input' not found.\"\n    exit 1\nfi\n\n# check if vocab_size is a positive integer\nif ! [[ \"$vocab_size\" =~ ^[0-9]+$ ]] || [ \"$vocab_size\" -lt 1 ]; then\n    echo \"Usage: $0 <input> <model_prefix> <vocab_size>\"\n    echo \"vocab_size size must be a positive integer.\"\n    exit 1\nfi\n\n# Print the processed inputs\necho \"Input: $input\"\necho \"Model Prefix: $model_prefix\"\necho \"Vocabulary Size: $vocab_size\"\n\n# train a sentencepiece tokenizer model\n# Llama 2 config can be printed as follows:\n\n# import sentencepiece.sentencepiece_model_pb2\n# mp = sentencepiece.sentencepiece_model_pb2.ModelProto()\n# mp.ParseFromString(open(\"tokenizer.model\", \"rb\").read())\n# print(mp.trainer_spec)\n# print(mp.normalizer_spec)\n\n# this gives:\n\n# trainer_spec {\n#   input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n#   model_prefix: \"spm_model_32k_200M_charcov099995_allowWSO__v2\"\n#   model_type: BPE\n#   vocab_size: 32000\n#   self_test_sample_size: 0\n#   input_format: \"text\"\n#   character_coverage: 0.9999499917030334\n#   input_sentence_size: 200000000\n#   seed_sentencepiece_size: 1000000\n#   shrinking_factor: 0.75\n#   num_threads: 80\n#   num_sub_iterations: 2\n#   max_sentence_length: 4192\n#   shuffle_input_sentence: true\n#   max_sentencepiece_length: 16\n#   split_by_unicode_script: true\n#   split_by_whitespace: true\n#   split_by_number: true\n#   treat_whitespace_as_suffix: false\n#   split_digits: true\n#   allow_whitespace_only_pieces: true\n#   vocabulary_output_piece_score: true\n#   hard_vocab_limit: true\n#   use_all_vocab: false\n#   byte_fallback: true\n#   required_chars: \"\"\n#   unk_id: 0\n#   bos_id: 1\n#   eos_id: 2\n#   pad_id: -1\n#   unk_surface: \" \\342\\201\\207 \"\n#   unk_piece: \"<unk>\"\n#   bos_piece: \"<s>\"\n#   eos_piece: \"</s>\"\n#   pad_piece: \"<pad>\"\n#   train_extremely_large_corpus: false\n#   enable_differential_privacy: false\n#   differential_privacy_noise_level: 0.0\n#   differential_privacy_clipping_threshold: 0\n# }\n# normalizer_spec {\n#   name: \"identity\"\n#   precompiled_charsmap: \"\"\n#   add_dummy_prefix: true\n#   remove_extra_whitespaces: false\n#   normalization_rule_tsv: \"\"\n# }\n\n# let's now use spm_train to train this exact model\n# options docs: https://github.com/google/sentencepiece/blob/master/doc/options.md\n\n# we'll depart on a few settings:\n# character_coverage -> 1.0\n\n# other important notes:\n# --split-digits = true, per the paper\n# --allow_whitespace_only_pieces is true, default in spm is false\n# --byte_fallback is true, default in spm is false\n# --normalization_rule_name is identity, default in spm is nmt_nfkc\n\nspm_train --input=\"$input\" \\\n          --model_prefix=\"$model_prefix\" \\\n          --model_type=bpe \\\n          --vocab_size=\"$vocab_size\" \\\n          --self_test_sample_size=0 \\\n          --input_format=\"text\" \\\n          --character_coverage=1.0 \\\n          --num_threads=\"$(nproc)\" \\\n          --split_digits=true \\\n          --allow_whitespace_only_pieces=true \\\n          --byte_fallback=true \\\n          --unk_surface=\" \\342\\201\\207 \" \\\n          --normalization_rule_name=identity \\\n", "previous_id": "1386edfd9090e2ef1da6636f3ba3220362079024", "new_code_file": null, "diff": null, "status": "removed"}, {"commit_id": "32cecbfe4adc5bcb60435c3baa5dcd84df6320e7", "commit_message": "freeing tokenizer in test.c", "relative_path": "test.c", "previous_code_file": "#define TESTING\n#include \"run.c\"\n\nvoid assert_eq(int a, int b) {\n    if (a != b) {\n        printf(\"Assertion failed: %d != %d\\n\", a, b);\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid test_prompt_encoding(Tokenizer* tokenizer, char* prompt, int* expected_tokens, int num_expected_tokens) {\n    // encode\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int));\n    int num_prompt_tokens = 0; // the total number of prompt tokens\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n\n    #if VERBOSITY == 1\n    // print maybe\n    printf(\"expected tokens:\\n\");\n    for (int i = 0; i < num_expected_tokens; i++) printf(\"%d \", expected_tokens[i]);\n    printf(\"\\n\");\n    printf(\"actual tokens:\\n\");\n    for (int i = 0; i < num_prompt_tokens; i++) printf(\"%d \", prompt_tokens[i]);\n    printf(\"\\n\");\n    #endif\n\n    // verify\n    assert_eq(num_prompt_tokens, num_expected_tokens);\n    for (int i = 0; i < num_prompt_tokens; i++) {\n        assert_eq(prompt_tokens[i], expected_tokens[i]);\n    }\n\n    #if VERBOSITY == 1\n    printf(\"OK\\n\");\n    printf(\"---\\n\");\n    #endif\n    free(prompt_tokens);\n}\n\nvoid test_prompt_encodings() {\n    // let's verify that the Tokenizer works as expected\n\n    char *tokenizer_path = \"tokenizer.bin\";\n    int vocab_size = 32000;\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, vocab_size);\n\n    // test 0 (test the empty string) (I added this as a simple case)\n    char *prompt0 = \"\";\n    int expected_tokens0[] = {1};\n    test_prompt_encoding(&tokenizer, prompt0, expected_tokens0, sizeof(expected_tokens0) / sizeof(int));\n\n    // the tests below are taken from the Meta Llama 2 repo example code\n    // https://github.com/facebookresearch/llama/blob/main/example_text_completion.py\n    // and the expected tokens come from me breaking in the debugger in Python\n\n    // test 1\n    char *prompt = \"I believe the meaning of life is\";\n    int expected_tokens[] = {1, 306, 4658, 278, 6593, 310, 2834, 338};\n    test_prompt_encoding(&tokenizer, prompt, expected_tokens, sizeof(expected_tokens) / sizeof(int));\n\n    // test 2\n    char* prompt2 = \"Simply put, the theory of relativity states that \";\n    int expected_tokens2[] = {1, 3439, 17632, 1925, 29892, 278, 6368, 310, 14215, 537, 5922, 393, 29871};\n    test_prompt_encoding(&tokenizer, prompt2, expected_tokens2, sizeof(expected_tokens2) / sizeof(int));\n\n    // test 3\n    char* prompt3 = \"A brief message congratulating the team on the launch:\\n\\n        Hi everyone,\\n\\n        I just \";\n    int expected_tokens3[] = {1, 319, 11473, 2643, 378, 629, 271, 18099, 278, 3815, 373, 278, 6826, 29901, 13, 13, 4706, 6324, 14332, 29892, 13, 13, 4706, 306, 925, 29871};\n    test_prompt_encoding(&tokenizer, prompt3, expected_tokens3, sizeof(expected_tokens3) / sizeof(int));\n\n    // test 4\n    char* prompt4 = \"Translate English to French:\\n\\n        sea otter => loutre de mer\\n        peppermint => menthe poivr\u00e9e\\n        plush girafe => girafe peluche\\n        cheese =>\";\n    int expected_tokens4[] = {1, 4103, 9632, 4223, 304, 5176, 29901, 13, 13, 4706, 7205, 4932, 357, 1149, 301, 449, 276, 316, 2778, 13, 4706, 1236, 407, 837, 524, 1149, 6042, 354, 772, 440, 29878, 1318, 13, 4706, 715, 1878, 330, 3055, 1725, 1149, 330, 3055, 1725, 4639, 28754, 13, 4706, 923, 968, 1149};\n    test_prompt_encoding(&tokenizer, prompt4, expected_tokens4, sizeof(expected_tokens4) / sizeof(int));\n}\n\nint main(int argc, char *argv[]) {\n    test_prompt_encodings();\n    printf(\"ALL OK\\n\");\n}\n", "previous_id": "e47bacdc62e1b1336188b6edd533624aaa04d07c", "new_code_file": "#define TESTING\n#include \"run.c\"\n\nvoid assert_eq(int a, int b) {\n    if (a != b) {\n        printf(\"Assertion failed: %d != %d\\n\", a, b);\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid test_prompt_encoding(Tokenizer* tokenizer, char* prompt, int* expected_tokens, int num_expected_tokens) {\n    // encode\n    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int));\n    int num_prompt_tokens = 0; // the total number of prompt tokens\n    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n\n    #if VERBOSITY == 1\n    // print maybe\n    printf(\"expected tokens:\\n\");\n    for (int i = 0; i < num_expected_tokens; i++) printf(\"%d \", expected_tokens[i]);\n    printf(\"\\n\");\n    printf(\"actual tokens:\\n\");\n    for (int i = 0; i < num_prompt_tokens; i++) printf(\"%d \", prompt_tokens[i]);\n    printf(\"\\n\");\n    #endif\n\n    // verify\n    assert_eq(num_prompt_tokens, num_expected_tokens);\n    for (int i = 0; i < num_prompt_tokens; i++) {\n        assert_eq(prompt_tokens[i], expected_tokens[i]);\n    }\n\n    #if VERBOSITY == 1\n    printf(\"OK\\n\");\n    printf(\"---\\n\");\n    #endif\n    free(prompt_tokens);\n}\n\nvoid test_prompt_encodings() {\n    // let's verify that the Tokenizer works as expected\n\n    char *tokenizer_path = \"tokenizer.bin\";\n    int vocab_size = 32000;\n    Tokenizer tokenizer;\n    build_tokenizer(&tokenizer, tokenizer_path, vocab_size);\n\n    // test 0 (test the empty string) (I added this as a simple case)\n    char *prompt0 = \"\";\n    int expected_tokens0[] = {1};\n    test_prompt_encoding(&tokenizer, prompt0, expected_tokens0, sizeof(expected_tokens0) / sizeof(int));\n\n    // the tests below are taken from the Meta Llama 2 repo example code\n    // https://github.com/facebookresearch/llama/blob/main/example_text_completion.py\n    // and the expected tokens come from me breaking in the debugger in Python\n\n    // test 1\n    char *prompt = \"I believe the meaning of life is\";\n    int expected_tokens[] = {1, 306, 4658, 278, 6593, 310, 2834, 338};\n    test_prompt_encoding(&tokenizer, prompt, expected_tokens, sizeof(expected_tokens) / sizeof(int));\n\n    // test 2\n    char* prompt2 = \"Simply put, the theory of relativity states that \";\n    int expected_tokens2[] = {1, 3439, 17632, 1925, 29892, 278, 6368, 310, 14215, 537, 5922, 393, 29871};\n    test_prompt_encoding(&tokenizer, prompt2, expected_tokens2, sizeof(expected_tokens2) / sizeof(int));\n\n    // test 3\n    char* prompt3 = \"A brief message congratulating the team on the launch:\\n\\n        Hi everyone,\\n\\n        I just \";\n    int expected_tokens3[] = {1, 319, 11473, 2643, 378, 629, 271, 18099, 278, 3815, 373, 278, 6826, 29901, 13, 13, 4706, 6324, 14332, 29892, 13, 13, 4706, 306, 925, 29871};\n    test_prompt_encoding(&tokenizer, prompt3, expected_tokens3, sizeof(expected_tokens3) / sizeof(int));\n\n    // test 4\n    char* prompt4 = \"Translate English to French:\\n\\n        sea otter => loutre de mer\\n        peppermint => menthe poivr\u00e9e\\n        plush girafe => girafe peluche\\n        cheese =>\";\n    int expected_tokens4[] = {1, 4103, 9632, 4223, 304, 5176, 29901, 13, 13, 4706, 7205, 4932, 357, 1149, 301, 449, 276, 316, 2778, 13, 4706, 1236, 407, 837, 524, 1149, 6042, 354, 772, 440, 29878, 1318, 13, 4706, 715, 1878, 330, 3055, 1725, 1149, 330, 3055, 1725, 4639, 28754, 13, 4706, 923, 968, 1149};\n    test_prompt_encoding(&tokenizer, prompt4, expected_tokens4, sizeof(expected_tokens4) / sizeof(int));\n\n    // memory and file handles cleanup\n    free_tokenizer(&tokenizer);\n}\n\nint main(int argc, char *argv[]) {\n    test_prompt_encodings();\n    printf(\"ALL OK\\n\");\n}\n", "diff": "@@ -73,6 +73,9 @@ void test_prompt_encodings() {\n     char* prompt4 = \"Translate English to French:\\n\\n        sea otter => loutre de mer\\n        peppermint => menthe poivr\u00e9e\\n        plush girafe => girafe peluche\\n        cheese =>\";\n     int expected_tokens4[] = {1, 4103, 9632, 4223, 304, 5176, 29901, 13, 13, 4706, 7205, 4932, 357, 1149, 301, 449, 276, 316, 2778, 13, 4706, 1236, 407, 837, 524, 1149, 6042, 354, 772, 440, 29878, 1318, 13, 4706, 715, 1878, 330, 3055, 1725, 1149, 330, 3055, 1725, 4639, 28754, 13, 4706, 923, 968, 1149};\n     test_prompt_encoding(&tokenizer, prompt4, expected_tokens4, sizeof(expected_tokens4) / sizeof(int));\n+\n+    // memory and file handles cleanup\n+    free_tokenizer(&tokenizer);\n }\n \n int main(int argc, char *argv[]) {", "status": "modified"}, {"commit_id": "e47bacdc62e1b1336188b6edd533624aaa04d07c", "commit_message": "Merge pull request #355 from janimo/export-vocab-size\n\nExport vocab size and Code Llama usage docs", "relative_path": "export.py", "previous_code_file": "\"\"\"\nThis script has functions and utilties for model export.\nBasically, we have a bunch of versions of the model, and we\nwant to export them to .bin files to be read from and inferenced in C.\n\nAmong the \"input\" versions of PyTorch files/models:\n- Official Llama 2 weights released by Meta\n- Huggingface weights available on the hub\n- llama2.c (this repo) trained models\n\nAmong the \"output\" versions of .bin files:\n- v0: Legacy files of the original llama2.c repo (will eventually be DEPRECATED)\n- v1-vN: Improved .bin files with a proper header, cache alignment, etc.\n\nThis script aspires to provide all of these conversions.\n\"\"\"\nimport os\nimport gzip\nimport shutil\nimport struct\nimport argparse\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom model import ModelArgs, Transformer\n\n# -----------------------------------------------------------------------------\n# common utilities\n\ndef serialize_fp32(file, tensor):\n    \"\"\" writes one fp32 tensor to file that is open in wb mode \"\"\"\n    d = tensor.detach().cpu().view(-1).to(torch.float32).numpy()\n    b = struct.pack(f'{len(d)}f', *d)\n    file.write(b)\n\ndef serialize_int8(file, tensor):\n    \"\"\" writes one int8 tensor to file that is open in wb mode \"\"\"\n    d = tensor.detach().cpu().view(-1).numpy().astype(np.int8)\n    b = struct.pack(f'{len(d)}b', *d)\n    file.write(b)\n\ndef quantize_q80(w, group_size):\n    \"\"\"\n    takes a tensor and returns the Q8_0 quantized version\n    i.e. symmetric quantization into int8, range [-127,127]\n    \"\"\"\n    assert w.numel() % group_size == 0\n    ori_shape = w.shape\n    w = w.float() # convert to float32\n    w = w.reshape(-1, group_size)\n    # find the max in each group\n    wmax = torch.abs(w).max(dim=1).values\n    # calculate the scaling factor such that float = quant * scale\n    scale = wmax / 127.0\n    # scale into range [-127, 127]\n    quant = w / scale[:,None]\n    # round to nearest integer\n    int8val = torch.round(quant).to(torch.int8)\n    # dequantize by rescaling\n    fp32val = (int8val.float() * scale[:,None]).view(-1)\n    fp32valr = fp32val.reshape(-1, group_size)\n    # calculate the max error in each group\n    err = torch.abs(fp32valr - w).max(dim=1).values\n    # find the max error across all groups\n    maxerr = err.max().item()\n    return int8val, scale, maxerr\n\n# -----------------------------------------------------------------------------\n# legacy\n\ndef legacy_export(model, filepath):\n    \"\"\" Original export of llama2.c bin files, i.e. version v0 \"\"\"\n    out_file = open(filepath, 'wb')\n\n    # first write out the header\n    hidden_dim = model.layers[0].feed_forward.w1.weight.shape[0]\n    p = model.params\n    shared_classifier = torch.equal(model.tok_embeddings.weight, model.output.weight)\n    # legacy format uses negative/positive vocab size as a shared classifier flag\n    if not shared_classifier:\n        p.vocab_size = -p.vocab_size\n    n_kv_heads = p.n_heads if p.n_kv_heads is None else p.n_kv_heads\n    header = struct.pack('iiiiiii', p.dim, hidden_dim, p.n_layers, p.n_heads,\n                                    n_kv_heads, p.vocab_size, p.max_seq_len)\n    out_file.write(header)\n\n    # next write out the embedding weights\n    serialize_fp32(out_file, model.tok_embeddings.weight)\n\n    # now all the layers\n    # attention weights\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention_norm.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wq.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wk.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wv.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wo.weight)\n    # ffn weights\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.ffn_norm.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.feed_forward.w1.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.feed_forward.w2.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.feed_forward.w3.weight)\n    # final rmsnorm\n    serialize_fp32(out_file, model.norm.weight)\n    # freqs_cis\n    serialize_fp32(out_file, model.freqs_cos[:p.max_seq_len])\n    serialize_fp32(out_file, model.freqs_sin[:p.max_seq_len])\n\n    # final classifier weights\n    if not shared_classifier:\n        serialize_fp32(out_file, model.output.weight)\n\n    # write to binary file\n    out_file.close()\n    print(f\"wrote {filepath}\")\n\n# -----------------------------------------------------------------------------\n# new version\n\ndef version1_export(model, filepath):\n    \"\"\"\n    Export the model weights in full float32 .bin file to be read from C.\n    This is same as legacy_export, but with a proper header.\n    \"\"\"\n    version = 1\n\n    out_file = open(filepath, 'wb')\n    # first write out the header. the header will be 256 bytes\n    # 1) write magic, which will be uint32 of \"ak42\" in ASCII\n    out_file.write(struct.pack('I', 0x616b3432))\n    # 2) write version, which will be int\n    out_file.write(struct.pack('i', version))\n    # 3) write the params, which will be 7 ints\n    p = model.params\n    hidden_dim = model.layers[0].feed_forward.w1.weight.shape[0]\n    n_kv_heads = p.n_heads if p.n_kv_heads is None else p.n_kv_heads\n    header = struct.pack('iiiiiii', p.dim, hidden_dim, p.n_layers, p.n_heads,\n                                    n_kv_heads, p.vocab_size, p.max_seq_len)\n    out_file.write(header)\n    # 4) write some other flags\n    shared_classifier = torch.equal(model.tok_embeddings.weight, model.output.weight)\n    out_file.write(struct.pack('B', int(shared_classifier)))\n    pad = 256 - out_file.tell() # pad rest with zeros; tell returns current pos\n    assert pad >= 0\n    out_file.write(b'\\0' * pad)\n\n    # now let's write out all the params\n    weights = [\n        *[layer.attention_norm.weight for layer in model.layers],\n        *[layer.ffn_norm.weight for layer in model.layers],\n        model.norm.weight,\n        model.tok_embeddings.weight,\n        *[layer.attention.wq.weight for layer in model.layers],\n        *[layer.attention.wk.weight for layer in model.layers],\n        *[layer.attention.wv.weight for layer in model.layers],\n        *[layer.attention.wo.weight for layer in model.layers],\n        *[layer.feed_forward.w1.weight for layer in model.layers],\n        *[layer.feed_forward.w2.weight for layer in model.layers],\n        *[layer.feed_forward.w3.weight for layer in model.layers],\n    ]\n    if not shared_classifier:\n        weights.append(model.output.weight)\n    for w in weights:\n        serialize_fp32(out_file, w)\n\n    # write to binary file\n    out_file.close()\n    print(f\"wrote {filepath}\")\n\ndef version2_export(model, filepath, group_size=64):\n    \"\"\"\n    Export the model weights in Q8_0 into .bin file to be read from C.\n    That is:\n    - quantize all weights to symmetric int8, in range [-127, 127]\n    - all other tensors (the rmsnorm params) are kept and exported in fp32\n    - quantization is done in groups of group_size to reduce the effects of any outliers\n    \"\"\"\n    version = 2\n\n    # let's first do some validation for this export type\n    while model.params.dim % group_size != 0:\n        group_size //= 2\n        print(f\"BACKOFF: reducing group size to {group_size} to fit hidden_dim\")\n    weights = [\n        model.tok_embeddings.weight,\n        *[layer.attention.wq.weight for layer in model.layers],\n        *[layer.attention.wk.weight for layer in model.layers],\n        *[layer.attention.wv.weight for layer in model.layers],\n        *[layer.attention.wo.weight for layer in model.layers],\n        *[layer.feed_forward.w1.weight for layer in model.layers],\n        *[layer.feed_forward.w2.weight for layer in model.layers],\n        *[layer.feed_forward.w3.weight for layer in model.layers],\n    ]\n    shared_classifier = torch.equal(model.tok_embeddings.weight, model.output.weight)\n    if not shared_classifier:\n        weights.append(model.output.weight)\n    for w in weights:\n        assert w.numel() % group_size == 0, f\"weight {i} has numel {w.numel()}, not a multiple of group_size {group_size}\"\n\n    # write\n    out_file = open(filepath, 'wb')\n    # first write out the header. the header will be 256 bytes\n    # 1) write magic, which will be uint32 of \"ak42\" in ASCII\n    out_file.write(struct.pack('I', 0x616b3432))\n    # 2) write version, which will be int\n    out_file.write(struct.pack('i', version))\n    # 3) write the params, which will be 7 ints\n    p = model.params\n    hidden_dim = model.layers[0].feed_forward.w1.weight.shape[0]\n    n_kv_heads = p.n_heads if p.n_kv_heads is None else p.n_kv_heads\n    header = struct.pack('iiiiiii', p.dim, hidden_dim, p.n_layers, p.n_heads,\n                                    n_kv_heads, p.vocab_size, p.max_seq_len)\n    out_file.write(header)\n    # 4) write some other flags\n    out_file.write(struct.pack('B', int(shared_classifier)))\n    out_file.write(struct.pack('i', group_size)) # group size used for quantization\n    pad = 256 - out_file.tell() # pad rest with zeros; tell returns current pos\n    assert pad >= 0\n    out_file.write(b'\\0' * pad)\n    # now that the header is done, let's write out the model\n\n    # first let's write out all the params that we are keeping in fp32: the norms\n    for layer in model.layers: # attention norms\n        serialize_fp32(out_file, layer.attention_norm.weight)\n    for layer in model.layers: # MLP norms\n        serialize_fp32(out_file, layer.ffn_norm.weight)\n    serialize_fp32(out_file, model.norm.weight) # final pre-classifier norm\n\n    # now let's write out all the params that we are quantizing to Q8_0\n    # note we skip classifier weights, which are shared with the embedding\n    ew = []\n    scales = []\n    for i, w in enumerate(weights):\n        # quantize this weight\n        q, s, err = quantize_q80(w, group_size)\n        # save the int8 weights to file\n        serialize_int8(out_file, q) # save the tensor in int8\n        scales.append(s)  # we'll do all the scales after all the qs\n        # logging\n        ew.append((err, w.shape))\n        print(f\"{i+1}/{len(weights)} quantized {tuple(w.shape)} to Q8_0 with max error {err}\")\n\n    # save the scaling factors in fp32 here\n    # this is done to keep all the weights contiquous, making pointer arithmetic easier in C\n    for s in scales:\n        serialize_fp32(out_file, s)\n\n    # print the highest error across all weights, should be very small, e.g. O(~0.001)\n    ew.sort(reverse=True)\n    print(f\"max quantization group error across all weights: {ew[0][0]}\")\n\n    # write to binary file\n    out_file.close()\n    print(f\"wrote {filepath}\")\n\n\n# -----------------------------------------------------------------------------\n# Load / import functions\n\ndef load_checkpoint(checkpoint):\n\n    # load the provided model checkpoint\n    checkpoint_dict = torch.load(checkpoint, map_location='cpu')\n    gptconf = ModelArgs(**checkpoint_dict['model_args'])\n    model = Transformer(gptconf)\n    state_dict = checkpoint_dict['model']\n    unwanted_prefix = '_orig_mod.'\n    for k,v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict, strict=False)\n    model.eval()\n    return model\n\ndef load_meta_model(model_path):\n    params_path = os.path.join(model_path, 'params.json')\n    with open(params_path) as f:\n        params = json.load(f)\n        print(params)\n\n    model_paths = sorted(list(Path(model_path).glob('consolidated.*.pth')))\n    models = [torch.load(p, map_location='cpu') for p in model_paths]\n\n    def concat_weights(models):\n        state_dict = {}\n        for name in list(models[0]):\n            tensors = [model[name] for model in models]\n            if len(tensors) == 1 or len(tensors[0].shape) == 1:\n                state_dict[name] = tensors[0]\n                continue\n            is_axis_1 = (\n                name.startswith('tok_embeddings.')\n                or name.endswith('.attention.wo.weight')\n                or name.endswith('.feed_forward.w2.weight')\n            )\n            axis = 1 if is_axis_1 else 0\n            state_dict[name] = torch.cat(tensors, dim=axis)\n            for model in models:\n                del model[name]\n        return state_dict\n\n    state_dict = concat_weights(models)\n    del models\n\n    # set ModelArgs\n    config = ModelArgs()\n    config.dim = params[\"dim\"]\n    config.n_layers = params[\"n_layers\"]\n    config.n_heads = params[\"n_heads\"]\n    config.n_kv_heads = params.get('n_kv_heads') or params['n_heads']\n    config.multiple_of = params[\"multiple_of\"]\n    config.norm_eps = params[\"norm_eps\"]\n\n    config.vocab_size = 32000\n    config.max_seq_len = 2048\n\n    # create a new Transformer object and set weights\n    model = Transformer(config)\n\n    model.tok_embeddings.weight = nn.Parameter(state_dict['tok_embeddings.weight'])\n    model.norm.weight = nn.Parameter(state_dict['norm.weight'])\n\n    for layer in model.layers:\n        i = layer.layer_id\n        layer.attention_norm.weight = nn.Parameter(state_dict[f'layers.{i}.attention_norm.weight'])\n        layer.attention.wq.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wq.weight'])\n        layer.attention.wk.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wk.weight'])\n        layer.attention.wv.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wv.weight'])\n        layer.attention.wo.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wo.weight'])\n        layer.ffn_norm.weight = nn.Parameter(state_dict[f'layers.{i}.ffn_norm.weight'])\n        layer.feed_forward.w1.weight = nn.Parameter(state_dict[f'layers.{i}.feed_forward.w1.weight'])\n        layer.feed_forward.w2.weight = nn.Parameter(state_dict[f'layers.{i}.feed_forward.w2.weight'])\n        layer.feed_forward.w3.weight = nn.Parameter(state_dict[f'layers.{i}.feed_forward.w3.weight'])\n\n    # final classifier\n    model.output.weight = nn.Parameter(state_dict['output.weight'])\n    model.eval()\n    return model\n\ndef load_hf_model(model_path):\n\n    try:\n        from transformers import AutoModelForCausalLM\n    except ImportError:\n        print(\"Error: transformers package is required to load huggingface models\")\n        print(\"Please run `pip install transformers` to install it\")\n        return None\n\n    # load HF model\n    hf_model = AutoModelForCausalLM.from_pretrained(model_path)\n    hf_dict = hf_model.state_dict()\n\n    # convert LlamaConfig to ModelArgs\n    config = ModelArgs()\n    config.dim = hf_model.config.hidden_size\n    config.n_layers = hf_model.config.num_hidden_layers\n    config.n_heads = hf_model.config.num_attention_heads\n    config.n_kv_heads = hf_model.config.num_attention_heads\n    config.vocab_size = hf_model.config.vocab_size\n    config.hidden_dim = hf_model.config.intermediate_size\n    config.norm_eps = hf_model.config.rms_norm_eps\n    config.max_seq_len = hf_model.config.max_position_embeddings\n\n    # create a new Transformer object and set weights\n    model = Transformer(config)\n\n    model.tok_embeddings.weight = nn.Parameter(hf_dict['model.embed_tokens.weight'])\n    model.norm.weight = nn.Parameter(hf_dict['model.norm.weight'])\n\n    # huggingface permutes WQ and WK, this function reverses it\n    def permute_reverse(w, n_heads=config.n_heads, dim1=config.dim, dim2=config.dim):\n        return w.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\n    for layer in model.layers:\n        i = layer.layer_id\n        layer.attention_norm.weight = nn.Parameter(hf_dict[f'model.layers.{i}.input_layernorm.weight'])\n        layer.attention.wq.weight = nn.Parameter(permute_reverse(hf_dict[f'model.layers.{i}.self_attn.q_proj.weight']))\n        layer.attention.wk.weight = nn.Parameter(permute_reverse(hf_dict[f'model.layers.{i}.self_attn.k_proj.weight']))\n        layer.attention.wv.weight = nn.Parameter(hf_dict[f'model.layers.{i}.self_attn.v_proj.weight'])\n        layer.attention.wo.weight = nn.Parameter(hf_dict[f'model.layers.{i}.self_attn.o_proj.weight'])\n        layer.ffn_norm.weight = nn.Parameter(hf_dict[f'model.layers.{i}.post_attention_layernorm.weight'])\n        layer.feed_forward.w1.weight = nn.Parameter(hf_dict[f'model.layers.{i}.mlp.gate_proj.weight'])\n        layer.feed_forward.w2.weight = nn.Parameter(hf_dict[f'model.layers.{i}.mlp.down_proj.weight'])\n        layer.feed_forward.w3.weight = nn.Parameter(hf_dict[f'model.layers.{i}.mlp.up_proj.weight'])\n\n    # final classifier\n    model.output.weight = nn.Parameter(hf_dict['lm_head.weight'])\n    model.eval()\n    return model\n\n\n# -----------------------------------------------------------------------------\n# API entrypoint\n\ndef model_export(model, filepath, version):\n    if version == 0:\n        legacy_export(model, filepath)\n    elif version == 1:\n        version1_export(model, filepath)\n    elif version == 2:\n        version2_export(model, filepath)\n    else:\n        raise ValueError(f\"unknown version {version}\")\n\ndef torchscript_export(model, filepath, zero_params=False, gzip_output=False):\n    \"\"\"\n    (This was submitted via a PR earlier. Leaving it here, but \"orphaned\" for now)\n    Saves the model as a TorchScript.\n    The resulting file can be loaded in C++ code and then used for training or\n    inference with:\n        #include <torch/script.h>\n        torch::jit::Module module = torch::jit::load(\"model.pt\")\n    Note that the serialized model includes the initial parameters and with the default\n    ModelArgs the file is 59M and gzips down to 55M. If you want to serialize/distribute\n    the model parameters separately you can zero out the parameters before saving it and\n    it will gzip down to 780K.\n    \"\"\"\n\n    # If requested zero params before saving the model. This is useful in\n    # conjunction with gzip_output.\n    if zero_params:\n        for p in model.parameters():\n            p.detach().zero_()\n\n    torch.jit.save(torch.jit.script(model), filepath)\n\n    if gzip_output:\n        with open(filepath, \"rb\") as f_in:\n            with gzip.open(f\"{filepath}.gz\", \"wb\") as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        os.unlink(filepath)\n\n# -----------------------------------------------------------------------------\n# CLI entrypoint\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"filepath\", type=str, help=\"the output filepath\")\n    parser.add_argument(\"--version\", default=0, type=int, help=\"the version to export with\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--checkpoint\", type=str, help=\"model checkpoint, .pt file\")\n    group.add_argument(\"--meta-llama\", type=str, help=\"meta llama model path\")\n    group.add_argument(\"--hf\", type=str, help=\"huggingface model path\")\n    args = parser.parse_args()\n\n    if args.checkpoint:\n        model = load_checkpoint(args.checkpoint)\n    elif args.meta_llama:\n        model = load_meta_model(args.meta_llama)\n    elif args.hf:\n        model = load_hf_model(args.hf)\n\n    if model is None:\n        parser.error(\"Can't load input model!\")\n\n    # export\n    model_export(model, args.filepath, args.version)\n", "previous_id": "49daf18f2f85cab239e80b8a452a2f78f295032f", "new_code_file": "\"\"\"\nThis script has functions and utilties for model export.\nBasically, we have a bunch of versions of the model, and we\nwant to export them to .bin files to be read from and inferenced in C.\n\nAmong the \"input\" versions of PyTorch files/models:\n- Official Llama 2 weights released by Meta\n- Huggingface weights available on the hub\n- llama2.c (this repo) trained models\n\nAmong the \"output\" versions of .bin files:\n- v0: Legacy files of the original llama2.c repo (will eventually be DEPRECATED)\n- v1-vN: Improved .bin files with a proper header, cache alignment, etc.\n\nThis script aspires to provide all of these conversions.\n\"\"\"\nimport os\nimport gzip\nimport shutil\nimport struct\nimport argparse\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom model import ModelArgs, Transformer\n\n# -----------------------------------------------------------------------------\n# common utilities\n\ndef serialize_fp32(file, tensor):\n    \"\"\" writes one fp32 tensor to file that is open in wb mode \"\"\"\n    d = tensor.detach().cpu().view(-1).to(torch.float32).numpy()\n    b = struct.pack(f'{len(d)}f', *d)\n    file.write(b)\n\ndef serialize_int8(file, tensor):\n    \"\"\" writes one int8 tensor to file that is open in wb mode \"\"\"\n    d = tensor.detach().cpu().view(-1).numpy().astype(np.int8)\n    b = struct.pack(f'{len(d)}b', *d)\n    file.write(b)\n\ndef quantize_q80(w, group_size):\n    \"\"\"\n    takes a tensor and returns the Q8_0 quantized version\n    i.e. symmetric quantization into int8, range [-127,127]\n    \"\"\"\n    assert w.numel() % group_size == 0\n    ori_shape = w.shape\n    w = w.float() # convert to float32\n    w = w.reshape(-1, group_size)\n    # find the max in each group\n    wmax = torch.abs(w).max(dim=1).values\n    # calculate the scaling factor such that float = quant * scale\n    scale = wmax / 127.0\n    # scale into range [-127, 127]\n    quant = w / scale[:,None]\n    # round to nearest integer\n    int8val = torch.round(quant).to(torch.int8)\n    # dequantize by rescaling\n    fp32val = (int8val.float() * scale[:,None]).view(-1)\n    fp32valr = fp32val.reshape(-1, group_size)\n    # calculate the max error in each group\n    err = torch.abs(fp32valr - w).max(dim=1).values\n    # find the max error across all groups\n    maxerr = err.max().item()\n    return int8val, scale, maxerr\n\n# -----------------------------------------------------------------------------\n# legacy\n\ndef legacy_export(model, filepath):\n    \"\"\" Original export of llama2.c bin files, i.e. version v0 \"\"\"\n    out_file = open(filepath, 'wb')\n\n    # first write out the header\n    hidden_dim = model.layers[0].feed_forward.w1.weight.shape[0]\n    p = model.params\n    shared_classifier = torch.equal(model.tok_embeddings.weight, model.output.weight)\n    # legacy format uses negative/positive vocab size as a shared classifier flag\n    if not shared_classifier:\n        p.vocab_size = -p.vocab_size\n    n_kv_heads = p.n_heads if p.n_kv_heads is None else p.n_kv_heads\n    header = struct.pack('iiiiiii', p.dim, hidden_dim, p.n_layers, p.n_heads,\n                                    n_kv_heads, p.vocab_size, p.max_seq_len)\n    out_file.write(header)\n\n    # next write out the embedding weights\n    serialize_fp32(out_file, model.tok_embeddings.weight)\n\n    # now all the layers\n    # attention weights\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention_norm.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wq.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wk.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wv.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.attention.wo.weight)\n    # ffn weights\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.ffn_norm.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.feed_forward.w1.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.feed_forward.w2.weight)\n    for layer in model.layers:\n        serialize_fp32(out_file, layer.feed_forward.w3.weight)\n    # final rmsnorm\n    serialize_fp32(out_file, model.norm.weight)\n    # freqs_cis\n    serialize_fp32(out_file, model.freqs_cos[:p.max_seq_len])\n    serialize_fp32(out_file, model.freqs_sin[:p.max_seq_len])\n\n    # final classifier weights\n    if not shared_classifier:\n        serialize_fp32(out_file, model.output.weight)\n\n    # write to binary file\n    out_file.close()\n    print(f\"wrote {filepath}\")\n\n# -----------------------------------------------------------------------------\n# new version\n\ndef version1_export(model, filepath):\n    \"\"\"\n    Export the model weights in full float32 .bin file to be read from C.\n    This is same as legacy_export, but with a proper header.\n    \"\"\"\n    version = 1\n\n    out_file = open(filepath, 'wb')\n    # first write out the header. the header will be 256 bytes\n    # 1) write magic, which will be uint32 of \"ak42\" in ASCII\n    out_file.write(struct.pack('I', 0x616b3432))\n    # 2) write version, which will be int\n    out_file.write(struct.pack('i', version))\n    # 3) write the params, which will be 7 ints\n    p = model.params\n    hidden_dim = model.layers[0].feed_forward.w1.weight.shape[0]\n    n_kv_heads = p.n_heads if p.n_kv_heads is None else p.n_kv_heads\n    header = struct.pack('iiiiiii', p.dim, hidden_dim, p.n_layers, p.n_heads,\n                                    n_kv_heads, p.vocab_size, p.max_seq_len)\n    out_file.write(header)\n    # 4) write some other flags\n    shared_classifier = torch.equal(model.tok_embeddings.weight, model.output.weight)\n    out_file.write(struct.pack('B', int(shared_classifier)))\n    pad = 256 - out_file.tell() # pad rest with zeros; tell returns current pos\n    assert pad >= 0\n    out_file.write(b'\\0' * pad)\n\n    # now let's write out all the params\n    weights = [\n        *[layer.attention_norm.weight for layer in model.layers],\n        *[layer.ffn_norm.weight for layer in model.layers],\n        model.norm.weight,\n        model.tok_embeddings.weight,\n        *[layer.attention.wq.weight for layer in model.layers],\n        *[layer.attention.wk.weight for layer in model.layers],\n        *[layer.attention.wv.weight for layer in model.layers],\n        *[layer.attention.wo.weight for layer in model.layers],\n        *[layer.feed_forward.w1.weight for layer in model.layers],\n        *[layer.feed_forward.w2.weight for layer in model.layers],\n        *[layer.feed_forward.w3.weight for layer in model.layers],\n    ]\n    if not shared_classifier:\n        weights.append(model.output.weight)\n    for w in weights:\n        serialize_fp32(out_file, w)\n\n    # write to binary file\n    out_file.close()\n    print(f\"wrote {filepath}\")\n\ndef version2_export(model, filepath, group_size=64):\n    \"\"\"\n    Export the model weights in Q8_0 into .bin file to be read from C.\n    That is:\n    - quantize all weights to symmetric int8, in range [-127, 127]\n    - all other tensors (the rmsnorm params) are kept and exported in fp32\n    - quantization is done in groups of group_size to reduce the effects of any outliers\n    \"\"\"\n    version = 2\n\n    # let's first do some validation for this export type\n    while model.params.dim % group_size != 0:\n        group_size //= 2\n        print(f\"BACKOFF: reducing group size to {group_size} to fit hidden_dim\")\n    weights = [\n        model.tok_embeddings.weight,\n        *[layer.attention.wq.weight for layer in model.layers],\n        *[layer.attention.wk.weight for layer in model.layers],\n        *[layer.attention.wv.weight for layer in model.layers],\n        *[layer.attention.wo.weight for layer in model.layers],\n        *[layer.feed_forward.w1.weight for layer in model.layers],\n        *[layer.feed_forward.w2.weight for layer in model.layers],\n        *[layer.feed_forward.w3.weight for layer in model.layers],\n    ]\n    shared_classifier = torch.equal(model.tok_embeddings.weight, model.output.weight)\n    if not shared_classifier:\n        weights.append(model.output.weight)\n    for w in weights:\n        assert w.numel() % group_size == 0, f\"weight {i} has numel {w.numel()}, not a multiple of group_size {group_size}\"\n\n    # write\n    out_file = open(filepath, 'wb')\n    # first write out the header. the header will be 256 bytes\n    # 1) write magic, which will be uint32 of \"ak42\" in ASCII\n    out_file.write(struct.pack('I', 0x616b3432))\n    # 2) write version, which will be int\n    out_file.write(struct.pack('i', version))\n    # 3) write the params, which will be 7 ints\n    p = model.params\n    hidden_dim = model.layers[0].feed_forward.w1.weight.shape[0]\n    n_kv_heads = p.n_heads if p.n_kv_heads is None else p.n_kv_heads\n    header = struct.pack('iiiiiii', p.dim, hidden_dim, p.n_layers, p.n_heads,\n                                    n_kv_heads, p.vocab_size, p.max_seq_len)\n    out_file.write(header)\n    # 4) write some other flags\n    out_file.write(struct.pack('B', int(shared_classifier)))\n    out_file.write(struct.pack('i', group_size)) # group size used for quantization\n    pad = 256 - out_file.tell() # pad rest with zeros; tell returns current pos\n    assert pad >= 0\n    out_file.write(b'\\0' * pad)\n    # now that the header is done, let's write out the model\n\n    # first let's write out all the params that we are keeping in fp32: the norms\n    for layer in model.layers: # attention norms\n        serialize_fp32(out_file, layer.attention_norm.weight)\n    for layer in model.layers: # MLP norms\n        serialize_fp32(out_file, layer.ffn_norm.weight)\n    serialize_fp32(out_file, model.norm.weight) # final pre-classifier norm\n\n    # now let's write out all the params that we are quantizing to Q8_0\n    # note we skip classifier weights, which are shared with the embedding\n    ew = []\n    scales = []\n    for i, w in enumerate(weights):\n        # quantize this weight\n        q, s, err = quantize_q80(w, group_size)\n        # save the int8 weights to file\n        serialize_int8(out_file, q) # save the tensor in int8\n        scales.append(s)  # we'll do all the scales after all the qs\n        # logging\n        ew.append((err, w.shape))\n        print(f\"{i+1}/{len(weights)} quantized {tuple(w.shape)} to Q8_0 with max error {err}\")\n\n    # save the scaling factors in fp32 here\n    # this is done to keep all the weights contiquous, making pointer arithmetic easier in C\n    for s in scales:\n        serialize_fp32(out_file, s)\n\n    # print the highest error across all weights, should be very small, e.g. O(~0.001)\n    ew.sort(reverse=True)\n    print(f\"max quantization group error across all weights: {ew[0][0]}\")\n\n    # write to binary file\n    out_file.close()\n    print(f\"wrote {filepath}\")\n\n\n# -----------------------------------------------------------------------------\n# Load / import functions\n\ndef load_checkpoint(checkpoint):\n\n    # load the provided model checkpoint\n    checkpoint_dict = torch.load(checkpoint, map_location='cpu')\n    gptconf = ModelArgs(**checkpoint_dict['model_args'])\n    model = Transformer(gptconf)\n    state_dict = checkpoint_dict['model']\n    unwanted_prefix = '_orig_mod.'\n    for k,v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict, strict=False)\n    model.eval()\n    return model\n\ndef load_meta_model(model_path):\n    params_path = os.path.join(model_path, 'params.json')\n    with open(params_path) as f:\n        params = json.load(f)\n        print(params)\n\n    model_paths = sorted(list(Path(model_path).glob('consolidated.*.pth')))\n    models = [torch.load(p, map_location='cpu') for p in model_paths]\n\n    def concat_weights(models):\n        state_dict = {}\n        for name in list(models[0]):\n            tensors = [model[name] for model in models]\n            if len(tensors) == 1 or len(tensors[0].shape) == 1:\n                state_dict[name] = tensors[0]\n                continue\n            is_axis_1 = (\n                name.startswith('tok_embeddings.')\n                or name.endswith('.attention.wo.weight')\n                or name.endswith('.feed_forward.w2.weight')\n            )\n            axis = 1 if is_axis_1 else 0\n            state_dict[name] = torch.cat(tensors, dim=axis)\n            for model in models:\n                del model[name]\n        return state_dict\n\n    state_dict = concat_weights(models)\n    del models\n\n    # set ModelArgs\n    config = ModelArgs()\n    config.dim = params[\"dim\"]\n    config.n_layers = params[\"n_layers\"]\n    config.n_heads = params[\"n_heads\"]\n    config.n_kv_heads = params.get('n_kv_heads') or params['n_heads']\n    config.multiple_of = params[\"multiple_of\"]\n    config.norm_eps = params[\"norm_eps\"]\n\n    config.vocab_size = state_dict['tok_embeddings.weight'].shape[0]\n    config.max_seq_len = 2048\n\n\n    # create a new Transformer object and set weights\n    model = Transformer(config)\n\n    model.tok_embeddings.weight = nn.Parameter(state_dict['tok_embeddings.weight'])\n    model.norm.weight = nn.Parameter(state_dict['norm.weight'])\n\n    for layer in model.layers:\n        i = layer.layer_id\n        layer.attention_norm.weight = nn.Parameter(state_dict[f'layers.{i}.attention_norm.weight'])\n        layer.attention.wq.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wq.weight'])\n        layer.attention.wk.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wk.weight'])\n        layer.attention.wv.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wv.weight'])\n        layer.attention.wo.weight = nn.Parameter(state_dict[f'layers.{i}.attention.wo.weight'])\n        layer.ffn_norm.weight = nn.Parameter(state_dict[f'layers.{i}.ffn_norm.weight'])\n        layer.feed_forward.w1.weight = nn.Parameter(state_dict[f'layers.{i}.feed_forward.w1.weight'])\n        layer.feed_forward.w2.weight = nn.Parameter(state_dict[f'layers.{i}.feed_forward.w2.weight'])\n        layer.feed_forward.w3.weight = nn.Parameter(state_dict[f'layers.{i}.feed_forward.w3.weight'])\n\n    # final classifier\n    model.output.weight = nn.Parameter(state_dict['output.weight'])\n    model.eval()\n    return model\n\ndef load_hf_model(model_path):\n\n    try:\n        from transformers import AutoModelForCausalLM\n    except ImportError:\n        print(\"Error: transformers package is required to load huggingface models\")\n        print(\"Please run `pip install transformers` to install it\")\n        return None\n\n    # load HF model\n    hf_model = AutoModelForCausalLM.from_pretrained(model_path)\n    hf_dict = hf_model.state_dict()\n\n    # convert LlamaConfig to ModelArgs\n    config = ModelArgs()\n    config.dim = hf_model.config.hidden_size\n    config.n_layers = hf_model.config.num_hidden_layers\n    config.n_heads = hf_model.config.num_attention_heads\n    config.n_kv_heads = hf_model.config.num_attention_heads\n    config.vocab_size = hf_model.config.vocab_size\n    config.hidden_dim = hf_model.config.intermediate_size\n    config.norm_eps = hf_model.config.rms_norm_eps\n    config.max_seq_len = hf_model.config.max_position_embeddings\n\n    # create a new Transformer object and set weights\n    model = Transformer(config)\n\n    model.tok_embeddings.weight = nn.Parameter(hf_dict['model.embed_tokens.weight'])\n    model.norm.weight = nn.Parameter(hf_dict['model.norm.weight'])\n\n    # huggingface permutes WQ and WK, this function reverses it\n    def permute_reverse(w, n_heads=config.n_heads, dim1=config.dim, dim2=config.dim):\n        return w.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\n    for layer in model.layers:\n        i = layer.layer_id\n        layer.attention_norm.weight = nn.Parameter(hf_dict[f'model.layers.{i}.input_layernorm.weight'])\n        layer.attention.wq.weight = nn.Parameter(permute_reverse(hf_dict[f'model.layers.{i}.self_attn.q_proj.weight']))\n        layer.attention.wk.weight = nn.Parameter(permute_reverse(hf_dict[f'model.layers.{i}.self_attn.k_proj.weight']))\n        layer.attention.wv.weight = nn.Parameter(hf_dict[f'model.layers.{i}.self_attn.v_proj.weight'])\n        layer.attention.wo.weight = nn.Parameter(hf_dict[f'model.layers.{i}.self_attn.o_proj.weight'])\n        layer.ffn_norm.weight = nn.Parameter(hf_dict[f'model.layers.{i}.post_attention_layernorm.weight'])\n        layer.feed_forward.w1.weight = nn.Parameter(hf_dict[f'model.layers.{i}.mlp.gate_proj.weight'])\n        layer.feed_forward.w2.weight = nn.Parameter(hf_dict[f'model.layers.{i}.mlp.down_proj.weight'])\n        layer.feed_forward.w3.weight = nn.Parameter(hf_dict[f'model.layers.{i}.mlp.up_proj.weight'])\n\n    # final classifier\n    model.output.weight = nn.Parameter(hf_dict['lm_head.weight'])\n    model.eval()\n    return model\n\n\n# -----------------------------------------------------------------------------\n# API entrypoint\n\ndef model_export(model, filepath, version):\n    if version == 0:\n        legacy_export(model, filepath)\n    elif version == 1:\n        version1_export(model, filepath)\n    elif version == 2:\n        version2_export(model, filepath)\n    else:\n        raise ValueError(f\"unknown version {version}\")\n\ndef torchscript_export(model, filepath, zero_params=False, gzip_output=False):\n    \"\"\"\n    (This was submitted via a PR earlier. Leaving it here, but \"orphaned\" for now)\n    Saves the model as a TorchScript.\n    The resulting file can be loaded in C++ code and then used for training or\n    inference with:\n        #include <torch/script.h>\n        torch::jit::Module module = torch::jit::load(\"model.pt\")\n    Note that the serialized model includes the initial parameters and with the default\n    ModelArgs the file is 59M and gzips down to 55M. If you want to serialize/distribute\n    the model parameters separately you can zero out the parameters before saving it and\n    it will gzip down to 780K.\n    \"\"\"\n\n    # If requested zero params before saving the model. This is useful in\n    # conjunction with gzip_output.\n    if zero_params:\n        for p in model.parameters():\n            p.detach().zero_()\n\n    torch.jit.save(torch.jit.script(model), filepath)\n\n    if gzip_output:\n        with open(filepath, \"rb\") as f_in:\n            with gzip.open(f\"{filepath}.gz\", \"wb\") as f_out:\n                shutil.copyfileobj(f_in, f_out)\n        os.unlink(filepath)\n\n# -----------------------------------------------------------------------------\n# CLI entrypoint\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"filepath\", type=str, help=\"the output filepath\")\n    parser.add_argument(\"--version\", default=0, type=int, help=\"the version to export with\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--checkpoint\", type=str, help=\"model checkpoint, .pt file\")\n    group.add_argument(\"--meta-llama\", type=str, help=\"meta llama model path\")\n    group.add_argument(\"--hf\", type=str, help=\"huggingface model path\")\n    args = parser.parse_args()\n\n    if args.checkpoint:\n        model = load_checkpoint(args.checkpoint)\n    elif args.meta_llama:\n        model = load_meta_model(args.meta_llama)\n    elif args.hf:\n        model = load_hf_model(args.hf)\n\n    if model is None:\n        parser.error(\"Can't load input model!\")\n\n    # export\n    model_export(model, args.filepath, args.version)\n", "diff": "@@ -323,9 +323,10 @@ def concat_weights(models):\n     config.multiple_of = params[\"multiple_of\"]\n     config.norm_eps = params[\"norm_eps\"]\n \n-    config.vocab_size = 32000\n+    config.vocab_size = state_dict['tok_embeddings.weight'].shape[0]\n     config.max_seq_len = 2048\n \n+\n     # create a new Transformer object and set weights\n     model = Transformer(config)\n ", "status": "modified"}]